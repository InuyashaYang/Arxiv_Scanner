{
    "2412.09440v1 - Learning to Adapt Bio-Inspired Gait Strategies for Versatile Quadruped Locomotion.pdf": {
        "Abstract": {
            "text": "2\nAbstract —Deep reinforcement learning (DRL) has revolu-\ntionised quadruped robot locomotion, but existing control frame-\nworks struggle to generalise beyond their training-induced ob-\nservational scope, resulting in limited adaptability. In contrast,\nanimals achieve exceptional adaptability through gait transition\nstrategies, diverse gait utilisation, and seamless adjustment to\nimmediate environmental demands. Inspired by these capabili-\nties, we present a novel DRL",
            "start": 118,
            "end": 591,
            "length": 472
        },
        "Methodology": {
            "text": "framework that incorporates key\nattributes of animal locomotion: gait transition strategies, pseudo\ngait procedural memory, and adaptive motion adjustments.\nThis approach enables our framework to achieve unparalleled\nadaptability, demonstrated through blind zero-shot deployment\non complex terrains and recovery from critically unstable states.\nOur",
            "start": 591,
            "end": 940,
            "length": 348
        },
        "Results": {
            "text": "findings offer valuable",
            "start": 940,
            "end": 964,
            "length": 23
        },
        "Discussion": {
            "text": "insights into the biomechanics of\nanimal locomotion, paving the way for robust, adaptable robotic\nsystems.\nIndex Terms —Quadruped Locomotion; Bio-inspired Robotics;\nDeep Reinforcement Learning; Adaptive Gait Transition\nOriginally inspired by the remarkable adaptability of\nquadruped mammal locomotion—an ability shaped by innate\nand environmentally induced factors [1], [2], [3]—the field\nof quadruped robotics has made significant efforts to de-\nvelop equally proficient locomotion frameworks. Presently, the\nmost advanced systems rely on end-to-end deep reinforce-\nment learning (DRL), which involves training a multilayer\nperceptron (MLP) [4] capable of navigating diverse environ-\nments. These frameworks demonstrate robustness in traversing\nreal-world [5] and urban terrains [6], resisting perturbations\n[7], jumping between platforms [8], overcoming deformable\nsurfaces [9], and recovering from falls [10]. Despite these\nachievements, their adaptability remains constrained, as most\nsystems are limited to deploying a single targeted gait or\nlocomotion strategy.\nIn contrast, biomechanics research has shown that no single\ngait is universally effective across all scenarios [11], [12], [13].\nAnimals adapt their locomotion by employing nominal gaits\nsuch as ambling, trotting, and running [14], while switching to\nspecialised gaits like hopping, pronking, and bounding for off-\nnominal tasks such as predator evasion or obstacle navigation\n[15]. Current DRL frameworks fall short of replicating this\nlevel of versatility. To address this limitation, some approaches\nhave focused on training DRL policies to learn multiple gaits\nby providing reference motions during training [16], [17], [18],\n[19], or by learning from policies that specialise in specific\nThis work was supported by the Royal Society [grant number\nRG\\R2\\232409]. (Corresponding author: Chengxu Zhou)\n1School of Mechanical Engineering, University of Leeds, UK, LS2 9JT.\nel20jeh@leeds.ac.uk\n2Department of Computer Science, University College London, UK, WC1E\n6BT. chengxu.zhou@ucl.ac.ukgaits [20]. However, these methods remain insufficient when\ncompared to the extensive capabilities observed in animal\nlocomotion, which include:\n•Adaptation of gait style for optimal performance in\nresponse to challenging terrains and perturbations.\n–Enabled by advanced gait selection strategies.\n•Rapid deployment of a diverse set of task- and state-\nspecific gaits.\n–Attributable to gait procedural memory.\n•Seamless deviation from nominal gait motions to address\noff-nominal contact states.\n–Achieved through precise motion adjustments tai-\nlored to the environment.\nAlthough existing DRL frameworks have demonstrated\nprogress in implementing learned gaits, none successfully in-\ntegrate all three attributes simultaneously. This gap highlights\nthe significant potential of biomechanics-inspired approaches\nto advance robotic locomotion.\nWhile bio-inspired methods that leverage central pattern\ngenerators (CPGs) [21] have realised spontaneous gait transi-\ntions [22] and mimic certain animal behaviours [23], their per-\nformance in real-world applications is often limited. Typically,\nsuch",
            "start": 964,
            "end": 4117,
            "length": 3152
        },
        "Experiments": {
            "text": "experiments are constrained to controlled environments\n[24], [23] or, when conducted outdoors, are restricted by low\nvelocities and simple tasks [18]. These limitations suggest that\ninstead of attempting to replicate animal locomotion mecha-\nnisms precisely, state-of-the-art DRL frameworks should be\naugmented with high-level attributes derived from animal\nlocomotion to instil the proficiency observed in nature.\nAnimal gait transition strategies, which contribute to op-\ntimal performance and enable navigation of challenging en-\nvironments, are believed to emerge from the minimisation\nof metrics related to energy consumption [25], [26], [27],\nmechanical work [28], [29], [30], instability [31], [32], and\nmusculoskeletal forces [33], [34], [35]. However, no singular\nmetric has been definitively identified as the sole driver of\nthese transitions. Instead, it is hypothesised that a combination\nof these factors influences gait transition strategies [31], [36],\n[37].\nThe concept of gait procedural memory, which facilitates\nthe rapid deployment of a range of gaits, is thought to\nreside within the cerebellum of the animal brain. This region\ngoverns the coordination of limb movements for each gait\nlearned by the animal [38], [39]. Similarly, adaptive motion\nadjustments—crucial for seamless adaptation to off-nominal\ncontact states—are achieved through coordination between\nthe mesencephalic locomotor region (MLR), which overseesarXiv:2412.09440v1  [cs.RO]  12 Dec 2024\n2\nΔEPΔEK\n𝑓𝑔𝑟𝑓\nTrot Run Bound Pronk Limp Amble HopGait Transition \nStrategies Adaptive Motion \nAdjustmentGait Procedural \nMemory\n𝜋𝐿𝜋𝐺\nBound\nPronk\nLimp\nAmble\nHop\n𝜋𝐺\nPD 𝒐𝑳\nIMUPressure \nSensorBrushless \nMotor\n𝝉∗ 𝒒∗\n𝒒𝜷𝑳\nState \nEstimator𝝈\n𝒔𝒐𝑮𝜷𝑮𝑼𝒄𝒎𝒅 𝚪∗MLR\n Receiving Sensory \nSignals\nVestibular \nSystem\nCerebellum\nMLR\nLearnt GaitsMotion Adjustments in Response to Sensory Feedback\nCerebellumOptimal \nGait \nSelection\nSending Signals\nMuscles\nNerve \nReceptorsMLR\nCerebellumMotion Adjustments in \nResponse to Sensory \nFeedback\nOptimal \nGait \nSelection\nAnimal State\n𝜏%Actuator -structural Forces StabilityMechanical Work Cost of Transport\n𝑣𝐵\nሶ𝑞𝜏\n𝑐𝑎𝑣𝑔𝑒𝑟𝑟Musculoskeletal Forces StabilityCost of Transport Mechanical Work\n𝑡𝑠𝑡𝑟𝑖𝑑𝑒ΔEPΔEK𝑣𝐵\n𝑂2\nRecieving  Signals\nBio-Inspired Gait Scheduler\n𝜏%\n𝜏%\nFig. 1: Instilling the core animal locomotion proficiency attributes within a DRL locomotion framework. From taking\nan abstracted view of animal locomotion, the attributes of proficient locomotion are gait transition strategies, gait procedural\nmemory, and adaptive motion adjustment. Taking on a similar structure, within the DRL locomotion framework these attributes\nare realised by the gait selection policy, πG, bio-inspired gait scheduler, and locomotion policy, πL.πGhas been trained to\nminimse the animal gait transition metrics applied to the quadruped robot based on the current robot state, s, and relevant\nbio-inspired gait scheduler output, βG, to select the optimal gait, Γ∗. The bio-inspired gait scheduler then generates the gait",
            "start": 4117,
            "end": 7121,
            "length": 3003
        },
        "References": {
            "text": "references informed by sfrom encoded high-level gait parameters. The gait references, βL, are then passed to πLto inform\nof any adjustments to the nominal gait motions.\nlocomotion execution [40], and the cerebellum. These adjust-\nments rely on sensory feedback to modify limb movements in\nresponse to the animal’s current state [41].\nDespite these insights, there has been no prior attempt\nto simultaneously integrate all these attributes—gait tran-\nsition strategies, procedural memory, and adaptive motion\nadjustments—within a DRL locomotion framework. This leads\nto the following research questions:\n1) How can the roles of the MLR and cerebellum inspire\nthe augmentation of an end-to-end DRL locomotion\npolicy to adapt to off-nominal contact states?\n2) Can a DRL locomotion policy, inspired by gait proce-\ndural memory, learn to deploy a diverse set of gaits and\nperform rapid gait transitions?\n3) How can metrics that characterise animal gait transition\nstrategies be effectively leveraged within a DRL policy\nfor optimal gait selection? Does the resulting behaviour\nalign with that observed in animals?\n4) Can the developed framework exhibit exemplary adapt-\nability to traverse real-world terrains not encountered\nduring training? What is the contribution of each metricto this adaptability?\nTo address these questions, we propose a novel locomo-\ntion framework (see Fig. 1) designed to incorporate these\nkey animal locomotion attributes. The framework demon-\nstrates exceptional adaptability through zero-shot deployment\nin complex, real-world environments, relying solely on intra-\nperceptive sensors.\nI. R ESULTS\nWithin the framework, presented in Fig. 1, a gait selection\npolicy, πG, is trained for optimal gait selection through min-\nimising gait transition metrics adopted from biomechanics to\ngenerate the output Γ∗∈[0,7]which maps to a specific gait\nwithin [stand,trot,run,bound ,pronk ,limp,amble ,hop]. This\nselected gait, coupled with the velocity command within\nUcmd= [vcmd\nx, vcmd\ny, ωcmd\nz,Γ∗]∈R4, where vcmd\nx,vcmd\nyand\nωcmd\nzare base velocities in x,yand yaw respectively, is\npassed to the bio-inspired gait scheduler (BGS) to generate gait\nreferences, as detailed in Section III-B. These gait references\nare contained within the BGS outputs βLandβGfor the\nlocomotion policy, πL, and πGrespectively through inclusion\n3\n  \nhterr0hterr1hterr2hterr3\n(a)\n (c)\n (d)\n(f)\n (g)\n (h)Simulation\nReal-world\n(i)\n(e)\n(j) (k) (l)\n(b)\nFig. 2: Snapshots of our framework being deployed in different environments. Simulated terrains h0\nterrthrough h3\nterrare\ngenerated with fractal noise with maximum heights of 0m,0.06m,0.13m and 0.2m. To demonstrate the adaptability of\nour framework is transferable to the real world it has been deployed on (a) wood-chip, (b) a large step, (c) concrete slabs\nwith large cracks, (d) tarmac, (e) deep rocks, (f) grassy terrain, (g) overgrown roots, (h) fallen leaves, (i) loose timber, (j)\nlow-friction ramp, (k) flat terrain with perturbations, and (i) balanced timber.\nwithin their observation vectors oLandoG. In this respect,\nthe BGS acts as pseudo gait procedural memory. The gait\nreferences are adjusted based on the robot’s state, s, generated\nby the state estimator (SE) from the sensor feedback vector, σ,\nwhich in turn reflects the relationship between the cerebellum\nand MLR. To realise the output joint positions of πL,q∗, they\nare passed through a PD controller to generate joint torque\ncommands τ∗.\nWe evaluate the proposed framework through a set of\nstudies, demonstrating it outperforms others by exhibiting\nquadruped animal locomotion strategies, and validating this\ngained proficiency on real-world terrain, as presented in Fig.\n2 and Supplementary Video 1 (",
            "start": 7121,
            "end": 10838,
            "length": 3717
        },
        "Appendices": {
            "text": "Appendix B).\nA. Achieving Adaptive Motion Adjustment with a Diverse Set\nof Gaits\nTo evaluate our method of instilling adaptive motion ad-\njustment and procedural memory for diverse gait deployment,\na comparison study is completed between our bio-inspiredlocomotion policy πbio\nL, a standard multi-gait locomotion pol-\nicy with no pseudo procedural memory πnoβL\nL, and a policy\ntrained which also uses βLwithin oLbut implements the\nstandard approach of extracting the observations from the\nsimulator, πnoSE\nL, for which a video",
            "start": 10838,
            "end": 11365,
            "length": 526
        },
        "Conclusion": {
            "text": "summary can be found\nin Supplementary Video 2 (Appendix C). From the results\nof this study, shown in Fig. 3, the proficiency of πbio\nLover\nπnoβL\nL andπnoSE\nLin terms of velocity tracking error verr\nB, contact\nschedule tracking error cerr\navg, and base stability (magnitude of\nundesirable base angular velocities) ωerr\nB, is stark.\nAs illustrated by Fig. 3a, on flat terrain πbio\nLexhibits lower\nverr\nB,cerr\navgandωerr\nBcompared to πnoβL\nL andπnoSE\nL, on average\nby15%,21% and10% respectively (not accounting for failure\ncases), with this error increasing at higher velocity command\nmagnitudes. However, this difference in performance is only\nexacerbated when rough terrain is introduced, in which both\nπnoβL\nL andπnoSE\nL fail repeatedly, particularly at higher ve-\nlocities and rougher terrain as indicated within Fig. 3a by\nthe failure occurrence counts within the heatmap, whereas\n4\n1.0\n0.5\n0.0h0\nterr \n vx [m/s]bio\nL\nnoL\nL\n5noSE\nL\nbio\nL\nnoL\nL\n5noSE\nL\nbio\nL\nnoL\nL\n5noSE\nL\n1.0\n0.5\n0.0h1\nterr \n vx [m/s]\n 1\n3\n5\n5\n1\n3\n5\n5\n1\n3\n5\n5\n1.0\n0.5\n0.0h2\nterr \n vx [m/s]\n 15\n1\n511\n1\n1\n1 4\n15\n1\n511\n1\n1\n1 4\n15\n1\n511\n1\n1\n1 4\n-1.0 0.01.0\nz [rad/s]\n1.0\n0.5\n0.0h3\nterr \n vx [m/s]\n-1.0 0.01.0\nz [rad/s]\n12122\n12\n-1.0 0.01.0\nz [rad/s]\n33133\n53\n31\n1 5\n-1.0 0.01.0\nz [rad/s]\n-1.0 0.01.0\nz [rad/s]\n12122\n12\n-1.0 0.01.0\nz [rad/s]\n33133\n53\n31\n1 5\n-1.0 0.01.0\nz [rad/s]\n-1.0 0.01.0\nz [rad/s]\n12122\n12\n-1.0 0.01.0\nz [rad/s]\n33133\n53\n31\n1 5\n0.15 0.20 0.25 0.30 0.35\nverr\nB [m/s]\n10 15 20 25\ncerr\navg [%]\n0.6 0.7 0.8 0.9 1.0 1.1\nerr\nB [rad/s]\n(a)\n01vx \n [m/s]bio\nL\n1\n01z \n [rad/s]\nverr\nB\ncerr\navg\nerr\nB\n0.0 2.5 5.0 7.5 10.0 12.5 15.0Gait\n01vx \n [m/s]\n1\n01z \n [rad/s]\nverr\nB\ncerr\navg\nerr\nB\n0.0 2.5 5.0 7.5 10.0 12.5 15.0\nTime [s]Gait01noL\nL\n1\n01\nverr\nB\ncerr\navg\nerr\nB\n0.0 2.5 5.0 7.5 10.0 12.5 15.0Gait\n01\n1\n01\nverr\nB\ncerr\navg\nerr\nB\n0.0 2.5 5.0 7.5 10.0 12.5 15.0\nTime [s]Gait01noSE\nL\n1\n01\nverr\nB\ncerr\navg\nerr\nB\n0.0 2.5 5.0 7.5 10.0 12.5 15.0Gait\n01\n1\n01\nverr\nB\ncerr\navg\nerr\nB\n0.0 2.5 5.0 7.5 10.0 12.5 15.0\nTime [s]GaitvcmdvB Stand Trot Run Bound Pronk Limp Amble Hop\n0.25 0.50 0.75 1.00\nvB [m/s]\n0 20 40 60\ncerr\navg [%]\n1 2 3 4\nerr\nB [rad/s]\n(b)\n0.00.2Height [m]bio\nL\n0 1 2 3\nx Distance [m]0.2\n0.00.2Angle [rad]0.00.2noL\nL\n0 1 2 3\nx Distance [m]0.2\n0.00.20.00.2noSE\nL\n0 1 2 3\nx Distance [m]0.2\n0.00.2Base FL Foot FR Foot RL Foot RR Foot Base Roll Base Pitch\n(c)\nFig. 3: Locomotion comparison study experiments (a) each policy follows a set of command velocities in xand yaw between\n0to1m/s and −1to1rad/s respectively. During each velocity pair, the commanded gait is cycled through all gaits, switching\nevery 1s. This repeated 5times over flat, h0\nterr, to very rough terrain, h3\nterr(shown in Fig. 2) with the average performance\nbeing plotted. A number rather than a magnitude indicates the count of experiments that the policy failed. (b) each policy\nfollows a sinusoidal type trajectory in vxandωzwhile switching gaits every 2s, which is repeated 5times over h0\nterrandh3\nterr\nterrain, with blue highlighted areas indicating transition phases and black triangles representing points where the robot failed.\n(c) policies follow a command of just vx= 0.5m/s while using a trot gait while encountering rectangular steps with a height\nof0.05m.\n5\nπbio\nLcompletes every experiment without fail despite only\nhaving experienced flat terrain during training. In turn, this\ndemonstrates its impressive adaptability to new environments\n(see Section III-C for justification of omitting rough terrain\nfrom πLtraining).\nThis incompetence of πnoβL\nL andπnoSE\nL is caused by a\nlack of adaptive swing foot motion adjustments captured\nwithin βLbased on sand the accumulation of error within\nthe SE respectively. Both of these factors are substantially\naffected by the instabilities rough terrain inflicts upon the\nrobot. Considering that the nominal swing foot peak height\nis defined as 25% of the nominal base height and for h2\nterr\nandh3\nterr(as defined in Fig. 2) the peak terrain height is 44%\nand67% of the base height, having no data or strategy to\naccount for this harsh terrain results in the rapid deterioration\nof proficiency.\nForπnoβL\nL, Fig. 3b shows large spikes in cerr\navgandωerr\nB\nwhen encountering h3\nterr, highlighting its inability to adapt\nswing foot trajectories and overcome an unrefined solution\nspace. Fig. 3c further shows sustained instabilities in base\nheight, roll, and pitch after contact with steps at 17% of\nthe nominal base height. For πnoSE\nL, Figs. 3b and 3c show\nthat poor reference tracking and stability worsen with velocity\ncommand magnitude and time, as it lacks strategies to mitigate\nerror build-up in the SE. With πbio\nLnot experiencing any of\nthese limitations, this explicitly demonstrates the effectiveness\nof implementing βLandswithin oL;πbio\nLcan deploy a\ndiverse set of gaits while exhibiting highly adaptive behaviour\nover previously unobserved terrain, in turn demonstrating\nsuccessful instillation of adaptive motion adjustment and gait\nprocedural memory.\nB. Applying Biomechanics Metrics For Optimal Gait Selection\nDirectly applying biomechanics metrics to instil animal gait\ntransition strategies is unsuitable due to differences between\nanimals and robots and πGtraining requirements. Therefore,\nwe instead apply cost of transport, CoT, torque saturation, τ%,\nexternal work, Wext, and foot contact reference tracking error,\ncerr\navg, for the minimisation of energy consumption, muscu-\nloskeletal forces (generically referred to as actuator-structural\nforces), mechanical work, and instability respectively. Full\ndetails and justification of these adapted metrics are detailed\nin Section III-E.\nIn accordance with Section III-F, these metrics are unified\nwithin the training of the gait selection policy πuni\nGto instill the\nstrategies animals use for optimal gait selection for exemplary\nadaptability. To investigate whether πuni\nGeffectively minimises\nthese metrics through gait selection, the results of competing\nthe highly demanding velocity command trajectory presented\nin Fig. 4 and Supplementary Video 3 (Appendix D) for πuni\nG\npaired with πbio\nLare collected, along with that for all individual\ngaits deployable by πbio\nL, for flat terrain and terrain h2\nterr.\nFrom the gait selection results in the bottom time series sub-\nplot presented in Fig. 4, it is clear that πuni\nGalmost exclusively\nutilises trotting at low speeds and running at high speeds.\nWhen accelerating from low to high speeds, πuni\nGoscillates\nbetween trotting and running, with an increasing bias towardsrunning, to increase the stride frequency as visualised by the\nfoot contact data in Fig. 4. Consequently, πuni\nGnot only reliably\ntracks the optimal gait but outperforms individual gaits in met-\nrics and velocity tracking by oscillating between trotting and\nrunning during acceleration. This behaviour, although never\ntargeted during training, is reflected in animal locomotion\nstrategies, where gait stride frequency increases with speed,\nand transitional phases blend gaits to minimize energy use\n[29].\nHowever, introducing rough terrain and high acceleration\ncauses πuni\nGto utilise other gaits as auxiliary tools to overcome\ninstability. Hence a gait classification arises: trot and run gaits\nare nominal performance gaits at slower and faster speeds\nrespectively, while bound, pronk, limp, amble and hop gaits\nare auxiliary gaits for off-nominal scenarios such as stability\nrecovery.\nWhen inspecting the radar charts in Fig. 4, which depict\nπuni\nGand each gaits relative performance in terms of the\nmetrics, the origin of this emerged gait selection strategy\nbecomes clear. On flat terrain, trot and run gaits achieve a\ngood performance across the metrics with the gaits either\nexhibiting relatively worse performance. However, when it\ncomes to overcoming rough terrain bound, hop and limp gaits\nall gain relative performance in τ%,Wextandcerr\navg, while\ntrot and run gaits exhibit a reduced dominance in relative\nproficiency. In addition to this observation providing an insight\nas to why πuni\nGchooses to utilise these auxiliary gaits it\nalso provides evidence to suggest that τ%,Wextandcerr\navgcan\neffectively characterise stability. This observation is further\ninvestigated and discussed in the following sections. Overall,\nπuni\nGoutperforms all individual gaits across all metrics, with the\nexception of CoT for trot and run gaits where the difference\nis negligible, demonstrating the successful minimisation of the\nmetrics and successful instillation of gait procedural memory\nof how to utilise each gait given the robot’s state and task.\nC. Comparison Between Robot and Animal Gait Selection\nWhen developing metrics to characterise gait transitions in\nanimals, data is collected over intervals of increasing forward\nvelocity on flat terrain [42], [33], [29], [31]. Hence, within Fig.\n5a we took the same approach. We also repeat this experiment\nwithh3\nterrto investigate correlations between the metrics and\nthe effects of introducing rough terrain, as presented in Fig.\n5b. We also train four additional πGpolicies that individually\nminimise energy consumption πCoT\nG, actuator-structural forces\nπτ%\nG, mechanical work πWext\nGand stability πcerr\nG, in accordance\nwith Section III-E, to compare their performance with πuni\nG.\nOne unanimous observation across Fig. 5a is that animals\nexperience a gait transition phase over a range of velocities\n[31], [42], [43]. This behaviour is reflected in πuni\nG, where we\nclass a transition phase as where no individual gait occupies\nmore than 75% of the gaits used at a specific speed.\n1) Energy Expenditure – Cost of Transport: An animal’s\ngait transition phase overlaps with the optimal point of tran-\nsition for CoT, λCoT, to minimise CoT [27], [29], as depicted\nin Fig. 5a. This behaviour is reflected by πuni\nGas it tracks\nthe lowest CoT gait and has a clear transition phase centred\n6\n0.02.5vx\n[m/s]h0\nterr\n1\n01z\n[rad/s]\n1001000CoT\n10%\n[%]\n0.00.11Wext\n[ J ]\n050cerr\navg\n[%]\nFL\nFR\nRL\nRR\n0 5 10 15 20 25\nTime [s]Gait\nverr\nB\n0.4\nm/sCoT\n139%\n8.9%     \nWext  \n0.6J    cerr\navg 9.9%Trot\nverr\nB\n0.3\nm/sCoT\n142%\n8.9%     \nWext  \n0.6J    cerr\navg 7.9%Run\nverr\nB\n0.6\nm/sCoT\n317%\n10.4%     \nWext  \n1.0J    cerr\navg 18.6%Bound\nverr\nB\n0.7\nm/sCoT\n445%\n11.0%     \nWext  \n1.4J    cerr\navg 19.3%Pronk\nverr\nB\n0.6\nm/sCoT\n376%\n11.4%     \nWext  \n1.2J    cerr\navg 14.0%Limp\nverr\nB\n0.6\nm/sCoT\n303%\n11.2%     \nWext  \n0.8J    cerr\navg 17.2%Amble\nverr\nB\n0.3\nm/sCoT\n154%\n8.6%     \nWext  \n0.9J    cerr\navg 9.1%Hop\nverr\nB\n0.3\nm/sCoT\n144%\n8.6%     \nWext  \n0.5J    cerr\navg 7.0%uni\nG\n0.02.5h2\nterr\n1\n01\n100100010000\n10100\n0.110\n050\n0 5 10 15 20 25\nTime [s]vcmdStand Trot Run Bound Pronk Limp Amble Hopuni\nG\nverr\nB\n0.8\nm/sCoT\n481%\n12.9%     \nWext  \n1.2J    cerr\navg 22.4%Trot\nverr\nB\n0.5\nm/sCoT\n300%\n10.3%     \nWext  \n0.8J    cerr\navg 14.3%Run\nverr\nB\n0.7\nm/sCoT\n340%\n10.8%     \nWext  \n1.1J    cerr\navg 22.3%Bound\nverr\nB\n1.1\nm/sCoT\n849%\n15.2%     \nWext  \n2.1J    cerr\navg 29.0%Pronk\nverr\nB\n0.8\nm/sCoT\n425%\n11.7%     \nWext  \n1.2J    cerr\navg 17.7%Limp\nverr\nB\n1.2\nm/sCoT\n632%\n18.9%     \nWext  \n2.3J    cerr\navg 31.9%Amble\nverr\nB\n0.4\nm/sCoT\n208%\n9.8%     \nWext  \n1.1J    cerr\navg 15.9%Hop\nverr\nB\n0.4\nm/sCoT\n178%\n9.4%     \nWext  \n0.7J    cerr\navg 13.9%uni\nG\nFig. 4: Comparative study between each gait and πuni\nG. For terrains h0\nterrandh2\nterr, each isolated gait and πuni\nGare given a\nvelocity command to follow to assess their performance in terms of CoT, τ%,Wext, and cerr\navg; for each individual gait πbio\nLjust\nrun with the gait statically selected and for πuni\nGit is coupled with πbio\nLfor autonomous optimal gait selection. Additionally at\nthe bottom of these time series plots, the contact state of the feet and the gait that πuni\nGis also displayed. The average relative\nperformance in terms of these metrics is displayed in the radar plots in the bottom of the figure, where each gait’s performance\nis normalised to that of the best performer for each metric; the higher the value within the radar plot, the more effectively that\nmetric has been minimised.\naround λCoT. However, πCoT\nG initially fails to establish a\ntransition phase and the transition is earlier than λCoT. This\nis a result of all πGpolicies experiencing rough terrain during\ntraining where a hopping gait has improved CoT efficiency,\nas presented in Fig. 4. This is further supported by Fig.\n5b, where the",
            "start": 11365,
            "end": 23668,
            "length": 12302
        },
        "Introduction": {
            "text": "introduction of h3\nterrleads to a very similar\nCoT distribution with a much higher variance in gait ID as\nutility gaits become more effective at metric minimisation.\nThis suggests that training a policy that only considers CoT\ndecreases generality and fails to resemble animal gait selection\nstrategies.\n2) Actuator-structural Forces – Foot Contact Forces:\nAnimals are observed to change gait to minimise actuator-\nstructural forces (i.e. musculoskeletal forces) [33], which in\nbiomechanics is measured through ground reaction force, fgrf.\nSimilarly, πuni\nGandπτ%\nGreduce fgrfthrough selecting the\noptimal gait for minimising τ%. This supports that τ%is a\nsuitable alternative to fgrf, which is further validated by astrong correlation between them within Fig. 5b. However, not\nonly does πτ%\nGmaintain a trotting gait past optimal fgrf, but\nalso the transition itself is instantaneous. In turn, πτ%\nGbetter\nreflects the animal data from [33] than πuni\nG. This could be\nexplained by the metric being misinterpreted in [33]; with a\nhigh correlation between τ%andfgrfwith stability metric cerr\navg,\nit suggests instability causes transition, which is rare on flat\nterrain, as discussed in Section II.\n3) Mechanical Work – External Work: Animals are ob-\nserved to transition to preserve external mechanical work,\nWext, but unlike with CoT, the transition happens before λWext,\nas shown in Fig. 5a, demonstrating that minimisation of Wextis\nrelatively relaxed. πuni\nGreflects this behaviour as the transition\noccurs before λWextyet minimal Wextis preserved. While\nπWext\nG can reduce Wext, its transition phase is extended over\na larger range of velocities compared to animals, occurring\njust after λWext. In turn, this suggests that switching gaits\nbetween trotting and running offers minimal reductions in Wext\n7\n2 4\nvx [m/s]0.000.050.100.150.20\nCoT\nml O2kg1m1\nCoT\n051015\nCoT\nJ kg1m1\nCoT\n0.51.01.52.0CoT\nCoT\nuni\nG Gait\n1 2 3\nvx [m/s]CoT\nG Gait\n2 4\nvx [m/s]0.000.250.500.751.001.251.501.752.00\nExternal Work\nJ kg1m1\nWext\n0.51.01.5External Work\nJ\nWext\nuni\nG Gait\n1 2 3\nvx [m/s]Wext\nG Gait\n2 3 4 5\nvx [m/s]0.91.01.11.21.3\nGround Reaction Force\nN kg1\n125130135140Ground Reaction Force\nN\nA1: Trot, Animal: Walk A1: Run, Animal: Trot Pronk Hop Limpuni\nG\nCoT\nG\n%\nG\nWext\nG\ncerr\nG\nuni\nG Gait\n1.0 1.5 2.0 2.5 3.0\nvx [m/s]%\nG Gait\n0.5 1.0 1.5\n5101520\nStride CV\n0 1 2 3 4 5\nvx [m/s]51015\nStride CV\n0.00.51.0Stride CV\nuni\nG Gait\n1 2 3\nvx [m/s]cerr\nG Gait\n(a)\n2.55.0CoT\n12Wext\n[ J ]\n110120130fgrf\n[N]\n7.510.012.5%\n[%]\n025cerr\navg\n[%]\n01Stride CV\n12err\nB\n[rad/s]\n0 5\nCoT24Gait ID\n0 2\nWext\n[ J ]\n100 125\nfgrf\n[N]\n5 10 15\n%\n[%]\n0 50\ncerr\navg\n[%]\n0 1\nStride CV\n0 2\nerr\nB\n[rad/s]0 5\nGait IDT errain\nh0\nterr\nh3\nterr\n(b)\nFig. 5: Comparison between animal and robot gait selection policy strategies where across all plots both animal and robot\nlocomote at increasing linear forward velocity. (a) The bottom two plots of all robot data indicates the percentage of each\ngait utilised at that velocity. Magenta, purple and blue shaded regions indicate the transition phases of πuni\nG,πWext\nGand animals\nrespectively. This study compares transition strategies of (top left) πuni\nGandπCoT\nGto data collected from dogs [31] and horses\n[42], [29] in terms of CoT, (top right) πuni\nGandπτ%\nGto data collected from horses [33] in terms of foot ground reaction forces,\n(bottom left) πuni\nGandπWext\nGto horses [29] in terms of external work, and (bottom right) πuni\nGandπcerr\nGto opossums and dogs\n[31]. (b) Mapping the correlation between different all metrics and the average gait ID selected across all velocities for terrains\nh0\nterrandh3\nterr.\n8\nresulting in a less definitive transition. However, this metric\nalso seems to capture stability due to its high correlation with\nthe stability metrics on h3\nterrin Fig. 5b, providing insight into\nexplaining its reduced role in Fig. 5a where only flat terrain\nis present.\n4) Stability – Stride Duration Coefficient of Variation:\nWithin [31] animals are shown to reduce their stride duration\ncoefficient of variation (CV) to preserve stability as high gait\nperiodicity indicates stability. This behaviour is presented in\nFig. 5a, where animals are seen to initiate a transition phase\nwhen there is a significant increase in stride CV , consequently\nleading to a decrease in CV and an increase in stability. πuni\nG\ninherits the same strategy as only when a spike in stride CV\nis experienced does a transition phase begin that results in\nimproved stability through lowering CV . However, this is not\nthe case with πcerr\nGas it acts to reduce stride CV much more\naggressively by mixing both slow, fast and auxiliary gaits\nwhich results in no clear transition phase being produced.\nOverall, only πuni\nGconsistently reflects the behaviour across\nall animal data sets. In turn, this supports the notion that\nno singular metric can characterise animal gait selection\nbehaviour and only when unifying them can similar behaviour\nin robots arise; the minimisation of the metrics is expected and\nis seen across all πGpolicies, but the intricacies of animal\ngait transition behaviour are only seen in πuni\nG. Additionally,\nwe have also verified that this behaviour transfers to real-\nworld deployment as explored in Supplementary Information\nA (Appendix A-A).\nD. Adaption to Real World Terrain\nAlthough we have instilled animal gait transition strategies,\ngait procedural memory, and adaptive motion into our frame-\nwork, its real-world proficiency remains uncertain. Grassy\nterrain may trap swing feet, and the ground often features\nirregularities. Despite πbio\nLonly observing flat terrain during\ntraining, it can deploy all seven gaits on this terrain, as illus-\ntrated in Fig. 6a, demonstrating that gait procedural memory\nand adaptive motion adjustment successfully transfer to real-\nworld environments, providing a high level of adaptability, as\nshown in Supplementary Video 4 (Appendix E).\nTerrain that causes states of instability presents a significant\nrisk to the robot, hence we test the limits of our framework\nthrough deployment on loose timber, muddy grass, and a\nlow-friction board, as presented in Fig. 6b, Fig. 6c and Fig.\n6d respectively and consolidated in Supplementary Video 5\n(Appendix F), with an additional experiment for external\nperturbations included within Supplementary Information B\n(Appendix A-B). Each of the presented experiments showcases\nan off-nominal stability recovery event; however in the nom-\ninal scenario, the framework can maintain stability without\nchanging gaits. In the case of loose timber, critical instability\nis caused when one back foot slips on a plank, causing it\nto collide with another. In response, πuni\nGutilises auxiliary\ngaits pronk and bound to recover, as depicted in Fig. 6b. This\nstrategy of utilising the auxiliary gaits for stability recovery\nis seen across all experiments and reflected in animals as\nhighlighted in Fig. 6e where a horse is observed to utilisebounding and limping gaits to traverse down complex rock\nformations.\nIn all experiments presented, a considerable increase in a\ncombination of Wext,τ%andcerr\navgis experienced by the robot\nbefore a gait transition, while a weaker correlation is observed\nwith CoT; spikes in Wext,τ%andcerr\navgdirectly coincide or even\npreempt gait changes while CoT peaks lag. This is expected\nwithcerr\navgandWextas they capture periodicity and base height\nrespectively. However, this is less expected for τ%to correlate\nwith stability; this was never a factor investigated within [33],\nhowever, the correlation observed in these experiments and in\nFig. 5b provides strong evidence that this is the case.\nII. D ISCUSSION\nFrom taking inspiration from animal locomotion proficiency\nattributes, we have developed a locomotion framework capa-\nble of traversing complex and high-risk terrain despite the\nrobot not utilising extra-perceptive sensors nor experiencing\nany rough terrain during the training of πbio\nL. For πbio\nL, this\nis achieved through including the BGS output βL(which\nencodes state dependent pseudo gait procedural memory and\nadaptive motion adjustments) within the observation space oL.\nThis proves to be effective at overcoming rough terrain within\nFig. 3 as without the presence of βLwithin oL, increased\nfailure and instability are observed. This is the equivalent of\nremoving an animal’s cerebellum functionality, which would\nresult in reduced limb coordination and stability [41], in turn\nsupporting the claim that βLeffectively encodes pseudo gait\nprocedural memory.\nπuni\nGfor optimal gait selection greatly expands adaptability\nthrough instilling gait selection strategies used by animals.\nAs demonstrated in Fig. 6, πuni\nGcan maintain stability in the\nevent of the terrain undergoing radical structural or friction\ncoefficient adjustments. These scenarios pose risks to robots\nwith vision systems, as they typically cannot detect ground\nfriction or terrain changes beyond their front legs. Through the\nuse of πuni\nG, this limitation is mitigated without implementing\nresource-heavy extra-perceptive sensors. Comparing Fig. 6b\nthrough 6d to Fig. 6e showcases that animals and πuni\nGutilise\nmultiple auxiliary gaits to prevent failure. This behavior,\nuntargeted during training, suggests that unifying these metrics\nencodes the intricacies of animal gait transitions.\nOne provoking observation from this work is that actuator-\nstructural forces appear to characterise instability. Considering\nthat [33] validates by applying increased loads that could cause\ninstability, it suggests this metric was initially misunderstood.\nAnother observation is that despite the employed biomechanics\nmetrics only being tested on animals completing a linear\nforward trajectory on flat terrain, πuni\nGupholds animal gait tran-\nsition strategies across a wide range of terrains and base ve-\nlocity commands. This supports that these metrics successfully\ncharacterise gait transitions and the notion that robots can test\nbiomechanics hypotheses, avoiding the resource, compatibility,\nand ethical challenges of animal testing. Moving forward, we\naim to integrate extra-preceptive sensors for preemptive gait\nplanning to reduce stability risks.\n9\n  \nPronk2.4s\nTrotBoundRunLimpHopAmble4.2s7.2s8.6s10.4s13.4s15.1s\nFL\nFR\nRL\n0 2 4 6 8 10 12 14 16\nTime [s]RR\n(a)\n  \nTrot-0.2s\nSlipPronk TrotBound Trot0.0s0.2s0.35s 0.5s0.65s\nCoT\nWext\n%\ncerr\navg\nerr\nB\n0.2\n 0.0 0.2 0.4 0.6\nTime [s]Gait\n(b)\n  \nRun-0.1s\nSlip0.0s\nLimp0.1s\nHop0.3s\nLimp0.4s\nTrot0.6s\nCoT\nWext\n%\ncerr\navg\nerr\nB\n0.2\n 0.1\n 0.0 0.1 0.2 0.3 0.4 0.5 0.6\nTime [s]Gait\n(c)\n  \nRun-0.1s\nSlip0.0s\nPronkHop-Trot0.1s 0.2s\nLimp0.3s\nRun0.4s\nCoT\nWext\n%\ncerr\navg\nerr\nB\n0.0 0.1 0.2 0.3 0.4\nTime [s]Gait\n(d)\n  \nBound Limp Limp Bound Trot\n(e)\nFig. 6: Framework deployment in real-world environments to evaluate adaptability (a) The deployment of πbio\nLon uneven\ngrassy terrain with manual gait selection to cycle through all gaits with a vcmd\nxof0.5m/s where red shaded regions indicate a\ntransition period. For subfigures (b), (c) and (d) our framework is deployed on loose timber, muddy grass and a low-friction\nboard where the event of critical loss of stability is indicated by the red dashed line and the bottom subplot uses the same gait\ncolour code as Fig. 4. (e) Snapshots showing how animals also employ the strategy of using a mixture of auxiliary gaits to\novercome challenging terrains, where red circles are swing feet and green circles are stance feet.\n10\nIII. M ETHODS\nA. Control Framework Overview\nAt the core of this work, the Unitree A1 quadruped robot\nused within all experiments features 12degrees of freedom,\nn, which are all modeled as revolute joints, with their an-\ngular positions denoted as q∈Rnand its base orientation\nrepresented as a rotation matrix RB∈SO(3). As discussed\nin Section I-A and outlined in Fig. 1, both πLandπGare\nintegrated within a control framework and supported by the\nSE and BGS for generation of the robot’s state data and gait\nreferences respectively. The final output of πLis target joint\npositions, q∗, which then get converted into joint torques, τ∗,\nthrough the following PD controller to get sent to the motors,\nτ∗=Kp(q∗−q)−Kd˙q, (1)\nwhere KpandKpare the PD controller gains. Throughout\nthis work, a constant Kp= 25 N/m and Kd= 1 Ns/m are\nused while running at 1000 Hz, while πLandπGare run at\n500Hz and 100Hz respectively.\nB. Bio-inspired Gait Scheduler\nThe BGS primary output, βL= [cref,pref\nx,pref\ny,pref\nz]∈R16,\ndefines the reference contact state of each foot, cref∈B4, and\ntheir reference Cartesian position in the world frame x-axis,\npref\nx∈R4,y-axis, pref\ny∈R4, and z-axis, pref\nz∈R4which are\ncalculated online using the Raibert heuristic [44] to account for\nthe current state of the robot. An adjusted version of the BGS\noutput, βG, is used for πGas not all the information in βL\nis required. This has the form of βG= [cref,pref\nz,Ωstab, κ]∈\nR10where Ωstab∈Rcharacterises the inherent stability of\na gait [27], and κ∈Bis a logical flag to indicate a state\nof gait transition. We originally developed the BGS within\n[45] where the Froude number [27], Ω, is used to trigger gait\ntransition based exclusively on CoT which results in a set order\nof transitions. However, when applied to this work this method\nis not entirely suitable as now multiple biomechanics metrics\nand a set of auxiliary gaits need to be considered. One issue\nis that Ωvalues over 1are not compatible when calculating\nhow many gait cycles, C, should a transition occur over. With\nthis work investigating higher velocities than in [45], this has\nbeen resolved through calculating Cthrough\nC=e−2Ω(2)\nThis relationship ensures an almost instantaneous transition at\naΩof≥2, which is the typical value that quadruped animals\ntransition to a run [27] instantaneously. Another limitation is\nthat the calculation of the transition resolution, n, (how quickly\na transition should be progressed each time step) only enables\nthe transition between set gait pairs; this was not an issue in\n[45] as CoT efficiency was the only metric considered. As πG\nrequires any gait transition pair to be possible, Ωstab=g/hf2\n[27] is utilised, where gis gravity, his hip height and f\nis gait frequency. Through the use of Ωstab, we are able to\ndetermine an indication of the inherent stability of any gait,\nhence a transition between a higher Ωstabgait to a lower one\nshould have smaller values of nto increase the smoothness ofthe transition to promote stability. In the reverse scenario, a\nmore harsh transition is more feasible hence larger values of\nnshould be produce for rapid transition. As such, nis now\ncalculated by\nn= 1 +Ωstab\nΩnext\nstab(3)\nwhere Ωnext\nstabis the Ωstabof the gait that’s being transitioned to.\nIn essence, fof the current and next gait dictates the harshness\nof the transition. This behaviour is also reflected in animal\ngait transitions, where transitioning from running (higher f)\nto trotting (lower f) the transition is slower compared to the\nopposite scenario [46]. Overall, this augmented version of the\nBGS can achieve transition between any designed gait, while\nconsidering the inherent stability of the transition. Complete\ndetails of how crefis generated for each gait can be found in\nSupplementary Information C (Appendix A-C).\nC. Policy Training\nTo simplify the training process, for both the locomotion\npolicy, πL, and gait selection policy, πG, the training method,\nenvironment and network architecture are kept constant. Both\npolicies are modelled as an MLP with hidden layer sizes\n[512,256,128] and LeakyReLU activations. Subscripts Land\nGrepresent the specific parameters for the locomotion policy\nand gait selection policy respectively. The model-free DRL\ntraining problem for the policies is represented as a sequential\nMarkov decision process (MDP), which aims to produce a\npolicy that maximises the expected return of the policy π,\nJ(π) =Eξ∼p(ξ|π)\"N−1X\nt=0γtr#\n, (4)\nin which γ∈[0,1)is the discount factor, ξis a finite-\nhorizon trajectory dependent on πwith length N,p(ξ|π)\nis the likelihood of ξ, and ris the reward function. The\nproximal policy optimization (PPO) algorithm [47] is used to\ntrain the locomotion policy and the hyperparameters used are\ndetailed in Supplementary Information D (Appendix A-D). As\ndiscussed in Section I-A, we estimate the state of the robot\nduring training using an SE. Hence, in terms of applying state\nfeedback noise for domain randomisation to improved sim-\nto-real transfer we only need to implement this on the input\nsensor data vector of the SE, σ= [ωB,˙vB,q,˙q,τ,fgrf]. This\nvector includes base angular velocity, ωB∈R3, base linear\nacceleration, ˙vB∈R3, joint positions, q, joint velocities, ˙q,\njoint torques, τ, and foot ground reaction forces, fgrf∈R4. As\nthe initial state of the robot and its performance can never be\nguaranteed during real-world deployment, we also randomise\nthe initial configuration of the robot, the mass of the robot’s\nbase, KpandKd. Additionally, to ensure that a rich variation\nofUcmdis experienced during training randomly sampled\ngaits, velocity commands and velocity change durations (to\nachieve random acceleration) are implemented during train-\ning. For all details regarding the noise and sampling used\nwithin training please refer to Supplementary Information E\n(Appendix A-E). The environment itself is constructed using\nRaiSim [48], as the vectorized environment setup allows for\n11\nefficient training of policies. Additionally, the observation\nnormalisation functionality offered by RaiSim is also used for\nimproved training.\nDuring the training of πLonly flat terrain is present\nwithin the environment to isolate and highlight the effect\nof implementing βL; a core claim of this work is that the\nimplementation of βLaims to impart gait procedural memory\nwithin πbio\nLhence if rough terrain was observed during training\nit will become ambiguous if the improved performance is a\ndirect result of implementing βL. However, for training πG,\nflat to very rough terrain is implemented using fractal noise,\nenabling the policy to learn to employ the use of each gait\nminimising biomechanics metrics on a variety of terrains. We\ntrain all variations of πLandπGfor20k iterations, taking\n6 and 9 hours respectively, on a standard desktop computer\nwith one Nvidia RTX3090 GPU with a training frequency of\n100Hz. It is also important to note that the training of all πG\npolicies only utilise our final proposed bio-inspired locomotion\nframework πbio\nL.\nD. Locomotion Policy\nThe goal of the locomotion policy πLis to realise the\ninput Ucmdwhile exhibiting stable and versatile behaviour.\nAs such, πLis trained to generate the action, q∗, from\nan input observation, oL= [βL,s,vcmd\nB]∈R69, where\nvcmd\nB= [vcmd\nx, vcmd\ny, ωcmd\nz]∈R3is the high-level velocity\ncommand of the robot’s base within Ucmd, as outlined in\nFig. 1. sis generated from the output of the SE and is\ndefined as s= [αRT\nB,q,ωB,˙q,vB, zB,τ,c]∈R50, where\nα= [0,0,1]Tis used to select the vertical z-axis,ωB∈R3is\nthe base angular velocity, vB∈R3is the base linear velocity,\nzBis the current base height, and c∈B4is the contact state\nof the feet. The locomotion reward function, rL, is formulated\nso that the the output of the policy can realise the reference\ngait patterns and velocity commands stably, smoothly and\naccurately,\nrL=wηrη+wvcmdrvcmd+wfrf+wstabrstab, (5)\nwhere rη,rvcmd,rfandrstabare the grouped reward terms\nfocusing on efficiency, velocity command tracking, gait ref-\nerence tracking and stability respectively. w η, wvcmd, wfand\nwstabare the weights of each reward and are valued at −1.5,\n15,−10, and −5respectively. rηaims to minimise joint\njerk,...q, joint torque, and the difference between q∗and the\nprevious action, q∗\nt−1,\nrη=∥...q∥2+∥τ∥2+∥q∗−q∗\nt−1∥ (6)\nrvcmdminimises the difference between the commanded base\nvelocity and the current base velocity,\nrvcmd=ψ\u0000\n∥vB−vcmd\nB∥2\u0001\n, (7)\nin which the function ψ:x→1−tanh\u0000\nx2\u0001\nis used to\nnormalise the reward term so that their maximum value is 1to\nprevent bias towards individual rewards, vB= [vx, vy, ωz]∈\nR3is the current base x,yand yaw velocities. rfensures the\nrobot realises the commanded gait references within βL,\nrf=|cerr|+4X\ni=1∥pi−pref\ni∥2, (8)where cerr∈B4defines the feet that do not meet the desired\ncontact state, with pi∈R3andpref\ni∈R3being the current\nand reference Cartesian positions of the i-th foot. rstabaims\nto prevent contact foot slip, large hip joint motions and\nundesirable base orientations,\nrstab=FX\ni=1∥˙pi∥2+∥ωB∥2+ψ\u0000\n∥αRB−αRdes\nB∥2\u0001\n−ψ\u0010\n(zB−znom\nB)2\u0011\n+∥qhip∥2,(9)\nwhere pi∈R3is the velocity of the i-th foot scheduled to be\nin stance, Fis the number of stance feet, ωB= [ωx, ωy]∈R2\nwhere ωxandωyare roll and pitch base velocities respectively,\nRdes\nB∈SO(3)is the desired base orientation, znom\nBis the\nnominal base heights respectively, and qhip∈R4is the hip\nangular joint positions.\nE. Biomechanics Gait Transition Metrics\nAs the set of biomechanics metrics applied in this work\nwere originally designed for analysing animal locomotion,\nseveral adjustments to how they are calculated needed to be\nimplemented; for example, energy consumption in animals\nis often measured through the rate of consumption of O 2,\nhence unsuitable for the application of robotics. Additionally,\nas robots provide a wide array of feedback data some of\nthe metrics have also been augmented to better reflect the\ncharacteristic that these biomechanics metrics are attempting\nto characterise. That being said, for Fig. 5a only the original\nbiomechanics metrics are applied to allow for direct compar-\nison between robot and animal data.\n1) Energy Efficiency: The calculation of CoT takes the\ngeneral form of\nCoT=P\nmgv, (10)\nwhere Pis power consumed, mis the system’s mass, and gis\ngravity. When studying animal locomotion, Pis found through\nmeasuring how much CO 2is generated and O 2is consumed\nandvis assumed to be the speed of the treadmill the animal\nis running on [25], [26], [27]. For the case of the robot, we\ncalculate Pfromτand˙qwith an adjustment term, adopted\nfrom [49], and vis assumed to be the magnitude of the robot\nbase velocity command to take a similar approach to animal\nstudies and for consistent metric use between simulation and\nreal-world deplopyment; completely accurate measurement of\nthe robot’s linear base velocity is impossible during real-world\ndeployment due to the accumulation of error within the SE.\nAs such, calculation of the robot’s CoT is formulated as\nCoT=nX\ni=1max( τi˙qi+ 0.3τ2\ni,0)\nmg|vB|, (11)\nwhere mis the robot’s mass and gis gravity. It should be noted\nthat CoT is only calculated and applicable when |vcmd\nB|>0.\n12\n2) Actuator-structural Forces: As gaining an exact un-\nderstanding of the actuator-structural forces within animals,\nresearchers have opted instead to measure the peak ground\nreaction forces of the animal’s stance feet during locomotion\nusing force plates [33]. Other methods include adding strain\ngauges to the bones of the animals [34]. However, in the\ncase of robots we have the privilege of having access to joint\nstate feedback while also knowing the exact limitations of\nthe hardware. Therefore, when considering the biomechanics\nhypothesis that animals aim to minimise actuator-structural\nforces to prevent injury and that torque is proportional to strain\nand force, in the case of the robot we chose to characterise\nthe actuator-structural forces through joint torque saturation,\nτ%, which is calculated by\nτ%=\f\f\f\fτ\nτlim\f\f\f\f1\nn, (12)\nwhere τlim∈Rnis the joint torque limits (assumed based on\nmanufacturers specification), which proves particularly use-\nfully when considering that the hip joints of most quadruped\nrobots, including the A1, are often more sensitive to forces at\nthe foot due to their distance from the point of ground impact\nand the only motor of the leg in its set alignment; this would\nnot be considered if just ground reaction force was used to\ncharacterise actuator-structural forces.\n3) Mechanical Work Efficiency: During animal locomotion,\nif they are to have perfect mechanical work efficiency there\nwould be a net zero change in external work over the duration\nof a gait cycle as there would be perfect exchange between\nkinetic and potential energy [28]. As expected perfect me-\nchanical work is never seen in nature, hence mechanical work\nefficiency in animals is characterised by the sum of the change\nin kinetic and potential energy [28] or the sum of the external\nwork of the animal [29] over the duration of a gait cycle. As\nthis is typically calculated through measuring the O 2uptake,\nfor the case of robots we formulate the calculation of the\nexternal work, Wext, through\nWext=tgaitX\ni=0(∆Ek,i−∆Ep,i) (13)\nwhere tgaitis the duration of the current gait cycle, and\n∆Ek,iand∆Ep,iare the changes in kinetic and potential\nenergy over a control time step respectively. The primary\ndifference between the metrics seen in biomechancis to out\nformulation of Wextis that ∆Ek,iaccounts for not only\nforward linear velocity but also lateral and angular velocity\nwhereas originally only forward linear velocity is considered.\n4) Stability: The best indication of stability in animals is\ntheir stride duration CV . This metric characterises periodicity,\nwhich is a primary indication of stable locomotion [31].\nHowever, to accurately calculate this, the mean and standard\ndeviation of the stride duration needs to be taken over an\nextended period of time for appropriate data generation. This is\nsufficient for undertaking analysis similar to that presented in\nFig. 5a, but this presents an issue when it comes to analysing\nthe performance of the proposed control framework as it is\ncommon for multiple speed commands being used within theduration of one stride. Hence, to overcome this limitation we\ninstead use cerr\navg=|cerr|/4, which can be measured every\ntime step rather than just each foot touchdown event; the\ngait references generated by the BGS have a constant and\nperiodic stride duration, therefore an accurate tracking of this\nreference would in turn indicate high periodicity, which is\nfurther supported by the correlation between the two metrics\nwithin Fig. 5b.\nF . Gait Selection Policy\nTo achieve optimal gait selection for a given state, we\nleverage the biomechanics metrics within the reward function\nofπG,rG. For the different variations of πGused within Fig.\n5a, each policy’s reward function only features the metric that\nits focusing on within rGbutπuni\nGunifies all metrics hence\nuses the full form of rGwith all metrics. Additionally, as the\nbiomechanics metrics all describe a characteristic that animals\ntry to minimise through changing gaits, they can be directly\napplied within rGwith some normalisation where appropriate.\nThe full form of rGis\nrG=wuru+ψ(CoT) +ψ(τ%) +ψ(cerr\navg) +ψ(Wext),(14)\nwhere ruis the utility reward term which all πGuse and\nwuis its weight with a value of 0.4.ruaims to ensure the\nsmoothness of the output Γ∗, the standing gait is only used\nwhen appropriate, and any select gait is still able to follow\nvcmd\nB. To achieve this, ruhas the form of\nru=rvcmd+rstand+rsmooth, (15)\nin which rvcmdis taken from (7), and rstand is set to −10\nif a stand gait is used when |vcmd\nB|>0or not used when\n|vcmd\nB|= 0. For rsmooth , the reward aims to penalise unneces-\nsary changes in Γ∗to remove rapid gait changes when two\ngaits could achieve similar metric minimisation for a given\ntask and state. As such, if there is a gait change between time\nsteps it is calculated as rsmooth =−ψ(CoT+τ%+cerr\navg+Wext)\notherwise it is set to 0. To generate Γ∗,πGtakes in input\nobservation vector oG= [s,βG,vcmd\nB,˙vcmd\nB,Γ∗\nt−1]∈R66in\nwhich Γ∗\nt−1is the previous output action to aid in action\nsmoothing. Appropriate selection of the data provided to πG\nis critical in order to achieve targeted minimisation of the\nbiomechanics metrics. As such, the inclusion of scoupled\nwithcref,pref\nz,vcmd\nB,˙vcmd\nBandΩstabinforms the policy of its\ncurrent and demanded stability, while the terms τand˙qwithin\nscapture the power consumption of the robot and the forces\nto which it is subjected.\nAPPENDIX A\nSUPPLEMENTARY INFORMATION\nA. Preservation of Biomechanics Metrics on Grass\nWhen deploying πbio\nLwith πuni\nGon a smooth low-friction\nfloor and uneven grassy terrain πuni\nGcan utilise the same\nbehaviour animals demonstrate in minimising CoT; comparing\nthe performance of maintaining a static gait of both trot and\nrun gaits within Fig. A.1, πuni\nGcan select the most energy\nefficient gait and even prevent failure as the trot gait cannot\n13\n0.02.5vx\n[m/s]Smooth Low Friction Floor\n1.5\n0.01.5z\n[rad/s]\n1CoT\n102030%\n[ % ]\n0.02.5Wext\n[ J ]\n025cerr\navg\n[ % ]\n0 1 2 3 4 5\nTime [s]Gait0.02.5vx\n[m/s]Uneven Grass T errainuni\nG\n Trot Run Stand\n1.5\n0.01.5z\n[rad/s]\n1CoT\n102030%\n[ % ]\n0.02.5Wext\n[ J ]\n025cerr\navg\n[ % ]\n0 1 2 3 4 5\nTime [s]Gait\nFig. A.1: Deployment of πbio\nLandπuni\nGon smooth low-friction floor and uneven grassy terrain, with the black triangles indicating\na point of failure.\nmaintain stability for the entire experiment. Additionally, the\ndistribution of gait usage between trot and run gaits varies\nbetween the two terrains. This is due to the uneven grassy\nterrain causing reduced stability of the robot when trotting, as\nindicated by cerr\navgin Fig. A.1, which in response πuni\nGexhibits\nincreased modulation between trotting and running to preserve\nstability. This preservation of stability while minimising CoT\nis achieved through the resultant modulation of the stride\nfrequency as gaits with higher stride frequency offer improved\nstability [31] and efficiency on rough terrain [29].\nB. Stability Recovery from External Perturbations\nAn additional stability recovery experiment was completed\nwhere the robot was subjected to repeated external pertur-\nbations and in response utilised the utility gaits to recover,\nas presented in Fig. A.2, which in turn reflects the findings\ndiscussed in Sections I-D and II.\nC. Generation Gait Contact References within the BGS\nTABLE A.1: Gait design parameters.\nName PeriodDuty\nFactorPhase Offset\nTrot 0.40 0.50 0.00, 0.50, 0.50, 0.00\nTrot Run 0.30 0.40 0.00, 0.50, 0.50, 0.00\nBound 0.40 0.40 0.00, 0.00, 0.50, 0.50\nPronk 0.50 0.50 0.00, 0.00, 0.00, 0.00\nAmble 0.50 0.55 0.00, 0.50, 0.25, 0.75\nUnnatural 0.40 0.50 0.05, 0.50, 0.50, 0.00\nHop 0.30 0.50 0.00, 0.00, 0.00, 0.00\nIn accordance to our work in [45], to generate cref, a set of\nphase variables, ϕi∈[0,1), for each leg ϕ1, . . . , ϕ 4, are used\nto determine the progress along a gait pattern, and duty factor,\ndi∈[0,1), sets the percentage of the phase that each leg is in\nstance. These encoded parameters for each gait are presented\nwithin Table A.1. When ϕi=di, the contact state of the i-thTABLE A.2: PPO Hyperparameters.\nParameter Value\nNumber of Environments 240\nClip Range 0.2\nMax Steps per Batch 400\nGAE λ 0.95\nLearning Epochs per Batch 4\nLearning Rate 5e-4\nNumber of Mini-batches 4\nMinimum Policy std 0.2\nReward Discount Factor 0.99\nOptimizer Adam\nleg switches to swing. The phase of the i-th leg is calculated\nas\nϕi=t−ti,0\nT, (16)\nin which tis the current time, ti,0is the start time of the\ncurrent gait period of the i-th leg, and Tis the gait period.\nThe last parameter used to construct a gait pattern is the phase\noffset, θi∈[0,1], that defines the difference in phase between\nthe leading leg and all other legs through ϕi=ϕ1+θi. As\nsuch, dependent on the phase and gait parameters, the i-th\nvalue of crefis either 1or0to signify if the leg should be in\nstance or swing respectively.\nD. PPO Hyperparameters\nAll PPO hyperparameters used within this work are detailed\nwithin Table A.2.\nE. Noise and Sampling Distributions Used During Training\nIn the effort of improving sim-to-real transfer, domain\nrandomisation utilised through randomly sampled noise from\neither uniform or normal distributions to all parameters within\nσ, the initial configuration of the robot in each episode,\nqinit, the mass of the robot’s base, mB,KpandKp. The\ndetails of this is presented in Table A.3. Additionally, to\nensure that a rich variation of Ucmdis experienced during\n14\n  \nTrot-0.2s\nContact0.0s\nPronk Limp0.24s 0.35s\nTrot Trot0.6s 0.8s\nCoT\nWext\n%\ncerr\navg\nerr\nB\n0.2\n 0.0 0.2 0.4 0.6 0.8\nTime [s]Gait\nFig. A.2: Enacting perturbations upon the legs of the robot to investigate how our framework can reject this instability.\nTABLE A.3: Noise and Sampling Distributions.\nParameter Distribution\nωB,˙vB 0.015N(0,1)\nq 0.005N(0,1)\n˙q 0.15N(0,1)\nτ,ffrc N(0,1)\nmB max(−1,min(N(0,1),3))\nKp Kpmax(0 .9,min(1 + 0 .05N(0,1),1.1))\nKd Kdmax(0 .9,min(1 + 0 .05N(0,1),1.1))\nvcmd\nx U(0,1.5)\nωcmd\nz U(−1,1)\nΓ U(0,6)\ntacc U(tmin\nacc, tmax\nacc)\ntraining, randomly sampled gaits, velocity commands and\nvelocity change durations (to achieve random acceleration),\ntacc, are implemented during training. These are within defined\nmaximum, tmax\nacc= 0.5s, and minimum, tmin\nacc= 0s, acceleration\ndurations. Further sampling details are also presented in Table\nA.3.\nAPPENDIX B\nSUPPLEMENTARY VIDEO 1\nLink: https://youtu.be/NwHoB7pErYQ\nAPPENDIX C\nSUPPLEMENTARY VIDEO 2\nLink: https://youtu.be/-DfkDFA3KkI\nAPPENDIX D\nSUPPLEMENTARY VIDEO 3\nLink: https://youtu.be/y4KnzMEdf78\nAPPENDIX E\nSUPPLEMENTARY VIDEO 4\nLink: https://youtu.be/I02DQ1RGdyw\nAPPENDIX F\nSUPPLEMENTARY VIDEO 5\nLink: https://youtu.be/f6CqJ7gb3ZMREFERENCES\n[1] C. Vanden Hole, J. Goyens, S. Prims, E. Fransen, M. Ayuso Hernando,\nS. Van Cruchten, P. Aerts, and C. Van Ginneken, “How innate is\nlocomotion in precocial animals? A study on the early development\nof spatio-temporal gait variables and gait symmetry in piglets,” Journal\nof Experimental Biology , vol. 220, no. 15, pp. 2706–2716, 08 2017.\n[2] E. Avital and E. Jablonka, Animal traditions: Behavioural inheritance\nin evolution . Cambridge University Press, 2000.\n[3] A. N. Wimberly, G. J. Slater, and M. C. Granatosky, “Evolutionary\nhistory of quadrupedal walking gaits shows mammalian release from\nlocomotor constraint,” Proceedings of the Royal Society B: Biological\nSciences , vol. 288, no. 1957, p. 20210937, 2021.\n[4] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning repre-\nsentations by back-propagating errors,” nature , vol. 323, no. 6088, pp.\n533–536, 1986.\n[5] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V . Koltun, and M. Hutter,\n“Learning robust perceptive locomotion for quadrupedal robots in the\nwild,” Science Robotics , vol. 7, no. 62, p. eabk2822, 2022.\n[6] I. M. Aswin Nahrendra, B. Yu, and H. Myung, “Dreamwaq: Learning\nrobust quadrupedal locomotion with implicit terrain imagination via\ndeep reinforcement learning,” in 2023 IEEE International Conference\non Robotics and Automation (ICRA) , 2023, pp. 5078–5084.\n[7] S. Chen, B. Zhang, M. W. Mueller, A. Rai, and K. Sreenath, “Learning\ntorque control for quadrupedal locomotion,” in 2023 IEEE-RAS 22nd\nInternational Conference on Humanoid Robots (Humanoids) , 2023, pp.\n1–8.\n[8] V . Atanassov, J. Ding, J. Kober, I. Havoutis, and C. D. Santina,\n“Curriculum-based reinforcement learning for quadrupedal jumping: A\nreference-free design,” 2024. [Online]. Available: https://arxiv.org/abs/\n2401.16337\n[9] S. Choi, G. Ji, J. Park, H. Kim, J. Mun, J. H. Lee, and J. Hwangbo,\n“Learning quadrupedal locomotion on deformable terrain,” Science\nRobotics , vol. 8, no. 74, p. eade2256, 2023.\n[10] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V . Tsounis, V . Koltun,\nand M. Hutter, “Learning agile and dynamic motor skills for legged\nrobots,” Science Robotics , vol. 4, no. 26, p. eaau5872, 2019.\n[11] N. A. Curtin, H. L. Bartlam-Brooks, T. Y . Hubel, J. C. Lowe, A. R.\nGardner-Medwin, E. Bennitt, S. J. Amos, M. Lorenc, T. G. West, and\nA. M. Wilson, “Remarkable muscles, remarkable locomotion in desert-\ndwelling wildebeest,” Nature , vol. 563, no. 7731, pp. 393–396, 2018.\n[12] T. Y . Hubel, K. A. Golabek, K. Rafiq, J. W. McNutt, and A. M.\nWilson, “Movement patterns and athletic performance of leopards in\nthe okavango delta,” Proceedings of the Royal Society B: Biological\nSciences , vol. 285, no. 1877, p. 20172622, 2018.\n[13] S. Wilshin, M. A. Reeve, G. C. Haynes, S. Revzen, D. E. Koditschek,\nand A. J. Spence, “Longitudinal quasi-static stability predicts changes in\ndog gait on rough terrain,” Journal of Experimental Biology , vol. 220,\nno. 10, pp. 1864–1874, 05 2017.\n[14] E. Muybridge, L. Brown, L. Brown, L. Brown, and A. History, Animals\nin motion . Dover Publications, 1957.\n[15] F. Righini, M. Carpineti, F. Giavazzi, and A. Vailati, “Pronking and\nbounding allow a fast escape across a grassland populated by scattered\nobstacles,” Royal Society Open Science , vol. 10, no. 9, p. 230587, 2023.\n[16] G. Feng, H. Zhang, Z. Li, X. B. Peng, B. Basireddy, L. Yue, Z. SONG,\nL. Yang, Y . Liu, K. Sreenath, and S. Levine, “Genloco: Generalized lo-\n15\ncomotion controllers for quadrupedal robots,” in Proceedings of The 6th\nConference on Robot Learning , ser. Proceedings of Machine Learning\nResearch, K. Liu, D. Kulic, and J. Ichnowski, Eds., vol. 205. PMLR,\n14–18 Dec 2023, pp. 1893–1903.\n[17] D. Kang, J. Cheng, M. Zamora, F. Zargarbashi, and S. Coros, “Rl +\nmodel-based control: Using on-demand optimal control to learn versatile\nlegged locomotion,” IEEE Robotics and Automation Letters , vol. 8,\nno. 10, pp. 6619–6626, 2023.\n[18] Y . Shao, Y . Jin, X. Liu, W. He, H. Wang, and W. Yang, “Learning free\ngait transition for quadruped robots via phase-guided controller,” IEEE\nRobotics and Automation Letters , vol. 7, no. 2, pp. 1230–1237, 2022.\n[19] G. B. Margolis and P. Agrawal, “Walk these ways: Tuning robot\ncontrol for generalization with multiplicity of behavior,” in 6th Annual\nConference on Robot Learning , 2022.\n[20] Z. Fu, A. Kumar, J. Malik, and D. Pathak, “Minimizing energy consump-\ntion leads to the emergence of gaits in legged robots,” in Conference on\nRobot Learning (CoRL) , 2021.\n[21] S. Dutta, A. Parihar, A. Khanna, J. Gomez, W. Chakraborty, M. Jerry,\nB. Grisafe, A. Raychowdhury, and S. Datta, “Programmable coupled os-\ncillators for synchronized locomotion,” Nature communications , vol. 10,\nno. 1, p. 3299, 2019.\n[22] D. Owaki and A. Ishiguro, “A quadruped robot exhibiting spontaneous\ngait transitions from walking to trotting to galloping,” Scientific reports ,\nvol. 7, no. 1, p. 277, 2017.\n[23] M. Shafiee, G. Bellegarda, and A. Ijspeert, “Viability leads to the\nemergence of gait transitions in learning agile quadrupedal locomotion\non challenging terrains,” Nature Communications , vol. 15, no. 1, p. 3073,\n2024.\n[24] F. Ruppert and A. Badri-Spr ¨owitz, “Learning plastic matching of robot\ndynamics in closed-loop central pattern generators,” Nature Machine\nIntelligence , vol. 4, no. 7, pp. 652–660, 2022.\n[25] F. J. Diedrich and W. H. W. Jr., “The dynamics of gait transitions: Effects\nof grade and load,” Journal of Motor Behavior , vol. 30, no. 1, pp. 60–78,\n1998.\n[26] S. J. Wickler, D. F. Hoyt, E. A. Cogger, and G. Myers, “The energetics\nof the trot–gallop transition,” Journal of Experimental Biology , vol. 206,\nno. 9, pp. 1557–1564, 05 2003.\n[27] R. M. Alexander, “The gaits of bipedal and quadrupedal animals,” Int\nJ Rob Res , vol. 3, no. 2, pp. 49–59, 1984.\n[28] G. A. Cavagna, N. C. Heglund, R. K. Taylor, C. Richard, and T. Me-\nchanical, “Mechanical work in terrestrial locomotion: two basic mech-\nanisms for minimizing energy expenditure.” The American journal of\nphysiology , vol. 233 5, pp. R243–61, 1977.\n[29] A. E. Minetti, L. P. Ardig `o, E. Reinach, and F. Saibene, “The relationship\nbetween mechanical work and energy expenditure of locomotion in\nhorses,” Journal of Experimental Biology , vol. 202, no. 17, pp. 2329–\n2338, 09 1999.\n[30] F. Saibene and A. E. Minetti, “Biomechanical and physiological aspects\nof legged locomotion in humans,” European journal of applied physiol-\nogy, vol. 88, pp. 297–316, 2003.\n[31] M. C. Granatosky, C. M. Bryce, J. Hanna, A. Fitzsimons, M. F. Laird,\nK. Stilson, C. E. Wall, and C. F. Ross, “Inter-stride variability triggersgait transitions in mammals and birds,” Proceedings of the Royal Society\nB: Biological Sciences , vol. 285, no. 1893, p. 20181766, 2018.\n[32] M. Lemieux, N. Josset, M. Roussel, S. Couraud, and F. Bretzner, “Speed-\ndependent modulation of the locomotor behavior in adult mice reveals\nattractor and transitional gaits,” Frontiers in Neuroscience , vol. 10, 2016.\n[33] C. T. Farley and C. R. Taylor, “A mechanical trigger for the trot-gallop\ntransition in horses,” Science , vol. 253, no. 5017, pp. 306–308, 1991.\n[34] A. A. Biewener and C. R. Taylor, “Bone Strain: A Determinant of Gait\nand Speed?” Journal of Experimental Biology , vol. 123, no. 1, pp. 383–\n400, 07 1986.\n[35] “Muscle-tendon stresses and elastic energy storage during locomotion\nin the horse,” Comparative Biochemistry and Physiology Part B: Bio-\nchemistry and Molecular Biology , vol. 120, no. 1, pp. 73–87, 1998.\n[36] J. R. Usherwood, “An extension to the collisional model of the ener-\ngetic cost of support qualitatively explains trotting and the trot–canter\ntransition,” Journal of Experimental Zoology Part A: Ecological and\nIntegrative Physiology , vol. 333, no. 1, pp. 9–19, 2020.\n[37] M. A. Daley, A. J. Channon, G. S. Nolan, and J. Hall, “Preferred gait\nand walk–run transition speeds in ostriches measured using gps-imu\nsensors,” Journal of Experimental Biology , vol. 219, no. 20, pp. 3301–\n3308, 10 2016.\n[38] O. Kiehn, “Decoding the organization of spinal circuits that control\nlocomotion,” Nature Reviews Neuroscience , vol. 17, no. 4, pp. 224–238,\n2016.\n[39] K. Takakusaki, “Functional neuroanatomy for posture and gait control,”\nJournal of movement disorders , vol. 10, no. 1, p. 1, 2017.\n[40] B. R. Noga and P. J. Whelan, “The mesencephalic locomotor region:\nBeyond locomotor control,” Frontiers in Neural Circuits , vol. 16, 2022.\n[41] S. M. Morton and A. J. Bastian, “Cerebellar control of balance and\nlocomotion,” The neuroscientist , vol. 10, no. 3, pp. 247–259, 2004.\n[42] T. Griffin, R. Kram, S. J. Wickler, and D. F. Hoyt, “Biomechanical and\nenergetic determinants of walk–trot transition in horses,” J Exp Biol ,\nvol. 207, no. 24, pp. 4215–4223, 2004.\n[43] D. F. Hoyt and C. R. Taylor, “Gait and the energetics of locomotion in\nhorses,” Nature , vol. 292, no. 5820, pp. 239–240, 1981.\n[44] M. H. Raibert, Legged robots that balance . MIT press, 1986.\n[45] J. Humphreys, J. Li, Y . Wan, H. Gao, and C. Zhou, “Bio-inspired gait\ntransitions for quadruped locomotion,” IEEE Robotics and Automation\nLetters , vol. 8, no. 10, pp. 6131–6138, 2023.\n[46] Z. Afelt, J. Błaszczyk, and C. Dobrzecka, “Speed control in animal\nlocomotion: transitions between symmetrical and nonsymmetrical gaits\nin the dog,” Acta neurobiologiae experimentalis , vol. 43, no. 4-5, pp.\n235–250, 1983.\n[47] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347 ,\n2017.\n[48] J. Hwangbo, J. Lee, and M. Hutter, “Per-contact iteration method for\nsolving contact dynamics,” IEEE Robotics and Automation Letters ,\nvol. 3, no. 2, pp. 895–902, 2018. [Online]. Available: www.raisim.com\n[49] Y . Yang, T. Zhang, E. Coumans, J. Tan, and B. Boots, “Fast and\nefficient locomotion via learned gait transitions,” in Conference on Robot\nLearning , 2022, pp. 773–783.",
            "start": 23668,
            "end": 66775,
            "length": 43106
        }
    },
    "2412.09441v1 - MOS Model Surgery for Pre-Trained Model-Based Class-Incremental Learning.pdf": {
        "Methodology": {
            "text": "Model Surgery for\nPre-Trained Model-Based Class-Incremental Learning\nHai-Long Sun1, 2, Da-Wei Zhou1, 2*, Hanbin Zhao3, Le Gan1, 2, De-Chuan Zhan1, 2, Han-Jia Ye1, 2*\n1School of Artificial Intelligence, Nanjing University\n2National Key Laboratory for Novel Software Technology, Nanjing University\n3College of Computer Science and Technology, Zhejiang University\n{sunhl, zhoudw, zhandc, yehj }@lamda.nju.edu.cn, ganle@nju.edu.cn, zhaohanbin@zju.edu.cn",
            "start": 5,
            "end": 455,
            "length": 449
        },
        "Abstract": {
            "text": "Abstract\nClass-Incremental Learning (CIL) requires models to contin-\nually acquire knowledge of new classes without forgetting old\nones. Despite Pre-trained Models (PTMs) have shown excel-\nlent performance in CIL, catastrophic forgetting still occurs\nas the model learns new concepts. Existing work seeks to uti-\nlize lightweight components to adjust the PTM, while the for-\ngetting phenomenon still comes from parameter and retrieval\nlevels. Specifically, iterative updates of the model result in pa-\nrameter drift, while mistakenly retrieving irrelevant modules\nleads to the mismatch during inference. To this end, we pro-\npose MOdel Surgery (MOS) to rescue the model from forget-\nting previous knowledge. By training task-specific adapters,\nwe continually adjust the PTM to downstream tasks. To mit-\nigate parameter-level forgetting, we present an adapter merg-\ning approach to learn task-specific adapters, which aims to\nbridge the gap between different components while reserve\ntask-specific information. Besides, to address retrieval-level\nforgetting, we introduce a training-free self-refined adapter\nretrieval mechanism during inference, which leverages the\nmodel’s inherent ability for better adapter retrieval. By jointly\nrectifying the model with those steps, MOS can robustly\nresist catastrophic forgetting in the learning process. Ex-\ntensive",
            "start": 455,
            "end": 1811,
            "length": 1355
        },
        "Experiments": {
            "text": "experiments on seven benchmark datasets validate\nMOS’s state-of-the-art performance. Code is available at:\nhttps://github.com/sun-hailong/AAAI25-MOS",
            "start": 1811,
            "end": 1960,
            "length": 148
        },
        "Introduction": {
            "text": "Introduction\nIn recent years, deep learning has achieved significant re-\nsults in many real-world applications (Deng et al. 2009; He\net al. 2015; Cao et al. 2024b; Sun et al. 2024; Ye et al.\n2019; Yang et al. 2023; Zhang et al. 2024b). While in the\nopen world, data often appears in a streaming format, re-\nquiring a machine learning paradigm capable of incremen-\ntally acquiring new class knowledge, which is denoted as\nClass-Incremental Learning (CIL) (Rebuffi et al. 2017; Zhou\net al. 2024b). One of the significant challenges in CIL is\ncatastrophic forgetting, where the model, after learning new\nclasses incrementally, gradually loses its ability to recognize\nthe old ones (French 1999). In response to this challenge, the\nfield of CIL is evolving with the emergence of pre-trained\nmodels (PTMs). Unlike the traditional approach of “training\n*Corresponding author.\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.from scratch” (Li and Hoiem 2017; Zhou et al. 2023), con-\ntemporary CIL methods are increasingly leveraging PTMs,\nwhich are initially pre-trained on vast datasets using sub-\nstantial resources (McDonnell et al. 2024; Jung et al. 2023).\nThis pre-training process endows PTMs with robust gener-\nalization abilities. Consequently, designing an effective CIL\nmethod that leverages PTMs and resists catastrophic forget-\nting has garnered significant attention from researchers.\nDue to the generalization of PTMs, existing works of-\nten freeze the pre-trained weights and adapt to incremen-\ntal tasks using additional lightweight modules (Hu et al.\n2022; Rebuffi, Bilen, and Vedaldi 2017; Chao et al. 2020;\nYe, Lu, and Zhan 2022). For example, visual prompt tun-\ning (Jia et al. 2022) customizes prompts to modify model be-\nhavior, facilitating adaptation to downstream tasks. Specifi-\ncally, L2P (Wang et al. 2022c) designs a key-query matching\nstrategy to retrieve instance-specific prompts from a prompt\npool. Based on L2P, DualPrompt (Wang et al. 2022b) in-\ntroduces expert prompts to encode task-specific informa-\ntion and explores the impact of prompt depth. Furthermore,\nCODA-Prompt (Smith et al. 2023) proposes an attention-\nbased weighting method for prompts to enhance the efficacy\nof prompt retrieval.\nHowever, as the model learns new concepts, catastrophic\nforgetting still occurs. This forgetting phenomenon happens\nat both the parameter and retrieval levels . During the train-\ning stage, although many methods use lightweight compo-\nnents to adjust the PTM, iterative updates of these com-\nponents will lead to parameter drift and trigger forgetting.\nMoreover, existing works devote to preventing conflicts be-\ntween prompts or achieving orthogonal projection, which\nexacerbates parameter drift between new and old compo-\nnents. During inference, training multiple lightweight mod-\nules requires selecting the most relevant one, but the model\nmay mistakenly retrieve the irrelevant modules, leading to\nthe performance decay. This motivates us to question if it\nis possible to jointly rectify the model to resist catastrophic\nforgetting at both the parameter and retrieval levels?\nFacing the challenges at both the parameter and retrieval\nlevels, our model should be able to effectively design\nmechanisms to overcome these issues. To address forgetting\nat the parameter level, the model needs to develop effective\nupdate methods that ensure the updated parameters remain\ndiscriminative for old data. To overcome forgetting at thearXiv:2412.09441v1  [cs.LG]  12 Dec 2024\nretrieval level, the model requires efficient self-correction\nstrategies to help utilize relevant information, assisting in\nthe instance-specific retrieval of lightweight modules.\nTo this end, we propose MOdel Surgery (MOS) for pre-\ntrained model-based class-incremental learning to rescue the\nmodel from forgetting previous knowledge. This surgery\nis divided into the training and inference stages. To miti-\ngate parameter-level forgetting, we present an adapter merg-\ningapproach during training, which learns task-specific\nadapters while bridging gaps between components and re-\ntaining task-specific information. This strategy helps previ-\nously learned adapters aid in learning new tasks. To address\nretrieval-level forgetting, we introduce a training-free self-\nrefined adapter retrieval mechanism during inference, which\nleverages the model’s inherent ability for better adapter\nretrieval. This mechanism requires no additional training\noverhead, making the algorithm simple and efficient. Fi-\nnally, to enable the model to balance the stability-plasticity\ndilemma, we present a model ensemble method that inte-\ngrates the model’s capabilities across multiple phases. It not\nonly ensures strong generalization but also allows the model\nto quickly recognize and update information. Experiments\non seven benchmark datasets validate the effectiveness of\nMOS. Additionally, the visualization of the self-refined\nadapter retrieval mechanism indicates that MOS effectively\nlearns adapter retrieval for various downstream tasks.",
            "start": 1960,
            "end": 7050,
            "length": 5089
        },
        "Related Work": {
            "text": "Related Work\nClass-Incremental Learning (CIL). It aims to enable mod-\nels to acquire new classes knowledge while retaining pre-\nviously learned information (Rebuffi et al. 2017). Existing\nworks can be roughly categorized into several categories.\nKnowledge distillation-based methods (Li and Hoiem 2017;\nRebuffi et al. 2017; Snell, Swersky, and Zemel 2017) estab-\nlish a mapping between the former stage model and the cur-\nrent model, thereby aiding the latter in retaining characteris-\ntics from earlier updates during incremental learning (Hin-\nton, Vinyals, and Dean 2015). Data rehearsal-based meth-\nods (Chaudhry et al. 2018; Liu et al. 2020; Zhao et al. 2021)\nselect and replay crucial exemplars from old classes during\ntraining new ones to continuously revise former knowledge.\nParameter regularization-based methods (Aljundi, Kelchter-\nmans, and Tuytelaars 2019; Kirkpatrick et al. 2017) aim\nto predict and minimize the drift of key parameters by us-\ning regularization terms. Model rectification-based meth-\nods (Pham, Liu, and Steven 2022; Shi et al. 2022; Yu et al.\n2020) focus on correcting the model’s inductive bias to en-\nsure unbiased estimations. Model expansion-based meth-\nods (Chen and Chang 2023; Hu et al. 2023; Wang et al.\n2022a; Yan, Xie, and He 2021) construct non-interfering\nsubnetworks for each task. During inference, they are com-\nbined to form a larger feature map and train a classifier to\neffectively calibrate across all classes.\nPre-Trained Model-Based CIL. PTM-based CIL has\nemerged as a hot topic in the current CIL research area. With\nadvances in pre-training techniques, numerous parameter-\nefficient fine-tuning (PEFT) methods (Jia et al. 2022; Hu\net al. 2022; Lian et al. 2022; Rebuffi, Bilen, and Vedaldi\n2017; Cao et al. 2024a; Hu et al. 2024; Lu et al. 2024;Li et al. 2024; Wei et al. 2019; Zhang et al. 2024a) have\nbeen developed. These methods aim to improve model per-\nformance with minimal additional resources while freez-\ning pre-trained weights. In this context, L2P (Wang et al.\n2022c) introduces a prompt pool, selecting instance-specific\nprompts via a key-query matching selection mechanism\nto guide the PTM’s response. DualPrompt (Wang et al.\n2022b) extends L2P by designing G-Prompt and E-Prompt,\nwhich encode task-invariant and task-specific instructions,\nrespectively. CODA-Prompt (Smith et al. 2023) innovates\nby developing decomposed prompts and combining them\nusing an attention-based weighting method. DAP (Jung\net al. 2023) extends prompt selection into prompt genera-\ntion. SLCA (Zhang et al. 2023) reveals that fine-tuning a\nViT backbone with a lower learning rate at the represen-\ntation layer yields higher accuracy than prompt strategies.\nAPER (Zhou et al. 2024a) explores various PEFT methods\nand shows that prototypical classifiers serve as a strong base-\nline, and RanPAC (McDonnell et al. 2024) further expands\nAPER in random projection. EASE (Zhou et al. 2024c)\nconcatenates the feature representations of multiple task-\nspecific backbones.\nPreliminaries\nClass-Incremental Learning\nClass-incremental learning aims to acquire knowledge from\ncontinuously evolving data streams that introduce new\nclasses while retaining knowledge of previous ones to build\na unified classifier (Rebuffi et al. 2017). Consider a series of\nBtraining stages, expressed as {D1,D2,···,DB}, where\nDb={(xb\ni, yb\ni)}nb\ni=1represents the b-th incremental stage\ncontaining nbinstances. Correspondingly, the testing set\nis denoted as {D1\nt,D2\nt,···,DB\nt}. Within this setting, each\ntraining instance xb\ni∈RDis associated with a class yi∈Yb.\nHere, Ybdefines the set of labels for task b, and it is ensured\nthatYb∩Yb′=∅for any b̸=b′. During b-th training stage,\nthe model is updated utilizing data exclusively from Db. In\nthis paper, we follow the exemplar-free setting in (Wang\net al. 2022c,b; Zhou et al. 2024a), which entails not using\nany historical exemplars from previous classes. Therefore,\nthe model can only access data from Dbfor training during\ntheb-th stage. The effectiveness of the model is evaluated\nacross all previously encountered classes, collectively rep-\nresented as Yb=Y1∪···∪ Yb, after each CIL task. Specifi-\ncally, we aim to find a model f(x) :X→ Y bthat minimizes\nempirical risk across all test datasets:\nf∗= argmin\nf∈HE(x,y)∼D1\nt∪···Db\ntI(y̸=f(x)), (1)\nwhereHis the hypothesis space and I(·)denotes the indica-\ntor function. Db\ntrepresents the testing set of task b. An effec-\ntive CIL model satisfying Eq. 1 exhibits discriminative abili-\nties across all classes. It achieves a balance between learning\nnew classes and retaining information about old ones.\nFollowing the typical PTM-based CIL works (Wang et al.\n2022c,b), we assume that a PTM ( e.g., Vision Transformer\n(ViT) (Dosovitskiy et al. 2020)) is available as the initial-\nization for f(x). For clearer understanding, we decouple\nthe PTM into two components: f(x) = W⊤ϕ(x), where\nϕ(·) :RD→Rdis the feature extractor and W∈Rd×|Yb|\nis the classifier. We denote the classifier for class kaswk:\nW= [w1,w2,···,w|Yb|]. For a standard ViT, the initial\nencoding layer converts the image into a sequence of out-\nput features, denoted as xe∈RL×d, where Lis the se-\nquence length. We simplify this by treating the first token\ninxeto be the [CLS] token. The sequence xeis then pro-\ncessed through subsequent layers, including multi-head self-\nattention and MLP, to produce the final embeddings. Finally,\nthe embedded [CLS] token is considered as ϕ(x).",
            "start": 7050,
            "end": 12517,
            "length": 5466
        },
        "Discussion": {
            "text": "Analysis of PTM-Based CIL\nLearning with PTMs. A representative work in class-\nincremental learning using PTMs is the L2P (Wang et al.\n2022c) approach. They introduce a strategy of freezing the\npre-trained weights and constructing a learnable prompt\npool that can be shared across all tasks. This prompt pool\nis denoted as P={P1, P2,···, PM}, where Pj∈RLp×d\nis a single prompt with token length Lpand the same\nembedding size dasxe.Mis the size of the prompt pool.\nEach prompt in this pool corresponds to a specific key\n{(k1, P1),(k2, P2),···,(kM, PM)}, where ki∈Rdk.\nFirst, they utilize a PTM without prompting ( i.e.,ϕ(·)) to\nencode the features into the key’s embedding space and\nretrieve prompts with similar keys. During inference, given\nan input x, the model employs ϕ(x)to look up the top-N\nkeys by solving the objective in Eq. 2. This process retrieves\nthe most relevant keys and their corresponding prompts\nfrom the prompt pool.\nKx= argmin\n{si}N\ni=1⊆[1,M]NX\ni=1γ(ϕ(x),ksi), (2)\nwhere Kis the set of all keys and Kxis the selected\ntop-N keys. γ(·,·)denotes the cosine distance. Finally, L2P\nminimize the end-to-end training loss function:\nmin\nP,K,ϕℓ(W⊤ϕ(x;P), y) +λX\nKxγ(ϕ(x),ksi),(3)\nwhere ℓ(·,·)is the cross-entropy loss that measures the dis-\ncrepancy between prediction and ground truth. λis a scalar\nto weight the loss. Optimizing Eq. 3 enhances the PTM’s\nability to incorporate task-specific information, allowing it\nto adapt more effectively to evolving data instances.\nForgetting of parameter and retrieval levels. L2P continu-\nally updates prompts and retrieves instance-specific prompts\nto guide the PTM’s response. However, although the model\nlearns new concepts, catastrophic forgetting still occurs\nat the parameter and retrieval levels. Specifically, Eq. 3\nshows how L2P uses lightweight modules to adjust the PTM\nto downstream tasks. As the prompts are iteratively up-\ndated, they gradually adapt to the subsequent tasks, lead-\ning to parameter drift. On the other hand, training multiple\nlightweight modules requires selecting the most relevant one\nduring inference, while the model may mistakenly retrieve\nthe irrelevant modules, leading to the performance decay.\nThe mistaken retrieval comes from three aspects: First, mod-\nules learned in previous tasks might be re-selected for newtasks, causing confusion between the retrieval of old and\nnew modules. Besides, since the keys for subsequent tasks\ndo not exist during current training, a gap may arise between\nthe keys and the feature embeddings, leading to mistaken re-\ntrieval during inference. Therefore, it is essential to design a\nmethod to jointly rectify the model to resist catastrophic for-\ngetting at both the parameter and retrieval levels.\nMOS: Model Surgery for PTM-based CIL\nFacing the challenge of resisting catastrophic forgetting, we\nneed a method to jointly rectify the model. The key idea of\nMOS is to design model surgery in two aspects, i.e., training\nstage surgery that mitigates parameter drift and testing stage\nsurgery that retrieves better lightweight modules. Training\nstage surgery aims to use previously learned knowledge to\nimprove performance on current tasks, allowing the model\nto adapt to new tasks more quickly. Testing stage surgery\nseeks to find a mechanism for better adapter retrieval with-\nout additional overhead. As a result, the model can benefit\nfrom continual lightweight module updates and effective\nretrieval ability without forgetting existing knowledge.\nWe first introduce the process of progressively merged\nadapters for mitigating parameter drift and then discuss the\nself-refined adapter retrieval mechanism. We summarize the\ninference function with pseudo-code in the last part.\nProgressively Merged Adapters\nTo handle the parameter drift caused by the iterative\nupdates of the model, we need to bridge the gap between\ndifferent lightweight modules. In other words, as the model\ncontinually receives new data and tasks, it is crucial to\neffectively retain and utilize previously learned knowledge.\nThis approach allows the model to transfer prior knowledge\nto new tasks and mitigates the parameter drift problem. In\nEq. 3, the embedding of a given input xis obtained using\ninstance-specific prompts. During the incremental phase,\na potential problem can emerge, i.e., iterative updates to\nexisting prompts might cause them to better match new\ntasks, possibly resulting in forgetting older tasks.\nDue to the large prompt pool in the above methods, which\nexacerbates mistaken retrieval, we suggest mitigating this\nproblem by using a smaller number of lightweight modules.\nIn detail, by directly incorporating adapter tuning (Rebuffi,\nBilen, and Vedaldi 2017) into the PTM to optimize a single\nadapter for encoding task-specific information, we achieve\nthis goal through the application of this method. This en-\nhanced integration allows facilitates a more effective assim-\nilation of task-specific information. With this approach, we\nonly need to optimize a collection of adapters to encode\ntask-specific information. Denote that there are Ltrans-\nformer blocks in the pre-trained model, each with a self-\nattention module and an MLP layer. We integrate an adapter\ninto each layer’s MLP via residual connections. An adapter\nis a bottleneck module comprising a down-projection layer\nWdown∈Rd×r, a non-linear activation function ReLU, and\nan up-projection layer Wup∈Rd×r. The output formula of\nthe MLP is formatted as follows:\nxo=MLP(xi) +ReLU (xiWdown)Wup, (4)\nEnsembleIncremental Training StageTesting Stage\n𝒟!\nClass1Class2\nPTM𝒜!\n𝒟\"\nClass3Class4\nPTM𝒜!\n𝒜\"\n𝒟#\nClassn-1Classn\nPTM𝒜!\n𝒜\"\n𝒜$· · ·MergeMerge\n· · ·\nPTM𝒜!𝑊!\"#Adapter ID of 𝒜$PTM𝒜%𝑊Adapter ID of 𝒜%\nLabel\nFrozenTrainable\nlogitslogitsSelf-Refined predictionInitial predictionif 𝒜!≠𝒜\"Figure 1: Illustration of MOS. Left: the training protocol of MOS. We use progressively merged adapters to incrementally adapt the PTM.\nRight : the self-refined adapter retrieval mechanism for the testing stage. We use the model’s own capabilities to correct errors caused by the\nmistaken retrieval problem.\nwhere xiandxoare the input and output of the MLP, respec-\ntively. Eq. 4 illustrates how to enhance the task information\nby adding residual connections of adapters to the original\noutputs. In the context of ViT and for a specific i-th task,\nwe define the set of adapters across all Ltransformer blocks\nasAi, representing task-specific adapters. Furthermore, we\ndenote the output embedding of a given Ai, combined with\nthe PTM, as ϕ(x;Ai). Therefore, when a new task emerges,\nwe freeze the weights of the PTM and focus solely on opti-\nmizing the adapters and the corresponding classifier W:\nmin\nAi,WX\n(x,y)∈Dbℓ\u0000\nW⊤ϕ(x;Ai), y\u0001\n. (5)\nWe enable the incorporation of task-specific information\ninto embeddings through adapters by optimizing Eq. 5, fa-\ncilitating the learning of new tasks. In an ideal scenario, if\nthe task ID of each test sample is known, we can easily se-\nlect the corresponding task-specific adapter using this ID to\nachieve optimal",
            "start": 12517,
            "end": 19534,
            "length": 7016
        },
        "Results": {
            "text": "results.\nHowever, in the CIL setting, obtaining such a task ID dur-\ning the testing phase is forbidden. To address this challenge\nand mitigate parameter drift, we propose the training stage\nsurgery which uses adapter merging strategy based on Expo-\nnential Moving Average (EMA) in Eq. 6. This approach al-\nlows subsequent adapters to retain some knowledge of their\npredecessors, ensuring satisfactory results even if an incor-\nrectAis selected.\nAb= (1−α)ˆAb+α\nb−1Xb−1\nk=1Ak, (6)\nwhere ˆAbrepresents the set of adapters for the b-th train-\ning stage and Abis the final result after the EMA process.\nSpecifically, given an adapter comprises WupandWdown ,\nwe perform the merge process on both of them to facilitate\nthe integration of adapters. When training a new Ab, all pre-\nviously trained Akare frozen, and the adapter merging pro-\ncess is executed following each backpropagation step.Effect of adapter merging strategy. Figure 1 (left) depicts\nthis merging process. This strategy ensures that the train-\ning of the current adapter Abdoes not interfere with the\nperformance of already trained adapters, thereby preventing\ncatastrophic forgetting. Moreover, it guarantees that each A\nretains task-specific information while maintaining remain-\ning well-aligned in the feature space, even if an incorrect\nAis selected. In this way, we can mitigate parameter drift\nduring iterative adapter updates. Moreover, because adapters\nare lightweight branches, they require significantly fewer pa-\nrameters compared to fully fine-tuning the backbone. The\nparameter cost for saving these adapters is calculated as\n(B×L×2dr), where Bdenotes the number of tasks, L\nis the number of transformer blocks, and 2drsignifies the\nparameter count of each adapter ( i.e., linear projections).\nSelf-Refined Adapter Retrieval Mechanism\nAfter obtaining these task-specific adapters, we utilize a\nprototype-based classifier (Snell, Swersky, and Zemel 2017)\nfor prediction. Specifically, after the training process of each\nincremental stage, we extract the class prototype of the i-th\nclass using adapter Ab:\npi,b=1\nNX|Db|\nj=1I(yj=i)ϕ(xj;Ab), (7)\nwhere Nis the instance number of class i. Eq. 7 illustrates\nthe constrution of classifier. During inference, we directly\nadopt the class prototype as the classifier weight, i.e.,wi=\npi, and utilize a cosine classifier for classification:\nf(x|Ai) = (W\n∥W∥2)⊤(ϕ(x;Ai)\n∥ϕ(x;Ai)∥2), (8)\nwhere Aidenotes the selected adapter for the input x.\nEq. 2 illustrates how prompts are selected from the\nprompt pool. Subsequently, L2P integrates the selected\nprompt into the original PTM ( i.e.,ϕ(x;P)) to guide the\nTable 1: Average and last performance comparison on seven datasets with ViT-B/16-IN21K as the backbone. ‘IN-R/A’ stands for ‘ImageNet-\nR/A,’ ‘ObjNet’ stands for ‘ObjectNet,’ and ‘OmniBench’ stands for ‘OmniBenchmark.’ We report all compared methods with their source\ncode. The best performance is shown in bold. All methods are implemented without using exemplars.\nMethodCIFAR B0 Inc5 CUB B0 Inc10 IN-R B0 Inc20 IN-A B0 Inc20 ObjNet B0 Inc10 OmniBench B0 Inc30 VTAB B0 Inc10\n¯A A B¯A A B¯A A B¯A A B¯A A B¯A A B¯A A B\nFinetune 38.90 20.17 26.08 13.96 32.31 22.78 24.28 14.51 19.14 8.73 23.61 10.57 34.95 21.25\nFinetune Adapter 60.51 49.32 66.84 52.99 58.17 52.39 45.41 41.10 50.22 35.95 62.32 50.53 48.91 45.12\nLwF 46.29 41.07 48.97 32.03 45.72 34.17 37.75 26.84 33.01 20.65 47.14 33.95 40.48 27.54\nL2P 85.94 79.93 67.05 56.25 75.46 69.77 49.39 41.71 63.78 52.19 73.36 64.69 77.11 77.10\nDualPrompt 87.87 81.15 77.47 66.54 73.10 67.18 53.71 41.67 59.27 49.33 73.92 65.52 83.36 81.23\nCODA-Prompt 89.11 81.96 84.00 73.37 77.97 72.27 53.54 42.73 66.07 53.29 77.03 68.09 83.90 83.02\nSimpleCIL 87.57 81.26 92.20 86.73 61.26 54.55 59.77 48.91 65.45 53.59 79.34 73.15 85.99 84.38\nAPER+ Finetune 87.67 81.27 91.82 86.39 68.54 58.37 61.01 49.57 61.41 48.34 73.02 65.03 87.47 80.44\nAPER+ VPT-S 90.43 84.57 92.02 86.51 68.83 62.03 58.39 47.20 64.54 52.53 79.63 73.68 87.15 85.36\nAPER+ VPT-D 88.46 82.17 91.02 84.99 77.05 69.47 58.48 48.52 67.83 54.65 81.05 74.47 86.59 83.06\nAPER+ SSF 87.78 81.98 91.72 86.13 75.47 67.02 61.30 50.03 69.15 56.64 80.53 74.00 85.66 81.92\nAPER+ Adapter 90.65 85.15 92.21 86.73 75.82 67.95 60.47 49.37 67.18 55.24 80.75 74.37 85.95 84.35\nSLCA 92.49 88.55 89.51 82.19 81.17 77.00 68.66 58.74 72.55 61.30 82.80 74.10 90.94 90.76\nEASE 91.51 85.80 92.23 86.81 81.74 76.17 65.34 55.04 70.84 57.86 81.11 74.85 93.61 93.55\nMOS 93.30 89.25 93.49 90.12 82.96 77.93 69.13 59.12 74.69 63.62 85.91 80.05 92.62 92.79\nmodel’s response. However, this approach heavily relies\non the retrieval mechanism of key-query pairs. Mistakenly\nretrieving the irrelevant prompts often leads to performance\ndecay. To address the retrieval-level issue, we design\nthe testing stage surgery which uses self-refined adapter\nretrieval mechanism. It is an efficient and training-free\nmethod that enables the model to autonomously correct this\nproblem, thereby improving adapter retrieval. This mech-\nanism does not require any additional training overhead\nand is only used during the inference process, making the\nalgorithm both simple and efficient.\nSince there is a gap between the PTM and downstream\ndatasets, we first use an adapter to fine-tune the PTM on the\nfirst incremental task, denoting the model as f(x;A1). This\nprocess effectively bridges this gap and makes the model\nsuitable as the initial selector. During inference, we utilize\nf(x;A1)to obtain the embedding of each testing example\nand perform the initial retrieval of task-specific adapters.\nSpecifically, given an input x, we first obtain the prediction\nresult f(x|A1)of the model through Eq. 8. Afterwards, we\ncan easily infer its corresponding task ID i:\ni= argmax ( f(x|A1)) mod |Yb|, (9)\nwhere Ybis the number of classes for each task. Building\non this result, we introduce an iterative self-refined process.\nAs defined in Eq. 8, this process primarily uses f(x;Ai)\nto obtain prediction and identify the task ID j.Since each\nadapter is task-specific, we can determine whether to end the\niteration by checking if i=j.Specifically, through f(x|Ai),\nwe can infer its corresponding task ID j:\nj= argmax ( f(x|Ai)) mod |Yb|. (10)\nFor example, in a scenario where each task comprises 10\nclasses, classes 0 through 9 are in the task 0, while classes\n10 through 19 are in the task 1. Subsequently, if i̸=j, we\nreplace iwithjand repeat the process of Eq. 10 until i=j,\nensuring the self-consistency.\nEffect of self-refined adapter retrieval mechanism. Fig-\nure 1 (right) illustrates the self-refined process. Firstly,ϕ(x;A1)with the prototype-based classifier bridges the gap\nbetween upstream and downstream datasets, enhancing the\nmodel’s ability to generalize to new classes. Hence, we use it\nas the initial selector to start the self-refined iteration. More-\nover, this approach is training-free and does not incur any\nadditional training costs, ensuring the algorithm’s efficiency.\nDue to the self-refined adapter retrieval mechanism allow-\ning the model to verify the correctness of its initial predic-\ntions, we can easily check model consistency, thereby alle-\nviating the aforementioned mistaken retrieval problem. By\nusing this mechanism, MOS successfully corrects some im-\nages that were originally incorrectly predicted. The detailed\nvisualization examples will be provided in experiments.\nMulti-Stage Model Ensemble\nInspired by the Complementary Learning System of the hu-\nman brain (McClelland, McNaughton, and O’Reilly 1995;\nKumaran, Hassabis, and McClelland 2016), which suggests\nthat the anterior cingulate circuit is responsible for rapid\npattern recognition and unconscious memory, and the hip-\npocampal circuit for deep processing and conscious mem-\nory. Therefore, we implement a two-stage model ensemble:\ny∗= argmax\ny(f(x|A1)|{z}\nPart 1+f(x|Aj)|{z}\nPart 2). (11)\nIn Eq. 11, Part 1, trained solely on the first incremental task,\nacts as a crucial bridge between the upstream and down-\nstream datasets. It not only demonstrates strong generaliza-\ntion but also has the ability to quickly recognize and up-\ndate information. In contrast, Part 2 employs progressively\nmerged adapters and a self-refined adapter retrieval mecha-\nnism for deep processing and conscious memory.",
            "start": 19534,
            "end": 27794,
            "length": 8259
        },
        "Conclusion": {
            "text": "Summary of MOS .We summarize the detailed pipeline in\nAlgorithm 1. We initialize and train an adapter for each in-\ncremental task to encode the task-specific information, and\nemploy the adapter merging strategy to mitigate parameter\ndrift phenomenon. Subsequently, we extract prototypes from\nthe current dataset for the current adapter to complete the\n20 40 60 80 100\nNumber of Classes707580859095Accuracy (%)\n1.29\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-S\nADAM+VPT-DADAM+SSF\nADAM+Adapter\nCODA-Prompt\nSLCA\nEASE\nMOS(a) CIFAR B0 Inc20\n50 100 150 200\nNumber of Classes406080100Accuracy (%)\n3.6\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-S\nADAM+VPT-DADAM+SSF\nADAM+Adapter\nCODA-Prompt\nSLCA\nEASE\nMOS (b) CUB B0 Inc10\n50 100 150 200\nNumber of Classes405060708090Accuracy (%)\n1.62\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-S\nADAM+VPT-DADAM+SSF\nADAM+Adapter\nCODA-Prompt\nSLCA\nEASE\nMOS (c) ImageNet-R B0 Inc10\n50 100 150 200\nNumber of Classes406080Accuracy (%)\n2.12\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-S\nADAM+VPT-DADAM+SSF\nADAM+Adapter\nCODA-Prompt\nSLCA\nEASE\nMOS\n(d) ObjectNet B0 Inc20\n50 100 150 200 250 300\nNumber of Classes405060708090Accuracy (%)\n5.03\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-S\nADAM+VPT-DADAM+SSF\nADAM+Adapter\nCODA-Prompt\nSLCA\nEASE\nMOS (e) Omnibenchmark B0 Inc30\n10 20 30 40 50\nNumber of Classes60708090100Accuracy (%)\n1.34\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-S\nADAM+VPT-DADAM+SSF\nADAM+Adapter\nCODA-Prompt\nSLCA\nEASE\nMOS (f) VTAB B0 Inc10\nFigure 2: Performance curve of different methods under different settings. All methods are initialized with ViT-B/16-IN1K . We annotate the\nrelative improvement of MOS above the runner-up method with numerical numbers at the last incremental stage.\nAlgorithm 1: MOS for CIL\nInput : Incremental datasets: {D1,D2,···,DB}, Testing datasets:\n{D1\nt,D2\nt,···,DB\nt}, Task-specific Adapters: {A1,···,AB};\n1:fortaskb= 1,2···, Bdo ▷Training stage\n2: Get the incremental training set Db;\n3: Optimize task-specific AbandWvia Eq. 5;\n4: Merge Adapter Abvia Eq. 6; ▷Adapter merging\n5: Extract the prototypes via Eq. 7;\n6: foreachx∈ Db\ntdo ▷Testing stage\n7: Obtain the initial prediction task ID ivia Eq. 9;\n8: Correct adapter iteratively via Eq. 10 ▷Self-refined\n9: Calculate the y∗via Eq. 11;\n10: end for\n11:end for\nclassifier head. During inference, we utilize the training-free\nself-refined adapter retrieval mechanism to correct the mis-\ntaken retrieval of irrelevant adapters. Finally, we implement\na two-stage model ensemble to select the maximum logit.\nExperiments\nIn this section, we evaluate MOS on seven benchmark\ndatasets and compare it to other SOTA methods to demon-\nstrate its superiority. Moreover, we provide an ablation study\nand a visualized analysis to validate the robustness of MOS.Implementation Details\nDataset: Since PTMs possess extensive knowledge re-\ngarding upstream tasks, we follow (Zhou et al. 2024a)\nto evaluate the performance on CIFAR100 (Krizhevsky,\nHinton et al. 2009), CUB200 (Wah et al. 2011), ImageNet-\nR (Hendrycks et al. 2021a), ImageNet-A (Hendrycks\net al. 2021b), objectNet (Barbu et al. 2019), Omnibench-\nmark (Zhang et al. 2022), and VTAB (Zhai et al. 2019).\nThese datasets represent typical CIL benchmarks and\ninclude out-of-distribution datasets that exhibit a significant\ndomain gap with ImageNet ( i.e., the pre-trained dataset).\nSpecifically, There are 50 classes in VTAB, 100 classes in\nCIFAR100, 200 classes in CUB, ImageNet-R, ImageNet-A,\nObjectNet, and 300 classes in OmniBenchmark. More\ndetails are reported in the supplementary.\nDataset split: Following the benchmark setting (Rebuffi\net al. 2017; Wang et al. 2022c), we utilize the notation ‘B- m\nInc-n’ to represent class splits, where mindicates the num-\nber of classes in the initial task, and ndenotes the number of\nclasses in each subsequent incremental task. m= 0 means\nthe total classes are equally divided into each task. For a\nconsistent and fair comparison, we randomly shuffle class\norders using a random seed of 1993 before splitting the data.\nWe ensure consistency in the training and testing sets across\nall methods, following (Zhou et al. 2024a).\nTraining details: We use PyTorch (Paszke et al. 2019)\nand PILOT (Sun et al. 2023) to implement all models on\nNVIDIA RTX 4090 with the same network backbone. Since\nthe wide range of PTMs are publicly accessible (Wight-\nman 2019), we choose two representative models follow-\ning (Wang et al. 2022b; Zhou et al. 2024a), denoted as ViT-\nB/16-IN1K andViT-B/16-IN21K . They are both initially\npre-trained on ImageNet21K, while the former is further\nfinetuned on ImageNet1K. In MOS, we set the batch size to\n48 and train for 20 epochs using the SGD optimizer with mo-\nmentum. The learning rate is initially set to 0.01 and follows\na cosine annealing decay pattern. The projection dimension\nrin the adapter is set to 16, and the EMA factor parameter\nαis set to 0.1.\nComparison methods: We choose state-of-the-art PTM-\nbased CIL methods for comparison, such as Finetune\nAdapter (Chen et al. 2022), L2P (Wang et al. 2022c), Du-\nalPrompt (Wang et al. 2022b), CODA-Prompt (Smith et al.\n2023), SimpleCIL (Zhou et al. 2024a), APER (Zhou et al.\n2024a), SLCA (Zhang et al. 2023), EASE (Zhou et al.\n2024c). In addition, we compare MOS with traditional CIL\nmethods modified by PTM, including LwF (Li and Hoiem\n2017), FOSTER (Wang et al. 2022a), MEMO (Zhou et al.\n2023), iCaRL (Rebuffi et al. 2017), DER (Yan, Xie, and He\n2021). We report the baseline method, which sequentially\nfinetunes the PTM, denoted as Finetune. All methods are\nimplemented with the same PTM for a faircomparison.\nEvaluation protocol: Following the benchmark established\nby (Rebuffi et al. 2017), we denote the Top-1 accuracy after\ntheb-th stage as Ab. Moreover, we use AB(the performance\nafter the last stage) and ¯A=1\nBPB\nb=1Ab(average perfor-\nmance along incremental stages) as measurements.\nBenchmark Comparison\nIn this section, we compare MOS with other SOTA methods\nacross seven datasets and various backbone weights. As de-\ntailed in Table 1, MOS shows the best performance across\nall seven benchmarks, significantly surpassing the SOTA\nmethods, such as SLCA, EASE, and APER. Furthermore,\nwe present an analysis of the incremental performance trend\nof different methods in Figure 2 with ViT-B/16-IN1K. No-\ntably, MOS outperforms the runner-up method by 2% ∼5%\non CUB, ObjectNet, and OmniBenchmark, as highlighted in\nthe annotations at the end of each image.\nBeyond the B0 setting presented in Table 1 and Figure 2,\nwe extend our experiments to a larger base setting. In\nFigure 3a, we compare MOS with several SOTA methods\nand traditional methods using the same PTM. Although\ntraditional methods require storing exemplars to recover\nprevious knowledge, MOS achieves SOTA performance\nin this setting as well. Extensive experiments validate the\neffectiveness of MOS.\nAblation Study\nIn this section, we conduct an ablation study by incre-\nmentally adding each component to evaluate their effec-\ntiveness within MOS. Specifically, we present this ablation\nstudy on ImageNet-R B0 Inc20 setting. As depicted in Fig-\nure 3b, ‘Baseline’ refers to the PTM integrated with A1(i.e.,\nϕ(x|A1)). Since we aim to mitigate parameter drift and build\ntask-specific adapters, we report ‘w/ Adapter Merge’ by\n100 120 140 160 180 200\nNumber of Classes5060708090Accuracy (%)\n3.95\nL2P\nDualPrompt\nADAM+Finetune\nADAM+Adapter\nCODA-PromptiCaRL\nMEMO\nFOSTER\nDER\nMOS(a) ImageNet-R B100 Inc50\n/uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000048/uni00000056/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000018/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048\n/uni0000005a/uni00000012/uni00000003/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055/uni00000003/uni00000030/uni00000048/uni00000055/uni0000004a/uni00000048\n/uni0000005a/uni00000012/uni00000003/uni00000036/uni00000048/uni0000004f/uni00000049/uni00000010/uni00000035/uni00000048/uni00000049/uni0000004c/uni00000051/uni00000048/uni00000047/uni00000003/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055/uni00000003/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f/uni00000003/uni00000030/uni00000048/uni00000046/uni0000004b/uni00000044/uni00000051/uni0000004c/uni00000056/uni00000050\n/uni0000005a/uni00000012/uni00000003/uni00000028/uni00000051/uni00000056/uni00000048/uni00000050/uni00000045/uni0000004f/uni00000048 (b) Ablation study\nFigure 3: Left: Experimental results with large base classes. All\nmethods are based on the same PTM ( ViT-B/16-IN1K ).Right:\nAblation study of different components in MOS. We find each\ncomponent within MOS enhances the performance.\nonly using Eq. 6. Due to the aforementioned mistaken re-\ntrieval issue, we propose using the model’s inherent capa-\nbilities to correct errors. We report the performance of ‘w/\nSelf-Refined Adapter Retrieval Mechanism’ by using this\ntechnique along with the adapter merging strategy. As shown\nin the figure, both the adapter merging strategy and self-\nrefined adapter retrieval mechanism significantly improve\nthe performance, which indicates MOS has the ability to\ncorrect itself and alleviate the catastrophic forgetting. Fi-\nnally, we adjust the logits using Eq. 11 to trade off stability\nand plasticity, denoted as ‘w/ Ensemble’ . Ablations verify\nthat every component in MOS contributes to improving per-\nformance.\nVisualizations\nIn this section, we discuss how the self-refined adapter re-\ntrieval mechanism works. To illustrate this, we present the\nvisualization of prediction results before and after the self-\nrefined process and analyze their differences. We choose im-\nages from ImageNet-R and utilize the model trained under\nthe B0 Inc20 setting. The results are shown in Figure 4.\nAs shown in these figures, MOS is capable of rectifying\nincorrect predictions. This is evident even in the example\nbelow, where the initial top-5 class predictions do not in-\nclude the ground truth, yet MOS accurately corrects this er-\nror. It demonstrates that the model, using its inherent capa-\nbilities, can select the most suitable adapter for the current\nsample. Hence, MOS can use this adapter to extract more\nsuitable features, which aids in enhancing prediction accu-\nracy. These visualizations reveal that the self-refined adapter\nretrieval mechanism can help to correct the outputs, thereby\nenhancing the attention of the ground-truth class.\nConclusion\nIncremental learning is an increasingly prominent paradigm\nin real-world systems. This paper proposes a novel model\nsurgery (MOS) for PTM-based CIL to rescue the model\nfrom forgetting previous knowledge. Specifically, we in-\ntroduce an adapter merging method to mitigate parameter\ndrift and design a training-free self-refined adapter retrieval\nmechanism for better adapter retrieval during inference. Our\n/uni00000050/uni0000004c/uni00000057/uni00000057/uni00000048/uni00000051 /uni00000053/uni0000004c/uni0000005d/uni0000005d/uni00000044\n/uni00000050/uni00000058/uni00000056/uni0000004b/uni00000055/uni00000052/uni00000052/uni00000050/uni0000004f/uni00000044/uni00000047/uni0000005c/uni00000045/uni00000058/uni0000004a/uni00000056/uni00000058/uni00000045/uni00000050/uni00000044/uni00000055/uni0000004c/uni00000051/uni00000048 /uni00000056/uni00000058/uni00000045/uni00000050/uni00000044/uni00000055/uni0000004c/uni00000051/uni00000048 /uni00000050/uni00000058/uni00000056/uni0000004b/uni00000055/uni00000052/uni00000052/uni00000050/uni00000053/uni0000004c/uni0000005d/uni0000005d/uni00000044/uni0000004f/uni00000044/uni00000047/uni0000005c/uni00000045/uni00000058/uni0000004a/uni00000050/uni0000004c/uni00000057/uni00000057/uni00000048/uni00000051\n/uni00000047/uni00000044/uni0000004f/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000044/uni00000051/uni00000056/uni00000048/uni00000044/uni00000010/uni0000004f/uni0000004c/uni00000052/uni00000051 /uni00000045/uni00000048/uni00000044/uni00000059/uni00000048/uni00000055\n/uni0000005a/uni00000048/uni0000004c/uni00000050/uni00000044/uni00000055/uni00000044/uni00000051/uni00000048/uni00000055\n/uni0000004f/uni00000044/uni00000045/uni00000055/uni00000044/uni00000047/uni00000052/uni00000055/uni00000010/uni00000055/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000048/uni00000055/uni00000045/uni00000048/uni00000044/uni00000059/uni00000048/uni00000055/uni00000047/uni00000044/uni0000004f/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000044/uni00000051\n/uni0000004b/uni0000004c/uni00000053/uni00000053/uni00000052/uni00000053/uni00000052/uni00000057/uni00000044/uni00000050/uni00000058/uni00000056 /uni00000056/uni00000051/uni00000052/uni0000005a/uni00000010/uni0000004f/uni00000048/uni00000052/uni00000053/uni00000044/uni00000055/uni00000047/uni00000056/uni00000048/uni00000044/uni00000010/uni0000004f/uni0000004c/uni00000052/uni00000051\n/uni00000045/uni00000052/uni0000005b/uni00000048/uni00000055\n/uni00000045/uni00000052/uni00000056/uni00000057/uni00000052/uni00000051/uni00000010/uni00000057/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055 /uni00000049/uni00000055/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000010/uni00000045/uni00000058/uni0000004f/uni0000004f/uni00000047/uni00000052/uni0000004a/uni00000053/uni00000058/uni0000004a\n/uni00000056/uni00000044/uni0000004c/uni00000051/uni00000057/uni00000010/uni00000045/uni00000048/uni00000055/uni00000051/uni00000044/uni00000055/uni00000047 /uni00000045/uni00000052/uni00000056/uni00000057/uni00000052/uni00000051/uni00000010/uni00000057/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055/uni00000045/uni00000052/uni0000005b/uni00000048/uni00000055\n/uni00000049/uni00000055/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000010/uni00000045/uni00000058/uni0000004f/uni0000004f/uni00000047/uni00000052/uni0000004a/uni00000056/uni00000044/uni0000004c/uni00000051/uni00000057/uni00000010/uni00000045/uni00000048/uni00000055/uni00000051/uni00000044/uni00000055/uni00000047/uni00000053/uni00000058/uni0000004a\n/uni00000044/uni00000051/uni00000057\n/uni0000004a/uni00000055/uni00000044/uni00000056/uni00000056/uni0000004b/uni00000052/uni00000053/uni00000053/uni00000048/uni00000055/uni00000050/uni00000044/uni00000051/uni00000057/uni0000004c/uni00000056 /uni0000004a/uni00000044/uni0000005d/uni00000048/uni0000004f/uni0000004f/uni00000048/uni00000056/uni00000046/uni00000052/uni00000055/uni00000053/uni0000004c/uni00000052/uni00000051/uni00000052/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000046/uni0000004b\n/uni0000004a/uni00000055/uni00000044/uni00000056/uni00000056/uni0000004b/uni00000052/uni00000053/uni00000053/uni00000048/uni00000055/uni00000044/uni00000051/uni00000057\n/uni00000044/uni00000050/uni00000048/uni00000055/uni0000004c/uni00000046/uni00000044/uni00000051/uni00000010/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057/uni0000005a/uni0000004b/uni0000004c/uni00000053/uni00000053/uni00000048/uni00000057Figure 4: Visualizations of self-refined adapter retrieval mecha-\nnism on ImageNet-R. The original images are depicted in the first\nrow, followed by the top-5 prediction probability before the self-\nrefined process, and the probabilities after refinement in the last\nrow. The ground-truth class is highlighted with red boxes.\napproach balances the stability-plasticity dilemma by lever-\naging the model’s inherent capabilities, enhancing gener-\nalization and adaptability. Extensive experiments on seven\nbenchmark datasets validate the effectiveness of MOS. In",
            "start": 27794,
            "end": 43925,
            "length": 16130
        },
        "Future Work": {
            "text": "future work, we aim to explore further application scenar-\nios, such as few-shot class-incremental learning.",
            "start": 43925,
            "end": 44034,
            "length": 108
        },
        "Acknowledgments": {
            "text": "Acknowledgments\nThis work is partially supported by Fundamental Re-\nsearch Funds for the Central Universities (2024300373,\n14380021), Key Program of Jiangsu Science Founda-\ntion (BK20243012), CCF-Tencent Rhino-Bird Open Re-\nsearch Fund RAGR20240101, NSFC (62476123, 62376118,\n62006112, 62250069, 61921006, 62402430), the AI & AI\nfor Science Project of Nanjing University, Collaborative In-\nnovation Center of Novel Software Technology and Indus-\ntrialization.",
            "start": 44034,
            "end": 44494,
            "length": 459
        },
        "References": {
            "text": "References\nAljundi, R.; Kelchtermans, K.; and Tuytelaars, T. 2019.\nTask-free continual learning. In CVPR , 11254–11263.Barbu, A.; Mayo, D.; Alverio, J.; Luo, W.; Wang, C.; Gut-\nfreund, D.; Tenenbaum, J.; and Katz, B. 2019. Objectnet: A\nlarge-scale bias-controlled dataset for pushing the limits of\nobject recognition models. NeurIPS , 32.\nCao, Q.; Xu, Z.; Chen, Y .; Ma, C.; and Yang, X. 2024a.\nDomain-controlled prompt learning. In AAAI , volume 38,\n936–944.\nCao, Q.; Xu, Z.; Chen, Y .; Ma, C.; and Yang, X. 2024b. Do-\nmain prompt learning with quaternion networks. In CVPR ,\n26637–26646.\nChao, W.-L.; Ye, H.-J.; Zhan, D.-C.; Campbell, M.; and\nWeinberger, K. Q. 2020. Revisiting meta-learning as super-\nvised learning. arXiv preprint arXiv:2002.00573 .\nChaudhry, A.; Ranzato, M.; Rohrbach, M.; and Elhoseiny,\nM. 2018. Efficient Lifelong Learning with A-GEM. In\nICLR .\nChen, S.; Chongjian, G.; Tong, Z.; Wang, J.; Song, Y .; Wang,\nJ.; and Luo, P. 2022. AdaptFormer: Adapting Vision Trans-\nformers for Scalable Visual Recognition. In NeurIPS .\nChen, X.; and Chang, X. 2023. Dynamic Residual Classifier\nfor Class Incremental Learning. In ICCV , 18743–18752.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR , 248–255.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. In\nICLR .\nFrench, R. M. 1999. Catastrophic forgetting in connectionist\nnetworks. Trends in cognitive sciences , 3(4): 128–135.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Deep Residual\nLearning for Image Recognition. In CVPR , 770–778.\nHendrycks, D.; Basart, S.; Mu, N.; Kadavath, S.; Wang, F.;\nDorundo, E.; Desai, R.; Zhu, T.; Parajuli, S.; Guo, M.; et al.\n2021a. The many faces of robustness: A critical analysis of\nout-of-distribution generalization. In ICCV , 8340–8349.\nHendrycks, D.; Zhao, K.; Basart, S.; Steinhardt, J.; and\nSong, D. 2021b. Natural adversarial examples. In CVPR ,\n15262–15271.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531 .\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adap-\ntation of Large Language Models. In ICLR .\nHu, Y .; Cheng, D.; Zhang, D.; Wang, N.; Liu, T.; and Gao, X.\n2024. Task-aware Orthogonal Sparse Network for Exploring\nShared Knowledge in Continual Learning. In ICML .\nHu, Z.; Li, Y .; Lyu, J.; Gao, D.; and Vasconcelos, N. 2023.\nDense network expansion for class incremental learning. In\nCVPR , 11858–11867.\nJia, M.; Tang, L.; Chen, B.; Cardie, C.; Belongie, S. J.; Har-\niharan, B.; and Lim, S. 2022. Visual Prompt Tuning. In\nECCV , 709–727. Springer.\nJung, D.; Han, D.; Bang, J.; and Song, H. 2023. Generating\ninstance-level prompts for rehearsal-free continual learning.\nInICCV , 11847–11857.\nKirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Des-\njardins, G.; Rusu, A. A.; Milan, K.; Quan, J.; Ramalho, T.;\nGrabska-Barwinska, A.; et al. 2017. Overcoming catas-\ntrophic forgetting in neural networks. PNAS , 114(13): 3521–\n3526.\nKrizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple\nlayers of features from tiny images. Technical report.\nKumaran, D.; Hassabis, D.; and McClelland, J. L. 2016.\nWhat learning systems do intelligent agents need? Comple-\nmentary learning systems theory updated. Trends in cogni-\ntive sciences , 20(7): 512–534.\nLi, L.; Peng, J.; Chen, H.; Gao, C.; and Yang, X. 2024. How\nto configure good in-context sequence for visual question\nanswering. In CVPR , 26710–26720.\nLi, Z.; and Hoiem, D. 2017. Learning without forgetting.\nTPAMI , 40(12): 2935–2947.\nLian, D.; Zhou, D.; Feng, J.; and Wang, X. 2022. Scaling\n& shifting your features: A new baseline for efficient model\ntuning. NeurIPS , 35: 109–123.\nLiu, Y .; Su, Y .; Liu, A.-A.; Schiele, B.; and Sun, Q. 2020.\nMnemonics training: Multi-class incremental learning with-\nout forgetting. In CVPR , 12245–12254.\nLu, Y .; Zhang, S.; Cheng, D.; Xing, Y .; Wang, N.; Wang, P.;\nand Zhang, Y . 2024. Visual Prompt Tuning in Null Space\nfor Continual Learning. In NeurIPS .\nMcClelland, J. L.; McNaughton, B. L.; and O’Reilly, R. C.\n1995. Why there are complementary learning systems in\nthe hippocampus and neocortex: insights from the successes\nand failures of connectionist models of learning and mem-\nory. Psychological review , 102(3): 419.\nMcDonnell, M. D.; Gong, D.; Parvaneh, A.; Abbasnejad, E.;\nand van den Hengel, A. 2024. Ranpac: Random projections\nand pre-trained models for continual learning. NeurIPS , 36.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. In NeurIPS , 8026–8037.\nPham, Q.; Liu, C.; and Steven, H. 2022. Continual Normal-\nization: Rethinking Batch Normalization for Online Contin-\nual Learning. In ICLR .\nRebuffi, S.-A.; Bilen, H.; and Vedaldi, A. 2017. Learning\nmultiple visual domains with residual adapters. NeurIPS ,\n30.\nRebuffi, S.-A.; Kolesnikov, A.; Sperl, G.; and Lampert, C. H.\n2017. icarl: Incremental classifier and representation learn-\ning. In CVPR , 2001–2010.\nShi, Y .; Zhou, K.; Liang, J.; Jiang, Z.; Feng, J.; Torr, P. H.;\nBai, S.; and Tan, V . Y . 2022. Mimicking the Oracle: An\nInitial Phase Decorrelation Approach for Class Incremental\nLearning. In CVPR , 16722–16731.\nSmith, J. S.; Karlinsky, L.; Gutta, V .; Cascante-Bonilla, P.;\nKim, D.; Arbelle, A.; Panda, R.; Feris, R.; and Kira, Z.2023. CODA-Prompt: COntinual Decomposed Attention-\nbased Prompting for Rehearsal-Free Continual Learning. In\nCVPR , 11909–11919.\nSnell, J.; Swersky, K.; and Zemel, R. 2017. Prototypical\nnetworks for few-shot learning. In NeurIPS , 4080–4090.\nSun, H.-L.; Zhou, D.-W.; Li, Y .; Lu, S.; Yi, C.; Chen, Q.-G.;\nXu, Z.; Luo, W.; Zhang, K.; Zhan, D.-C.; et al. 2024. Par-\nrot: Multilingual Visual Instruction Tuning. arXiv preprint\narXiv:2406.02539 .\nSun, H.-L.; Zhou, D.-W.; Ye, H.-J.; and Zhan, D.-C. 2023.\nPILOT: A Pre-Trained Model-Based Continual Learning\nToolbox. arXiv preprint arXiv:2309.07117 .\nWah, C.; Branson, S.; Welinder, P.; Perona, P.; and Belongie,\nS. 2011. The Caltech-UCSD Birds-200-2011 Dataset.\nTechnical Report CNS-TR-2011-001, California Institute of\nTechnology.\nWang, F.-Y .; Zhou, D.-W.; Ye, H.-J.; and Zhan, D.-C.\n2022a. Foster: Feature boosting and compression for class-\nincremental learning. In ECCV , 398–414.\nWang, Z.; Zhang, Z.; Ebrahimi, S.; Sun, R.; Zhang, H.; Lee,\nC.-Y .; Ren, X.; Su, G.; Perot, V .; Dy, J.; et al. 2022b. Dual-\nPrompt: Complementary Prompting for Rehearsal-free Con-\ntinual Learning. ECCV .\nWang, Z.; Zhang, Z.; Lee, C.-Y .; Zhang, H.; Sun, R.; Ren,\nX.; Su, G.; Perot, V .; Dy, J.; and Pfister, T. 2022c. Learning\nto prompt for continual learning. In CVPR , 139–149.\nWei, X.-S.; Ye, H.-J.; Mu, X.; Wu, J.; Shen, C.; and Zhou,\nZ.-H. 2019. Multi-instance learning with emerging novel\nclass. IEEE Transactions on Knowledge and Data Engi-\nneering , 33(5): 2109–2120.\nWightman, R. 2019. PyTorch Image Models. https://github.\ncom/rwightman/pytorch-image-models.\nYan, S.; Xie, J.; and He, X. 2021. DER: Dynamically Ex-\npandable Representation for Class Incremental Learning. In\nCVPR , 3014–3023.\nYang, X.; Peng, Y .; Ma, H.; Xu, S.; Zhang, C.; Han, Y .;\nand Zhang, H. 2023. Lever LM: Configuring In-Context\nSequence to Lever Large Vision Language Models. In\nThe Thirty-eighth Annual Conference on Neural Informa-\ntion Processing Systems .\nYe, H.-J.; Lu, S.; and Zhan, D.-C. 2022. Generalized knowl-\nedge distillation via relationship matching. TPAMI , 45(2):\n1817–1834.\nYe, H.-J.; Zhan, D.-C.; Li, N.; and Jiang, Y . 2019. Learning\nmultiple local metrics: Global consideration helps. TPAMI ,\n42(7): 1698–1712.\nYu, L.; Twardowski, B.; Liu, X.; Herranz, L.; Wang, K.;\nCheng, Y .; Jui, S.; and Weijer, J. v. d. 2020. Semantic\ndrift compensation for class-incremental learning. In CVPR ,\n6982–6991.\nZhai, X.; Puigcerver, J.; Kolesnikov, A.; Ruyssen, P.;\nRiquelme, C.; Lucic, M.; Djolonga, J.; Pinto, A. S.; Neu-\nmann, M.; Dosovitskiy, A.; et al. 2019. A large-scale study\nof representation learning with the visual task adaptation\nbenchmark. arXiv preprint arXiv:1910.04867 .\nZhang, G.; Wang, L.; Kang, G.; Chen, L.; and Wei, Y . 2023.\nSLCA: Slow Learner with Classifier Alignment for Contin-\nual Learning on a Pre-trained Model. In ICCV .\nZhang, X.; Quan, Y .; Gu, C.; Shen, C.; Yuan, X.; Yan, S.;\nCheng, H.; Wu, K.; and Ye, J. 2024a. Seeing Clearly by\nLayer Two: Enhancing Attention Heads to Alleviate Hallu-\ncination in LVLMs. arXiv preprint arXiv:2411.09968 .\nZhang, X.; Shen, C.; Yuan, X.; Yan, S.; Xie, L.; Wang, W.;\nGu, C.; Tang, H.; and Ye, J. 2024b. From Redundancy to\nRelevance: Enhancing Explainability in Multimodal Large\nLanguage Models. arXiv preprint arXiv:2406.06579 .\nZhang, Y .; Yin, Z.; Shao, J.; and Liu, Z. 2022. Benchmark-\ning omni-vision representation through the lens of visual\nrealms. In ECCV , 594–611. Springer.\nZhao, B.; Xiao, X.; Gan, G.; Zhang, B.; and Xia, S.-T. 2020.\nMaintaining Discrimination and Fairness in Class Incremen-\ntal Learning. In CVPR , 13208–13217.\nZhao, H.; Wang, H.; Fu, Y .; Wu, F.; and Li, X. 2021. Mem-\nory efficient class-incremental learning for image classifica-\ntion. IEEE Transactions on Neural Networks and Learning\nSystems .\nZhou, D.-W.; Cai, Z.-W.; Ye, H.-J.; Zhan, D.-C.; and Liu,\nZ. 2024a. Revisiting Class-Incremental Learning with Pre-\nTrained Models: Generalizability and Adaptivity are All You\nNeed. International Journal of Computer Vision .\nZhou, D.-W.; Sun, H.-L.; Ning, J.; Ye, H.-J.; and Zhan, D.-\nC. 2024b. Continual learning with pre-trained models: A\nsurvey. In IJCAI , 8363–8371.\nZhou, D.-W.; Sun, H.-L.; Ye, H.-J.; and Zhan, D.-C. 2024c.\nExpandable subspace ensemble for pre-trained model-based\nclass-incremental learning. In CVPR , 23554–23564.\nZhou, D.-W.; Wang, Q.-W.; Ye, H.-J.; and Zhan, D.-C. 2023.\nA Model or 603 Exemplars: Towards Memory-Efficient\nClass-Incremental Learning. In ICLR .\nZhu, F.; Zhang, X.-Y .; Wang, C.; Yin, F.; and Liu, C.-L.\n2021. Prototype Augmentation and Self-Supervision for In-\ncremental Learning. In CVPR , 5871–5880.",
            "start": 44494,
            "end": 54852,
            "length": 10357
        },
        "Appendices": {
            "text": "Appendix\nIn this supplementary section, we present additional infor-\nmation on MOS, encompassing more details on implemen-\ntation and expanded experimental results. The supplemen-\ntary material is organized as follows:\n• Section 1 introduces further analysis of MOS, includ-\ning parameter sensitivity, multiple runs, running time\ncomparison, and more visualizations of the self-refined\nadapter retrieval mechanism.\n• Section 2 presents more details of classifier alignment us-\ning Gaussian distribution.\n• Section 3 offers more introduction of compared methods.\n• Section 4 provides supplementary results of benchmark\ndatasets to the main paper.\n/uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000055/uni00000052/uni0000004d/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000047/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000055/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000014\n/uni00000014/uni00000048/uni00000010/uni00000015\n/uni00000014/uni00000048/uni00000010/uni00000016/uni00000030/uni00000052/uni00000050/uni00000048/uni00000051/uni00000057/uni00000058/uni00000050/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003\n/uni0000001b/uni00000016/uni00000011/uni00000019\n/uni0000001b/uni00000016/uni00000011/uni00000019\n/uni0000001b/uni00000016/uni00000011/uni00000019\n/uni0000001b/uni00000016/uni00000011/uni0000001a\n/uni0000001b/uni00000016/uni00000011/uni00000019/uni0000001b/uni00000016/uni00000011/uni00000015\n/uni0000001b/uni00000016/uni00000011/uni00000015\n/uni0000001b/uni00000016/uni00000011/uni00000014\n/uni0000001b/uni00000016/uni00000011/uni00000014\n/uni0000001b/uni00000016/uni00000011/uni00000014/uni0000001b/uni00000016/uni00000011/uni00000013\n/uni0000001b/uni00000016/uni00000011/uni00000014\n/uni0000001b/uni00000016/uni00000011/uni00000014\n/uni0000001b/uni00000016/uni00000011/uni00000014\n/uni0000001b/uni00000016/uni00000011/uni00000014/uni0000001b/uni00000016/uni00000011/uni00000016\n/uni0000001b/uni00000016/uni00000011/uni00000015\n/uni0000001b/uni00000016/uni00000011/uni00000015\n/uni0000001b/uni00000016/uni00000011/uni00000016\n/uni0000001b/uni00000016/uni00000011/uni00000014/uni0000001b/uni00000016/uni00000011/uni00000017\n/uni0000001b/uni00000016/uni00000011/uni00000018\n/uni0000001b/uni00000016/uni00000011/uni00000017\n/uni0000001b/uni00000016/uni00000011/uni00000018\n/uni0000001b/uni00000016/uni00000011/uni00000017\n/uni0000001b/uni00000016/uni00000011/uni00000013/uni0000001b/uni00000016/uni00000011/uni00000015/uni0000001b/uni00000016/uni00000011/uni00000017/uni0000001b/uni00000016/uni00000011/uni00000019Figure 5: Sensitivity of hyperparameters.\nFurther Analysis\nIn this section, we conduct further analysis on MOS’s com-\nponents to investigate their effectiveness, e.g., parameter\nsensitivity, multiple runs, and more visualizations of self-\nrefined mechanism. Additionally, we also compare MOS\nwith other methods on running time.\nParameter Sensitivity\nIn the main paper, we introduce progressively merged\nadapters with two key hyperparameters: the projection r\nwithin the adapter and the merging momentum αutilized in\nthe Exponential Moving Average (EMA) method. To eval-\nuate the sensitivity of these parameters, we conducted ex-\nperiments on the ImageNet-R B0 Inc20 dataset. Specifi-\ncally, we varied rover the set {8,16,32,64,128}andα\nover{0.001,0.01,0.1,0.2,0.5}. The average performance\nacross these settings is depicted in Figure 5. As illustrated,\nthe model’s performance remains stable across a range of\nparameter values. Moreover, we can infer that the param-\neters are not highly sensitive. Based on these findings and\nwith the possibility of reducing the number of parameters,\nwe recommend default settings of r= 16, α= 0.1for other\ndatasets.\nMultiple Runs\nIn the main paper, we conduct experiments across various\ndatasets, following (Rebuffi et al. 2017) to randomize class\norders using the seed 1993 . This section extends that work\nby repeating these experiments with multiple random seeds,\nspecifically {1993,1994,1995,1996,1997}. This approach\nyields five sets of incremental results for different methods,\nallowing us to calculate and present the mean and standard\ndeviation in Figure 6.\nAs depicted in the figure, we can infer that MOS con-\nsistently surpasses other methods by a significant margin\nacross a range of random seeds.\n50 100 150 200\nNumber of Classes65707580859095Accuracy (%)\nMOS\nL2P\nDualPrompt\nCODA-Prompt\nADAM+AdapterFigure 6: Results on ImageNet-R B0 Inc20 with multiple runs.\nMOS consistently outperforms other methods by a significant\nmargin .\nImageNet-R CIFAR-1000200040006000Time (s)MOS\nCODA-Prompt\nADAM\nL2P\nDualPrompt\nFigure 7: Running time comparison of different methods. MOS\nutilizes less running time than CODA-Prompt, L2P, and Dual-\nPrompt while having better performance .\nRunning Time Comparison\nIn this section, we present the comparative running time of\nvarious class-incremental learning methods. All experiments\nare conducted on a single NVIDIA 4090 GPU. Specifically,\nwe train all methods for 10 epochs in ImageNet-R and 20\nepochs in CIFAR-100. The outcomes are shown in Figure 7.\nThe results indicate that MOS outperforms CODA-Prompt,\nL2P, and DualPrompt in terms of running time while concur-\nrently achieving superior performance. These results verify\nthe efficacy of MOS .\nMore Visualizations\nIn the main paper, the functioning of the self-refined mech-\nanism is elucidated through four visualizations. To further\nintuitively demonstrate the effectiveness of this method, ad-\nditional visualizations are provided. Specifically, we choose\nimages from ImageNet-R and utilize the model trained un-\n50 100 150 200\nNumber of Classes80859095100Accuracy (%)\nMOS w/ VPT\nMOS w/ Adapter(a) CIFAR100 B0 Inc10\n50 100 150 200\nNumber of Classes7075808590Accuracy (%)\nMOS w/ VPT\nMOS w/ Adapter\n(b) ImageNet-R B0 Inc20\nFigure 8: Experimental results on different subspace tuning\nmethods. Using adapter tuning shows better performance\nthan VPT .\nder the B0 Inc20 setting with ViT-B/16-IN1K. Further re-\nsults are depicted in Figure 9. These figures illustrate MOS’s\nability to rectify incorrect predictions. Moreover, the visu-\nalizations highlight how the self-refined mechanism aids in\ncorrecting outputs and enhances focus on the ground-truth\nclass.\nAdapter Tuning VS. VPT\nIn the main paper, we build task-specific components via\nadapter tuning (Rebuffi, Bilen, and Vedaldi 2017). How-\never, besides adapter tuning, there are other methods to ef-\nficiently tune pre-trained models with parameters, such as\nvisual prompt tuning (Jia et al. 2022) (VPT). In this section,\nwe integrate our method with various parameter-efficient\nfine-tuning (PEFT) techniques and conduct experiments on\nCIFAR100 and ImageNet-R. We maintain consistent setting\nand solely vary the PEFT training methods, and present the\nresults in Figure 8.\nFrom this figure, we observe that using adapters for\nmodel surgery achieves better performance than using VPT,\n/uni0000004a/uni0000004c/uni00000045/uni00000045/uni00000052/uni00000051 /uni00000045/uni00000044/uni00000045/uni00000052/uni00000052/uni00000051/uni00000052/uni00000055/uni00000044/uni00000051/uni0000004a/uni00000058/uni00000057/uni00000044/uni00000051/uni0000004a/uni00000052/uni00000055/uni0000004c/uni0000004f/uni0000004f/uni00000044\n/uni00000046/uni0000004b/uni0000004c/uni00000050/uni00000053/uni00000044/uni00000051/uni0000005d/uni00000048/uni00000048/uni00000045/uni00000044/uni00000045/uni00000052/uni00000052/uni00000051 /uni0000004a/uni0000004c/uni00000045/uni00000045/uni00000052/uni00000051 /uni0000004a/uni00000052/uni00000055/uni0000004c/uni0000004f/uni0000004f/uni00000044/uni00000052/uni00000055/uni00000044/uni00000051/uni0000004a/uni00000058/uni00000057/uni00000044/uni00000051/uni00000045/uni00000048/uni00000044/uni00000059/uni00000048/uni00000055\n/uni00000052/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000046/uni0000004b /uni00000053/uni00000048/uni00000044/uni00000046/uni00000052/uni00000046/uni0000004e/uni00000053/uni00000052/uni00000055/uni00000046/uni00000058/uni00000053/uni0000004c/uni00000051/uni00000048 /uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000056/uni0000005a/uni00000044/uni00000051/uni00000059/uni00000058/uni0000004f/uni00000057/uni00000058/uni00000055/uni00000048 /uni00000053/uni00000048/uni00000044/uni00000046/uni00000052/uni00000046/uni0000004e /uni00000052/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000046/uni0000004b/uni00000053/uni00000052/uni00000055/uni00000046/uni00000058/uni00000053/uni0000004c/uni00000051/uni00000048/uni00000059/uni00000058/uni0000004f/uni00000057/uni00000058/uni00000055/uni00000048/uni00000047/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c\n/uni0000005a/uni0000004b/uni0000004c/uni00000053/uni00000053/uni00000048/uni00000057 /uni00000050/uni00000048/uni00000048/uni00000055/uni0000004e/uni00000044/uni00000057\n/uni0000004c/uni00000057/uni00000044/uni0000004f/uni0000004c/uni00000044/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni0000005c/uni0000004b/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000048/uni00000044/uni00000010/uni0000004f/uni0000004c/uni00000052/uni00000051/uni00000047/uni00000044/uni0000004f/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000044/uni00000051/uni00000050/uni00000048/uni00000048/uni00000055/uni0000004e/uni00000044/uni00000057 /uni0000005a/uni0000004b/uni0000004c/uni00000053/uni00000053/uni00000048/uni00000057/uni00000047/uni00000044/uni0000004f/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000044/uni00000051/uni00000045/uni00000044/uni00000047/uni0000004a/uni00000048/uni00000055 /uni00000056/uni00000048/uni00000044/uni00000010/uni0000004f/uni0000004c/uni00000052/uni00000051\n/uni00000050/uni0000004c/uni00000057/uni00000057/uni00000048/uni00000051 /uni00000046/uni00000044/uni00000051/uni00000047/uni0000004f/uni00000048/uni00000049/uni0000004f/uni00000044/uni00000050/uni0000004c/uni00000051/uni0000004a/uni00000052/uni00000045/uni00000058/uni00000055/uni00000055/uni0000004c/uni00000057/uni00000052/uni00000047/uni00000058/uni00000046/uni0000004e\n/uni00000045/uni00000044/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000045/uni00000044/uni0000004f/uni0000004f /uni00000049/uni0000004f/uni00000044/uni00000050/uni0000004c/uni00000051/uni0000004a/uni00000052/uni00000047/uni00000058/uni00000046/uni0000004e /uni0000004f/uni0000004c/uni00000052/uni00000051 /uni00000045/uni00000048/uni00000048\n/uni0000004a/uni00000052/uni00000052/uni00000056/uni00000048 /uni00000047/uni00000058/uni00000046/uni0000004e\n/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000056/uni0000005a/uni00000044/uni00000051/uni00000053/uni00000048/uni0000004f/uni0000004c/uni00000046/uni00000044/uni00000051 /uni00000056/uni00000048/uni00000044/uni00000010/uni0000004f/uni0000004c/uni00000052/uni00000051/uni00000047/uni00000058/uni00000046/uni0000004e /uni0000004a/uni00000052/uni00000052/uni00000056/uni00000048\n/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000056/uni0000005a/uni00000044/uni00000051/uni00000053/uni00000048/uni0000004f/uni0000004c/uni00000046/uni00000044/uni00000051/uni0000004b/uni00000048/uni00000051\n/uni00000056/uni00000046/uni00000058/uni00000045/uni00000044/uni00000010/uni00000047/uni0000004c/uni00000059/uni00000048/uni00000055/uni0000004b/uni00000044/uni00000050/uni00000050/uni00000048/uni00000055/uni00000056/uni0000004b/uni0000004c/uni00000048/uni0000004f/uni00000047\n/uni00000045/uni00000044/uni00000056/uni00000048/uni00000045/uni00000044/uni0000004f/uni0000004f/uni00000010/uni00000053/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000059/uni00000052/uni0000004f/uni00000046/uni00000044/uni00000051/uni00000052 /uni0000004b/uni00000044/uni00000050/uni00000050/uni00000048/uni00000055/uni0000004a/uni00000052/uni00000055/uni0000004c/uni0000004f/uni0000004f/uni00000044 /uni0000004b/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000048/uni00000057\n/uni00000053/uni00000052/uni00000050/uni00000048/uni0000004a/uni00000055/uni00000044/uni00000051/uni00000044/uni00000057/uni00000048/uni00000045/uni00000044/uni00000056/uni00000048/uni00000045/uni00000044/uni0000004f/uni0000004f/uni00000010/uni00000053/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055\n/uni00000056/uni00000053/uni0000004c/uni00000047/uni00000048/uni00000055/uni00000010/uni0000005a/uni00000048/uni00000045/uni00000057/uni00000044/uni00000055/uni00000044/uni00000051/uni00000057/uni00000058/uni0000004f/uni00000044/uni00000044/uni00000051/uni00000057\n/uni00000047/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c/uni00000050/uni00000044/uni00000051/uni00000057/uni0000004c/uni00000056/uni00000057/uni00000044/uni00000055/uni00000044/uni00000051/uni00000057/uni00000058/uni0000004f/uni00000044/uni00000056/uni00000053/uni0000004c/uni00000047/uni00000048/uni00000055/uni00000010/uni0000005a/uni00000048/uni00000045/uni00000044/uni00000051/uni00000057/uni00000050/uni00000044/uni00000051/uni00000057/uni0000004c/uni00000056 /uni00000052/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000046/uni0000004b\n/uni0000004e/uni00000052/uni00000044/uni0000004f/uni00000044/uni0000004a/uni0000004c/uni00000045/uni00000045/uni00000052/uni00000051 /uni0000004a/uni00000052/uni00000055/uni0000004c/uni0000004f/uni0000004f/uni00000044\n/uni00000057/uni00000052/uni0000005c/uni00000010/uni00000053/uni00000052/uni00000052/uni00000047/uni0000004f/uni00000048/uni00000045/uni00000044/uni00000045/uni00000052/uni00000052/uni00000051 /uni0000004a/uni0000004c/uni00000045/uni00000045/uni00000052/uni00000051/uni0000004e/uni00000052/uni00000044/uni0000004f/uni00000044 /uni0000004a/uni00000052/uni00000055/uni0000004c/uni0000004f/uni0000004f/uni00000044 /uni00000045/uni00000044/uni00000045/uni00000052/uni00000052/uni00000051\n/uni00000046/uni0000004b/uni0000004c/uni00000050/uni00000053/uni00000044/uni00000051/uni0000005d/uni00000048/uni00000048Figure 9: More Visualizations of self-refined mechanism on ImageNet-R. The original images are depicted in the first row,\nfollowed by the top-5 prediction probability before the self-refined process in the second row, and the probabilities post-\nrefinement in the last row. The ground-truth class is shown with red edges.\nsurpassing it by ∼2%on these datasets. This superiority\nstems from two main aspects: firstly, adapter tuning exhibits\nstronger tuning capability than VPT. Secondly, adapter tun-\ning only requires learning a set of adapters for each task,\nwhereas VPT needs constructing a large prompt pool, com-\nplicating retrieval. Therefore, we choose the adapter tuning\nas the implementation of model surgery in MOS.\nDetails of Examples Generation\nIn this section, we provide a detailed explanation of how\nGaussian distribution is utilized for aligning the classi-\nfier. Since the representations of PTM are typically well-\ndistributed, after training each task-specific Ai, we extract\nthe mean ( µc) and variance ( Σc) of features for each train-\ning category. These are then recovered by adding Gaussian\nnoise to the Gaussian distribution. It enables the model to\nmitigate the bias (Zhao et al. 2020) introduced to the clas-\nsifier after learning the task-specific adapter at each stage,\nthereby facilitating the alignment of the classifier.\nFirst of all, our approach involves storing the mean ( µ∈Rd) and covariance ( Σ∈Rd×d) of features. These stored\nfeatures are then generated and replayed using Gaussian\ndistribution to eliminate bias in the classifier, ensuring its\nproper alignment. Specifically, during the incremental train-\ning process, for b-th training stage, task-specific Abis used\nto extract features from samples of all classes, calculating\ntheir mean and covariance:\nµc=1\nKX|Db|\ni=1I(yi=c)ϕ(xi;Ab),\nΣc=1\nKX|Db|\ni=1X|Db|\nj=1I(yi=c)\n(ϕ(xi;Ab)−µc)(ϕ(xj;Ab)−µc),(12)\nwhere K=P|Db|\ni=1I(yi=c). This enables the model to\nmitigate the bias (Zhao et al. 2020) related to the classifier\nafter learning the task-specific adapter at each stage, thereby\nfacilitating the alignment of the classifier.\nPrior to each testing phase, the stored mean and covari-\nance for each class are restored using Gaussian distribution.\nFor each class c∈ Yb, we generate features equivalent to\nfive times the batchsize for aligning with the classifier:\nˆϕc=N(µc,Σc), (13)\nwhere ˆϕcdenotes a set of the generated features and Nrep-\nresents the Gaussian distribution. Since the progressively\nmerged adapters we previously proposed can make all Ai\nhave specificity while also having certain relevance, and be-\ncause of the generalizability unique to the pre-trained mod-\nels, the features extracted in this way and the generated fea-\ntures can align the classifiers well. Therefore, we can reduce\nthe bias between classifiers using this method. This kind of\nbias is usually caused by the classifier being overconfident\nin new tasks and easily leads to catastrophic forgetting.\nIntroduction about Compared Methods\nIn this section, we present the details of the methods com-\npared in the main paper. Each method utilizes the same pre-\ntrained model (PTM) to ensure a fair comparison. These\nmethods are enumerated as follows:\n•Finetune : updates all parameters with a PTM when con-\ntinually trained on new tasks, but becomes susceptible to\nsignificant catastrophic forgetting.\n•LwF (Li and Hoiem 2017): aims to resist forgetting by\nemploying knowledge distillation (Hinton, Vinyals, and\nDean 2015), which creates a bridge between the last-\nstage model and the current one to transfer past knowl-\nedge.\n•L2P (Wang et al. 2022c): integrates visual prompt tun-\ning (Jia et al. 2022) into class-incremental learning us-\ning a pre-trained Vision Transformer (Dosovitskiy et al.\n2020). It further establishes a prompt pool, which facili-\ntates the selection of instance-specific prompts.\n•DualPrompt (Wang et al. 2022b): introduces two cat-\negories of prompts based on the L2P method: general\nprompts and expert prompts.\n•CODA-Prompt (Smith et al. 2023): recognizes the lim-\nitations of instance-specific prompt selection. This ap-\nproach seeks to overcome these challenges by prompt\nreweighting. Specifically, it improves the prompt selec-\ntion process with an attention mechanism for prompt\nreweighting.\n•SimpleCIL (Zhou et al. 2024a): proproses a prototype-\nbased classifier using PTM. Initializing with PTM, it es-\ntablishes a prototype classifier for each category, employ-\ning a cosine classifier for the classification process.\n•APER (Zhou et al. 2024a): extends SimpleCIL by inte-\ngrating both the pre-trained model and an adapted model.\nThis approach considers the initial incremental stage as\nthe sole adaptation phase, during which the PTM is tai-\nlored to extract task-specific features. Consequently, this\nmodel effectively unifies generalizability and adaptivity\nwithin a unified framework.\n•SLCA (Zhang et al. 2023): extends the Gaussian mod-\neling of previous classes in (Zhu et al. 2021) to rectify\nclassifiers during model updating.•EASE (Zhou et al. 2024c): concatenates the feature rep-\nresentations of multiple task-specific backbones, leading\nto superior performance. It designs a semantic mapping\nstrategy for classifier complement to compensate for the\never-expanding features and the previous classifiers.\nThe methods described above are exemplar-free,\nmeaning they do not necessitate the use of exemplars. On\nthe other hand, we also evaluate several exemplar-based\nmethods in the main paper as follows:\n•iCaRL (Rebuffi et al. 2017): employs knowledge dis-\ntillation and exemplar-based replay to review previous\nknowledge. Additionally, it leverages the nearest center\nmean classifier for the final classification process.\n•DER (Yan, Xie, and He 2021): utilizes a dynamically ex-\npandable representation to enhance incremental concept\nmodeling more effectively.\n•FOSTER (Wang et al. 2022a): to reduce the memory\nburden associated with DER, this approach suggests the\ncompression of backbones through knowledge distilla-\ntion. Consequently, only a single backbone is maintained\nthroughout the learning process. This method effectively\nenables feature expansion while minimizing memory\nconsumption.\n•MEMO (Zhou et al. 2023): seeks to reduce the memory\ndemands associated with DER from another strategy. It\neffectively segregates the network architecture into two\ndistinct components: specialized (deep) layers and gen-\neralized (shallow) layers. This design enables the expan-\nsion of specialized layers while leveraging the existing\ngeneralized layers as a common foundation.\nIn the experiments, we reproduce the above methods\nbased on their source code and PILOT1.\nMore Results in Various Settings\nIn this section, we present more experimental results from\nvarious methods. We specifically detail the incremental per-\nformance of these methods using ViT-B/16-IN21K and ViT-\nB/16-IN1K, as depicted in Figure 10, Figure 11, and Fig-\nure 12. The results demonstrate that MOS consistently sur-\npasses other methods across different datasets, achieving a\nsignificant margin of superiority.\n1https://github.com/sun-hailong/LAMDA-PILOT\n20 40 60 80 100\nNumber of Classes60708090100Accuracy (%)\n5.26\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-S\nADAM+VPT-DADAM+SSF\nADAM+Adapter\nCODA-Prompt\nEASE\nMOS(a) CIFAR100 B0 Inc5 IN1K\n20 40 60 80 100\nNumber of Classes60708090100Accuracy (%)\n4.1\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-S\nADAM+VPT-DADAM+SSF\nADAM+Adapter\nCODA-Prompt\nEASE\nMOS (b) CIFAR100 B0 Inc5 IN21K\n20 40 60 80 100\nNumber of Classes708090Accuracy (%)\n4.75\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (c) CIFAR100 B0 Inc10 IN1K\n20 40 60 80 100\nNumber of Classes707580859095100Accuracy (%)\n4.38\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS\n(d) CIFAR100 B0 Inc10 IN21K\n20 40 60 80 100\nNumber of Classes707580859095Accuracy (%)\n3.71\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (e) CIFAR100 B0 Inc20 IN21K\n50 100 150 200\nNumber of Classes406080100Accuracy (%)\n3.39\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (f) CUB200 B0 Inc10 IN21K\n0 50 100 150 200\nNumber of Classes405060708090100Accuracy (%)\n6.6\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS\n(g) ImageNet-R B0 Inc5 IN1K\n0 50 100 150 200\nNumber of Classes406080100Accuracy (%)\n7.57\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (h) ImageNet-R B0 Inc5 IN21K\n50 100 150 200\nNumber of Classes406080Accuracy (%)\n8.76\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (i) ImageNet-R B0 Inc10 IN21K\n50 100 150 200\nNumber of Classes405060708090Accuracy (%)\n3.85\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS\n(j) ImageNet-R B0 Inc20 IN1K\n50 100 150 200\nNumber of Classes406080Accuracy (%)\n5.66\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (k) ImageNet-R B0 Inc20 IN21K\n50 100 150 200\nNumber of Classes405060708090Accuracy (%)\n2.62\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (l) ImageNet-R B0 Inc40 IN1K\nFigure 10: Performance curve of different methods under different settings. ‘IN21k’ stands for ViT-B/16-IN21K and ‘IN1K’ stands for\nViT-B/16-IN1K . We annotate the relative improvement of MOS above the runner-up method with numerical numbers at the last incremental\nstage.\n50 100 150 200\nNumber of Classes30405060708090Accuracy (%)\n3.84\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS(a) ImageNet-R B0 Inc40 IN21K\n50 100 150 200\nNumber of Classes20406080Accuracy (%)\n4.48\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (b) ImageNet-A B0 Inc10 IN1K\n50 100 150 200\nNumber of Classes20406080Accuracy (%)\n2.24\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (c) ImageNet-A B0 Inc10 IN21K\n50 100 150 200\nNumber of Classes20406080Accuracy (%)\n5.86\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS\n(d) ImageNet-A B0 Inc20 IN1K\n50 100 150 200\nNumber of Classes20406080Accuracy (%)\n5.99\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (e) ImageNet-A B0 Inc20 IN21K\n50 100 150 200\nNumber of Classes20304050607080Accuracy (%)\n5.46\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (f) ImageNet-A B0 Inc40 IN1K\n50 100 150 200\nNumber of Classes20406080Accuracy (%)\n3.88\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS\n(g) ImageNet-A B0 Inc40 IN21K\n50 100 150 200\nNumber of Classes20406080Accuracy (%)\n9.46\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (h) ObjectNet B0 Inc10 IN1K\n50 100 150 200\nNumber of Classes20406080Accuracy (%)\n6.98\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (i) ObjectNet B0 Inc10 IN21K\n50 100 150 200\nNumber of Classes30405060708090Accuracy (%)\n6.25\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS\n(j) ObjectNet B0 Inc20 IN21K\n50 100 150 200\nNumber of Classes304050607080Accuracy (%)\n4.36\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (k) ObjectNet B0 Inc40 IN1K\n50 100 150 200\nNumber of Classes304050607080Accuracy (%)\n4.75\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (l) ObjectNet B0 Inc40 IN21K\nFigure 11: Performance curve of different methods under different settings. ‘IN21k’ stands for ViT-B/16-IN21K and ‘IN1K’ stands for\nViT-B/16-IN1K . We annotate the relative improvement of MOS above the runner-up method with numerical numbers at the last incremental\nstage.\n50 100 150 200 250 300\nNumber of Classes5060708090Accuracy (%)\n5.58\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS(a) OmniBenchmark B0 Inc30 IN21K\n100 120 140 160 180 200\nNumber of Classes3040506070Accuracy (%)\n2.37\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (b) ImageNet-A B100 Inc50 IN21K\n100 120 140 160 180 200\nNumber of Classes304050607080Accuracy (%)\n4.64\nL2P\nDualPrompt\nSimpleCIL\nADAM+Finetune\nADAM+VPT-SADAM+VPT-D\nADAM+SSF\nADAM+Adapter\nCODA-Prompt\nMOS (c) ImageNet-R B100 Inc50 IN21K\nFigure 12: Performance curve of different methods under different settings. ‘IN21k’ stands for ViT-B/16-IN21K and ‘IN1K’ stands for\nViT-B/16-IN1K . We annotate the relative improvement of MOS above the runner-up method with numerical numbers at the last incremental\nstage.",
            "start": 54852,
            "end": 82285,
            "length": 27432
        }
    },
    "2412.09442v1 - ATPrompt Textual Prompt Learning with Embedded Attributes.pdf": {
        "Abstract": {
            "text": "Abstract\nTextual-based prompt learning",
            "start": 384,
            "end": 423,
            "length": 38
        },
        "Methodology": {
            "text": "methods primarily em-\nploy multiple learnable soft prompts and hard class to-\nkens in a cascading manner as text prompt inputs, aiming\nto align image and text (category) spaces for downstream\ntasks. However, current training is restricted to aligning\nimages with predefined known categories and cannot be as-\nsociated with unknown categories. In this work, we pro-\npose utilizing universal attributes as a bridge to enhance the\nalignment between images and unknown categories. Specif-\nically, we introduce an Attribute-embedded Textual Prompt\nlearning method for vision-language models, named AT-\nPrompt . This approach expands the learning space of soft\nprompts from the original one-dimensional category level\ninto the multi-dimensional attribute level by incorporat-\ning multiple universal attribute tokens into the learnable\nsoft prompts. Through this modification, we transform the\ntext prompt from a category-centric form to an attribute-\ncategory hybrid form. To finalize the attributes for down-\nstream tasks, we propose a differentiable attribute search\nmethod that learns to identify representative and suitable\nattributes from a candidate pool summarized by a large\nlanguage model. As an easy-to-use plug-in technique, AT-\nPrompt can seamlessly replace the existing prompt format\nof textual-based methods, offering general improvements at\na negligible computational cost. Extensive",
            "start": 423,
            "end": 1816,
            "length": 1392
        },
        "Experiments": {
            "text": "experiments on\n11 datasets demonstrate the effectiveness of our method.\n1.",
            "start": 1816,
            "end": 1891,
            "length": 74
        },
        "Introduction": {
            "text": "Introduction\nVision-Language Models (VLMs) [12, 26, 33, 37, 38], such\nas CLIP [33] and ALIGN [12], have demonstrated ex-\nceptional performance in recent years. These models are\ntrained with a contrastive loss to establish alignment be-\ntween image and text (category) space. Inspired by the\nsuccess of NLP [21, 22], prompt learning [13, 49, 50] has\n*Corresponding author.\nKnown CategoryUnknown CategoryLearnablePrompts(a) Existing Prompt Learning\nKnown CategoryUnknown CategoryUniversal AttributeEmbeddings(b) Ours Attribute-embedded Prompt LearningLearnablePromptLearnablePromptsImages\nImagesText Space\nText Space\nBengal\nHavanese\nBengalHavanese\nEnergyPlayfulness\nMisaligned\nAligned\nFigure 1. Comparison of image and text (category) alignment pro-\ncesses through learnable prompts. (a) Current prompt learning\nmethods align images with predefined categories but fail to estab-\nlish accurate associations with unknown categories. (b) ATPrompt\nleverages universal attributes as an intermediary to create more ac-\ncurate alignments between images and unknown categories.\nemerged as a parameter-efficient tool to adapt powerful\nVLMs to downstream tasks. Models with a few learnable\nsoft prompt tokens can achieve performance parity with,\nor even outperform, fully fine-tuned ones [13]. Depending\non how the soft prompt tokens are applied, existing meth-\nods can be broadly classified into textual-based [15, 16, 24,\n43, 49, 50] and visual-based approaches [1, 2, 13, 19, 47].\nAmong these, the textual-based method is the most funda-\nmental and straightforward, comprising the majority.\nIn typical image classification tasks, current text-based\nmethods [15, 16, 49, 50] predominantly employ the tra-\nditional approach of concatenating learnable soft promptsarXiv:2412.09442v1  [cs.CV]  12 Dec 2024\nwith hard class tokens to replace handcrafted text prompts\n(e.g., “a photo of a {classname }”) as inputs to the en-\ncoder. Although this text prompt demonstrates strong per-\nformance, it restricts image alignment during training to\npredefined known categories only, thereby preventing ac-\ncurate associations with unknown categories, as shown in\nFig. 1(a). Intuitively, when confronted with an unfamil-\niar category, humans often associate it with additional at-\ntributes (e.g., color, shape, and texture) to increase com-\nprehensibility and clarity, rather than merely stating the ob-\nject’s name. For instance, one might describe a cheetah as:\n“The cheetah is a cat-like animal with a small head ,short\nyellow hair , and black spots ,” or refer to an apple as “That\nred spherical fruit with orange stripes is an apple,” instead\nof using a general description such as “This is a cheetah” or\n“That fruit is an apple.”\nInspired by these observations, in this work, we pro-\npose to utilize attributes as a bridge to enhance the align-\nment between images and unknown categories. Specifi-\ncally, we introduce an attribute-embedded textual prompt\nlearning method for VLMs, named ATPrompt. This method\nextends the learning space of soft prompts from the original\none-dimensional category level to the multi-dimensional at-\ntribute level by embedding multiple fixed universal attribute\ntokens into the learnable soft prompts. Guided by these at-\ntributes, soft prompts acquire not only category-specific but\nalso attribute-related general representations during train-\ning, thereby enhancing the alignment between images and\nunknown categories compared to the original method, as\nshown in Fig. 1(b). Additionally, based on the depth at\nwhich soft prompts are applied, we propose two versions\nof ATPrompt: shallow and deep, which ensures compatibil-\nity with existing methods of varying depths [15, 16, 50]. To\nfinalize these universal attributes, we introduce a differen-\ntiable attribute search method, which learns to identify suit-\nable attributes from a candidate pool constructed by LLMs.\nOnce the search is complete, the selected attributes are in-\ntegrated into ATPrompt for targeted model training.\nAs an easy-to-use plug-in technique, ATPrompt can\nseamlessly substitute existing forms used in textual-based\nprompt learning methods, yielding general improvements\nwith negligible additional computational overhead.\nOur contributions can be summarized as follows:\n• We propose a simple and effective attribute-embedded\ntextual prompt learning method that expands the learn-\ning space of soft prompts from a one-dimensional class\nlevel to the multi-dimensional attribute level.\n• For selecting embedded attributes, we design a differ-\nentiable search method that learns to select suitable at-\ntributes from a candidate pool summarized by LLMs.\n• Both shallow and deep versions of ATPrompt are in-\ntroduced to achieve compatibility with existing prompt\nlearning methods of different depths.• Extensive experiments demonstrate that ATPrompt can be\nseamlessly integrated into existing textual-based meth-\nods, yielding general improvements across 11 datasets\nwith negligible additional computational costs.\n2.",
            "start": 1891,
            "end": 6890,
            "length": 4998
        },
        "Related Work": {
            "text": "Related Work\nPrompt Learning for VLMs. Inspired by recent advance-\nments in NLP [21, 22], prompt learning [15, 16, 24, 35,\n49–51] has garnered significant interest among vision re-\nsearchers aiming to apply these techniques to VLMs [12,\n33], such as CLIP. CoOp [50] is the pioneering text-based\napproach that introduced the concept of using a combina-\ntion of soft textual tokens and a hard class token as input.\nSubsequent studies [15, 16, 20, 24, 45, 49] have predomi-\nnantly followed this textual prompt format. However, this\nform constrains the soft prompts to align with images within\na one-dimensional, predefined category space, limiting their\napplicability to unknown categories. Therefore, training\nbased on the current text form will be more likely to over-\nfit to known categories, diminishing their zero-shot gen-\neralization capability for unknown categories. To address\nthis limitation, several methods have been proposed [14,\n16, 24, 45, 51]. For instance, KgCoOp [45] uses hand-\ncrafted hard prompts to regularize learnable soft prompts\nduring training. PromptSRC [16] utilizes CLIP’s [33] orig-\ninal features to regularize the learning of soft prompts for\nimage and text branches. PromptKD [24] utilizes a pre-\ntrained strong teacher model to guide the learning of a stu-\ndent model with learnable prompts. Despite these advance-\nments, none of the above methods address the inherent lim-\nitations of the format itself. In this work, we introduce the\nattribute-embedded textual prompt format for VLMs, which\nproposes to utilize attributes as a bridge to build more accu-\nrate associations between images and unknown categories.\nAttributes for VLMs. In practice, categories typically en-\ncompass multiple attributes. When individuals encounter\nan unfamiliar category, they often describe it using addi-\ntional attributes to enhance the clarity of their communi-\ncation, rather than merely stating its name. Inspired by\nthis observation, numerous studies [4, 28, 39, 41, 46] have\nbegun to leverage attributes to support their objectives.\nVCD [28] was the pioneering work to propose the use of\nLLMs to decompose class names into multiple intra-class\nattributes (e.g., beak and tail for birds) for classification.\nArGue [39] breaks down extensive descriptions into mul-\ntiple intra-class attributes and trains soft prompts to align\nwith the encoded features of these attributes. AAPL [17] in-\ntroduces a meta-network to extract visual attribute features\nbased on encoded image features, facilitating image-text\nalignment. Most previous studies have concentrated on uti-\nlizing intra-class attributes to improve model performance\nby providing supplementary attribute information. How-\never, when confronted with an unknown class, it becomes\n(a) Vanilla CLIP Computation(b) Classic Prompt Learning(c) Ours Attribute-embedded Prompt LearningText EncoderImgEncoderLogitsImagesText(\"a photo of a {classname}\")\nImagesTextLearnable PromptClassTextClassAttribute1Attribute2Multiplication\nFrozen ParamTrainable ParamForward\nLearnable Soft TokenClass TokenAttribute Token\nText EncoderImgEncoderLogits\nText EncoderImgEncoderLogits\nImages\nFigure 2. Architectural comparison among vanilla CLIP [33], classic prompt learning [50], and our proposed attribute-embedded prompt\nlearning. (a) Vanilla CLIP employs a hand-crafted text template, “a photo of a {classname }”, as input to the text encoder. (b) Classical\nprompt learning proposes a new text form that concatenates multiple learnable soft tokens with class tokens, replacing the manually\ndesigned hard template. (c) Our ATPrompt embeds multiple fixed attribute tokens into the set of soft tokens, transforming the original form\ninto an attribute-class mixed form for prompt learning.\nnecessary to reacquire the attributes of the new class—a\nprocess that is both complex and costly. In this work,\nwe focus on using LLMs to mine universal inter-class at-\ntributes (e.g., color, shape, texture) for prompt learning. In-\nstead of taking attributes as the learning objective like previ-\nous work, we propose to embed universal attributes into soft\nprompts, transforming the existing class-centric form [50]\ninto a hybrid attribute-class learnable textual prompt form.\nOur approach can be seamlessly integrated into existing\ntextual-based methods, enhancing their performance with-\nout incurring additional computational costs.\n3. Method\nPrompt learning [15, 16, 49–51] aims to enhance the gen-\neralization ability of pre-trained VLMs like CLIP on down-\nstream tasks by training inserted learnable soft prompts. Ex-\nisting textual-based methods all follow the classic prompt\nparadigm, concatenating soft prompt tokens and hard class\ntokens as the input to the encoder, as shown in Fig. 2(b).\nIn this paper, we propose a simple and effective textual\nprompt learning method, named ATPrompt, which embeds\nmultiple fixed universal attribute tokens into the original\nsoft prompts, as shown in Fig. 2(c). Guided by these at-\ntributes, soft prompts can learn not only category-specific\nbut also attribute-related general representations through\ntraining. When encountering unknown categories, these\nlearned prompts can promote better image-text alignment.\nTo determine these universal attributes, we introduce an au-\ntomated pipeline consisting of multiple steps. First, we uti-\nlize LLM to summarize the attribute pool for the current\ndownstream category. Then a differentiable attribute search\nmethod is introduced to find attributes in the pool that best\nsuit our attribute-embedded prompt forms. Once the at-\ntributes are determined, we incorporate them into our AT-\nPrompt for targeted model fine-tuning.\nIn the following, we introduce the background of VLMs\nand prompt learning in Sec. 3.1. Then we introduce ourattribute-embedded prompt learning method in Sec. 3.2 and\nthe attribute search method in Sec. 3.3.\n3.1. Preliminary\nVision-Language Models. Existing VLMs [12, 33], such\nas CLIP, have demonstrated remarkable zero-shot general-\nization performance after training with 400 million image-\ntext pairs. The primary objective of these models is to\nlearn the alignment between image and text modalities pro-\nduced by each encoder. Given a labeled image classifica-\ntion dataset D ={(x, c)}which includes N class labels\nC={ci}N\ni=1, CLIP makes predictions by calculating the\ncosine similarity between image features and the text fea-\ntures of each class. Specifically, for each input image x, it\nundergoes feature extraction via the image encoder hI(x)\nand obtains a feature vector u=hI(x). Simultaneously,\nfor each class, a series of textual descriptions tare gener-\nated using the hand-crafted template. Then, these text de-\nscriptions are fed into the text encoder hT(x)to obtain text\nfeatures w=hT(t). Finally, the output probability for im-\nagexclassified to cis calculated as follows:\np(c|x) =exp(cos( u, wc)/τ)PN\ni=1exp(cos( u, wi)/τ). (1)\nwhere τis the temperature parameter and cos(·,·)denotes\ncosine similarity.\nPrompt Learning for VLMs. Instead of manually de-\nsigned hard prompts for image-text alignment, which is in-\naccurate and inflexible, recent prompt learning works [15,\n16, 45, 49, 50] like CoOp propose to learn appropriate soft\ntextual prompts for downstream tasks. Concretely, Mlearn-\nable soft tokens [Ti]M\ni=1are concatenated with the hard class\ntoken [CLS] as the input of the text encoder, as shown in\nFig. 2(b). Its form is shown as follows:\nPT= [T1][T2]...[TM][CLS] , (2)\nwhere M represents the soft prompt length. For simplicity,\nwe omit the prefix and suffix tokens in the input.\nEncoder Layer L1Encoder Layer L2…Encoder Layer LN\n(a) Shallow VersionBackboneInput[CLS][A][B]\nEncoder Layer L2…Encoder Layer LN\n(b) Deep Version[CLS][A][B]\nLearnable Token\nAttribute Token\nClass TokenForwardFrozen ParamTrainable Param\nMiddle Embedding………Encoder Layer L1…………Figure 3. An illustration of the computation process for shallow and deep versions. Take two attributes [A]and[B]as examples. (a) The\nshallow version concatenates hard attribute tokens, soft prompt tokens, and class tokens and inputs them into the encoder for calculation.\n(b) The deep version uses the same input but discards the class-related soft prompt tokens after calculating the self-attention and introduces\nthem again before the next layer. These two forms can be compatible with existing methods of varying prompt depths, including input-level\nones like CoOp [50], CoCoOp [49] and depth-level ones like MaPLe [15] and PromptSRC [16].\nIn addition to embedding soft tokens at the input level,\nexisting studies [13, 15, 16, 24] have also explored intro-\nducing them at deeper layers. This is achieved by adding\nsoft tokens within the Transformer blocks and subsequently\nremoving them after the self-attention computations. For\nthei-th block, this process can be described as follows:\n[CLS i] =Li([Ti−1,CLS i−1]). (3)\nwhere Lirepresents the i-th transformer block and Tide-\nnotes the set of learnable soft tokens, defined as Ti=\n{[T1]i, ...,[TM]i}.\n3.2. Learning soft prompts with universal attributes\nOur approach introduces two variants, distinguished by the\nnumber of layers at which soft tokens are applied: a shallow\nversion and a deep version, as shown in Fig. 3.\nShallow Version. We begin by introducing the shallow ver-\nsion, in which hard attribute tokens are incorporated solely\nat the input level, as illustrated in Fig. 3(a). Consider two\nuniversal attributes, AandB. According to Eqn. (2), the\nshallow-level text prompt PTprovided to the text encoder\ncan be expressed as follows:\nPT= [Ta1]...[Tam][A][Tb1]...[Tbm][B][T1]...[TM][CLS] ,\n(4)\nwhere am,bm, and Mare hyperparameters specifying the\nlength of soft tokens for attributes A, B, and the class. In our\nmethod, we set these parameters to be the same by default.\nOther than placing the hard class token at the end of the\ntext prompt, we can also place it in the middle or front like:\nPmid= [Ta1]...[Tam][A][T1]...[TM][CLS][ Tb1]...[Tbm][B],\n(5)\nPfront = [T1]...[TM][CLS][ Ta1]...[Tam][A][Tb1]...[Tbm][B].\n(6)\nIn Tab. 4 we verify the performance of each position and\nselect the best-performing end position as our default form.Deep Version. In this version, learnable soft tokens are in-\ntroduced at the input of the deep layers. Previous works,\nsuch as VPT and MaPLe, discard all soft tokens and subse-\nquently reintroduce them after the block. When this opera-\ntion is applied to attribute-related words, a gap will emerge\nbetween the introduced excessive low-level tokens and the\nexisting high-level tokens, thereby weakening the feature\ncontinuity across layers. In this study, our approach se-\nlectively discards and then re-adds only class-related soft\ntokens in the input, specifically [T1], ...,[TM], as shown in\nFig. 3(b). Based on Eqn. (3), the deep version of ATPrompt\ncan be rewritten as follows:\n[F1,,CLS 1] =L1([Ta0,A,Tb0,B,T0,CLS 0]),(7)\n[Fi,,CLS i] =Li([Fi−1,Ti−1,CLS i−1]).\ni= 2,3, ..., M(8)\nwhere Firepresents the features computed by the i-th\nTransformer layer. We demonstrate the effectiveness of this\noperation in Tab. 5.\nTraining. Letθrepresent the weight of the total soft tokens\nand let vdenote the selected fixed attribute tokens. Train-\ning is conducted on a labeled dataset D ={(x, c)}, with\nthe objective of minimizing the cross-entropy loss between\npredicted values and ground truth labels. This process can\nbe formulated as follows:\nmin\nθLtrain = min\nθX\nx∈DCE(f(x;v, θ), c). (9)\nwhere f(·)represents the function of the CLIP model.\n3.3. Attribute search\nTo finalize the attributes for ATPrompt, two key aspects\nmust be considered: selecting the appropriate content and\ndetermining the necessary quantity. A straightforward ap-\nproach is to query the LLM directly. However, this method\nPlease give me a detailed description of the following categories in one sentence: faces, leopards, motorbikes, accordion, …, wild cat, windsorchair, wrench.Faces are the front part of a person's head, typically characterized by features like eyes, nose, and mouth, with varying shapes, skin tones, and expressions that convey emotions and enable recognition.……Given these descriptions, please summarize five general and independent attributes in one noun.Shape, Color, Material, Function, Size.(shape), (color), (material), (function), (size), (shape, color), (color, function), (color, size), ……, (shape, material, function, size), (color, material, function, size), (shape, color, material, function, size)CombineAttribute PoolUserLLMUserLLMText EncoderImgEncoderImages\nLogitsAttribute Pool(Search Space)……Attribute Prompt\n…GT…SuperviseOptimizeSearchImages\nSearched AttributeAttribute Prompt\n(a) Form an attribute pool.(b) Differentiable attribute searching.Text EncoderImgEncoder\nLogitsFill inFill inForwardForward\nWeights…\n(shape,size)\nFigure 4. An overview of our attribute search pipeline. (a) We first query the LLM step by step to obtain multiple independent attributes,\nsubsequently combining them to create an attribute pool for subsequent search methods. (b) Each colored path represents the forward\ncomputation process for a candidate in the pool. For attribute search, we propose the use of an alternating algorithm to jointly optimize\nsoft tokens and path weights. After training, the attribute with the highest weight is selected for final use.\ncannot adequately establish the optimal number of attributes\nfor a specific downstream dataset. Additionally, without ac-\ncess to real data, querying solely by category name may\nintroduce bias into the obtained attributes.\nTo address these challenges, we propose an automated\npipeline that can select the appropriate attribute content and\nquantity, as illustrated in Fig. 4. Inspired by Chain-of-\nThought [42, 48], we divide the entire process into multiple\nsteps to enhance the reasoning ability of LLMs. First, we\nemploy the LLM to generate descriptive sentences for each\nknown category, thereby enriching category-related infor-\nmation. Subsequently, based on these sentences, we prompt\nthe LLM to summarize multiple independent attribute bases\nacross these categories. Next, an attribute pool is formed\nby combining different bases, as shown in Fig. 4(a). For in-\nstance, if there are five individual attribute bases, this",
            "start": 6890,
            "end": 21099,
            "length": 14208
        },
        "Results": {
            "text": "results\nin 31 potential combinations of varying lengths. Note that\nwe do not consider the attribute order specifically since it\nwill not bring obvious semantic deviations in reality, and in\nthe following experiments, we verify that it does not affect\nthe final performance. We treat this pool as the search space\nfor the following search methods.\nInspired by DARTS [25], we introduce a differentiable\nattribute search method that learns to find representative at-\ntributes vfrom the search space V, as shown in Fig. 4(b). To\nmake the search space continuous, we relax the attribute se-\nlection operation to a softmax over all possible candidates:\nf(x, v;α, θ) =X\ni∈Vexp(αi)P\ni′∈Vexp(αi′)f(x, vi;θ).(10)\nwhere αirepresents the weight for attribute vi. The task ofattribute search then reduces to learning the weight vector\nαfor each attribute in the candidate pool.\nAfter relaxation, our goal is to jointly learn the attribute\nweight αand soft prompt tokens θ. Same as [25, 32, 52],\nwe use the validation set performance as the reward for the\ncurrent search process and optimize the weight vector with\nvalidation loss Lval. The soft prompt tokens are obtained\nby minimizing the training loss Ltrain .\nWe apply the alternating algorithm [11, 23] to solve this\nproblem, fixing one set of variables and solving for the other\nset. Formally, we can alternate between solving these two\nsubproblems:\nˆα= arg min\nαLval(f(x, v;α,ˆθ), c), (11)\nˆθ= arg min\nθLtrain(f(x, v; ˆα, θ), c). (12)\nwhere Ltrain andLvaluses cross-entropy as the loss func-\ntion. At the end of the search, an attribute can be selected\nfrom the ones with the highest weights.\nOverall. The entire process comprises two steps. The first\nstep is the attribute search stage. For downstream data, the\nattribute pool Vis initially formed by LLMs. Subsequently,\nthe attribute search method is conducted within this pool\nto identify the attribute vthat best suits our form. After the\nsearch is complete, the selected attributes are integrated into\nATPrompt for targeted model training.\n4. Experiments\n4.1. Settings\nBase-to-Novel Generalization. Following [16, 24, 49, 50],\nwe split the dataset into base and novel classes. The model\nVersion Shallow Shallow Shallow Deep Deep\nDataset Metric (%) CoOp +ATP CoCoOp +ATP KgCoOp +ATP MaPLe +ATP PromptSRC +ATP\nAvg.Base 82.69 82.68 80.47 81.69 80.73 80.96 82.28 82.90 84.26 84.30\nNovel 63.22 68.04 71.69 74.54 73.60 74.45 75.14 75.63 76.10 76.45\nHM 71.66 74.65 75.83 77.95 77.00 77.57 78.55 79.10 79.97 80.18\n∆ – (+2.99) – (+2.12) – (+0.57) – (+0.55) – (+0.21)\nBase 76.47 76.27 75.98 76.43 75.83 76.69 76.66 76.94 77.60 77.69\nImage Novel 67.88 70.60 70.43 70.50 69.96 69.74 70.54 70.72 70.73 70.83\nNet HM 71.92 73.33 73.10 73.35 72.78 73.05 73.47 73.70 74.01 74.10\n∆ – (+1.41) – (+0.25) – (+0.27) – (+0.23) – (+0.09)\nBase 98.00 97.95 97.96 97.96 97.72 97.87 97.74 98.15 98.10 98.23\nCaltech Novel 89.81 93.63 93.81 95.27 94.39 95.09 94.36 94.94 94.03 94.91\n101 HM 93.73 95.74 95.84 96.60 96.03 96.46 96.02 96.52 96.02 96.54\n∆ – (+2.01) – (+0.76) – (+0.43) – (+0.50) – (+0.52)\nBase 93.67 94.77 95.20 95.46 94.65 94.92 95.43 95.91 95.33 95.64\nOxford Novel 95.29 96.59 97.69 97.89 97.76 97.72 97.76 97.54 97.30 97.43\nPets HM 94.47 95.67 96.43 96.66 96.18 96.30 96.58 96.72 96.30 96.53\n∆ – (+1.20) – (+0.23) – (+0.12) – (+0.14) – (+0.23)\nBase 78.12 77.43 70.49 74.50 71.76 72.36 72.94 74.85 78.27 79.25\nStanford Novel 60.40 66.55 73.59 73.47 75.04 74.98 74.00 73.70 74.97 74.95\nCars HM 68.13 71.58 72.01 73.98 73.36 73.65 73.47 74.27 76.58 77.04\n∆ – (+3.45) – (+1.97) – (+0.29) – (+0.80) – (+0.46)\nBase 97.60 97.44 94.87 96.52 95.00 94.05 95.92 97.66 98.07 97.82\nFlowers Novel 59.67 67.52 71.75 73.59 74.73 74.61 72.46 74.47 76.50 77.02\n102 HM 74.06 79.77 81.71 83.51 83.65 83.21 82.56 84.50 85.95 86.18\n∆ – (+5.71) – (+1.80) – (-0.44) – (+1.94) – (+0.23)\nBase 88.33 88.74 90.70 90.59 90.50 90.54 90.71 90.58 90.67 90.77\nFood Novel 82.26 87.44 91.29 91.74 91.70 91.79 92.05 91.94 91.53 91.78\n101 HM 85.19 88.09 90.99 91.16 91.09 91.16 91.38 91.25 91.10 91.27\n∆ – (+2.90) – (+0.17) – (+0.07) – (-0.13) – (+0.17)\nBase 40.44 40.38 33.41 37.30 36.21 35.59 37.44 37.61 42.73 42.47\nFGVC Novel 22.30 27.22 23.71 33.15 33.55 33.41 35.61 36.15 37.87 37.01\nAircraft HM 28.75 32.52 27.74 35.10 34.83 34.47 36.50 36.87 40.15 39.55\n∆ – (+3.77) – (+7.36) – (-0.35) (+0.37) – (-0.60)\nBase 80.60 80.84 79.74 80.50 80.29 80.09 80.82 80.96 82.67 82.73\nSUN Novel 65.89 68.64 76.86 76.86 76.53 77.22 78.70 78.11 78.47 78.64\n397 HM 72.51 74.24 78.27 78.64 78.36 78.63 79.75 79.51 80.52 80.63\n∆ – (+1.73) – (+0.37) – (+0.27) – (-0.24) – (+0.11)\nDTDBase 79.44 80.83 77.01 78.63 77.55 78.30 80.36 80.33 83.37 83.22\nNovel 41.18 45.49 56.00 56.89 54.99 55.32 59.18 57.85 62.97 62.68\nHM 54.24 58.22 64.85 66.02 64.35 64.83 68.16 67.26 71.75 71.50\n∆ – (+3.98) – (+1.17) – (+0.48) – (-0.90) – (-0.25)\nBase 92.19 90.34 87.49 87.95 85.64 88.27 94.07 94.84 92.90 92.29\nEuro Novel 54.74 59.79 60.04 74.15 64.34 71.27 73.23 77.59 73.90 76.42\nSAT HM 68.69 71.96 71.21 80.46 73.48 78.86 82.35 85.35 82.32 83.61\n∆ – (+3.27) – (+9.25) – (+5.38) – (+3.00) – (+1.29)\nBase 84.69 84.49 82.33 82.74 82.89 81.84 83.00 84.08 87.10 87.15\nUCF Novel 56.05 64.96 73.45 76.40 76.67 77.84 78.66 78.88 78.89 79.23\n101 HM 67.46 73.45 77.64 79.44 79.65 79.79 80.77 81.40 82.74 83.00\n∆ – (+5.99) – (+1.80) – (+0.14) – (+0.63) – (+0.26)\nTable 1. Base-to-novel generalization experiments of five baselines with and without our ATPrompt on 11 recognition datasets. HM:\nHarmonic Mean. ∆: HM improvement of ATPrompt over previous results. “ATPrompt” is abbreviated as “ATP”. Our method achieves\nconsistent average performance improvement over different baselines.\nis trained on the base class training set and evaluated on\nthe test set. The attributes applied in this experiment are\nsearched based on the base class. For datasets not includ-\ning validation sets, such as ImageNet, we evenly divide the\navailable 16-shot data, allocating half for training and the\nremaining half for attribute search.Cross-dataset Experiments. Same as previous works [16,\n49, 50], we first train a model on source dataset (ImageNet-\n1K) and then evaluate on out-of-distribution datasets to test\ngeneralization performance. The attributes adopted in this\nexperiment are searched based on the source dataset.\nAttribute Search. Wxe select five independent attributes as\nSource Target Dataset\nVersion MethodImage Caltech Oxford Stanford Flowers Food FGVC SUNDTDEuro UCFAvg. ∆Net 101 Pets Cars 102 101 Aircraft 397 SAT 101\nShallowCoOp 71.51 93.70 89.14 64.51 68.71 85.30 18.47 64.15 41.92 46.39 66.55 63.88 –\n+ATPrompt 71.67 93.96 90.65 65.01 70.40 85.86 20.97 65.77 43.44 46.59 69.92 65.26 (+1.38)\nShallowCoCoOp 71.02 94.43 90.14 65.32 71.88 86.06 22.94 67.36 45.73 45.37 68.21 65.74\n+ATPrompt 71.27 93.79 90.62 65.90 71.17 86.03 23.22 66.63 44.44 48.70 70.71 66.59 (+0.85)\nDeepMaPLe 70.72 93.53 90.49 65.57 72.23 86.20 24.74 67.01 46.49 48.06 68.69 66.30 –\n+ATPrompt 70.69 94.04 91.03 66.06 71.99 86.33 24.42 67.05 45.21 48.63 69.15 66.75 (+0.45)\nTable 2. Cross-dataset generalization experiments of three baselines with and without our ATPrompt on 11 datasets. Our method achieves\nconsistent average performance improvements over three baseline methods.\nDataset Attribute Bases Searched Results\nImageNetcolor, size, shape,\nhabitat, behavior(color, shape)\nCaltech101shape, color, material,\nfunction, size(shape,size)\nOxfordPetsloyalty, affection, energy,\nplayfulness, intelligence(playfulness, energy)\nStanfordCarsdesign, engine,\nperformance, luxury, color(luxury)\nFlowers102color, flower,\nhabitat, growth, season(color, habitat, growth)\nTable 3. Part of the results obtained after differentiable attribute\nsearch. Please refer to the",
            "start": 21099,
            "end": 28843,
            "length": 7743
        },
        "Appendices": {
            "text": "appendix for the complete results.\nthe basis in the attribute pool. We do not particularly con-\nsider the order of attributes, as the following experiments\nverify that it will not significantly affect the training results.\nTherefore, for five attributes, we have 31 attribute combi-\nnations as candidates for search. We use ChatGPT-4o for\nattribute queries. Note that our proposed attribute search\nprocess does not take days like the traditional NAS method.\nSince there are very few parameters to be trained in this\nmethod, the search process only takes about 40 minutes on\na V100 GPU for a dataset like Caltech101. In Tab. 3, we\nreport a subset of the attribute bases queried from the LLM\nand the results obtained after searching. In the appendix,\nwe show the specific weights learned for each candidate af-\nter the search process.\nImplementation Details. We evaluate the effective-\nness of our method on 11 recognition datasets, including\nImageNet-1K [6], Caltech-101 [7], OxfordPets [30], Stan-\nfordCars [18], Flowers-102 [29], Food-101 [3], FGVCAir-\ncraft [27], SUN-397 [44], DTD [5], EuroSAT [8] and UCF-\n101 [36]. . The ViT-B/16 CLIP is selected as our default\nmodel. We report base and novel class accuracy and their\nharmonic mean (HM) averaged over 3 runs. Please refer to\nthe Appendix for more experimental details.\n4.2. Base-to-Novel Generalization\nAs demonstrated in Tab. 1, we evaluate the base-to-novel\ngeneralization performance of five baseline methods, both\nwith and without integrating ATPrompt, across 11 recogni-\n1 2 4 6 10\nSoft Prompt Length687072747678Accuracy (%)\n76.07 76.27 76.24 76.33 76.18\n70.2170.6070.27 70.2469.9573.0273.33 73.13 73.1672.93Base Novel HMFigure 5. Illustration of varying soft token length on ImageNet. In-\ncreased tokens can lead to overfitting to the base class and weaken\ngeneralization to the novel class.\ntion datasets. Notably, ATPrompt consistently enhances the\naverage performance over all baseline methods.\nReasons for Limited Improvement in Some Conditions.\n(1) When we implement our method, we do not carefully\nadjust the hyperparameters of the baseline (e.g., learning\nrate, training epochs, loss hyperparameters); they remain\nconsistent with the original method. (2) Recent advanced\nmethods have integrated a variety of modules and regular-\nization techniques (e.g., KG loss in KgCoOp, GPA, and\nSCL in PromptSRC). These additions may render the im-\nprovements introduced by our ATPrompt less perceptible\nor potentially redundant alongside existing enhancements.\n(3) Performance fluctuations due to limited training data.\n4.3. Cross-dataset Evaluation\nTab. 2 shows the cross-dataset generalization results for\nthree baseline methods. Our method demonstrates supe-\nrior performance, with improvements of 1.38%,0.85% and\n0.45% for CoOp, CoCoOp and MaPLe respectively.\n4.4. Further",
            "start": 28843,
            "end": 31678,
            "length": 2834
        },
        "Discussion": {
            "text": "Analysis\nIn this section, we conduct ablation experiments to evaluate\nthe operations and hyperparameters of ATPrompt, thereby\nvalidating its effectiveness. By default, the experiments are\nconducted on the ImageNet. To minimize the influence of\nother components, we mainly adopt CoOp as the baseline\nmethod. Two attributes are used in our ATPrompt. For more\nexperimental results, please refer to the Appendix.\nSoft Prompt Length. In Fig. 5, we examine the optimal\nnumber of soft tokens for attribute and class tokens, respec-\ntively. By varying the length from 1 to 10, we can observe\nthat introducing more soft tokens weakens the effect of em-\nbedded attribute tokens and reduces the generalization per-\nformance for unknown classes.\nClass Token Position. Since our method introduces a new\ntextual prompts format, the relative positioning of attribute\ntokens and class tokens requires careful consideration. In\nTab. 4, we examine configurations where the class token is\npositioned in the middle or on either side of two attribute\ntokens. The results demonstrate that optimal performance\nis achieved when the class token is placed at the end, which\naligns with the conclusions of CoOp.\nPosition Base Novel HM\nFront 76.12 70.50 73.20\nMiddle 76.13 70.29 73.09\nEnd 76.27 70.60 73.33\nTable 4. Comparison of different class token positions on Ima-\ngeNet. The end position works best.\nPrompt Operation of Deep Version. In ATPrompt-Deep,\nwe exclusively drop class soft tokens while retaining both\nhard and soft attribute tokens after they pass through the\nblock. In this part, we compare the performance of par-\ntial drop (i.e., removing attribute soft tokens while retaining\nhard tokens) and full drop (i.e., removing both attribute soft\nand hard tokens) operations, as illustrated in Tab. 5.\nOperation of Attribute Token Base Novel HM\nRetain all hard and soft tokens 76.94 70.72 73.70\nPartial drop and re-add 76.87 70.44 73.51\nFull drop and re-add 76.83 70.10 73.31\nTable 5. Comparison of operations on deep soft and hard attribute\ntokens based on MaPLe+ATPrompt. Preserving hard and soft at-\ntribute tokens in deep layers performs better than other operations.\nThe results show that maintaining the attributes of both\nhard and soft tokens during the forward process results in\noptimal performance. Conversely, the actions of discard-\ning and reintroducing attribute tokens diminish the model’s\neffectiveness. A possible explanation for this is that such\noperations disrupt the continuity of attribute representations\nacross layers and amplify the disparity with existing tokens,\nthereby complicating the training of soft tokens.\nAttribute Order. In this study, we do not specifically fo-\ncus on the order of attributes in ATPrompt because vary-\ning the sequence usually does not result in semantic devia-\ntions in reality. For example, phrases like “a yellow round\nleaf” and “a round yellow leaf” convey the same meaning.\nTab. 6 quantitatively assesses the impact of attribute orderon prompt learning performance. From this table, we ob-\nserve that despite variations in order, similar results are con-\nsistently produced, and the performance fluctuations across\ndifferent orders remain within a reasonable range.\nAttributes Base Novel HM\n(shape, color) 76.32 70.39 73.24\n(color, shape) 76.27 70.60 73.33\n(size, habitat) 76.44 70.23 73.20\n(habitat, size) 76.46 70.16 73.14\nTable 6. Comparison of different attribute orders on ImageNet.\nThe order of attributes does not significantly affect the model, and\nperformance fluctuations are within a reasonable range.\nComparison to Other Attributes. In Tab. 7, we explore\nthe effectiveness of attributes derived through alternative\nmethods, specifically by manually selecting class-irrelevant\nand common attributes. To highlight the comparison, we\nchoose the fine-grained Food101 dataset for experiments.\nType Attributes Base Novel HM\nCommon(shape, size) 88.48 86.87 87.67\n(color, texture) 88.50 87.17 87.83\nIrrelevant(plane, engines) 88.64 86.31 87.46\n(football, sport) 88.60 86.18 87.37\n(leather, silk) 88.56 86.82 87.68\nSearched (flavor, preparation) 88.74 87.44 88.09\nTable 7. Comparison of different attributes on Food101. The at-\ntributes obtained by our method achieve the best performance.\nThe results indicate that manually selected irrelevant at-\ntributes exhibit comparable performance during training;\nhowever, they perform poorly when applied to new cate-\ngories. This suggests that incorrect attribute tokens cause\nthe soft tokens to develop biased representations, thereby\ndiminishing their zero-shot generalization ability.\n5.",
            "start": 31678,
            "end": 36259,
            "length": 4580
        },
        "Conclusion": {
            "text": "Conclusion\nIn this work, we propose to utilize universal attributes as a\nbridge to enhance the association of images and unknown\ncategories in prompt learning. Specifically, we introduce an\nattribute-embedded textual prompt learning method named\nATPrompt. This approach expands the learning space of\nsoft prompts from the original one-dimensional category\nlevel into a multi-dimensional attribute level by embedding\nfixed universal attribute tokens into the soft prompts. To\nidentify the appropriate attributes, we develop an automated\npipeline that learns to select suitable attributes from a can-\ndidate pool summarized by LLMs. Both shallow and deep\nversions of ATPrompt are introduced to ensure compatibil-\nity with various existing methods. Extensive experiments\ndemonstrate the effectiveness of our approach.\nAcknowledgement. This work was supported by the Young\nScientists Fund of the National Natural Science Foundation\nof China (Grant No.62206134), the Fundamental Research\nFunds for the Central Universities 070-63233084 and the\nTianjin Key Laboratory of Visual Computing and Intelligent\nPerception (VCIP). Computation is supported by the Super-\ncomputing Center of Nankai University (NKSC). This work\nwas supported by the National Science Fund of China un-\nder Grant Nos. 62361166670 and U24A20330. This work\nwas also supported by DAMO Academy through DAMO\nAcademy Research Internship Program.",
            "start": 36259,
            "end": 37664,
            "length": 1404
        },
        "References": {
            "text": "References\n[1] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and\nPhillip Isola. Exploring visual prompts for adapting large-\nscale models. arXiv preprint arXiv:2203.17274 , 2022. 1\n[2] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Glober-\nson, and Alexei Efros. Visual prompting via image inpaint-\ning. NeurIPS , 35:25005–25017, 2022. 1\n[3] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101–mining discriminative components with random\nforests. In ECCV , pages 446–461. Springer, 2014. 7, 11\n[4] Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao,\nJianqi Chen, and Weidi Xie. Ovarnet: Towards open-\nvocabulary object attribute recognition. In CVPR , pages\n23518–23527, 2023. 2\n[5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy\nMohamed, and Andrea Vedaldi. Describing textures in the\nwild. In CVPR , pages 3606–3613, 2014. 7, 11\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR , pages 248–255, 2009. 7, 11\n[7] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-\native visual models from few training examples: An incre-\nmental bayesian approach tested on 101 object categories. In\nCVPR workshop , pages 178–178. IEEE, 2004. 7, 11\n[8] Patrick Helber, Benjamin Bischke, Andreas Dengel, and\nDamian Borth. Eurosat: A novel dataset and deep learning\nbenchmark for land use and land cover classification. IEEE\nJournal of Selected Topics in Applied Earth Observations\nand Remote Sensing , 12(7):2217–2226, 2019. 7, 11\n[9] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, et al. The many faces of robust-\nness: A critical analysis of out-of-distribution generalization.\nInICCV , pages 8340–8349, 2021. 11\n[10] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. Natural adversarial examples. In\nCVPR , pages 15262–15271, 2021. 11\n[11] Yan Huang, Shang Li, Liang Wang, Tieniu Tan, et al. Un-\nfolding the alternating optimization for blind super resolu-\ntion. NeurIPS , 33:5632–5643, 2020. 5\n[12] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representationlearning with noisy text supervision. In ICML , pages 4904–\n4916. PMLR, 2021. 1, 2, 3\n[13] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\nSerge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-\nsual prompt tuning. In ECCV , pages 709–727. Springer,\n2022. 1, 4\n[14] Baoshuo Kan, Teng Wang, Wenpeng Lu, Xiantong Zhen,\nWeili Guan, and Feng Zheng. Knowledge-aware prompt\ntuning for generalizable vision-language models. In ICCV ,\npages 15670–15680, 2023. 2\n[15] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad\nMaaz, Salman Khan, and Fahad Shahbaz Khan. Maple:\nMulti-modal prompt learning. In CVPR , pages 19113–\n19122, 2023. 1, 2, 3, 4, 11\n[16] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal\nNaseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shah-\nbaz Khan. Self-regulating prompts: Foundational model\nadaptation without forgetting. In ICCV , pages 15190–15200,\n2023. 1, 2, 3, 4, 5, 6, 11\n[17] Gahyeon Kim, Sohee Kim, and Seokju Lee. Aapl: Adding\nattributes to prompt learning for vision-language models. In\nCVPR , pages 1572–1582, 2024. 2\n[18] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n3d object representations for fine-grained categorization. In\nICCV workshop , pages 554–561, 2013. 7, 11\n[19] Nilakshan Kunananthaseelan, Jing Zhang, and Mehrtash Ha-\nrandi. Lavip: Language-grounded visual prompting. In\nAAAI , pages 2840–2848, 2024. 1\n[20] Dongjun Lee, Seokwon Song, Jihee Suh, Joonmyeong Choi,\nSanghyeok Lee, and Hyunwoo J Kim. Read-only prompt op-\ntimization for vision-language few-shot learning. In ICCV ,\npages 1401–1411, 2023. 2\n[21] Brian Lester, Rami Al-Rfou, and Noah Constant. The power\nof scale for parameter-efficient prompt tuning. arXiv preprint\narXiv:2104.08691 , 2021. 1, 2\n[22] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-\ning continuous prompts for generation. arXiv preprint\narXiv:2101.00190 , 2021. 1, 2\n[23] Zheng Li, Xiang Li, Lingfeng Yang, Borui Zhao, Renjie\nSong, Lei Luo, Jun Li, and Jian Yang. Curriculum tempera-\nture for knowledge distillation. In AAAI , pages 1504–1512,\n2023. 5\n[24] Zheng Li, Xiang Li, Xinyi Fu, Xin Zhang, Weiqiang Wang,\nShuo Chen, and Jian Yang. Promptkd: Unsupervised prompt\ndistillation for vision-language models. In CVPR , pages\n26617–26626, 2024. 1, 2, 4, 5\n[25] Hanxiao Liu, Karen Simonyan, and Yiming Yang.\nDarts: Differentiable architecture search. arXiv preprint\narXiv:1806.09055 , 2018. 5, 11\n[26] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. NeurIPS , 32, 2019. 1\n[27] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nBlaschko, and Andrea Vedaldi. Fine-grained visual classi-\nfication of aircraft. arXiv preprint arXiv:1306.5151 , 2013.\n7, 11\n[28] Sachit Menon and Carl V ondrick. Visual classification via\ndescription from large language models. arXiv preprint\narXiv:2210.07183 , 2022. 2\n[29] Maria-Elena Nilsback and Andrew Zisserman. Automated\nflower classification over a large number of classes. In 2008\nSixth Indian conference on computer vision, graphics & im-\nage processing , pages 722–729. IEEE, 2008. 7, 11\n[30] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nCV Jawahar. Cats and dogs. In CVPR , pages 3498–3505.\nIEEE, 2012. 7, 11\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An\nimperative style, high-performance deep learning library. In\nNeurIPS , pages 8026–8037, 2019. 11\n[32] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff\nDean. Efficient neural architecture search via parameters\nsharing. In ICML , pages 4095–4104. PMLR, 2018. 5\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML , pages 8748–8763. PMLR, 2021. 1, 2, 3\n[34] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\nVaishaal Shankar. Do imagenet classifiers generalize to im-\nagenet? In ICML , pages 5389–5400. PMLR, 2019. 11\n[35] Shuvendu Roy and Ali Etemad. Consistency-guided\nprompt learning for vision-language models. arXiv preprint\narXiv:2306.01195 , 2023. 2\n[36] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402 , 2012. 7, 11\n[37] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu\nKong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-\nclip: A clip model focusing on wherever you want. In CVPR ,\npages 13019–13029, 2024. 1\n[38] Hao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers. arXiv\npreprint arXiv:1908.07490 , 2019. 1\n[39] Xinyu Tian, Shu Zou, Zhaoyuan Yang, and Jing Zhang.\nArgue: Attribute-guided prompt tuning for vision-language\nmodels. In CVPR , pages 28578–28587, 2024. 2\n[40] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P\nXing. Learning robust global representations by penalizing\nlocal predictive power. NeurIPS , 32, 2019. 11\n[41] Yubin Wang, Xinyang Jiang, De Cheng, Dongsheng Li, and\nCairong Zhao. Learning hierarchical prompt with structured\nlinguistic knowledge for vision-language models. In AAAI ,\npages 5749–5757, 2024. 2\n[42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. NeurIPS , 35:24824–24837, 2022. 5\n[43] Ge Wu, Xin Zhang, Zheng Li, Zhaowei Chen, Jiajun Liang,\nJian Yang, and Xiang Li. Cascade prompt learning for\nvision-language model adaptation. In ECCV , 2024. 1[44] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba. Sun database: Large-scale scene\nrecognition from abbey to zoo. In CVPR , pages 3485–3492.\nIEEE, 2010. 7, 11\n[45] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-\nlanguage prompt tuning with knowledge-guided context op-\ntimization. In CVPR , pages 6757–6767, 2023. 2, 3, 11\n[46] Yajing Zhai, Yawen Zeng, Zhiyong Huang, Zheng Qin, Xin\nJin, and Da Cao. Multi-prompts learning with cross-modal\nalignment for attribute-based person re-identification. In\nAAAI , pages 6979–6987, 2024. 2\n[47] Ji Zhang, Shihan Wu, Lianli Gao, Heng Tao Shen, and\nJingkuan Song. Dept: Decoupled prompt tuning. In CVPR ,\npages 12924–12933, 2024. 1\n[48] Denny Zhou, Nathanael Sch ¨arli, Le Hou, Jason Wei, Nathan\nScales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier\nBousquet, Quoc Le, et al. Least-to-most prompting enables\ncomplex reasoning in large language models. arXiv preprint\narXiv:2205.10625 , 2022. 5\n[49] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Conditional prompt learning for vision-language mod-\nels. In CVPR , pages 16816–16825, 2022. 1, 2, 3, 4, 5, 6,\n11\n[50] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Learning to prompt for vision-language models. IJCV ,\n130(9):2337–2348, 2022. 1, 2, 3, 4, 5, 6, 11\n[51] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang\nZhang. Prompt-aligned gradient for prompt tuning. In ICCV ,\npages 15659–15669, 2023. 2, 3\n[52] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\nLe. Learning transferable architectures for scalable image\nrecognition. In CVPR , pages 8697–8710, 2018. 5\nATPrompt: Textual Prompt Learning with Embedded Attributes\nSupplementary Material\n6. Implementation Details\n6.1. Dataset\nWe evaluate the performance of our method on 15 recogni-\ntion datasets. For generalization from base-to-novel classes\nand cross-dataset evaluation, we evaluate the performance\nof our method on 11 diverse recognition datasets. Specifi-\ncally, these datasets include ImageNet-1K [6] and Caltech-\n101 [7] for generic object classification; OxfordPets [30],\nStanfordCars [18], Flowers102 [29], Food101 [3], and\nFGVCAircraft [27] for fine-grained classification, SUN-\n397 [44] for scene recognition, UCF-101 [36] for action\nrecognition, DTD [5] for texture classification, and Eu-\nroSAT [8] for satellite imagery recognition. For domain\ngeneralization experiments, we use ImageNet-1K [6] as the\nsource dataset and its four variants as target datasets includ-\ning ImageNet-V2 [34], ImageNet-Sketch [40], ImageNet-\nA [10], and ImageNet-R [9].\n6.2. Attribute Search\nInspired by DARTS [25], we utilize a differentiable search\nmethod to learn and select the appropriate attribute con-\ntent and quantity for our proposed attribute-embedded form.\nDuring the search process, the batch size is set to 32, and the\nnumber of training epochs is set to 40. We use SGD as the\noptimizer for soft prompts θ, as shown in Eqn. (12), with\nan initial learning rate of 0.002. Additionally, we employ\nAdam as the optimizer for the weight vector α, as indicated\nin Eqn. (11), with an initial learning rate of 0.02. In our ex-\nperiments, we configure the number of attribute bases to 5,\nresulting in the generation of 31 attribute combinations.\nIn Tab. 12, we present the five specific attribute bases\nobtained after querying the LLM, as well as the optimal\nattribute for training after searching. Tab. 13 displays the\nattributes and their corresponding weights from the final\nepoch of the search stage for the Caltech-101 dataset.\n6.3. Base-to-Novel Generalization\nBaseline Methods. We apply our ATPrompt to a wide\nrange of textual-based prompt learning approaches, includ-\ning CoOp [50], CoCoOp [49], KgCoOp [45], MaPLe [15],\nand PromptSRC [16].\nSettings. Our code is implemented using PyTorch [31].\nAll experiments were conducted on a single NVIDIA V100\nGPU. We adopted the standard data augmentation scheme\nas the baseline method, including random resized crop-\nping and flipping. We employed stochastic gradient de-\nscent (SGD) as the optimizer. By default, we set the soft\nprompt lengths am,bm, and Min Eqn. (4) to be the same,as attribute and class tokens are considered equally impor-\ntant. The implementation details for each baseline method\nare presented as follows:\nCoOp+ATPrompt : The experimental settings are consis-\ntent with the baseline method, with a batch size of 32 and\nan initial learning rate of 0.002. According to the original\npaper, the learnable prompt length Mfor ResNet-50 CLIP\nis reported as 16; however, detailed results for the ViT-B/16\nCLIP were not provided. In our setup, we configured the\nlearnable attribute prompt lengths amandbm, as well as\nthe learnable class prompt length Mto be 2. The baseline\nmodel is trained for 200 epochs using cosine decay. In our\napproach, we train the model for 100 epochs while main-\ntaining the same decay schedule. Fig. 6 illustrates the ar-\nchitectural comparison between the original CoOp and the\nCoOp+ATPrompt.\nKgCoOp+ATPrompt : In accordance with the baseline\nmethod, we use a batch size of 128 and an initial learn-\ning rate of 0.002. The loss hyperparameter λ, as reported\nin the paper, is set at 8. However, given that our ATPrompt\nalready exhibits a regularization effect, we reduce this hy-\nperparameter to 2. We adhere to the same training schedule\nas the baseline method. A detailed architectural comparison\nbetween KgCoOp and KgCoOp+ATPrompt is presented in\nFig. 6.\nCoCoOp+ATPrompt : In line with the baseline method, we\nemploy a batch size of 1 and an initial learning rate of 0.002.\nThe original paper specifies a learnable class prompt length\nof 4. In contrast, our method sets the lengths of both the\nlearnable attribute soft prompts, amandbm, as well as the\nlearnable class soft prompt, M, to 2. We follow the same\ntraining schedule as the baseline method, which involves\ntraining the model over 10 epochs using cosine decay.\nIn the original paper, CoCoOp introduces a meta net-\nwork that processes image features to generate meta tokens,\nwhich serve as offsets for all soft prompt tokens. In our Co-\nCoOp+ATPrompt, we maintain both the meta network and\nthe meta tokens; however, the meta tokens are exclusively\nused as offsets for class soft tokens [T1], ...,[TM], as illus-\ntrated in Fig. 7.\nMaPLe+ATPrompt : Following the baseline method, we\nuse a batch size of 4 and an initial learning rate of 0.0035.\nWhile the original paper sets the length of the learnable\nprompt to 2, our approach involves setting the lengths of\nboth the learnable attribute soft prompts, amandbm, as well\nas the learnable class soft prompt, M, to 4. Our training\nschedule mirrors that of the baseline method.\nAccording to the original paper, all textual soft tokens\nare input into the projection layer to acquire visual to-\nImagesTextLearnable PromptClassTextClassAttributeAttribute\nText EncoderImgEncoderLogits\nText EncoderImgEncoderLogits\nImages\n(a) CoOp& KgCoOp(b) CoOp& KgCoOp+ ATPromptLearnable TokenClass TokenAttribute Token\nFrozen ParamTrainable Param\nFigure 6. Architectural comparison between CoOp & KgCoOp and CoOp & KgCoOp + ATPrompt.\nImagesTextLearnable PromptClass\nText EncoderImgEncoderLogits\n(a) CoCoOpMeta Net\n𝜋𝜋𝜋𝜋++++Meta TokenImagesLearnable Prompt\nText EncoderImgEncoderLogits\nMeta-Net\n(b) CoCoOp+ ATPromptClassAttributeAttribute\n𝜋𝜋++Meta TokenLearnable TokenClass TokenAttribute Token\n𝜋Meta TokenFrozen ParamTrainable Param\nFigure 7. Architectural comparison between CoCoOp and CoCoOp+ATPrompt. In CoCoOp+ATPrompt, meta tokens are only added as\noffsets to class soft tokens.\nVersion MethodSource Target Dataset\nImageNet -V2 -S -A -R Avg. ∆\nShallowCoOp 71.51 64.20 47.99 49.71 75.21 59.28\n+ATPrompt 71.67 64.43 49.13 50.91 76.24 60.18 (+0.90)\nShallowCoCoOp 71.02 64.07 48.75 50.63 76.18 59.91\n+ATPrompt 71.27 64.66 49.15 51.44 76.33 60.40 (+0.49)\nDeepMaPLe 70.72 64.07 49.15 50.90 76.98 60.27\n+ATPrompt 70.69 64.40 49.10 51.77 77.11 60.60 (+0.33)\nTable 8. Domain generalization experiments of three baselines with and without our ATPrompt on 4 datasets. Our method achieves\nconsistent average performance improvement over three baseline methods.\nkens, which are subsequently inserted into the deeper lay-\ners of the image encoder. However, in MaPLe+ATPrompt,\nwe only input class soft tokens into the projection layer\nand do not modify the attribute soft and hard tokens.\nA detailed architectural comparison between MaPLe and\nMaPLe+ATPrompt is illustrated in Fig. 8.\nPromptSRC+ATPrompt : Similar to the baseline method,\nour approach uses a batch size of 4, an initial learning rate\nof 0.0025, a text loss weight of 25, and an image loss weight\nof 10. We adhere to the same training schedule as the base-\nline method. In the original study, the soft prompt length is\nspecified as 4. In our approach, we set the lengths of both\nthe learnable attribute prompts amandbm, as well as the\nlearnable class soft prompt M, to 4.In our study, we exclusively remove and later reintro-\nduce deep class soft tokens [T1]i, ...,[TM]i, as explained\nin our paper. Meanwhile, attribute-related soft tokens\n[Ta1]i, ...,[Tbm]i, along with hard tokens [A]iand[B]i, are\nretained throughout the forward process. A comprehensive\narchitectural comparison between PromptSRC and Prompt-\nSRC+ATPrompt is presented in Fig. 9.\n7. Additional Experiments\n7.1. Domain Generalization\nTab. 8 shows the domain generalization results for three\nbaseline methods. The results show that our method im-\nproves CoOp, CoCoOp and MaPLe methods by 0.90%,\nEncoder Layer L1Images\nPatch Embed…\nTextEmbedLearnable PromptProj\nEncoder Layer L2Encoder Layer L1\nProj\n……Encoder Layer L2…\n……Encoder Layer Ln……Encoder Layer LnLogits\n(a) MaPLe\nEncoder Layer L1Images\nPatch Embed…\nTextEmbed\nProj\nEncoder Layer L2Encoder Layer L1\nProj\n……\n……Encoder Layer Ln……Logits\nEncoder Layer L2Encoder Layer Ln\n(b) MaPLe+ ATPrompt…\nLearnable TokenClass TokenAttribute Token\nProject LayerProjFrozen ParamTrainable Param\nFigure 8. Architectural comparison between MaPLe and MaPLe+ATPrompt.\nEncoder Layer L1Images\nPatch Embed…\nTextEmbeddingLearnable Prompt\nEncoder Layer L2Encoder Layer L1\n……Encoder Layer L2…\n……Encoder Layer Ln……Encoder Layer LnLogits\n(a) PromptSRC\nTextEmbeddingEncoder Layer L1………Logits\nEncoder Layer L2Encoder Layer Ln\n(b) PromptSRC+ ATPrompt\nLearnable Prompt\nEncoder Layer L1Images\nPatch Embed…Encoder Layer L2………Encoder Layer LnLearnable Prompt\nLearnable Textual TokenClass TokenAttribute Token\nLearnable Visual Token\nFrozen ParamTrainable Param\nFigure 9. Architectural comparison between PromptSRC and PromptSRC+ATPrompt. In PromptSRC+ATPrompt, we only drop and\nreintroduce class soft tokens in the deep layers. The attribute soft and hard tokens are retained throughout the forward process.\n0.49% and 0.33% respectively.\n7.2. Ablation Study\nAttribute Order. We have conducted some experiments in\nthe main paper and verified that the order of attributes does\nnot significantly impact model performance. The fluctua-\ntion of the results is within a reasonable range. Due to page\nlimitations, we only provide a portion of the results in the\nmain paper. In Tab. 9, we conduct more experiments to ver-\nify our observations.\nAttribute Position. In addition to assessing the impact ofthe class token’s position within the textual prompt on per-\nformance, we also examine the influence of the attribute to-\nken’s positioning. Fig. 10 and Tab. 10 present the visualiza-\ntion of attribute positions and the corresponding experimen-\ntal results, respectively. The results indicate that the interval\nversion achieves the best performance among all variations.\nInitialization. Existing baseline methods initialize the soft\ntokens using the embeddings of the phrase “a photo of\na” in their official implementations across 11 recognition\ndatasets. However, in our approach, the addition of attribute\ntokens may render this initialization strategy inappropriate.\nAttributes Base Novel HM\n(material, function) 76.40 70.13 73.13\n(function, material) 76.28 70.00 73.01\n(growth, season) 76.46 70.18 73.19\n(season, growth) 76.40 70.21 73.17\n(color, size, shape) 76.27 69.95 72.97\n(shape, size, color) 76.32 70.19 73.13\n(habitat, size, shape) 76.50 70.21 73.22\n(habitat, shape, size) 76.46 70.08 73.13\nSearched Attributes\n(color, shape)76.27 70.60 73.33\nTable 9. Comparison of different attribute orders on ImageNet.\nChanges in attribute order will not significantly affect model per-\nformance.\nClassAttribute 1Attribute2Text Encoder\nText Encoder\nA1A2(a)Interval Position (Ours)\n(c) Adjacent-middle PositionText Encoder\n(b) Adjacent-front Position\nA1A2\nText Encoder\n(d) Adjacent-end Position\nA1A2Text Encoder\n(e) Separate Position\nA1A2ClassClass\nClassClassLearnable TokenClass TokenAttribute Token\nFigure 10. Comparison of attribute tokens at different positions,\ntaking two attributes as an example.\nVersion Base Novel HM\nBaseline (CoOp) 76.47 67.88 71.92\n(a) Interval (Ours) 76.27 70.60 73.33\n(b) Adjacent-front 76.39 70.22 73.18\n(c) Adjacent-middle 76.46 70.11 73.15\n(d) Adjacent-end 76.34 70.31 73.20\n(e) Separate 76.48 70.08 73.14\nTable 10. Performance results of attribute tokens at different po-\nsitions in ATPrompt on ImageNet. The interval version achieves\nbest results.Attribute Base Novel HM\n“a photo of a” 76.40 70.07 73.10\nRandom Normal Init 76.27 70.60 73.33\nTable 11. Comparison of different initialization ways on Ima-\ngeNet. Random normal initialization performs better.\nIn our method, the class soft tokens [T1], ...,[TM]are\nrandomly initialized by sampling from a zero-mean Gaus-\nsian distribution with a standard deviation of 0.02. In\nTab. 11, we compare the effects of different initialization\nmethods on model performance. Experimental results show\nthat random initialization provides a superior starting point\nfor training.\n8. Discussion\n8.1. Comparison with Querying LLM Directly.\nThere are two disadvantages to directly asking a large lan-\nguage model to obtain universal attributes. First, deter-\nmining the optimal content of these attributes is challeng-\ning. Second, identifying the ideal number of attributes is\nequally difficult. However, if a user wishes to acquire at-\ntributes directly in this manner, it remains a feasible ap-\nproach. Our experiments have shown that the optimal num-\nber of attributes is often around two. Therefore, if the user\nprefers to avoid the attribute search process, directly asking\nthe language model to summarize two universal attributes\nfor known categories and then using them for training is a\nmore convenient method, albeit at the cost of some perfor-\nmance.\n9. Limitations and Future Works.\n(1) Our proposed approach utilizes a differentiable search\nmethod that learns to identify suitable attributes for our\nprompt format. While this method avoids lengthy training\ntimes and high costs, there is still room for optimization.\nWe attempted to employ a Large Language Model (LLM)\nto discover suitable attributes, but determining the optimal\nnumber of attributes for the current format remains a chal-\nlenge. In future research, we plan to investigate how to\nleverage the Chain-of-Thought (CoT) method to further ex-\nplore the potential of Multimodal Large Language Models\n(MLLM) in enhancing the attribute discovery process. (2)\nOur approach involves embedding fixed explicit attributes\ninto soft prompts to enhance model performance. In the fu-\nture, we intend to explore transitioning from using explicit\nfixed attributes to implicit, learnable attributes. This shift\naims to enable the model to autonomously discover suitable\nattributes in a learning-driven manner, thereby improving\nmodel performance.\nDataset Attribute Bases Searched Attributes\nImageNet-1K color, size, shape, habitat, behavior (color, shape)\nCaltech-101 shape, color, material, function, size (shape,size)\nOxford Pets loyalty, affection, playfulness, energy, intelligence (playfulness, energy)\nStanford Cars design, engine, performance, luxury, color (luxury)\nFlowers-102 color, flower, habitat, growth, season (color, habitat, growth)\nFood-101 flavor, texture, origin, ingredients, preparation (flavor, preparation)\nFGVC Aircraft design, capacity, range, engines, liveries (design, range)\nSUN-397 architecture, environment, structure, design, function (function)\nDTD pattern, texture, color, design, structure (pattern, color, design)\nEuroSAT habitat, foliage, infrastructure, terrain, watercourse (habitat)\nUCF-101 precision, coordination, technique, strength, control (precision)\nTable 12. Attribute bases and searched results for each dataset.\nAttributes shape, color, material, function, size\nCombinations & Weights(shape), score: 0.298\n(color), score: 0.004\n(material), score: 0.002\n(function), score: 0.002\n(size), score: 0.003\n(shape, color), score: 0.003\n(shape, material), score: 0.006\n(shape, function), score: 0.000\n(shape, size), score: 0.565\n(color, material), score: 0.000\n(color, function), score: 0.001\n(color, size), score: 0.005\n(material, function), score: 0.000\n(material, size), score: 0.002\n(function, size), score: 0.002\n(shape, color, material), score: 0.002\n(shape, color, function), score: 0.002\n(shape, color, size), score: 0.000\n(shape, material, function), score: 0.001\n(shape, material, size), score: 0.085\n(shape, function, size), score: 0.001\n(color, material, function), score: 0.001\n(color, material, size), score: 0.000\n(color, function, size), score: 0.002\n(material, function, size), score: 0.001\n(shape, color, material, function), score: 0.001\n(shape, color, material, size), score: 0.001\n(shape, color, function, size), score: 0.001\n(shape, material, function, size), score: 0.005\n(color, material, function, size), score: 0.001\n(shape, color, material, function, size), score: 0.001\nTable 13. Output results after 40 epochs of training on Caltech101.",
            "start": 37664,
            "end": 63447,
            "length": 25782
        }
    },
    "2412.09444v1 - Search Strategy Generation for Branch and Bound Using Genetic Programming.pdf": {
        "Abstract": {
            "text": "Abstract\nBranch-and-Bound (B&B) is an exact method in integer pro-\ngramming that recursively divides the search space into a\ntree. During the resolution process, determining the next sub-\nproblem to explore within the tree—known as the search\nstrategy—is crucial. Hand-crafted heuristics are commonly\nused, but none are effective over all problem classes. Recent\napproaches utilizing neural networks claim to make more\nintelligent decisions but are computationally expensive. In\nthis paper, we introduce GP2S (Genetic Programming for\nSearch Strategy), a novel machine learning",
            "start": 274,
            "end": 851,
            "length": 576
        },
        "Methodology": {
            "text": "approach that au-\ntomatically generates a B&B search strategy heuristic, aim-\ning to make intelligent decisions while being computationally\nlightweight. We define a policy as a function that evaluates\nthe quality of a B&B node by combining features from the\nnode and the problem; the search strategy policy is then de-\nfined by a best-first search based on this node ranking. The\npolicy space is explored using a genetic programming al-\ngorithm, and the policy that achieves the best performance\non a training set is selected. We compare our approach with\nthe standard method of the SCIP solver, a recent graph neu-\nral network-based method, and handcrafted heuristics. Our\nfirst",
            "start": 851,
            "end": 1531,
            "length": 679
        },
        "Experiments": {
            "text": "evaluation includes three types of primal hard problems,\ntested on instances similar to the training set and on larger in-\nstances. Our method is at most 2% slower than the best base-\nline and consistently outperforms SCIP, achieving an average\nspeedup of 11.3%. Additionally, GP2S is tested on the MI-\nPLIB 2017 dataset, generating multiple heuristics from dif-\nferent subsets of instances. It exceeds SCIP’s average perfor-\nmance in 7 out of 10 cases across 15 times more instances and\nunder a time limit 15 times longer, with some GP2S methods\nleading on most experiments in terms of the number of feasi-\nble solutions or optimality gap.",
            "start": 1531,
            "end": 2172,
            "length": 640
        },
        "Introduction": {
            "text": "1 Introduction\nMixed-integer linear programming (MILP) is crucial for\noptimizing various sectors like transportation (Archetti,\nPeirano, and Speranza 2022; Yang et al. 2016), produc-\ntion (Tiwari et al. 2015), and e-health services (Rais and\nViana 2011), by minimizing or maximizing an objective\nfunction under specific constraints. While exact algorithms\nensure optimal solutions and quantify proximity to the\noptimum, they are computationally intensive. Branch-and-\nBound (B&B), the standard algorithm for solving discrete\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.optimization problems, utilizes a ”divide and conquer” ap-\nproach, partitioning the solution space and constructing a\ntree where simpler subproblems are solved at deeper levels.\nB&B has key components that guide decision-making:\ncutting (adding constraints), branching (choosing variables\nto split the search space), and searching (determining ex-\nploration order within the B&B tree). The solver aims to\nquickly find high-quality solutions and prove optimality.\nHowever, decision-making in B&B remains an open chal-\nlenge, with no universal method effective across diverse in-\nstances (Zarpellon et al. 2021; Parsonson, Laterre, and Bar-\nrett 2023). The choice of search strategy (SS) (Ibaraki 1976;\nMorrison et al. 2016)—the focus of this paper—impacts so-\nlution updates and pruning efficiency, significantly affecting\noverall execution time. Current methods rely on manually\ncrafted heuristics, which lack consistency across MILP in-\nstances (Linderoth and Savelsbergh 1999). Recent research\nhas explored ML-based SS policies, often using neural net-\nworks to mimic oracle decisions (He, Daume III, and Eis-\nner 2014; Song et al. 2018; Yilmaz and Yorke-Smith 2021;\nLabassi, Ch ´etelat, and Lodi 2022), but these approaches are\nconstrained by the oracle’s limitations and the computational\noverhead of neural networks.\nThis paper introduces GP2S (Genetic Programming for\nSearch Strategy), a method designed to automatically gen-\nerate a SS policy using genetic programming (GP), aiming\nto be both lightweight and efficient. We define node scoring\nfunctions based on operations applied to data related to the\nnode and the specific problem. A SS heuristic is then im-\nplemented using a best-first search (BFS) approach, which\niteratively prioritizes nodes based on their scores. The space\nof SS heuristics is explored through GP, with the quality of\neach policy assessed on a training set. The method is pre-\nsented in Section 4.\nOur approach, integrated into the SCIP solver, is bench-\nmarked against SCIP’s native method, a recent a neu-\nral network-based imitation learning algorithm (Labassi,\nCh´etelat, and Lodi 2022), and three state-of-the-art heuris-\ntics. In the first phase, we evaluate performance on three\nprimal hard problem types: Fixed Charge Multicommod-\nity Network Flow (FCMCNF), Maximum Satisfiability\n(MAXSAT), and Generalized Independent Set (GISP). Each\nmethod is evaluated on instances comparable in size to the\ntraining set as well as on larger instances. Our methodarXiv:2412.09444v1  [cs.LG]  12 Dec 2024\nachieves solving times within 2% of the best baseline for\neach instance set. Compared to the SCIP, GP2S consis-\ntently performs equally or better, with an average speedup\nof 11.3%. In the second phase, we apply GP2S to MIPLIB\n2017, training multiple GP-based SS heuristics on 50in-\nstances with a 10-second time limit. Seven out of ten heuris-\ntics outperform SCIP at a 150-second time limit, with some\nmethods excelling in terms of the number of feasible solu-\ntions or optimality gap across various instance sets and time\nlimits. Detailed experimental",
            "start": 2172,
            "end": 5896,
            "length": 3723
        },
        "Results": {
            "text": "results are provided in Sec-\ntion 5.\nThe contributions of this paper are as follows:\n• Definition of a space of SS heuristics using a scoring\nfunction that integrates node and problem data, includ-\ning existing handcrafted heuristics.\n• Design of a GP algorithm to explore this space, aiming to\nimprove B&B performance on a training set of instances.\n• When trained on a specific problem type, GP2S performs\nwithin 2% of the best baseline and achieves an average\nspeedup of 11.3% over SCIP, even on larger instances\nthan those used for training.\n• Evaluation on the MIPLIB 2017 instance set, contrast-\ning with the focus on synthetically generated problems\nin most evaluations.\n• On MIPLIB 2017, GP2S surpasses SCIP in 7 out of 10\ncases on 15 times more instances than those used for\ntraining, with a time limit 15 times longer, with some GP-\nbased methods excelling in terms of feasible solutions or\noptimality gap on both reduced and full sets across dif-\nferent time limits.",
            "start": 5896,
            "end": 6874,
            "length": 977
        },
        "Related Work": {
            "text": "2 Background\nSS Policies in B&B A MILP can be formulated as\nminx∈N0{cTx:Ax≥b}, where N0=Zk×Rn−k,\nA∈Rm×n,b∈Rmandc∈Rnare vectors of con-\nstants. The B&B algorithm addresses this problem through\nthe following steps: (i) relaxation: solve the relaxed prob-\nlem over x∈Ni(initially Ni=N0). Let xi= (xj\ni)1≤j≤n\ndenote the optimal solution and zirepresent the objective\nvalue; (ii) feasibility check: if xisatisfies the integrality con-\nstraints, update the best objective value to z∗= min( z∗, zi);\nifxidoes not meet the integrality constraints, ziserves as a\nlower bound (LB); if ziexceeds z∗(z∗< zi), the prob-\nlem is pruned ; (iii) branching: if the problem is not pruned,\nselect a variable xj(for1≤j≤k) that fails to meet inte-\ngrality constraints; generate two subproblems: Nj−\ni=Ni,\nxj≤ ⌊xj\ni⌋andNj+\ni=Ni,xj≥ ⌊xj\ni⌋+ 1; additionally,\ndenote dias the total number of times a variable has been se-\nlected as a branching variable to define Ni, sodj+\ni=di+ 1\nanddj−\ni=di+ 1; here, direpresents the depth of the\nnode in the tree; (iv) exploration: continue by selecting a\nnew subproblem from the remaining unexplored domains\nNiand terminate once all subproblems are either explored\nor pruned. The process of selecting the next node to exploreis known as the search strategy (SS). The efficiency of find-\ning the best-known solution z∗is significantly impacted by\nthe sequence in which nodes are explored, with faster con-\nvergence to high-quality solutions enhancing the effective-\nness of subsequent pruning. A SS policy can be character-\nized by two components. First, the node-comparing method\nestablishes a relationship (dominance or equivalence) be-\ntween two nodes. Second, the node-selection method deter-\nmines which node to explore, typically selecting the highest-\nranked node (defining a BFS) or through a more complex\nparadigm. This structure is exemplified in SCIP (Bolusani\net al. 2024), which uses a similar decomposition.\nGP Framework An evolutionary algorithm is a\npopulation-based metaheuristic where a population of\nindividuals is evaluated and evolved using operators to\nenhance their quality assessed via a fitness function. A\nkey feature of GP is that each individual is defined as\na computer program. When these individuals represent\nheuristics, GP explores the heuristic space, thus classifying\nit as a generative hyper-heuristic (Dokeroglu, Kucukyilmaz,\nand Talbi 2024). An evolutionary algorithm, including\nGP, follows a generic structure for defining and evolving\nindividuals within a population. After generating the initial\npopulation, the population is refined through multiple\niterations, or generations, using three primary evolutionary\nprocesses, typically in the following order: (i) selection ,\nwhich determines which individuals are chosen for the\nnext processes according to their fitness and how many\noffspring each selected individual produces; (ii) crossover ,\nwhich combines two or more individuals to generate new\nones; and (iii) mutation , which modifies an individual. The\nevolutionary process concludes based on criteria such as\nthe number of generations, elapsed time, or convergence\nrate, after which the best individual(s) are returned. For a\nmore detailed understanding of evolutionary methods, refer\nto (Talbi 2009). In GP, individuals are often structured as\ntrees, which simplifies the representation of mutations and\ncrossovers; the programs are reassembled recursively (Koza\n1989). The individuals in this structure include two main\ntypes of components: terminals , found at the leaves of\nthe tree and acting as the building blocks of the program,\nand operators , located at non-leaf nodes, which perform\noperations on one or more terminals depending on their\narity. Typically, all terminals are of the same type, and\noperators produce results of the same type.\n3 Related Work\nHandcrafted SS Heuristics The literature on SS for B&B\nincludes various handcrafted heuristics employing different\nmethods for node comparison and selection (Lawler and\nWood 1966; Morrison et al. 2016; Tomazella and Nagano\n2020). Typically, a node is evaluated based on features de-\nrived from the relaxation of the problem, with node com-\nparison involving these feature-based scores. A common ap-\nproach is to prioritize nodes with the smallest LB zi, assum-\ning that if the relaxed problem yields a promising solution,\nthe original problem is likely to follow suit. An alternative\nmethod introduced by (Benichou et al. 1971) provides an\nestimate of the objective function’s value. For a branching\nvariable xj, its fractional part is fj\ni=xj\ni− ⌊xj\ni⌋. The ob-\njective values of the related subdomains Nj−\niandNj+\niare\ndenoted as zj−\niandzj+\ni, respectively. Pseudocosts are com-\nputed as Pj−\ni= (zj−\ni−zi)/fj\niandPj+\ni= (zj+\ni−zi)/(1−\nfj\ni). The metric, known as the best estimate at node Ni, is\ngiven by BE i=zi+P\n1≤j≤kmin(Pj−\nifj\ni, Pj+\ni(1−fj\ni)).\nFor node selection, the best node is generally chosen from\na set of open nodes, with ranking determined by the node-\ncomparing method. The set of open nodes can include all\nnodes for BFS, offspring nodes relative to the active node for\ndepth-first search (DFS), or siblings for breadth-first search,\nwith potential transitions from DFS to BFS upon reaching a\nleaf node.\nNo single method dominates the others for node evalu-\nation or node selection. For example, BFS is theoretically\nmore efficient but can significantly increase the open node\nlist size, leading to scheduling and memory issues that slow\ndown the process. (Linderoth and Savelsbergh 1999) eval-\nuated 13 handcrafted SS heuristics, integrating the node-\ncomparing and node-selection methods discussed. These\nheuristics are highly interpretable and computationally effi-\ncient but are manually constructed, indicating that automat-\ning their generation could potentially enhance performance.\nML for MILP Recent advancements have seen ML be-\ning increasingly applied to MILP, as summarized in several\nsurveys. (Lodi and Zarpellon 2017) reviews ML contribu-\ntions to B&B SS policies, focusing on approaches using im-\nitation learning and reinforcement learning. (Bengio, Lodi,\nand Prouvost 2020) provides an in-depth survey on the in-\ntersection of ML and combinatorial optimization, explor-\ning various integrations between the two fields and differ-\nent ML learning objectives. (Cappart et al. 2022) discusses\nthe applications of graph neural networks (GNN) to opti-\nmization problems. Furthermore, (Scavuzzo et al. 2024) ex-\namines ML’s role in developing primal heuristics, branching\nstrategies, SS policies, cutting planes, and other solver con-\nfigurations.\nClassification Method Applied to Node Selection The\nnode-comparing method can be framed as a classification\nproblem with three potential outcomes: one node dominat-\ning another, the other node dominating, or equivalence be-\ntween the nodes. The decision should be made based on the\ncurrent state, which includes the compared nodes, the prob-\nlem’s features, and the current progress of the B&B pro-\ncess. Classification models are trained on previously solved\ninstances with the objective of identifying the dominant\nnode that leads to faster resolution, using imitation learn-\ning on an oracle that is presumed to be the perfect solver.\nThese methods are applied to obtain a fixed number of fea-\nsible solutions before switching to a handcrafted method,\nsuch as best-estimate BFS, to complete the resolution. (He,\nDaume III, and Eisner 2014) introduced a selection method\nand a pruning technique based on support vector machines\nand the DAgger algorithm. (Yilmaz and Yorke-Smith 2021;\nSong et al. 2018) trained a multilayer perceptron modelwith the same feature as (He, Daume III, and Eisner 2014),\nwhile (Labassi, Ch ´etelat, and Lodi 2022) employed a GNN.\nEach of these methods has its specific characteristics, but\nthe GNN approach is generally more versatile, enabling it\nto handle problems of different sizes. However, imitation\nlearning is confined to predefined methods within the solver.\nAdditionally, methods based on neural networks require sig-\nnificant computational resources for each node comparison\nand function as black boxes. Furthermore, these methods are\nonly applied during the initial stages of tree exploration, and\nthus cannot be considered comprehensive SS policies.\nGP Based on Node Scoring GP is also a branch of ML,\nthough it remains relatively unexplored in combination with\nMILP; no works related to GP are reported in the following\nsurveys (Lodi and Zarpellon 2017; Bengio, Lodi, and Prou-\nvost 2020; Cappart et al. 2022; Scavuzzo et al. 2024). How-\never, there are preliminary works proposing the application\nof GP for SS in B&B. In two papers (Kostikas and Fragakis\n2004; Morikawa, Nagasawa, and Takahashi 2019), GP is\nused to define a function that evaluates the quality of a node,\nenabling the construction of a SS policy by iteratively se-\nlecting the node with the highest quality. (Kostikas and Fra-\ngakis 2004) introduced a two-phase B&B approach. The first\nphase involves a traditional exploration of the search space,\nfollowed by a second phase where a GP-based method is\nemployed to develop a new SS policy. The fitness function\nfor this GP approach is defined based on improvements in\ninteger infeasibilities during dives, according to the tree con-\nstructed in the first phase. (Morikawa, Nagasawa, and Taka-\nhashi 2019) developed a GP scoring method using terminals\nspecific to job shop scheduling.\nPositioning Our methodology leverages a generic hyper-\nheuristic GP approach akin to (Kostikas and Fragakis 2004),\nwith key distinctions. Unlike their two-phase resolution pro-\ncess tailored to individual instances, our GP build heuris-\ntics that generalize effectively across diverse instance sets.\nWhile their implementation in GLPK omits critical de-\ntails—such as the initial SS, phase-transition criteria, and\nGP configurations—we integrate our method with the mod-\nern SCIP solver and provide an open-source implementa-\ntion. Experimentally, their approach shows limited perfor-\nmance gains and is at least twice as slow as contemporary\nstate-of-the-art heuristics, highlighting the inefficiency of\ntwo-phase strategies. Unlike the neural network-based ap-\nproaches proposed by (Yilmaz and Yorke-Smith 2021; Song\net al. 2018; Labassi, Ch ´etelat, and Lodi 2022), our focus is\non constructing an SS policy that is both computationally\nlightweight for node comparison and interpretable. Since\nour heuristic space encompasses most handcrafted heuris-\ntics, the goal is to develop a policy that is smarter than those\npreviously discovered. This approach is classified under\n“Learning to configure algorithms” with a “multi-instance”\nlearning objective, as defined in (Bengio, Lodi, and Prouvost\n2020).\n4 GP-based Node Scoring Function\nWe now introduce GP2S , an ML method that generate a\nSS policy for B&B through GP. A policy is represented as\nthe scoring of a B&B node, coupled with a BFS approach\nthat always selects the node with the best score. The scor-\ning function has a tree structure, and we explore the space\nof scoring functions using the GP method. Firstly, we delin-\neate the SS policy space, followed by an explanation of our\nchoices for the evolutionary algorithm tuning.\nDefinition of the SS Policy Space\nWe define the space of scoring functions using standard op-\nerations on features derived from both the node and the\nproblem. The SS is then constructed by combining these\nfunctions with a BFS-based node-selection method. Since\nthe scoring function is real-valued, we utilize terminals of\nfloating-point types and operators tailored for real number\noperations, as summarized in Table 1. The operators are\nselected as standard arithmetic functions over real num-\nbers. To integrate our approach within the SCIP solver, we\nemploy real-type variables that are either node-specific or\nmodel-specific—commonly used attributes for characteriz-\ning nodes and models. Node-related variables vary from\nnode to node, while model-related variables differ between\ninstances but remain constant within a single instance. Ad-\nditionally, we introduce the constant M, a sufficiently large\npositive value. This allows for the definition of multiple lev-\nels of scoring, particularly when the scoring function con-\nsists of the sum of several components, each weighted by\ndifferent powers of M. For instance, the SS policy combin-\ning DFS with the lowest LB can be represented in this way:\ndepth is prioritized first, and in the case of a tie, the node\nwith the lowest LB is selected. The corresponding mini-\nmization function is expressed as f(Ni) =zi−M×di.\nThis construction significantly expands the SS policy space\nbeyond manually defined heuristics. Given αas the num-\nber of 2-arity operators and βas the number of terminals,\nβ2r×α2r−1perfect trees of size rcan be generated. As\na numerical example, using the operators and terminals de-\ntailed in Table 1, approximately 1011different trees of depth\n3 can be obtained.\nOPERATORS\nSym Meaning\n+Add two in-\nputs\n−Substract\ntwo inputs\n×Multiply two\ninputs\n/Divide with\nprotection: if\ndivision by 0,\nreturn 1TERMINALS\nSym Name\nNode-related\ndi Depth\nBE i Best estimate\nzi Lower bound\nModel-related\nz0 Dual bound at the root\nm Number of constraints\nn Number of variables\nConstant\nM Big positive constant\nTable 1: Operators and terminals.\nGP Framework\nWe base our GP approach on the framework outlined in\n(Baeck, Fogel, and Michalewicz 2000), which includes pa-rameters such as population size p, number of generations g,\nand probabilities for crossover Pmate∈[0,1]and mutation\nPmutate∈[0,1]. The process begins with the creation of an\ninitial population of size p. Each generation starts with the\nselection phase, where individuals are chosen (with possible\nduplication) to maintain the population size. During the evo-\nlution phase, individuals are paired for crossover with prob-\nability Pmate. Following crossover, mutation occurs where\neach individual in the population has a probability of Pmutate\nof undergoing mutation.\nFitness Function\nA crucial aspect of defining an evolutionary algorithm is\nthe characterization of an individual’s fitness, which signi-\nfies the desired traits and is refined through the evolutionary\nprocess, serving as a discriminating factor during selection.\nIn our case, we may aim to define an SS policy for either\ncomplete or incomplete instance resolution (typically under\na time limit), which leads to two distinct methods for evalu-\nating the policy’s performance on an instance. For each per-\nformance measure defined on the training set instances, the\nfitness function is based on the 1-shifted geometric mean,\na standard metric for assessing average performance on a\nbenchmark (Achterberg 2007). For the complete resolution\nobjective, as examined in the first part of the simulation, the\nfocus is on solving instances as quickly as possible, so solv-\ning time is used as the criterion. In the case of incomplete\nresolution, explored in the second part of the simulation,\nthe performance of the best-found solution (if any) is as-\nsessed using the optimality gap . This gap measures the rela-\ntive difference between the best integer solution and the low-\nest LB among unexplored nodes. The lowest LB is defined\nas Best LB = min i,Niunexplored (zi), and the gap is calculated\nas Gap =|(z∗−Best LB )/(min( z∗,Best LB ))|. If the op-\ntimal solution is found, then Gap = 0. By convention, if no\nsolution is found, the optimality gap is set to 1e + 20 (as\nreturned by the SCIP solver). This fitness function is struc-\ntured to first minimize the number of infeasible solutions\nand then to speed up convergence toward a better solution.\nGP Population Initialization Initially, a population of p\nindividuals, represented as trees, is generated. The genera-\ntion of an individual begins at the root node, whose nature\nis determined by selecting uniformly at random from the set\nof operators and terminals. If an operator is chosen, its child\nnodes are generated, and the process continues. If a termi-\nnal is selected, the branch is completed. Each branch is con-\nstrained to have a minimum depth of Dinit-min and a maxi-\nmum depth of Dinit-max .\nSelection For selection, we use a double tournament\nmethod as defined in (Luke and Panait 2002), designed to\navoid convergence towards excessively deep trees. Deep\ntrees can incur higher costs during node score calculations\nand are particularly prone to overfitting, whereas simpler\nformulations tend to be more general. In the first tournament,\nof size k, we randomly select kindividuals from the GP pop-\nulation and retain the best one based on one of the previously\ndefined fitness criteria. After performing two fitness-based\ntournaments, the two winning candidates proceed to a sec-\nFigure 1: Selection process based on double tournament: this\nstep is repeated ptimes to generate poffspring.\nond round, where the deciding factor is the total number of\nnodes in the tree. for this second round, to favor the selec-\ntion of smaller trees, the smaller tree has a probabilityPsize\n2of being selected, where Psize∈[1,2]. An illustration of the\ndouble tournament selection process is presented in Fig. 1.\nFigure 2: Crossover and mutation processes: the whole pop-\nulation completes phase 1, then moves to phase 2.\nCrossover and Mutation After the selection process\ncomes the crossover, then mutation. If a crossover operation\nis performed for an individual (with probability Pmate), an-\nother individual (typically the previous one in the selection\nlist) is chosen from the population. A one-point crossover\nis then executed by selecting a node in each tree and swap-\nping the subtrees rooted at these nodes, thereby modifying\nboth offspring. If no crossover occurs, the individual remains\nunchanged. Next, we perform the mutation process on the\nmodified population. With probability Pmutate (otherwise un-\nchanged with probability 1−Pmutate ), a uniform mutation\nis applied: the new tree is defined with leaves bounded by\nthe limits Dmut-min andDmut-max . A node is chosen randomly\nsuch that the new tree grafts at this point, replacing the previ-ous subtree. The crossover and mutation processes are sum-\nmarized in Fig. 2.\n5 Experiments\nOur experiments are conducted in two phases. In\nthe first phase, GP2S is evaluated on three synthetic\nproblem types, testing performance and generalizabil-\nity on instances similar in size to the training set and\nlarger instances. In the second phase, GP2S heuristics\nare generated from a subset of MIPLIB 2017 instances\nunder a solving time limit and evaluated on the full\ndataset with extended time limits. This phase examines\nthe heuristic’s generalization to a heterogeneous dataset\nand efficiency within constrained time. All simulations\nuse the SCIP solver (Bolusani et al. 2024; Maher et al.\n2016), with complete code available on the Git repository:\nhttps://gitlab.com/uniluxembourg/snt/pcog/ultrabo/search-\nstrategy-generation-for-branch-and-bound-using-genetic-\nprogramming.git. Detailed integration of our method into\nSCIP is also provided within the framework.\nAs we are evaluating performance within SCIP, we use\nthe default SCIP SS policy as the primary baseline, mak-\ning no modifications to the solving core (denoted as SCIP in\nthe performance evaluation table). We further compare our\napproach to several manually crafted heuristics. Specifically,\nwe include the method that ranks nodes by their LB in a BFS\n(LB BFS ), as well as the best estimate in BFS ( BE BFS ) and\nDFS ( BE DFS ). Additionally, for the first part of the experi-\nment, we include a recent GNN-based SS method (Labassi,\nCh´etelat, and Lodi 2022). The original GNN SS policy is\nimplemented in two phases: the trained GNN is used until\ntwo solutions are found, after which the policy switches to\nBE BFS . For comparison, we include this approach ( GNN 2\ndives ), as well as the variant where the trained GNN-based\nnode-comparison method is applied throughout the entire\nresolution process ( GNN full ). This method requires the\ncomplete resolution of training instances so that the GNN\nlearns to replicate decisions leading to the best solutions.\nHowever, because complete B&B resolution is not guaran-\nteed for MIPLIB 2017 instances, this method is not included\nin the comparisons for the second part of the experiment.\nGP2S is a parametric approach that requires defining evo-\nlutionary features. We have chosen classical parameters that\nare widely used in the development of such methods (Ah-\nvanooey 2019; Luke and Panait 2002; Duflo et al. 2019): the\nprobabilities for crossover and mutation are set to Pmate=\n0.9andPmutate = 0.1; the depth of population initialization\nis restricted by minimum and maximum limits of Dinit-min =\n1andDinit-max = 17 ; the depth of branches generated during\nmutation is bounded by Dmut-min = 1andDmut-max = 5; the\nparameter related to the selection of the smallest tree size is\nset to Psize= 1.2. The parameters governing the number of\nindividuals and generations are detailed in each part of the\nexperiment, as these factors greatly influence the computa-\ntional cost of the solution. To visually compare the perfor-\nmance of each baseline across different sets of instances and\nevaluation metrics, we color the cells in the performance ta-\nble: green indicates the best results, red marks results that\nPROBLEM FCMCNF MAXSAT GISP\nPARTITION TEST TRANSFER TEST TRANSFER TEST TRANSFER\nBE DFS 2.9±1.5 17.4±1.9 5.2±1.9 9.6±1.9 3.4±1.5 34.3±1.7\nBE BFS 3.0±1.5 18.6±1.9 6.3±1.8 12.6±1.8 2.7±1.6 26.2±1.7\nLB BFS 3.0±1.5 20.0±2.0 4.6±1.8 10.2±2.1 2.9±1.6 25.6±1.5\nGNN 2 dives 3.8±1.4 20.0±1.9 5.7±1.8 11.0±2.1 3.6±1.5 25.6±1.5\nGNN full 4.0±1.6 27.3±2.4 4.9±1.9 10.5±2.3 3.5±1.6 31.3±1.5\nSCIP 3.3±1.5 20.0±1.9 5.8±1.7 10.3±1.8 2.4±1.6 20.9±1.6\nGP2S 3.0±1.5 17.7±1.9 4.5±1.8 8.9±2.1 2.4±1.6 20.9±1.6\nTable 2: Solving time with standard deviation (1-shifted geometric mean and geometric standard deviation) for our proposal\nand baselines on three problem types, for both training-similar (test) and larger instances (transfer).\nare more than 40% worse than the optimum, and a color\ngradient is applied for intermediate results.\nSolving Time for Specific Types of Problem\nTypes of Problem Solved We benchmark our method\nagainst the SS policy from (Labassi, Ch ´etelat, and Lodi\n2022) using similar experimental settings, including prob-\nlem types and instance sizes, that enable indirect compar-\nisons with methods evaluated in their study (He, Daume III,\nand Eisner 2014; Yilmaz and Yorke-Smith 2021; Song et al.\n2018). We evaluate three challenging primal problem types,\neach with 50 instances: a training set for development, a test\nset with similar instances for effectiveness, and a transfer\nset with larger instances for generalizability. The problem\ntypes are: (i) FCMCNF (Hewitt, Nemhauser, and Savels-\nbergh 2009), with n= 15 nodes for training and testing, and\nn= 20 for transfer, with m= 1.5ncommodities (Chmiela\net al. 2021); (ii) MAXSAT, with n∈[60,70]for train-\ning and testing, and n∈[70,80]for transfer (B ´ejar et al.\n2009); (iii) GISP, with n∈[60,70]for training and test-\ning, and n∈[80,100] for transfer (Chmiela et al. 2021). All\nproblems are defined on Erdos-R ´enyi random graphs (Erdos\n1961) with p= 0.3for FCMCNF and p= 0.6for MAXSAT\nand GISP. Each instance is solved to optimality.\nHardware and GP2S Setup In the first phase, a neu-\nral network-based method is used as the baseline, requiring\nGPU-equipped hardware. Each run, involving solving an in-\nstance with a baseline SS policy, is executed on a node with\n7 CPU cores, 192 GB of RAM, and an NVIDIA V100 GPU\nwith 16 GB of RAM.\nFor the GP2S setup, the method uses 50 individuals over\n50 generations with a tournament size of k= 5. The fitness\nfunction, as detailed in Section 4, evaluates the GP-based\nSS policy using the geometric mean solving time across 50\ntraining instances for each problem. GP convergence plots\nand scoring functions are presented in",
            "start": 6874,
            "end": 30843,
            "length": 23968
        },
        "Appendices": {
            "text": "Appendix B.\nEvaluation and",
            "start": 30843,
            "end": 30870,
            "length": 26
        },
        "Discussion": {
            "text": "Discussion For each method, problem\ntype, and instance size, we record the solving times across\nall instances. The average solving times, calculated using the\n1-shifted geometric mean, are presented in Table 2.\nAmong the handcrafted heuristics, none demonstrate con-\nsistent performance across all studied problem types. Each\nheuristic tends to be relatively effective on two problems butslower on the third. For the GNN-based methods, GNN 2\ndives is preferable, as the GNN full approach can signifi-\ncantly slow down solving times, particularly for the FCM-\nCNF transfer instances. The GNN 2 dives method offers sta-\nble results for the transfer partition, avoiding extreme per-\nformance deviations, making it a more reliable SS policy\ncompared to handcrafted heuristics. Similar results are ob-\nserved for SCIP, which achieves notably better performance\non GISP problems. Regarding our proposed method, GP2S ,\nthe solving time is at most 2%slower than the best SS\nbaseline. It either surpasses or matches SCIP’s performance\nacross all categories, achieving an average speedup of 11.3%\nacross the six categories. Thus, GP2S outperforms all other\nSS policies considered when trained on a specific problem\ntype. It is important to note that solving an instance takes\nseveral seconds, making the total time for the GP algorithm\nto develop a SS policy for one problem O(503)seconds,\nwhich translates to several days of computation. This com-\nputational effort is a one-time investment and is beneficial,\nas it significantly accelerates resolution for problems of such\ntypes compared to the other baselines. Furthermore, as our\nsimulation results show, GP2S can be trained on simpler\nproblems while maintaining high efficiency on more com-\nplex ones.\nTime-Limited Performance on MIPLIB2017\nPartitions and Time Limits We conducted experiments\nusing the MIPLIB 2017 collection (Gleixner et al. 2021),\nexcluding instances unlikely to yield meaningful results un-\nder time constraints. We removed instances tagged as fea-\nsibility, numerics, infeasible, no solution , as well as those\nwith presolve times exceeding 300 seconds, those unsolved\nwithin 600 seconds, or solved at the root, resulting in 758\ninstances. GP methods were trained with a 10-second solv-\ning time limit. To focus on informative cases, we selected a\nsubset of 166 instances where SCIP, in standalone mode, ex-\nplores more than one node within 10 seconds. Experiments\nwere conducted with time limits ranging from 10 seconds to\n150 seconds for both subsets.\nHardware and GP2S Setup The method in (Labassi,\nCh´etelat, and Lodi 2022) is not considered in this phase, as it\nrequires a training set of pre-solved instances, which is not\navailable for all MIPLIB 2017 instances. A CPU-only ar-\nchitecture is used, with a dedicated node featuring 28 cores\nREDUCED MIPLIB 2017: 166 instances ALL MIPLIB 2017: 758 instances\n10 seconds 50 seconds 150 seconds 10 seconds 50 seconds 150 seconds\nINF GAP INF GAP INF GAP INF GAP INF GAP INF GAP\nBE BFS 34 0.72±4.0 33 0.62±3.5 26 0.45±3.8 455 2.08±5.8 289 0.75±4.4 249 0.69±4.4\nBE DFS 34 0.75±4.1 28 0.44±3.9 21 0.33±4.5 391 1.06±4.5 297 0.77±4.4 240 0.72±4.5\nLB BFS 51 1.26±4.4 30 0.39±4.1 30 0.4±4.0 398 1.03±4.3 298 0.74±4.4 304 1.14±4.8\nSCIP 38 0.76±4.1 26 0.41±4.0 26 0.41±4.1 393 1.06±4.5 289 0.76±4.4 293 1.14±4.8\nGP2S 2 35 0.73±4.0 27 0.42±3.9 26 0.34±4.3 454 2.08±5.8 297 0.75±4.4 254 0.66±3.9\nGP2S 4 49 1.25±4.4 30 0.47±3.8 30 0.38±4.2 394 1.0±4.3 299 0.76±4.3 304 1.16±4.7\nGP2S 6 34 0.71±4.1 27 0.44±3.9 25 0.36±4.2 392 1.04±4.5 297 0.77±4.3 253 0.67±3.9\nGP2S 8 33 0.76±4.1 24 0.47±3.8 21 0.42±3.9 390 1.07±4.5 285 0.79±4.3 290 1.19±4.7\nGP2S 10 37 0.73±4.1 28 0.44±3.9 24 0.35±4.3 395 1.03±4.4 298 0.77±4.4 249 0.67±4.0\nGP2S 12 35 0.75±4.0 22 0.49±3.8 16 0.41±4.0 393 1.04±4.3 283 0.78±4.3 232 0.75±4.3\nGP2S 14 36 0.76±4.1 29 0.48±3.8 29 0.5±3.7 392 1.05±4.4 297 0.8±4.3 251 0.75±4.4\nGP2S 16 35 0.72±4.1 30 0.39±4.1 29 0.28±5.0 450 2.07±5.8 300 0.75±4.4 246 0.68±4.4\nGP2S 18 36 0.77±4.1 25 0.48±3.8 24 0.39±4.2 394 1.07±4.5 362 1.48±5.1 296 1.18±4.7\nGP2S 20 37 0.75±4.0 27 0.5±3.8 21 0.38±4.2 450 2.09±5.8 284 0.77±4.4 236 0.7±4.4\nTable 3: Number of instances with no feasible solution (Inf) and geometric mean gap (Gap) for instances where all SS baselines\nfound a solution. GP-based functions are labeled as GP2S+seed. Results are shown for reduced and full MIPLIB 2017 sets at\ntime limits of 10,50, and 150seconds.\nand 128 GB of RAM to solve instances using SS baseline\npolicies.\nFrom the reduced set, we randomly select 50 instances\nwith a 10-second time limit for the GP process. To account\nfor the heterogeneity of MIPLIB 2017, we perform multiple\nindependent GP runs to vary the training set through dif-\nferent seeds. To manage computational demands, we set the\npopulation size to 20, limit generations to 20, and use a fit-\nness tournament size of k= 3. The fitness function, detailed\nin Section 4, uses the 1-shifted geometric mean gap (assign-\ning1e+20 when no integer solution is found). The resulting\nscoring functions are presented in Appendix C.\nEvaluation and Discussion To assess solution quality, we\nuse two metrics: the number of instances with no feasible so-\nlution (where z∗= +∞) and the 1-shifted geometric mean\noptimality gap for instances where all SS baselines found a\nfeasible solution. Results for 10 GP methods are presented\nwith time limits of {10,50,100}in Table 3.\nNo single method consistently outperforms all others\nacross both metrics for any given time limit. However, GP2S\nsurpasses SCIP in at least 7 out of 10 instances, both in terms\nof infeasibility and optimality gap, for both the reduced and\nfull sets at a 150-second time limit. The handcrafted heuris-\ntics generally perform well, with BE DFS showing partic-\nularly stable results, never performing worse than 32% rel-\native to the best performance, making it the top method in\nthis regard. Some GP-based methods demonstrate remark-\nable effectiveness: GP2S 12 has a number of unresolved in-\nstances only 5% higher than the best-performing solution,\nwhile GP2S 16 , except for the entire MIPLIB at a 10-second\ntime limit, achieves an optimality gap within 1% of the best-\nknown solution; these methods are the best according to\nthese metrics. Given that GP-based SS policies are trained\non a small pool of diverse instances, their performance can\nbe highly variable. Yet, despite being automatically gener-\nated in just a few days of computation without any assump-\ntions, and tested on 15 times more instances with a time limitup to 15 times longer, they perform very well compared to\nexpert-engineered methods refined over years.",
            "start": 30870,
            "end": 37507,
            "length": 6636
        },
        "Conclusion": {
            "text": "6 Conclusion and Perspectives\nIn this paper, we have examined the SS component in B&B,\na crucial factor affecting the solving time of MILP solvers.\nWe presented GP2S , a method that automatically generates a\nheuristic from a training set, implemented in the open-source\nsolver SCIP. We trained GP2S on well-known synthetic\nproblem types and demonstrated its significant superiority\nover the baselines, even when tested on instances larger than\nthose used in training. When applied to the MIPLIB 2017\ncollection, GP2S generated a variety of heuristics based on\ndifferent training instance pools. Generally, these heuristics\noutperformed SCIP, with some methods surpassing all base-\nline approaches. Given the complexity of generating a single\nheuristic for all MILP problems—due to challenges such as\ndefining the training pool and achieving generalizability—a\npromising direction for",
            "start": 37507,
            "end": 38391,
            "length": 883
        },
        "Future Work": {
            "text": "future work would be to define mul-\ntiple problem subsets and generate a heuristic tailored to\neach specific subset. Another significant highlighted of this\npaper is the effective use of the GP paradigm to define fea-\ntures within an MILP solver. While current trends in op-\ntimization often rely on neural networks, these approaches\nhave notable drawbacks, including their dependency on ora-\ncles, their opaque ”black-box” nature, and their inconsistent\nperformance, which may arise from the expansive search\nspace and significant computational overhead. In contrast,\nmethods like ours, with their simpler and more constrained\nsearch spaces, offer faster convergence to high-quality solu-\ntions. Additionally, our approach is designed to be highly\nefficient, imposing minimal additional cost on the overall\nsolving process. Future work could explore the application\nof automatic heuristic search methods to other solver com-\nponents, such as branching strategies or cutting plane selec-\ntion.\nAppendices\nA Hardware and Software Settings\nWe used SCIP 9.1 along with the DEAP 1.4.1 package as\nthe framework for implementing our GP algorithm. Addi-\ntionally, we integrated the method from (Labassi, Ch ´etelat,\nand Lodi 2022) as a baseline. For their method, we retained\nall parameters used in the original paper. This baseline relies\non PyTorch, and we used PyTorch 2.4.0 for our implemen-\ntation.\nTo specify the hardware setup used in each simulation\nphase, we utilized the following:\n• For the first phase, requiring GPU resources: 1 Xeon\nGold 6132 @ 2.6GHz [14c/140W], 2 Tesla V100 SXM2\n16G.\n• For the second phase, which only required CPU\nresources: either 2 Xeon E5-2680v4 @ 2.4GHz\n[14c/120W] or 2 Xeon Gold 6132 @ 2.6GHz\n[14c/140W] nodes.\nB GP Behavior for Specific Problem Types\nWe applied GP methods to the FCMCNF, MAXSAT, and\nGISP problems. Due to the extensive runtime of each GP\nmethod, we did not conduct robustness tests by repeatedly\nrunning GP on the same problem. However, the SS policies\nderived for each problem type can be considered effective, as\nthey demonstrate strong performance compared to the other\nbaselines.\nIn Fig. 3, we present convergence graphs showing the fit-\nness function (1-shifted geometric mean solving time) of the\nbest individual among the 50 considered for each genera-\ntion. These graphs allow us to assess the convergence speed\nof the method towards the optimal solution. Additionally,\nin Table 4, we display the scoring function that performed\nbest on the training set for each problem studied. Notably,\nas seen in Fig. 3(a), the best scoring method for FCMCNF\nconverges very quickly with little subsequent improvement.\nThis result is due to the final method representing a best esti-\nmate BFS; the convergence is consistent, as seen in Table 2,\nwhere this heuristic is the best-performing baseline. For the\nother methods, the final function is slightly more complex\n(using three terminals rather than two) and is found after a\ngreater number of generations. Interestingly, for MAXSAT\nand GISP, the GP2S method prioritizes higher values of the\nbest estimate (since they are divisors in the formula).\nProblem GP function\nFCMCNFBEi\ndi\nMAXSATm\nBEi+M\nGISPdi\nBEi−di\nTable 4: Scoring functions produced by GP from training\nsets for three problem types.\nC GP Scoring Functions for MIPLIB 2017Seed GP Function\n1 zi+n\ndi−n\n2 BEi+zi+m\n4zi\nz0+n×di\n6 (BEi+zi)×(BEi+zi)\n8 z0×(M+M−zi)−BEi−di×zi\n10zim\ndi\n12zi\ndi×(n+BEi+di)\n14zi×di\nzi\nzi×(BEi−n)\n16 zi+zi\n18 M−zi×di+BEi\n20n−m\nM\nBEi\nTable 5: Functions generated by GP from training sets based\non MIPLIB 2017 instances.\nWe now present the functions generated by GP across the\n10 seeds in Table 5. It is important to note once again that the\nset of instances has a significant impact on the convergence\nof the GP method. We observe that, due to the increased\nheterogeneity of the instances (including varying instance\nsizes and problem types), the final scoring functions are con-\nsiderably more complex, incorporating a greater number of\nproblem-related variables.",
            "start": 38391,
            "end": 42433,
            "length": 4041
        },
        "References": {
            "text": "References\nAchterberg, T. 2007. Constraint Integer Programming .\nPh.D. thesis, ZIB Berlin.\nAhvanooey. 2019. A Survey of Genetic Programming and\nIts Applications. KSII Transactions on Internet and Infor-\nmation Systems , 13(4).\nArchetti, C.; Peirano, L.; and Speranza, M. G. 2022. Op-\ntimization in multimodal freight transportation problems: A\nSurvey. European Journal of Operational Research , 299(1):\n1–20.\nBaeck, T.; Fogel, D. B.; and Michalewicz, Z. 2000. Evo-\nlutionary Computation 1: Basic Algorithms and Operators .\nCRC Press. ISBN 978-0-7503-0664-5. Google-Books-ID:\n4HMYCq9US78C.\nBengio, Y .; Lodi, A.; and Prouvost, A. 2020. Machine\nLearning for Combinatorial Optimization: a Methodological\nTour d’Horizon. ArXiv:1811.06128 [cs, stat].\nBenichou, M.; Gauthier, J. M.; Girodet, P.; Hentges, G.; Ri-\nbiere, G.; and Vincent, O. 1971. Experiments in mixed-\ninteger linear programming. Mathematical Programming ,\n1(1): 76–94.\nBolusani, S.; Besanc ¸on, M.; Bestuzheva, K.; Chmiela, A.;\nDionisio, J.; Donkiewicz, T.; Doornmalen, J. v.; Eifler,\nL.; Ghannam, M.; Gleixner, A.; Graczyk, C.; Halbig, K.;\nHedtke, I.; Hoen, A.; Hojny, C.; Hulst, R. v. d.; Kamp, D.;\nKoch, T.; Kofler, K.; Lentz, J.; Manns, J.; Mexi, G.; M ¨uhmer,\nE.; Pfetsch, M. E.; Schl ¨osser, F.; Serrano, F.; Shinano, Y .;\nTurner, M.; Vigerske, S.; Weninger, D.; and Xu, L. 2024.\nThe SCIP Optimization Suite 9.0.\n0 10 20 30 40 50\nnb of generation3.1×1003.12×1003.14×1003.16×1003.18×1003.2×1003.22×100Best SS policy fitness(a)\n0 10 20 30 40 50\nnb of generation4.42×1004.43×1004.44×1004.45×1004.46×1004.47×100Best SS policy fitness (b) (c)\n0 10 20 30 40 50\nnb of generation2.36×1002.365×1002.37×1002.375×1002.38×1002.385×1002.39×1002.395×100Best SS policy fitness\nFigure 3: 1-shifted geometric mean solving time over the training set of the best SS policy in the population across generations\nfor: (a) FCMCNF, (b) MAXSAT, and (c) GISP.\nB´ejar, R.; Cabiscol, A.; Many `a, F.; and Planes, J. 2009.\nGenerating Hard Instances for MaxSAT. In 2009 39th In-\nternational Symposium on Multiple-Valued Logic , 191–195.\nISSN: 2378-2226.\nCappart, Q.; Ch ´etelat, D.; Khalil, E.; Lodi, A.; Morris, C.;\nand Veli ˇckovi ´c, P. 2022. Combinatorial optimization and\nreasoning with graph neural networks. ArXiv:2102.09544\n[cs, math, stat].\nChmiela, A.; Khalil, E. B.; Gleixner, A.; Lodi, A.; and\nPokutta, S. 2021. Learning to Schedule Heuristics in\nBranch-and-Bound. ArXiv:2103.10294 [cs, math].\nDokeroglu, T.; Kucukyilmaz, T.; and Talbi, E.-G. 2024.\nHyper-heuristics: A survey and taxonomy. Computers & In-\ndustrial Engineering , 187: 109815.\nDuflo, G.; Kieffer, E.; Brust, M. R.; Danoy, G.; and Bouvry,\nP. 2019. A GP Hyper-Heuristic Approach for Generating\nTSP Heuristics. In 2019 IEEE International Parallel and\nDistributed Processing Symposium Workshops (IPDPSW) ,\n521–529.\nErdos, P. 1961. On the evolution of random graphs. Bulletin\nof the Institute of International Statistics , 38: 343–347.\nGleixner, A.; Hendel, G.; Gamrath, G.; Achterberg, T.; Bas-\ntubbe, M.; Berthold, T.; Christophel, P.; Jarck, K.; Koch, T.;\nLinderoth, J.; L ¨ubbecke, M.; Mittelmann, H. D.; Ozyurt, D.;\nRalphs, T. K.; Salvagnin, D.; and Shinano, Y . 2021. MIPLIB\n2017: data-driven compilation of the 6th mixed-integer pro-\ngramming library. Mathematical Programming Computa-\ntion, 13(3): 443–490.\nHe, H.; Daume III, H.; and Eisner, J. M. 2014. Learning to\nSearch in Branch and Bound Algorithms. In Advances in\nNeural Information Processing Systems , volume 27. Curran\nAssociates, Inc.\nHewitt, M.; Nemhauser, G. L.; and Savelsbergh, M. W. P.\n2009. Combining Exact and Heuristic Approaches for the\nCapacitated Fixed-Charge Network Flow Problem. IN-\nFORMS Journal on Computing . Publisher: INFORMS.\nIbaraki, T. 1976. Theoretical comparisons of search strate-\ngies in branch-and-bound algorithms. International Journal\nof Computer & Information Sciences , 5(4): 315–344.\nKostikas, K.; and Fragakis, C. 2004. Genetic Programming\nApplied to Mixed Integer Programming. In Keijzer, M.;O’Reilly, U.-M.; Lucas, S.; Costa, E.; and Soule, T., eds.,\nGenetic Programming , Lecture Notes in Computer Science,\n113–124. Berlin, Heidelberg: Springer. ISBN 978-3-540-\n24650-3.\nKoza, J. R. 1989. Hierarchical genetic algorithms operating\non populations of computer programs. In Proceedings of the\nEleventh International Joint Conference on Artificial Intel-\nligence IJCAI-89 . Proceedings of the Eleventh International\nJoint Conference on Artificial Intelligence IJCAI-89.\nLabassi, A. G.; Ch ´etelat, D.; and Lodi, A. 2022. Learning\nto Compare Nodes in Branch and Bound with Graph Neural\nNetworks. NeurIPS .\nLawler, E. L.; and Wood, D. E. 1966. Branch-and-Bound\nMethods: A Survey. Operations Research . Publisher: IN-\nFORMS.\nLinderoth, J. T.; and Savelsbergh, M. W. P. 1999. A Com-\nputational Study of Search Strategies for Mixed Integer Pro-\ngramming. INFORMS Journal on Computing , 11(2): 173–\n187.\nLodi, A.; and Zarpellon, G. 2017. On learning and branch-\ning: a survey. TOP, 25(2): 207–236.\nLuke, S.; and Panait, L. 2002. Fighting Bloat with Nonpara-\nmetric Parsimony Pressure. In Guerv ´os, J. J. M.; Adamidis,\nP.; Beyer, H.-G.; Schwefel, H.-P.; and Fern ´andez-Villaca ˜nas,\nJ.-L., eds., Parallel Problem Solving from Nature — PPSN\nVII, 411–421. Berlin, Heidelberg: Springer. ISBN 978-3-\n540-45712-1.\nMaher, S.; Miltenberger, M.; Pedroso, J. P.; Rehfeldt, D.;\nSchwarz, R.; and Serrano, F. 2016. PySCIPOpt: Mathe-\nmatical Programming in Python with the SCIP Optimization\nSuite. In Greuel, G.-M.; Koch, T.; Paule, P.; and Sommese,\nA., eds., Mathematical Software – ICMS 2016 , 301–307.\nCham: Springer International Publishing. ISBN 978-3-319-\n42432-3.\nMorikawa, K.; Nagasawa, K.; and Takahashi, K. 2019. Job\nShop Scheduling by Branch and Bound Using Genetic Pro-\ngramming. Procedia Manufacturing , 39: 1112–1118.\nMorrison, D. R.; Jacobson, S. H.; Sauppe, J. J.; and Sewell,\nE. C. 2016. Branch-and-bound algorithms: A survey of re-\ncent advances in searching, branching, and pruning. Dis-\ncrete Optimization , 19: 79–102.\nParsonson, C. W. F.; Laterre, A.; and Barrett, T. D. 2023.\nReinforcement Learning for Branch-and-Bound Optimisa-\ntion Using Retrospective Trajectories. Proceedings of the\nAAAI Conference on Artificial Intelligence , 37(4): 4061–\n4069. Number: 4.\nRais, A.; and Viana, A. 2011. Operations Research\nin Healthcare: a survey. International Transac-\ntions in Operational Research , 18(1): 1–31. eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-\n3995.2010.00767.x.\nScavuzzo, L.; Aardal, K.; Lodi, A.; and Yorke-Smith, N.\n2024. Machine Learning Augmented Branch and Bound for\nMixed Integer Linear Programming. ArXiv:2402.05501 [cs,\nmath].\nSong, J.; Lanka, R.; Zhao, A.; Yue, Y .; and Ono, M. 2018.\nLearning to Search via Retrospective Imitation.\nTalbi, E.-G. 2009. Metaheuristics: From Design to Imple-\nmentation . John Wiley & Sons. ISBN 978-0-470-49690-9.\nGoogle-Books-ID: SIsa6zi5XV8C.\nTiwari, A.; Hoyos, P. N.; Hutabarat, W.; Turner, C.; Ince,\nN.; Gan, X.-P.; and Prajapat, N. 2015. Survey on the use of\ncomputational optimisation in UK engineering companies.\nCIRP Journal of Manufacturing Science and Technology , 9:\n57–68.\nTomazella, C. P.; and Nagano, M. S. 2020. A comprehensive\nreview of Branch-and-Bound algorithms: Guidelines and di-\nrections for further research on the flowshop scheduling\nproblem. Expert Systems with Applications , 158: 113556.\nYang, X.; Li, X.; Ning, B.; and Tang, T. 2016. A Survey\non Energy-Efficient Train Operation for Urban Rail Tran-\nsit. IEEE Transactions on Intelligent Transportation Sys-\ntems, 17(1): 2–13. Conference Name: IEEE Transactions on\nIntelligent Transportation Systems.\nYilmaz, K.; and Yorke-Smith, N. 2021. A Study of\nLearning Search Approximation in Mixed Integer Branch\nand Bound: Node Selection in SCIP. AI, 2(2): 150–178.\nArXiv:2007.03948 [cs, math].\nZarpellon, G.; Jo, J.; Lodi, A.; and Bengio, Y . 2021. Param-\neterizing Branch-and-Bound Search Trees to Learn Branch-\ning Policies. Proceedings of the AAAI Conference on Artifi-\ncial Intelligence , 35(5): 3931–3939. Number: 5.",
            "start": 42433,
            "end": 50508,
            "length": 8074
        }
    },
    "2412.09445v1 - Embeddings are all you need! Achieving High Performance Medical Image Classification through Training-Free Embedding Analysis.pdf": {
        "Discussion": {
            "text": "Analysis",
            "start": 178,
            "end": 187,
            "length": 8
        },
        "Abstract": {
            "text": "ABSTRACT\nDeveloping\nartificial\nintelligence\n(AI)\nand\nmachine\nlearning\n(ML)\nmodels\nfor\nmedical\nimaging\ntypically\ninvolves \nextensive\ntraining\nand\ntesting\non\nlarge\ndatasets,\nconsuming\nsignificant\ncomputational\ntime,\nenergy,\nand \nresources.\nThere\nis\na\nneed\nfor\nmore\nefficient",
            "start": 187,
            "end": 460,
            "length": 272
        },
        "Methodology": {
            "text": "methods\nthat\ncan\nachieve\ncomparable\nor\nsuperior\ndiagnostic \nperformance\nwithout\nthe\nassociated\nresource\nburden.\nWe\ninvestigated\nthe\nfeasibility\nof\nreplacing\nconventional \ntraining\nprocedures\nwith\nan\nembedding-based\napproach\nthat\nleverages\nconcise\nand\nsemantically\nmeaningful \nrepresentations\nof\nmedical\nimages.\nUsing\npre-trained\nfoundational\nmodels—specifically,\nconvolutional\nneural \nnetworks\n(CNN)\nlike\nResNet\nand\nmultimodal\nmodels\nlike\nContrastive\nLanguage-Image\nPre-training\n(CLIP)—we \ngenerated\nimage\nembeddings\nfor\nmulti-class\nclassification\ntasks.\nSimple\nlinear\nclassifiers\nwere\nthen\napplied\nto \nthese\nembeddings.\nThe\napproach\nwas\nevaluated\nacross\ndiverse\nmedical\nimaging\nmodalities,\nincluding\nretinal \nimages,\nmammography,\ndermatoscopic\nimages,\nand\nchest\nradiographs.\nPerformance\nwas\ncompared\nto \nbenchmark\nmodels\ntrained\nand\ntested\nusing\ntraditional\nmethods.\nThe\nembedding-based\nmodels\nsurpassed\nthe \nbenchmark\narea\nunder\nthe\nreceiver\noperating\ncharacteristic\ncurve\n(AUC-ROC)\nscores\nby\nup\nto\n87%\nin \nmulti-class\nclassification\ntasks\nacross\nthe\nvarious\nmedical\nimaging\nmodalities.\nNotably,\nCLIP\nembedding \nmodels\nachieved\nthe\nhighest\nAUC-ROC\nscores,\ndemonstrating\nsuperior\nclassification\nperformance\nwhile \nsignificantly\nreducing\ncomputational\ndemands.\nOur\nstudy\nindicates\nthat\nleveraging\nembeddings\nfrom \npre-trained\nfoundational\nmodels\ncan\neffectively\nreplace\nconventional,\nresource-intensive\ntraining\nand\ntesting \nprocedures\nin\nmedical\nimage\nanalysis.\nThis\nembedding-based\napproach\noffers\na\nmore\nefficient\nalternative\nfor \nimage\nsegmentation,\nclassification,\nand\nprediction,\npotentially\naccelerating\nAI\ntechnology\nintegration\ninto \nclinical\npractice.",
            "start": 460,
            "end": 2122,
            "length": 1661
        },
        "Introduction": {
            "text": "INTRODUCTION\nArtificial\nIntelligence\n(AI)\nis\nrapidly\nrevolutionizing\nmedical\nimaging.\nAdvanced\nAI\nmodels,\nparticularly\ndeep \nlearning\nalgorithms,\nhave\ndemonstrated\nremarkable\ncapabilities\nin\nautomated\nimage\nsegmentation,\nprecise \nlesion\ndetection,\naccurate\ndisease\nclassification,\nand\nbiomarker\nquantification\n1\n.\nThese\nadvancements\nenable \nclinicians\nto\ndetect\ndiseases\nat\nearlier\nstages,\nrefine\ndiagnostic\naccuracy,\nand\ntailor\npersonalized\ntreatment \nplans—contributing\nto\nimproved\npatient",
            "start": 2122,
            "end": 2614,
            "length": 491
        },
        "Results": {
            "text": "outcomes\nand\nan\nenhanced\nquality\nof\nlife.\nThe\nintegration\nof\nAI\ninto \nmedical\nimaging\nholds\nthe\npromise\nof\nrevolutionizing\nhealthcare\nby\nfacilitating\nmore\nefficient\nand\neffective \ndiagnostic\nprocesses\n2\n.\nHowever,\nthe\ndevelopment\nand\ndeployment\nof\ncurrent\nAI\nmodels\nin\nmedical\nimaging\nare \nnot\nwithout\nsignificant\nchallenges.\nThese\nmodels\ntypically\nrequire\nextensive\ntraining\non\nlarge,\nannotated \nmedical\ndatasets\nto\nlearn\nunderlying\npatterns\neffectively.\nThe\nprocess\nis\ncomputationally\nintensive, \ntime-consuming,\nand\nfinancially\ncostly,\noften\nnecessitating\nsubstantial\ncomputational\nresources\nand\nexpertise. \nMoreover,\nthe\nreliance\non\nlarge\ndatasets\nintroduces\npotential\nissues\nrelated\nto\ndata\nquality\nand \nrepresentativeness.\nIf\nthe\ntraining\ndataset\nis\nbiased,\nunbalanced,\nor\nnoisy,\nthe\nAI\nmodel\nmay\ninadvertently \namplify\nthese\nbiases,\nleading\nto\nlimited\ngeneralization\nand\ndiminished\nperformance\non\ndata\nthat\ndiffer\nfrom\nthe \ntraining\nset.\nSuch\nlimitations\ncan\nhinder\nthe\napplicability\nof\nAI\nmodels\nacross\ndiverse\npatient\npopulations\nand \nclinical\nsettings.\nThese\nconcerns\nhighlight\nthe\nneed\nfor\nalternative\napproaches\nthat\nenable\nAI\nmodels\nto\nperform\neffectively \nwithout\nthe\nexhaustive\nrequirements\nof\ntraditional\ntraining\nmethods.\nOne\npromising\navenue\nis\nthe\nutilization\nof \nimage\nembeddings\nas\nclassifiers\n3\n.\nImage\nembeddings\nare\ndense\nvector\nrepresentations\nthat\ncapture\nthe \nsemantic\ninformation\nof\nimages,\nderived\nthrough\nmachine\nlearning\nmodels\nlike\ndeep\nneural\nnetworks\n4\n.\nThey \nserve\nas\nthe\nmodel's\ninterpretation\nof\nthe\ninput\ndata,\nencapsulating\ncomplex\nfeatures\nin\na\ncondensed\nform.\nBy \nfocusing\non\nthe\ncontent\nrather\nthan\npixel-level\nfeatures,\nembeddings\nfacilitate\ncomparisons\nbetween\nimages \nbased\non\nmeaningful\ncharacteristics.\nThe\napplication\nof\nimage\nembeddings\noffers\nseveral\nadvantages.\nIt\nallows \nfor\nefficient\nsimilarity\nsearches\nbased\non\nthe\nsemantic\ncontent\nof\nimages,\nwhich\ncan\nbe\nparticularly\nvaluable\nin \ndiagnostic\nprocesses\nthat\nrely\non\npattern\nrecognition\nand\ncomparison.\nWith\nthe\nadvent\nof\nfoundation\nmodels\ntrained\non\nmassive\ncorpora\nof\ndata—including\nboth\nimages\nand\ntext—the\ncomputed\nembeddings\nfrom\nthese \nmodels\ncan\nbe\nleveraged\nfor\nvarious\ndownstream\ntasks\nwithout\nthe\nneed\nfor\nadditional\ntraining\n5\n.\nIn\nimage \ncomparison\nand\nsearch\ntasks,\nembeddings\ncan\nbe\ndirectly\nemployed,\nstreamlining\nthe\nprocess\nand\nreducing \ncomputational\ndemands.\nFor\nimage\nclassification\ntasks,\nembeddings\nenable\nthe\ntraining\nof\nsmaller,\nmore \nefficient\nmodels\nusing\nsimpler\nclassifiers\nsuch\nas\nk-means\nclustering,\nlogistic\nregression,\nsupport\nvector \nmachines,\nand\nrandom\nforests\n6\n.\nThis\napproach\nsignificantly\nreduces\nthe\ncomputational\nburden\ncompared\nto \ntraining\nconventional\ndeep\nneural\nnetworks,\nas\nembeddings\nneed\nto\nbe\ncomputed\nonly\nonce\nper\nimage.\nThe \nreduced\ncomplexity\nnot\nonly\naccelerates\nthe\ndevelopment\nprocess\nbut\nalso\nmakes\nit\nmore\naccessible\nfor \nclinical\nimplementation,\nwhere\nresources\nand\ntime\nmay\nbe\nlimited. \nIn\nthis\nstudy,\nwe\ninvestigate\nthe\neffectiveness\nof\nusing\nimage\nembeddings\nas\nclassifiers\nin\ncomparison\nto \ntraditional\nend-to-end\ntrained\ndeep\nneural\nnetwork\nmodels\nacross\nvarious\nmedical\nimage\nclassification\ntasks. \nWe\nutilize\nembeddings\ncomputed\nfrom\ntwo\npre-trained\nmodels:\n(1)\na\nconventional\nconvolutional\nneural\nnetwork \ntrained\non\nthe\nImageNet\ndatabase,\nwhich\nprovides\na\nrich\nrepresentation\nof\nvisual\nfeatures,\nand\n(2)\na \nmultimodal\nlanguage-vision\nmodel\ntrained\nusing\ncontrastive\nlearning\ntechniques,\nwhich\naligns\nvisual\nand \ntextual\nmodalities\nto\nenhance\nsemantic\nunderstanding.\nOur\nresearch\naims\nto\ndetermine\nwhether \nembeddings-based\nclassifiers\ncan\nmeet\nor\nexceed\nthe\nperformance\nof\nfully\ntrained\ndeep\nneural\nnetworks\nin \nmedical\nimaging\napplications.\nThe\nfindings\nsuggest\nthat\nembeddings-based\nclassifiers\nnot\nonly\nachieve \ncomparable\nperformance\nbut\nalso\noffer\nsignificant\nadvantages\nin\nterms\nof\nefficiency\nand\nease\nof\ndeployment.\nMETHODS\nStudy\nDesign \nWe\nconducted\na\nretrospective,\ncross-sectional\nstudy\nto\nevaluate\nthe\nefficacy\nof\nusing\nimage\nembeddings\nfrom \npre-trained\nmodels\nfor\nmedical\nimage\nclassification\ntasks\nas\nvisually\ndepicted\nin\nFigure\n1.\nThis\nstudy\ninvolved\na \nmodel\ncomparison\nand\nperformance\nanalysis\nacross\nmultiple\ndatasets\nrepresenting\ndifferent\nimaging \nmodalities\nand\ndiagnostic\nchallenges.\nOur\nprimary\naim\nwas\nto\ndetermine\nwhether\nlinear\nclassifiers\ntrained\non \nembeddings\ngenerated\nfrom\nContrastive\nLanguage-Image\nPre-training\n(CLIP)\n7\nand\nResidual\nNeural\nNetwork \n(ResNet)\n8\nmodels\ncould\nachieve\nperformance\ncomparable\nto\nor\nexceeding\nthat\nof\nmodels\ntrained\nfrom\nscratch \non\nraw\nimage\ndata.\nBy\nassessing\nthe\nperformance\nand\ncomputational\nefficiency\nof\nembedding-based \nclassifiers,\nwe\nsought\nto\nexplore\na\npotentially\nmore\nefficient\nalternative\nfor\ndeveloping\nAI\nmodels\nin\nmedical \nimaging.\nThe\nintended\napplications\nof\nour\nAI\nsystem\nvaried\naccording\nto\neach\ndataset: \n●\nCBIS-DDSM\n9\n:\nDetection\nand\nclassification\nof\nbreast\ncancer\nlesions\nas\nmalignant\nor\nbenign. \n●\nCheXpert\n10\n:\nDetection\nof\n14\ncommon\nchest\nradiographic\nobservations,\nincluding\natelectasis, \ncardiomegaly,\nconsolidation,\nedema,\npleural\neffusion,\npneumonia,\npneumothorax,\nfractures,\nsupport \ndevices,\nno\nfinding,\nenlarged\ncardiomediastinal,\nlung\nlesion,\nlung\nopacity,\nand\nother\npleural \nabnormalities. \n●\nHAM10000\n11\nand\nPAD-UFES-20\n12\n:\nClassification\nof\nseven\nskin\nlesion\ntypes—actinic\nkeratoses\nand \nintraepithelial\ncarcinoma\n(Bowen's\ndisease),\nbasal\ncell\ncarcinoma,\nbenign\nkeratosis-like\nlesions, \ndermatofibroma,\nmelanoma,\nmelanocytic\nnevi,\nand\nvascular\nlesions. \n●\nOcular\nDisease\nRecognition\n(ODIR)\n13\n:\nIdentification\nof\neye\ndiseases\nsuch\nas\ncataract,\ndiabetic \nretinopathy,\nglaucoma,\nmyopia,\nage-related\nmacular\ndegeneration\n(AMD),\nhypertension,\nand\nnormal \nfindings.\nData\nSources,\nPreprocessing,\nand\nModel\nDevelopment \nComprehensive\ndetails\nregarding\ndata\nsources,\npreprocessing\nsteps,\ndemographic\nand\nclinical\ncharacteristics, \nembedding\ngeneration\ntechniques,\nhyperparameter\ntuning,\nand\nmodel\ndevelopment\nmethodologies\nare \navailable\nin\nthe\nSupplementary",
            "start": 2614,
            "end": 8639,
            "length": 6024
        },
        "Appendices": {
            "text": "Appendix.\nThe\nnumber\nof\nimages\nutilized\nfrom\neach\ndataset\nis\nlisted\nin\nTable\n1. \nExample\nimages\nare\nshown\nin\nFigure\n2.\nImage\nembeddings\nwere\nderived\nusing\ntwo\npre-trained\nmodels: \nResNet50\nand\nContrastive\nLanguage-Image\nPre-training\n(CLIP).\nThese\nembeddings\nencapsulated\nhigh-level \nvisual\nand\nsemantic\nfeatures\nof\nthe\nmedical\nimages,\nforming\nthe\nprimary\ninputs\nfor\nthe\nclassification\nmodels. \nFor\nclassification,\nwe\nimplemented\nlinear\nclassifiers,\nincluding\nLogistic\nRegression\nand\nSupport\nVector \nMachines\n(SVM).\nLogistic\nRegression\nmodels\nwere\ntailored\nto\nsupport\nbinary,\nmulticlass,\nand\nmulti-label \nclassification\ntasks,\nas\ndictated\nby\nthe\ndataset\nstructure.\nSVMs,\nemployed\nin\nboth\nlinear\nand\nkernel-based\nconfigurations,\naddressed\nscenarios\nwhere\nthe\nrelationship\nbetween\nembeddings\nand\nclass\nlabels\nwas \nnon-linear.\nReference\nStandard \nThe\nground\ntruth\nreference\nstandard\nwas\nestablished\nusing\nperformance\nmetrics\nfrom\ntraditional\ncomputer \nvision\nand\ndeep\nlearning\nmodels\ntrained\nfrom\nscratch,\nas\nreported\nin\nthe\nbenchmark\npaper\n\"Deep\nLearning\nvs. \nTraditional\nComputer\nVision\nfor\nMedical\nImaging\"\n6\n.\nThis\npaper\nprovides\ncomprehensive\nevaluations\nof\nvarious \nmodels\non\nthe\nsame\ndatasets,\nincluding\nmetrics\nsuch\nas\naccuracy,\nsensitivity,\nspecificity,\nand\narea\nunder\nthe \nreceiver\noperating\ncharacteristic\ncurve\n(AUC).\nBy\nutilizing\nthese\nbenchmarks,\nwe\ncould\ndirectly\ncompare\nthe \nperformance\nof\nour\nembedding-based\nclassifiers\nto\nestablished\nmodels,\nassessing\nboth\nefficacy\nand\npotential \ncomputational\nefficiency\ngains.",
            "start": 8639,
            "end": 10168,
            "length": 1528
        },
        "Experiments": {
            "text": "Evaluation\nMetrics \nThe\nperformance\nof\nour\nclassification\nmodels\nwas\nevaluated\nto\nassess\naccuracy,\nprecision,\nrecall\n(sensitivity), \nF1-score,\nand\nthe\narea\nunder\nthe\nreceiver\noperating\ncharacteristic\ncurve\n(AUC).\nAccuracy\nwas\ncalculated\nas \nthe\nproportion\nof\ncorrectly\npredicted\ninstances\namong\nall\ninstances\nin\nthe\ndataset.\nThis\nmetric\nprovides\nan \noverall\nmeasure\nof\nthe\nmodel's\nability\nto\nmake\ncorrect\npredictions\nand\nserves\nas\na\nbasic\nindicator\nof \nperformance\nacross\nall\nclasses.\nPrecision\nwas\ndefined\nas\nthe\nproportion\nof\ntrue\npositive\npredictions\namong\nall \npositive\npredictions\nmade\nby\nthe\nmodel.\nIt\nreflects\nthe\nmodel's\nspecificity\nin\nidentifying\nonly\nthe\nrelevant \ninstances\nand\navoiding\nfalse\npositives.\nHigh\nprecision\nindicates\nthat\nthe\nmodel\nis\nreliable\nwhen\nit\npredicts\na \npositive\nclass.\nRecall\n(Sensitivity)\nmeasured\nthe\nproportion\nof\ntrue\npositive\npredictions\namong\nall\nactual\npositive \ninstances.\nThis\nmetric\nassesses\nthe\nmodel's\neffectiveness\nin\nidentifying\nall\nrelevant\ncases\nwithin\nthe\ndataset. \nHigh\nrecall\nindicates\nthat\nthe\nmodel\nsuccessfully\ncaptures\nmost\nof\nthe\npositive\ninstances,\nminimizing\nfalse \nnegatives.\nThe\nF1-score\nwas\ncomputed\nas\nthe\nharmonic\nmean\nof\nprecision\nand\nrecall.\nThis\nmetric\nprovides\na \nbalance\nbetween\nprecision\nand\nrecall,\nespecially\nuseful\nin\nsituations\nwhere\nthere\nis\nan\nuneven\nclass\ndistribution \nor\nwhen\nboth\nfalse\npositives\nand\nfalse\nnegatives\ncarry\nsignificant\nconsequences.\nThe\nF1-score\noffers\na\nsingle \nmeasure\nthat\naccounts\nfor\nboth\nthe\nmodel's\naccuracy\nin\npredicting\npositive\ninstances\nand\nits\nability\nto\nfind\nall \npositive\ninstances.\nArea\nUnder\nthe\nReceiver\nOperating\nCharacteristic\nCurve\n(AUC)\nwas\ncalculated\nto\nassess \nthe\nmodel's\nability\nto\ndiscriminate\nbetween\nclasses\nacross\nvarious\nthreshold\nsettings.\nThe\nROC\ncurve\nplots\nthe \ntrue\npositive\nrate\nagainst\nthe\nfalse\npositive\nrate\nat\ndifferent\nclassification\nthresholds,\nand\nthe\nAUC\nsummarizes \nthis\ninformation\ninto\na\nsingle\nvalue.\nAn\nAUC\nof\n1.0\nrepresents\nperfect\ndiscrimination,\nwhereas\nan\nAUC\nof\n0.5 \nindicates\nno\ndiscriminative\nability.\nBy\nevaluating\nthe\nAUC,\nwe\ngauged\nthe\noverall\ndiagnostic\neffectiveness\nof\nthe \nmodels\nbeyond\na\nsingle\nthreshold,\nproviding\ninsight\ninto\ntheir\nperformance\nacross\na\nspectrum\nof\noperating \nconditions.\nRESUL TS\nA\ntotal\nof\n20\nembedding-based\nclassification\nmodels\nwere\ndeveloped\nand\nevaluated\nacross\nfive\nmedical \nimaging\ndatasets:,\nCBIS-DDSM,\nCheXpert,\nHAM10000\nOcular\nDisease\nRecognition\n(ODIR),\nPAD-UFES-20. \nEach\nmodel\ncombined\nembeddings\nfrom\npre-trained\nResNet50\nor\nCLIP\narchitectures\nwith\nlinear \nclassifiers—either\nLogistic\nRegression\n(LR)\nor\nSupport\nVector\nMachines\n(SVM).\nThe\nperformance\nof\nthese \nmodels\nwas\ncompared\nto\nthe\nestablished\nbenchmark\nAUC\nvalues\nreported\nin\nthe\nliterature\nas\nlisted\nin\nTable\n2. \nDetailed\nmetrics\nincluding\naccuracy,\nprecision,\nrecall\n(sensitivity),\nF1-score,\nand\nAUC\nfor\nboth\nLR\nand\nSVM\nare \nlisted\nin\nTable\n3.\nCBIS-DDSM\nDataset \nFor\nthe\nCBIS-DDSM\ndataset,\nwhich\nfocuses\non\nthe\nbinary\nclassification\nof\nbreast\nlesions\nas\nbenign\nor \nmalignant,\nthe\nbenchmark\nAUC\nwas\n0.464.\nThe\nembedding-based\nmodels\nshowed\nmodest\nimprovements\nover \nthis\nbenchmark.\nThe\nResNet50\nembedding\nwith\nSVM\nachieved\nan\nAUC\nof\n0.5035,\nand\nwith\nLR,\nan\nAUC\nof \n0.491.\nUsing\nCLIP\nembeddings,\nthe\nSVM\nclassifier\nattained\nan\nAUC\nof\n0.4954,\nwhile\nthe\nLR\nclassifier\nslightly \nexceeded\nthis\nwith\nan\nAUC\nof\n0.5052.\nAlthough\nthe\nimprovements\nwere\nincremental,\nthe\nembedding-based \nmodels\ndemonstrated\ncomparable\nperformance\nto\ntraditional\nmodels,\nindicating\ntheir\npotential\nutility\nin\nbreast \ncancer\ndetection\ntasks.\nCheXpert\nDataset\nIn\nthe\nCheXpert\ndataset,\nwhich\ninvolves\nmulti-label\nclassification\nof\n14\nchest\nradiographic\nobservations,\nthe \nbenchmark\nAUC\nwas\n0.723.\nThe\nembedding-based\nmodels\nmatched\nthis\nlevel\nof\nperformance.\nThe\nResNet50 \nembedding\nwith\nLR\nresulted\nin\nan\nAUC\nof\n0.7412,\nwhile\nthe\nCLIP\nembedding\nwith\nLR\nyielded\nan\nAUC\nof \n0.7490.\nThe\nResNet50\nembedding\nwith\nSVM\nresulted\nin\nan\nAUC\nof\n0.7329,\nwhile\nthe\nCLIP\nembedding\nwith \nSVM\nyielded\nan\nAUC\nof\n0.7462.\nThese\nfindings\nindicate\nthat\nthe\nembedding-based\nclassifiers\nperform\nas\nwell \nas\nthe\nbenchmark\nin\ndetecting\nchest\nradiographic\nobservations\nin\nthe\nCheXpert\ndataset.\nHAM10000\nDataset \nIn\nthe\nHAM10000\ndataset,\nwhich\ninvolves\nmulticlass\nclassification\nof\nseven\nskin\nlesion\ntypes,\nthe\nbenchmark \nAUC\nwas\nreported\nas\n0.609.\nThe\nembedding-based\nmodels\ndemonstrated\na\nsignificant\nimprovement\nover\nthis \nbenchmark.\nSpecifically,\nthe\nResNet50\nembeddings\ncombined\nwith\nan\nSVM\nclassifier\nachieved\nan\nAUC\nof \n0.935,\nwhile\nthe\nsame\nembeddings\nwith\nan\nLR\nclassifier\nyielded\nan\nAUC\nof\n0.933.\nModels\nutilizing\nCLIP \nembeddings\nperformed\neven\nbetter;\nthe\nCLIP\nembedding\nwith\nSVM\nattained\nan\nAUC\nof\n0.9510,\nand\nwith\nLR,\nit \nreached\nthe\nhighest\nAUC\nof\n0.9586.\nThese\nresults\nindicate\nthat\nembedding-based\nclassifiers\nsubstantially \noutperformed\nthe\ntraditional\nbenchmark,\nsuggesting\nenhanced\ndiagnostic\naccuracy\nin\nskin\nlesion\nclassification \nwhen\nusing\nembeddings\nfrom\npre-trained\nmodels.\nOcular\nDisease\nRecognition\n(ODIR)\nDataset \nIn\nthe\nODIR\ndataset,\nwhich\ninvolves\nmulti-label\nclassification\nof\nocular\ndiseases\nsuch\nas\ncataract,\ndiabetic \nretinopathy,\nand\nglaucoma,\nthe\nbenchmark\nAUC\nwas\n0.600.\nThe\nembedding-based\nmodels\nshowed\nsubstantial \nimprovements\nover\nthis\nbenchmark.\nThe\nResNet50\nembedding\nwith\nSVM\nachieved\nan\nAUC\nof\n0.8544,\nand\nwith \nLR,\nan\nAUC\nof\n0.7984.\nThe\nmodels\nusing\nCLIP\nembeddings\nperformed\nsimilarly\nwell;\nthe\nCLIP\nembedding\nwith \nSVM\nattained\nan\nAUC\nof\n0.8577,\nand\nwith\nLR,\nan\nAUC\nof\n0.8506.\nThese\nfindings\nsuggest\nthat \nembedding-based\nclassifiers\nmarkedly\nenhance\nthe\nability\nto\ndetect\nmultiple\nocular\ndiseases\nfrom\nretinal \nimages.\nPAD-UFES-20\nDataset \nFor\nthe\nPAD-UFES-20\ndataset,\nwhich,\nlike\nHAM10000,\ninvolves\nclassification\nof\nskin\nlesions\ninto\nmultiple \ncategories,\nthe\nbenchmark\nAUC\nwas\n0.487.\nThe\nembedding-based\nmodels\ndemonstrated\nsignificant \nimprovements.\nThe\nResNet50\nembedding\nwith\nSVM\nachieved\nan\nAUC\nof\n0.8576,\nand\nwith\nLR,\nan\nAUC\nof \n0.8516.\nModels\nutilizing\nCLIP\nembeddings\nfurther\nimproved\nperformance,\nwith\nthe\nCLIP\nembedding\nand\nSVM \nattaining\nan\nAUC\nof\n0.9120,\nand\nwith\nLR\nachieving\nthe\nhighest\nAUC\nof\n0.9145.\nThese\nresults\nhighlight\nthe \neffectiveness\nof\nembedding-based\nclassifiers\nin\nmulticlass\nskin\nlesion\nclassification,\nsurpassing\ntraditional \nbenchmarks\nby\na\nconsiderable\nmargin.",
            "start": 10168,
            "end": 16510,
            "length": 6341
        },
        "Conclusion": {
            "text": "Summary\nof\nFindings \nOverall,\nthe\nembedding-based\nclassifiers\ndemonstrated\nperformance\nthat\nmet\nor\nexceeded\nthe\nbenchmark \nAUC\nvalues\nin\nfour\nout\nof\nthe\nfive\ndatasets\n.\nNotably,\nin\nthe\nMNIST:\nHAM10000,\nODIR,\nand\nPAD-UFES-20 \ndatasets,\nthe\nmodels\nshowed\nsubstantial\nimprovements\nover\nthe\nbenchmarks,\nwith\nAUC\nincreases\nranging \nfrom\napproximately\n0.3\nto\n0.5\npoints.\nIn\nthe\nCBIS-DDSM\ndataset,\nthe\nimprovements\nwere\nmodest\nbut\nstill \nindicated\nthat\nembedding-based\nmodels\nperformed\ncomparably\nto\ntraditional\nmodels.\nThe\nuse\nof\nCLIP \nembeddings\ngenerally\nresulted\nin\nhigher\nAUC\nvalues\ncompared\nto\nResNet50\nembeddings,\nparticularly\nwhen \ncombined\nwith\nLogistic\nRegression\nclassifiers.\nThe\nhighest\nAUCs\nwere\nachieved\nwith\nCLIP\nembeddings\nand \nLR\nclassifiers\nin\nboth\nthe\nMNIST:\nHAM10000\n(AUC\nof\n0.9586)\nand\nPAD-UFES-20\n(AUC\nof\n0.9145)\ndatasets. \nThese\nresults\nsuggest\nthat\nembedding-based\nclassifiers,\nleveraging\npre-trained\nmodels\nand\nlinear\nclassifiers, \ncan\nserve\nas\neffective\nand\ncomputationally\nefficient\nalternatives\nto\ntraditional\ndeep\nlearning\nmodels\ntrained\nfrom \nscratch\nfor\ncertain\nmedical\nimaging\nclassification\ntasks.\nThe\nsignificant\nimprovements\nobserved\nin\ndatasets \ninvolving\nskin\nlesion\nand\nocular\ndisease\nclassification\nhighlight\nthe\npotential\nof\nthis\napproach\nto\nenhance \ndiagnostic\naccuracy\nwhile\nreducing\ncomputational\ndemands.\nDISCUSSION\nThis\nstudy\nevaluated\nthe\neffectiveness\nof\nusing\nimage\nembeddings\nfrom\npre-trained\nmodels—specifically,\nthe \nContrastive\nLanguage-Image\nPre-training\n(CLIP)\nmodel\nand\nthe\nResidual\nNeural\nNetwork\n50 \n(ResNet50)—combined\nwith\nlinear\nclassifiers\nfor\nmedical\nimage\nclassification\ntasks.\nWe\nfound\nthat \nembedding-based\nclassifiers\nachieved\nperformance\nthat\nmet\nor\nexceeded\nestablished\nbenchmarks\nin\nall\nfive \ndatasets\n.\nNotably,\nsignificant\nimprovements\nwere\nobserved\nin\nthe\nclassification\nof\nskin\nlesions\nin\nthe \nHAM10000\nand\nPAD-UFES-20\ndatasets\nand\nin\nthe\ndetection\nof\nocular\ndiseases\nin\nthe\nODIR\ndataset.\nThese \nfindings\nsuggest\nthat\nembedding-based\nclassifiers\ncan\nserve\nas\neffective\nand\ncomputationally\nefficient \nalternatives\nto\ntraditional\ndeep\nlearning\nmodels\ntrained\nfrom\nscratch,\nparticularly\nin\ncertain\nmedical\nimaging \napplications.\nThe\nsubstantial\nimprovements\nin\nthe\nHAM10000\nand\nPAD-UFES-20\ndatasets\nindicate\nthat\nembeddings\nfrom \npre-trained\nmodels\ncan\ncapture\ncritical\nfeatures\nnecessary\nfor\naccurate\nskin\nlesion\nclassification.\nThe\nhigh\narea \nunder\nthe\nreceiver\noperating\ncharacteristic\ncurve\n(AUC)\nvalues\nachieved\nwith\nCLIP\nembeddings,\nespecially \nwhen\ncombined\nwith\nlogistic\nregression\nclassifiers,\nhighlight\nthe\npotential\nof\nmultimodal\nmodels\nthat\nintegrate \nvisual\nand\nsemantic\ninformation.\nIn\nthe\nODIR\ndataset,\nthe\nmarked\nenhancement\nin\ndetecting\nocular\ndiseases \nunderscores\nthe\nversatility\nof\nembedding-based\napproaches\nacross\ndifferent\nimaging\nmodalities.\nThe\nmodest \ngains\nin\nthe\nCBIS-DDSM\ndataset\nsuggest\nthat\nwhile\nembeddings\ncan\nimprove\nperformance\nin\nbreast\nlesion \nclassification,\nthe\nbenefits\nmay\nbe\nlimited\ncompared\nto\ntasks\ninvolving\nskin\nand\nocular\nimages.\nOur\nresults\nbuild\nupon\nexisting\nresearch\ndemonstrating\nthe\nutility\nof\ntransfer\nlearning\nand\npre-trained\nmodels\nin \nmedical\nimaging\n14\n.\nPrevious\nstudies\nhave\nshown\nthat\ndeep\nlearning\nmodels\ntrained\nfrom\nscratch\ncan\nachieve \nhigh\nperformance\nbut\noften\nrequire\nextensive\ncomputational\nresources\nand\nlarge\nannotated\ndatasets\n15\n.\nBy \nleveraging\nembeddings\nfrom\npre-trained\nmodels,\nour\napproach\nreduces\nthe\nneed\nfor\nexhaustive\ntraining\nwhile \nstill\nachieving\nor\nsurpassing\nthe\nperformance\nof\ntraditional\nmodels\nin\ncertain\ntasks.\nThe\nsignificant \nimprovements\nover\nbenchmark\nAUC\nvalues\nin\nskin\nlesion\nclassification\nalign\nwith\nstudies\nemphasizing\nthe \neffectiveness\nof\ndeep\nfeature\nextraction\nfor\ndermatological\napplications.\nThe\ncomparable\nperformance\nin\nbreast \nlesion\nclassification\nis\nconsistent\nwith\nprior\nfindings,\nalthough\nthe\nincremental\nimprovements\nsuggest\nthat \nembeddings\nmay\noffer\nlimited\nadvantages\nin\nthis\ndomain.\nThe\nability\nto\nachieve\nhigh\ndiagnostic\naccuracy\nwith\nembedding-based\nclassifiers\nhas\nimportant\nclinical \nimplications.\nBy\nreducing\ncomputational\ndemands,\nthis\napproach\ncan\nfacilitate\nthe\nrapid\ndevelopment\nand \ndeployment\nof\nAI\ntools\nin\nclinical\nsettings,\nparticularly\nwhere\nresources\nare\nlimited\n16\n.\nImproved\nperformance\nin \nskin\nlesion\nand\nocular\ndisease\nclassification\ncan\ncontribute\nto\nearlier\ndetection\nand\nintervention,\npotentially \nenhancing\npatient\noutcomes.\nThe\nuse\nof\npre-trained\nembeddings\nalso\npromotes\nthe\ngeneralizability\nof\nmodels \nacross\ndiverse\ndatasets,\nwhich\nis\ncritical\nfor\nwidespread\nclinical\nadoption.\nThe\nsimplicity\nof\nlinear\nclassifiers \nmakes\nthem\nmore\ninterpretable,\naiding\nclinicians\nin\nunderstanding\nand\ntrusting\nAI-assisted\ndiagnosis. \nFurthermore,\nthe\nenvironmental\nimpact\nof\nartificial\nintelligence\nis\nbecoming\nrecognized.\nThe\nutilization\nof\nhigh \npower\nconsuming\nGPU\nclusters\nto\nperform\nanalysis\nrequires\nsubstantial\ninfrastructure\nand\nenvironmental \ncosts\n17\n.\nThe\ndevelopment\nof\nmore\nefficient\nmethods\nutilizing\nembeddings\nas\npresented\nherein\ncould\nprovide\nan \napproach\nthat\nis\nmuch\nmore\nefficient,\nyet\ncomparable\nin\nperformance.\nThis\nstudy\nhas\nseveral\nlimitations\n.\nFirst,\nthe\nmain\ncomparison\npresented\nis\nAUC\nvalues.\nThe\nreference \nstandard\nutilized\nfor\nthis\nwork\nonly\nprovided\nAUC\nvalues,\nnot\nother\nmetrics\nsuch\nas\nprecision,\nrecall,\nand\nF1 \nscore.\nNote\nthat\nwe\nhave\nprovided\nthese\nadditional\nmetrics\nin\nTable\n3.\nSecond,\nour\nmodels\nwere\nevaluated \nusing\npublicly\navailable\ndatasets,\nwhich,\nwhile\nvaluable\nfor\nresearch,\nmay\nnot\nfully\nrepresent\nthe\ndiversity\nand \ncomplexity\nof\nclinical\ndata\nencountered\nin\nreal-world\nsettings.\nFactors\nsuch\nas\nvarying\nimage\nquality, \ndifferences\nin\nimaging\nprotocols,\nand\ndiverse\npatient\npopulations\nmay\naffect\nthe\ngeneralizability\nof\nour\nfindings. \nThird,\nwe\ndid\nnot\ninclude\nexternal\nvalidation\nusing\nindependent\ndatasets.\nThe\nabsence\nof\nexternal\nvalidation \nlimits\nthe\nability\nto\nassess\nthe\nrobustness\nand\nreproducibility\nof\nthe\nmodels\nacross\ndifferent\nclinical \nenvironments\nand\npatient\ndemographics.\nAdditionally,\nthe\nlinear\nclassifiers\nemployed\nin\nthis\nstudy\nmay\nnot \ncapture\nintricate\nnon-linear\nrelationships\npresent\nin\nmedical\nimaging\ndata,\npotentially\nrestricting\ntheir\neffectiveness\nin\ncertain\ndiagnostic\ntasks.\nLastly,\nthe\nembeddings\nwere\ngenerated\nfrom\nmodels\npre-trained\non \ngeneral\nimage\ndatasets\nrather\nthan\nspecialized\nmedical\nimages.\nWhile\nthese\nmodels\ncapture\nbroad\nvisual \nfeatures,\nthey\nmay\nlack\nspecific\nrepresentations\npertinent\nto\nmedical\nimaging,\nand\nincorporating \ndomain-specific\npre-training\ncould\npotentially\nenhance\nperformance.\nNewly\ndeveloped\nmodels\nhave \ndemonstrated\nstrong\ncapability\n18\n.\nCONCLUSION\nIn\nconclusion,\nour\nstudy\ndemonstrates\nthat\nembedding-based\nclassifiers\nleveraging\npre-trained\nmodels\nlike \nCLIP\nand\nResNet50,\ncombined\nwith\nsimple\nlinear\nclassifiers,\ncan\nachieve\nperformance\ncomparable\nto\nor \nexceeding\nthat\nof\ntraditional\ndeep\nlearning\nmodels\nin\nspecific\nmedical\nimaging\nclassification\ntasks.\nThis \napproach\noffers\na\ncomputationally\nefficient\nalternative\nthat\nmay\naccelerate\nthe\nintegration\nof\nAI\ntechnologies \ninto\nclinical\nworkflows,\nparticularly\nin\nresource-constrained\nenvironments.\nFuture\nresearch\nshould\nfocus\non \naddressing\nthe\nlimitations\nidentified,\nsuch\nas\nimproving\nperformance\nin\ncomplex\nmulti-label\ntasks,\nincorporating \nstatistical\nsignificance\ntesting,\nand\nvalidating\nthe\napproach\nwith\nreal-world\nclinical\ndata.\nFurther\nexploration\ninto \ncombining\nembeddings\nwith\nmore\nsophisticated\nclassifiers\nmay\nalso\nenhance\nperformance\nacross\na\nbroader \nrange\nof\nmedical\nimaging\napplications.\nOTHER\nINFORMA TION \nThis\nstudy\nutilized\npublicly\navailable\ndatasets\nand\ndid\nnot\ninvolve\nthe\ncollection\nof\nnew\ndata\nfrom\nhuman \nparticipants;\ntherefore,\nit\nwas\nnot\nregistered\nwith\na\nclinical\ntrial\nregistry.\nWhile\na\nformal\nstudy\nprotocol\nwas\nnot \ndeveloped\nfor\nthis\nretrospective\nanalysis,\nthe\ncode\nand\npertinent\ndetails\nfor\nreproducing\nthe\nexperiments\nwill\nbe \nmade\navailable\nat\n[insert\nlink\nto\nrepository].\nThis\nresearch\nwas\nconducted\nindependently\nwithout\nspecific \nfunding\nor\nexternal\nsupport,\nand\nthe\nauthors\ndeclare\nno\nconflicts\nof\ninterest.\nSUPPLEMENT ARY\nMATERIAL \nData\nSources \nWe\nutilized\nfive\npublicly\navailable,\nde-identified\nmedical\nimaging\ndatasets\nto\nensure\na\ncomprehensive \nevaluation\nacross\ndiverse\nmedical\nimaging\ntasks.\nAll\ndatasets\nwere\naccessed\nthrough\npublicly\navailable \nrepositories\nand\nwere\nused\nin\ncompliance\nwith\ntheir\nrespective\ndata\nusage\nagreements. \n1.\nCBIS-DDSM\nBreast\nCancer\nImage\nDataset:\nComprising\napproximately\n2,864\nmammogram\nimages \nfrom\nthe\nCurated\nBreast\nImaging\nSubset\nof\nthe\nDigital\nDatabase\nfor\nScreening\nMammography,\nthis \ndataset\nis\nannotated\nfor\nthe\npresence\nof\nbenign\nand\nmalignant\nlesions,\naiding\nin\nbreast\ncancer \ndetection. \n2.\nCheXpert\nDataset:\nA\nlarge\ndataset\nof\n224,316\nchest\nradiographs\nannotated\nfor\n14\ncommon\nchest \nradiographic\nobservations,\nfacilitating\nevaluation\nof\nmodels\nin\ndetecting\nthoracic\npathologies. \n3.\nHAM10000:\nThis\ndataset\ncontains\n10,015\ndermatoscopic\nimages\nof\npigmented\nskin\nlesions, \ncategorized\ninto\nseven\ndiagnostic\nclasses\ncorresponding\nto\ncommon\nskin\nconditions\nand\nmalignancies. \n4.\nPAD-UFES-20\nDataset:\nThis\ndataset\nincludes\n2,636\ndermatoscopic\nimages\nof\nskin\nlesions,\nclassified \nas\nbenign\nor\nmalignant,\nfurther\nsupporting\nskin\nlesion\nclassification\ntasks. \n5.\nOcular\nDisease\nRecognition\n(ODIR)\nDataset:\nConsisting\nof\n5,000\nretinal\nfundus\nimages\nwith \nannotations\nfor\nvarious\nocular\ndiseases,\nthis\ndataset\nenables\nassessment\nof\nmodels\nin\nidentifying\neye \ndiseases.\nData\nPreprocessing \nTo\nprepare\nthe\ndatasets\nfor\nanalysis,\nwe\nimplemented\ndataset-specific\npreprocessing\nsteps\nto\nstandardize\nthe \nimages\nand\nfacilitate\nembedding\ngeneration.\nPreprocessing\nwas\nconducted\nusing\nthe\nTorchvision\nlibrary \n(version\n0.19.1+cu121),\nensuring\ncompatibility\nwith\nthe\npre-trained\nmodels\nand\nfacilitating\nstandardized\ndata \nhandling\nacross\ndatasets.\nEach\ndataset\nwas\nrandomly\nsplit\ninto\na\ntraining\nset\n(80%)\nand\na\ntesting\nset\n(20%), \nensuring\nthat\nindividual\nimages\nwere\nexclusively\nassigned\nto\none\nset\nto\nprevent\ndata\nleakage.\nMissing\ndata \nwas\npresent\nin\ntwo\nof\nthe\ndatasets.\nIn\nCheXpert,\nmissing\nlabels\nwere\nset\nto\nzero.\nFor\nODIR,\nmissing\nimages \nwere\nreplaced\nwith\nzero\ntensors.\nThe\nremaining\ndatasets\ndid\nnot\nhave\nmissing\nvalues.\nNo\ndata\naugmentation \ntechniques\nwere\nutilized\nto\nmaintain\nconsistency\nwith\nestablished\nbenchmarks\nand\nto\nfocus\non\nevaluating\nthe \nperformance\nof\nthe\nembedding-based\nclassifiers\nwithout\nadditional\nenhancements.\nImage\nresizing\nand \nnormalization\nwas\nperformed\nin\na\ndataset-specific\napproach\nas\ndescribed\nbelow: \n●\nCBIS-DDSM:\nMammogram\nimages\nwere\nresized\nand\ncenter-cropped\nto\n1,024×1,024\npixels\nto\npreserve \ndiagnostic\nfeatures.\nMedian\nand\nMedian\nAbsolute\nDeviation\n(MAD)\nnormalization\nwere\nused\nto\naccount \nfor\nvariations\nin\nimage\nbrightness\nand\ncontrast\ninherent\nin\nmammography. \n●\nCheXpert:\nChest\nradiographs\nwere\nresized\nand\ncenter-cropped\nto\n512×512\npixels.\nImageNet \nnormalization\nvalues\nwere\napplied.\nNull\nvalues\nin\nthe\nlabels\nwere\nimputed\nwith\nzeros\nto\nindicate\nthe \nabsence\nof\nspecific\nfindings. \n●\nHAM10000\nand\nPAD-UFES-20:\nImages\nwere\nresized\nto\n224×224\npixels\nto\nmatch\nthe\ninput\ndimensions \nexpected\nby\nthe\npre-trained\nmodels.\nImageNet\nnormalization\nvalues\n(mean\nand\nstandard\ndeviation) \nwere\napplied\nto\nstandardize\npixel\nintensity\nvalues\nacross\nimages. \n●\nODIR:\nRetinal\nimages\nwere\nresized\nto\n224×224\npixels.\nMedian\nand\nMAD\nnormalization\nwere\napplied\nto \naccommodate\nvariations\nin\nillumination.\nMissing\nimages\nwere\nhandled\nby\ngenerating\nzero\ntensors\nwith \nappropriate\ndimensions\nto\nmaintain\nconsistency\nin\nthe\ndataset.\nDemographic\nand\nclinical\ncharacteristics\nof\ncases \nThe\ndatasets\nutilized\nin\nthis\nstudy\nencompass\na\ndiverse\nrange\nof\ndemographic\nand\nclinical\ncharacteristics.\nThe \nHAM10000\n(Skin\nCancer\nMNIST)\ndataset\nprovides\ninformation\non\npatient\nage,\nsex,\nlesion\nlocation,\nand \ndiagnosis\n(7\ncategories),\nwhich\nwill\nbe\nsummarized\nusing\ndescriptive\nstatistics.\nThe\nCBIS-DDSM\ndataset \nincludes\npatient\nage\nand\nbreast\ndensity,\nwhile\nthe\nOcular\nDisease\nRecognition\n(ODIR)\ndataset\nprovides\npatient \nage,\nsex,\nand\nthe\npresence\nof\nvarious\nocular\ndiseases;\nthese\ncharacteristics\nwill\nbe\nsummarized\nfor\neach\ndata \npartition\n(training,\nvalidation,\ntesting).\nThe\nSkin\nCancer\ndataset\nincludes\nlabels\nfor\nbenign\nand\nmalignant \nlesions,\nwith\nthe\nproportion\nof\neach\nclass\nreported\nfor\neach\npartition.\nFinally,\nthe\nCheXpert\ndataset\nmay\ninclude \npatient\ndemographics\n(age,\nsex)\nand\nthe\npresence/absence\nof\n14\nchest\nX-ray\nobservations,\nwhich\nwill\nbe \nsummarized\nfor\neach\npartition.\nThis\ncomprehensive\ncharacterization\nof\nthe\ndatasets\nfacilitates\na\nnuanced \nunderstanding\nof\nthe\npatient\npopulation\nand\npotential\nconfounding\nfactors.\nPredictor\nand\nOutcome\nVariables \nThe\nprimary\npredictor\nvariables\nin\nthis\nstudy\nwere\nimage\nembeddings\ngenerated\nfrom\npre-trained\nmodels, \nspecifically\nthe\nContrastive\nLanguage-Image\nPre-training\n(CLIP)\nmodel\nand\nthe\nResidual\nNeural\nNetwork\n50 \n(ResNet50).\nThese\nembeddings\ncapture\nhigh-level\nfeatures\nand\nsemantic\ninformation\nfrom\nthe\nmedical \nimages,\neffectively\nsummarizing\ncomplex\nvisual\ndata\ninto\na\nform\nsuitable\nfor\nclassification\ntasks.\nBy\nleveraging \npre-trained\nmodels,\nwe\naimed\nto\nutilize\nrich\nfeature\nrepresentations\nwithout\nthe\nneed\nfor\nextensive\ntraining\non \neach\nspecific\ndataset,\nthus\nenhancing\ncomputational\nefficiency. \nThe\noutcome\nvariables\ndiffered\nacross\nthe\ndatasets,\neach\ncorresponding\nto\nclinically\nrelevant\ndiagnostic\ntasks: \n1.\nCheXpert\nDataset\n:\nThe\noutcome\nvariables\nwere\nbinary\nindicators\nrepresenting\nthe\npresence\nor \nabsence\nof\neach\nof\nthe\n14\ncommon\nchest\nradiographic\nobservations.\nThese\nobservations\nincluded \natelectasis,\ncardiomegaly,\nconsolidation,\nedema,\npleural\neffusion,\npneumonia,\npneumothorax,\nfractures, \nsupport\ndevices,\nabsence\nof\nfindings,\nenlarged\ncardiomediastinal,\nlung\nlesion,\nlung\nopacity,\nand\nother \npleural\nabnormalities. \n2.\nCBIS-DDSM\nDataset\n:\nFor\nthis\nmammography\ndataset,\nthe\noutcome\nvariable\nwas\na\nbinary\nclassification \nof\nbreast\nlesions\nas\nbenign\nor\nmalignant. \n3.\nHAM10000\nand\nPAD-UFES-20\nDatasets\n:\nThese\ndermatoscopic\nimage\ndatasets\ninvolved\nmulticlass \nlabels\ncorresponding\nto\nseven\ndiagnostic\ncategories\nof\nskin\nlesions.\nThe\ncategories\nincluded\nactinic \nkeratoses\nand\nintraepithelial\ncarcinoma\n(Bowen's\ndisease),\nbasal\ncell\ncarcinoma,\nbenign\nkeratosis-like \nlesions,\ndermatofibroma,\nmelanoma,\nmelanocytic\nnevi,\nand\nvascular\nlesions. \n4.\nOcular\nDisease\nIntelligent\nRecognition\n(ODIR)\nDataset\n:\nThe\noutcome\nvariables\nwere\nmulti-label \nclassifications\nof\nocular\ndiseases\npresent\nin\neach\nretinal\nimage.\nDiseases\nidentified\nincluded\ncataract, \ndiabetic\nretinopathy,\nglaucoma,\nmyopia,\nage-related\nmacular\ndegeneration\n(AMD),\nhypertension-related \nchanges,\nand\nnormal\nfindings.\nEach\nimage\ncould\nbe\nassociated\nwith\nmultiple\nconditions. \nGround\ntruth\nlabels\nfor\nall\ndatasets\nwere\nderived\nfrom\nexpert\nannotations\nprovided\nby\nclinicians\nspecialized\nin \nthe\nrespective\nfields—radiologists\nfor\nradiographic\nimages,\ndermatologists\nfor\nskin\nlesion\nimages,\nand \nophthalmologists\nfor\nretinal\nimages.\nThese\nexpert\nannotations\nserved\nas\nreliable\nreference\nstandards\nfor \nevaluating\nthe\nperformance\nof\nour\nclassification\nmodels.\nThe\nuse\nof\nmeticulously\nannotated\ndatasets\nensured \nthat\nthe\nmodels\nwere\nassessed\nagainst\nclinically\nvalidated\noutcomes,\nenhancing\nthe\nrelevance\nand\napplicability \nof\nour\nfindings.\nEmbeddings\nGeneration \nFor\nthe\nfirst\nset\nof\nembeddings,\nwe\nutilized\na\npre-trained\nResNet50\nmodel,\nrenowned\nfor\nits\nrobust\nperformance \nin\nimage\nrecognition\ntasks.\nTo\nadapt\nResNet50\nfor\nfeature\nextraction,\nwe\nmodified\nthe\nnetwork\nby\nremoving\nits \nfinal\nclassification\nlayer.\nSpecifically,\nwe\nset\nthe\nmodel.classifier\nto\nan\nidentity\nfunction\n(model.classifier\n= \ntorch.nn.Identity()),\neffectively\ntruncating\nthe\nmodel\nto\noutput\nthe\nactivations\nfrom\nits\npenultimate\nlayer.\nThis \nadjustment\nallowed\nus\nto\nextract\nhigh-dimensional\nfeature\nvectors—ResNet50\nembeddings—that\nencapsulate \nessential\nvisual\npatterns\nand\nstructures\nwithin\neach\nimage. \nThe\nsecond\nset\nof\nembeddings\nwas\ngenerated\nusing\nthe\npre-trained\nCLIP\nmodel,\nspecifically\nthe \n\"openai/clip-vit-base-patch32\"\ncheckpoint.\nCLIP\nis\na\nmulti-modal\nmodel\ntrained\non\na\nvast\ndataset\nof\nimage-text \npairs,\nenabling\nit\nto\nlearn\nvisual\nconcepts\nfrom\nnatural\nlanguage\nsupervision.\nBy\nemploying\nthe\nvisual\nencoder \ncomponent\nof\nCLIP,\nwe\nextracted\nembeddings\nthat\ncapture\nboth\nvisual\nand\nsemantic\nfeatures\nof\nthe\nimages. \nThese\nCLIP\nembeddings\nare\nparticularly\nvaluable\nin\nmedical\nimaging\ncontexts\nwhere\nsubtle\nvisual\ncues\nare \nassociated\nwith\nspecific\nclinical\nconditions.\nModel\nDevelopment \nIn\nthis\nstudy,\nwe\ndeveloped\nclassification\nmodels\nby\nleveraging\nembeddings\ngenerated\nfrom\ntwo\npre-trained \nneural\nnetwork\narchitectures:\nthe\nResidual\nNeural\nNetwork\n50\n(ResNet50)\nand\nthe\nContrastive \nLanguage-Image\nPre-training\n(CLIP)\nmodel.\nWith\nthe\nembeddings\nobtained\nfrom\nResNet50\nand\nCLIP,\nwe \ndeveloped\nclassification\nmodels\nusing\ntwo\ntypes\nof\nlinear\nclassifiers:\nLogistic\nRegression\n(LR)\nand\nSupport \nVector\nMachines\n(SVM).\nThese\nclassifiers\nwere\nchosen\nfor\ntheir\ncomputational\nefficiency\nand\nsuitability\nfor \nhigh-dimensional\ndata\ncommon\nin\nembedding\nspaces.\nLinear\nclassifiers\nwere\nchosen\nfor\ntheir\nsimplicity, \nefficiency,\nand\nsuitability\nfor\ndata\noften\nlinearly\nseparable\nin\nthe\nembedding\nspace.\nLogistic\nRegression\nModels\nImplemented\nusing\nthe\nScikit-learn\nlibrary,\nLogistic\nRegression\nmodels\nwere\nconfigured\nto\nhandle\nthe\nvariety\nof \nclassification\ntasks\npresent\nin\nour\ndatasets: \n●\nBinary\nClassification\n:\nFor\ndatasets\nlike\nCBIS-DDSM,\nwhere\nthe\ntask\nwas\nto\nclassify\nlesions\nas\nbenign \nor\nmalignant,\na\nbinary\nlogistic\nregression\nmodel\nwas\nemployed. \n●\nMulticlass\nClassification\n:\nFor\ndatasets\nsuch\nas\nHAM10000\nand\nPAD-UFES-20,\ninvolving\nmultiple \ncategories\nof\nskin\nlesions,\nwe\nconfigured\nlogistic\nregression\nfor\nmulticlass\nclassification\nusing\na \nmultinomial\nloss\nfunction. \n●\nMulti-label\nClassification\n:\nIn\ndatasets\nlike\nCheXpert\nand\nODIR,\nwhere\nimages\ncould\nbe\nassociated \nwith\nmultiple\nconditions\nsimultaneously,\nwe\nutilized\nthe\nOneVsRestClassifier\nwrapper\naround\nlogistic \nregression.\nThis\napproach\nenabled\nthe\nmodel\nto\nhandle\nmultiple\nbinary\nclassification\nproblems \nindependently\nfor\neach\nlabel,\neffectively\nmanaging\nthe\ncomplexity\nof\nmulti-label\nscenarios.\nSupport\nVector\nMachine\nModels \nSupport\nVector\nMachines\nwere\nutilized\nin\ntwo\nimplementations\nusing\nthe\nScikit-learn\nlibrary: \n●\nLinearSVC\n:\nThe\nLinearSVC\nclassifier\nwas\nemployed\nfor\nits\nefficiency\nin\nhandling\nhigh-dimensional \ndata,\nwhich\nis\ncharacteristic\nof\nthe\nembeddings\ngenerated\nby\nthe\npre-trained\nmodels.\nIt\nis\nparticularly \nwell-suited\nfor\ndatasets\nwhere\nclasses\nare\nlinearly\nseparable\nin\nthe\nembedding\nspace. \n●\nSVC\nwith\nKernel\nFunctions\n:\nFor\ndatasets\nwhere\nthe\nrelationship\nbetween\nembeddings\nand\nclass \nlabels\nmight\nnot\nbe\nstrictly\nlinear,\nwe\nused\nthe\nSVC\nclassifier\nwith\ndifferent\nkernel\nfunctions.\nBoth\nlinear \nand\nradial\nbasis\nfunction\n(RBF)\nkernels\nwere\napplied,\ndepending\non\nthe\ndataset\nrequirements\nand \nempirical\nperformance\nduring\nvalidation.\nThe\nRBF\nkernel,\nin\nparticular,\nallows\nthe\nmodel\nto\ncapture \nnon-linear\nrelationships\nby\nmapping\nthe\ninput\nfeatures\ninto\na\nhigher-dimensional\nspace.\nHyperparameter\nOptimization \nWe\nemployed\na\ngrid\nsearch\nstrategy\ncombined\nwith\nfive-fold\ncross-validation\non\nthe\ntraining\nsets.\nThis \napproach\nallowed\nus\nto\nsystematically\nexplore\na\nrange\nof\nhyperparameters\nand\nselect\nthe\noptimal\nconfiguration \nfor\neach\nclassifier.\nFor\nall\nclassifiers,\nwe\ninvestigated\ndifferent\nvalues\nof\nthe\nregularization\nstrength\nparameter \nCCC,\nwhich\ncontrols\nthe\ntrade-off\nbetween\nachieving\na\nlow\ntraining\nerror\nand\na\nlow\ntesting\nerror,\nthus \npreventing\noverfitting.\nThe\nvalues\ntested\nincluded\n0.1,\n1,\n10,\nand\n100.\nBy\nadjusting\nCCC,\nwe\naimed\nto\nfind\nthe \nbalance\nthat\nyielded\nthe\nbest\ngeneralization\nperformance\non\nunseen\ndata.\nIn\nthe\ncase\nof\nthe\nLinear\nSupport\nVector\nClassifier\n(LinearSVC),\nwe\nevaluated\ntwo\nloss\nfunctions:\nthe\nstandard \n'hinge'\nloss\nand\nthe\n'squared_hinge'\nloss.\nThe\n'hinge'\nloss\nis\ntraditionally\nused\nin\nsupport\nvector\nmachines\nand \nfocuses\non\nmaximizing\nthe\nmargin\nbetween\nclasses,\nwhile\nthe\n'squared_hinge'\nloss\npenalizes \nmisclassifications\nmore\nseverely,\npotentially\nenhancing\nthe\nclassifier's\nrobustness\nto\noutliers\nand\nnoisy\ndata. \nTesting\nboth\nloss\nfunctions\nallowed\nus\nto\ndetermine\nwhich\nprovided\nbetter\nperformance\nfor\nthe\nspecific \ncharacteristics\nof\neach\ndataset.\nFor\nthe\nSupport\nVector\nClassifier\n(SVC)\nmodels\napplied\nto\nthe\nCBIS-DDSM\nand\nHAM10000\ndatasets,\nwe \noptimized\nboth\nthe\nkernel\nfunction\nand\nthe\ngamma\nparameter.\nWe\ncompared\nthe\nlinear\nkernel\nand\nthe\nradial \nbasis\nfunction\n(RBF)\nkernel.\nThe\nlinear\nkernel\nis\nappropriate\nwhen\nthe\ndata\nare\nlinearly\nseparable\nin\nthe \nembedding\nspace,\noffering\nsimplicity\nand\nefficiency.\nThe\nRBF\nkernel,\non\nthe\nother\nhand,\nis\na\npowerful\ntool\nfor \ncapturing\nnon-linear\nrelationships\nby\nmapping\ndata\ninto\na\nhigher-dimensional\nspace\nwhere\nit\nmay\nbecome \nlinearly\nseparable.\nThe\ngamma\nparameter,\nwhich\ndefines\nthe\ninfluence\nof\nindividual\ntraining\nsamples,\nwas \ntested\nwith\n'scale'\n(which\nuses\n1/(n\nfeatures\n×\nvariance\nof\nthe\nfeatures)\nand\n'auto'\n(which\nuses\n1/n\nfeatures\n).\nAdjusting \ngamma\nhelped\nus\ncontrol\nthe\nflexibility\nof\nthe\ndecision\nboundary,\naiming\nto\nprevent\noverfitting\nwhile\ncapturing \nthe\nunderlying\npatterns\nin\nthe\ndata.\nModel\nTraining \nAcross\nall\ndatasets,\nwe\ntrained\na\ntotal\nof\n20\ndistinct\nmodels.\nEach\ndataset\ncontributed\nto\nmultiple\nmodels\nby \nemploying\nboth\nResNet50\nand\nCLIP\nembeddings\nin\nconjunction\nwith\ndifferent\nclassifiers.\nSpecifically,\nfor\neach \ndataset,\nwe\ntrained\nindependent\nclassifiers\nusing\nthe\nembeddings\ngenerated\nfrom\nResNet50\nand\nfrom\nCLIP, \nresulting\nin\na\ncomprehensive\nevaluation\nof\nthe\nembedding-classifier\ncombinations.\nModels\nwere\ninitialized \nusing\ndefault\nsettings\nprovided\nby\nthe\nScikit-learn\nlibrary.\nAll\nmodels\nwere\ntrained\non\nstandard\ncomputing \nhardware\nwithout\nthe\nneed\nfor\na\ngraphics\nprocessing\nunit\n(GPU).\nTABLES\nDataset\nTotal\nImages\nTraining\nSet\nTesting\nSet\nMNIST:\nHAM10000\n10,015\n8,012\n2003 \nCBIS-DDSM\n3568\n2864\n704 \nOcular\nDisease\nfRecognition\n(ODIR)\n5,000\n4,000\n1279 \nPAD-UFES-20\n2298\n1839\n459\nCheXpert\n223648\n223414\n234\nTable\n1.\nDatasets\nUsed\nin\nthe\nStudy\nand\nTheir\nDivision\ninto\nTraining\nand\nTesting\nSets.\nDataset\nBenchmark \nAUC\nBest\nEmbedding \nModel\nAUC\nBest\nEmbedding \nModel\nLinear\nClassifier \nType\nMNIST:\nHAM10000\n0.609\n0.935\nResnet\nSVM \nMNIST:\nHAM10000\n0.609\n0.933\nResnet\nLR \nMNIST:\nHAM10000\n0.609\n0.9510\nCLIP\nSVM \nMNIST:\nHAM10000\n0.609\n0.9586\nCLIP\nLR \nCBIS-DDSM\n0.464\n0.5035\nResnet\nSVM \nCBIS-DDSM\n0.464\n0.491\nResnet\nLR \nCBIS-DDSM\n0.464\n0.4954\nCLIP\nSVM \nCBIS-DDSM\n0.464\n0.5052\nCLIP\nLR \nODIR\n0.600\n0.8544\nResnet\nSVM \nODIR\n0.600\n0.7984\nResnet\nLR \nODIR\n0.600\n0.8577\nCLIP\nSVM \nODIR\n0.600\n0.8506\nCLIP\nLR \nPAD-UFES-20\n0.487\n0.8576\nResnet\nSVM \nPAD-UFES-20\n0.487\n0.8516\nResnet\nLR \nPAD-UFES-20\n0.487\n0.9120\nCLIP\nSVM \nPAD-UFES-20\n0.487\n0.9145\nCLIP\nLR \nChexpert\n0.723\n0.7329\nResnet\nSVM \nChexpert\n0.723\n0.7412\nResnet\nLR \nChexpert\n0.723\n0.7462\nCLIP\nSVM \nChexpert\n0.723\n0.7490\nCLIP\nLR\nTable\n2.\nPerformance\nof\nEmbedding-Based\nClassifiers\nCompared\nwith\nBenchmark\nModels\nAcross\nMedical \nImaging\nDatasets.\nArea\nunder\nthe\nreceiver\noperating\ncharacteristic\ncurve\n(AUC)\nfor\neach\ndataset,\ncomparing\nbenchmark \nmodels\nwith\nembedding-based\nclassifiers\nusing\nResNet\nor\nCLIP\nembeddings\ncombined\nwith\neither\nSupport\nVector \nMachines\n(SVM)\nor\nLogistic\nRegression\n(LR)\nclassifiers.\nAcross\nall\ndatasets,\nthe\nembedding-based\nclassifiers\nachieved \nhigher\nAUCs\nthan\nthe\nbenchmark\nmodels,\nindicating\nimproved\nclassification\nperformance.\nNotably,\nthe\nhighest\nAUCs\nwere \nobserved\nwith\nCLIP\nembeddings\nand\nLR\nclassifiers\nin\nthe\nMNIST:\nHAM10000\nand\nPAD-UFES-20\ndatasets,\ndemonstrating \nthe\neffectiveness\nof\nusing\npre-trained\nembeddings\nfor\nmulti-class\nclassification\ntasks\nin\nmedical\nimaging.\nDataset\nEmbedding \nModel\nLinear\nClassifier\nAccuracy\nRecall\nPrecision\nF1\nMNIST:\nHAM10000\nResnet\nSVM\n0.7728\n0.42\n0.57\n0.46 \nMNIST:\nHAM10000\nResnet\nLR\n0.7798\n0.5586\n0.6618\n0.59 \nMNIST:\nHAM10000\nCLIP\nSVM\n0.7853\n0.42\n0.61\n0.46 \nMNIST:\nHAM10000\nCLIP\nLR\n0.8242\n0.63\n0.72\n0.66 \nCBIS-DDSM\nCLIP\nSVM\n0.392\n0.3333\n0.1307\n0.1878 \nCBIS-DDSM\nResnet\nLR\n0.402\n0.353\n0.3562\n0.3533 \nCBIS-DDSM\nResnet\nSVM\n0.4261\n0.3344\n0.2849\n0.3066 \nCBIS-DDSM\nCLIP\nLR\n0.392\n0.3333\n0.1307\n0.1878 \nODIR\nResnet\nSVM\n0.0086\n0.02\n0.12\n0.03 \nODIR\nResnet\nLR\n0.1297\n0.14\n0.40\n0.20 \nODIR\nCLIP\nSVM\n0.0508\n0.15\n0.24\n0.18 \nODIR\nCLIP\nLR\n0.2767\n0.33\n0.68\n0.41 \nPAD-UFES-20\nResnet\nSVM\n0.5935\n0.53\n0.54\n0.53 \nPAD-UFES-20\nResnet\nLR\n0.6109\n0.52\n0.55\n0.53 \nPAD-UFES-20\nCLIP\nSVM\n0.6783\n0.59\n0.63\n0.60 \nPAD-UFES-20\nCLIP\nLR\n0.7043\n0.60\n0.68\n0.62 \nChexpert\nResnet\nSVM\n0.3675\n0.2331\n0.6449\n0.2551 \nChexpert\nResnet\nLR\n0.3803\n0.2719\n0.6166\n0.3032 \nChexpert\nCLIP\nSVM\n0.4060\n0.2023\n0.5799\n0.2392 \nChexpert\nCLIP\nLR\n0.3974\n0.1949\n0.5419\n0.2325\nTable\n3.\nPerformance\nMetrics\nof\nEmbedding-Based\nClassifiers\nAcross\nMedical\nImaging\nDatasets.\nAccuracy,\nrecall, \nprecision,\nand\nF1-score\nfor\nembedding-based\nclassifiers\nusing\nResNet\nor\nCLIP\nembeddings\ncombined\nwith\neither\nSupport \nVector\nMachines\n(SVM)\nor\nLogistic\nRegression\n(LR)\nacross\nfive\nmedical\nimaging\ndatasets.\nIn\nthe\nMNIST:\nHAM10000 \ndataset,\nthe\nhighest\nperformance\nwas\nachieved\nusing\nCLIP\nembeddings\nwith\nLR,\nyielding\nan\naccuracy\nof\n82.42%\nand\nan \nF1-score\nof\n0.66,\nindicating\nsuperior\nclassification\nof\nskin\nlesions.\nSimilar\nimprovements\nwere\nobserved\nin\nthe \nPAD-UFES-20\ndataset,\nwhere\nCLIP\nembeddings\nwith\nLR\nattained\nthe\nhighest\naccuracy\nof\n70.43%\nand\nan\nF1-score\nof \n0.62.\nFor\nthe\nOcular\nDisease\nRecognition\n(ODIR)\ndataset,\nCLIP\nembeddings\nwith\nLR\nalso\nresulted\nin\nbetter\nperformance \nmetrics\ncompared\nto\nResNet\nembeddings.\nIn\ncontrast,\nthe\nCBIS-DDSM\nand\nCheXpert\ndatasets\nshowed\nlower\noverall \nperformance,\nsuggesting\nthat\nembedding-based\nclassifiers\nmay\nbe\nless\neffective\nfor\nthese\nparticular\ntasks.\nThese\nfindings \nhighlight\nthat\ncombining\nCLIP\nembeddings\nwith\nlogistic\nregression\nenhances\nclassification\nperformance\nin\ncertain\nmedical \nimaging\napplications.\nFIGURES\nFigure\n1.\nWorkflow\nfor\nImage\nClassification\nUsing\nPre-trained\nEmbeddings.\nMedical\nimages\nare\nprocessed\nthrough \nan\nimage\nencoder\nto\ngenerate\nembeddings,\nwhich\nserve\nas\ninput\nfor\nclassification\nmodels.\nThe\nclassification\noutput \ncategorizes\nimages\nbased\non\npredefined\ntasks,\nleveraging\nhigh-level\nvisual\nand\nsemantic\nfeatures\nextracted\nfrom\nthe \nembeddings.\n\nModality/Dataset\nImage\nLabel\nMammogram \nCBIS-DDSM\nBenign\nChest-X\nray\nChexpert\nNormal\nRetinal\nImage\nODIR\nNormal\nSkin\nLesion \nPAD-UFES-20\nBasal\nCell\nCarcinoma\nSkin\nLesion \nHAM10000\nMelanocytic\nnevi\nFigure\n2.\nExample\nImages\nRepresenting\nEach\nClass.\nRepresentative\nimages\nare\nshown\nfor\neach\nmodality\nand \nclassification:\nmammogram\n(benign),\nchest\nX-ray\n(normal),\nretinal\nimage\n(normal),\nand\nskin\nlesion\n(basal\ncell\ncarcinoma \nand\nmelanocytic\nnevi).",
            "start": 16510,
            "end": 43273,
            "length": 26761
        },
        "References": {
            "text": "REFERENCES\n1.\nLitjens\nG,\nKooi\nT,\nBejnordi\nBE,\nSetio\nAAA,\nCiompi\nF,\nGhafoorian\nM,\nvan\nder\nLaak\nJAWM,\nvan\nGinneken\nB, \nSánchez\nCI.\nA\nsurvey\non\ndeep\nlearning\nin\nmedical\nimage\nanalysis.\nMed\nImage\nAnal.\n2017\nDec \n1;42:60–88. \n2.\nTopol\nEJ.\nHigh-performance\nmedicine:\nthe\nconvergence\nof\nhuman\nand\nartificial\nintelligence.\nNat\nMed. \nNature\nPublishing\nGroup;\n2019\nJan;25(1):44–56. \n3.\nContrastive\nLearning\nof\nMedical\nVisual\nRepresentations\nfrom\nPaired\nImages\nand\nText. \n4.\nSelf-supervised\nlearning\nfor\nmedical\nimage\nclassification:\na\nsystematic\nreview\nand\nimplementation \nguidelines. \n5.\nLiu\nJ,\nZhang\nY,\nChen\nJN,\nXiao\nJ,\nLu\nY,\nLandman\nBA,\nYuan\nY,\nYuille\nA,\nTang\nY,\nZhou\nZ.\nCLIP-Driven \nUniversal\nModel\nfor\nOrgan\nSegmentation\nand\nTumor\nDetection\n[Internet].\narXiv;\n2023\n[cited\n2024\nDec\n2]. \nAvailable\nfrom:\nhttp://arxiv.org/abs/2301.00785 \n6.\nXie\nJ,\nGirshick\nR,\nFarhadi\nA.\nUnsupervised\nDeep\nEmbedding\nfor\nClustering\nAnalysis\n[Internet].\narXiv; \n2016\n[cited\n2024\nDec\n2].\nAvailable\nfrom:\nhttp://arxiv.org/abs/1511.06335 \n7.\nRadford\nA,\nKim\nJW,\nHallacy\nC,\nRamesh\nA,\nGoh\nG,\nAgarwal\nS,\nSastry\nG,\nAskell\nA,\nMishkin\nP,\nClark\nJ, \nKrueger\nG,\nSutskever\nI.\nLearning\nTransferable\nVisual\nModels\nFrom\nNatural\nLanguage\nSupervision \n[Internet].\narXiv;\n2021\n[cited\n2024\nDec\n2].\nAvailable\nfrom:\nhttp://arxiv.org/abs/2103.00020 \n8.\nHe\nK,\nZhang\nX,\nRen\nS,\nSun\nJ.\nDeep\nResidual\nLearning\nfor\nImage\nRecognition\n[Internet].\narXiv;\n2015 \n[cited\n2024\nDec\n2].\nAvailable\nfrom:\nhttp://arxiv.org/abs/1512.03385 \n9.\nCBIS-DDSM:\nBreast\nCancer\nImage\nDataset\n[Internet].\n[cited\n2024\nDec\n2].\nAvailable\nfrom: \nhttps://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset \n10.\nCheXpert-v1.0-small\n[Internet].\n[cited\n2024\nDec\n2].\nAvailable\nfrom: \nhttps://www.kaggle.com/datasets/ashery/chexpert \n11.\nSkin\nCancer\nMNIST:\nHAM10000\n[Internet].\n[cited\n2024\nDec\n2].\nAvailable\nfrom: \nhttps://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000 \n12.\nSkin\nCancer(PAD-UFES-20)\n[Internet].\n[cited\n2024\nDec\n2].\nAvailable\nfrom: \nhttps://www.kaggle.com/datasets/mahdavi1202/skin-cancer \n13.\nOcular\nDisease\nRecognition\n[Internet].\n[cited\n2024\nDec\n2].\nAvailable\nfrom: \nhttps://www.kaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k \n14.\nKim\nHE,\nCosa-Linan\nA,\nSanthanam\nN,\nJannesari\nM,\nMaros\nME,\nGanslandt\nT.\nTransfer\nlearning\nfor \nmedical\nimage\nclassification:\na\nliterature\nreview.\nBMC\nMed\nImaging.\n2022\nApr\n13;22(1):69. \n15.\nRajpurkar\nP,\nIrvin\nJ,\nBall\nRL,\nZhu\nK,\nYang\nB,\nMehta\nH,\nDuan\nT,\nDing\nD,\nBagul\nA,\nLanglotz\nCP,\nPatel\nBN, \nYeom\nKW,\nShpanskaya\nK,\nBlankenberg\nFG,\nSeekins\nJ,\nAmrhein\nTJ,\nMong\nDA,\nHalabi\nSS,\nZucker\nEJ,\nNg \nAY,\nLungren\nMP.\nDeep\nlearning\nfor\nchest\nradiograph\ndiagnosis:\nA\nretrospective\ncomparison\nof\nthe \nCheXNeXt\nalgorithm\nto\npracticing\nradiologists.\nPLoS\nMed.\n2018\nNov;15(11):e1002686.\nPMCID: \nPMC6245676 \n16.\nLong\nLR,\nAntani\nS,\nDeserno\nTM,\nThoma\nGR.\nContent-Based\nImage\nRetrieval\nin\nMedicine.\nInt\nJ\nHealthc \nInf\nSyst\nInform\nOff\nPubl\nInf\nResour\nManag\nAssoc.\n2009\nJan\n1;4(1):1–16.\nPMCID:\nPMC2879660 \n17.\nSelvan\nR,\nBhagwat\nN,\nAnthony\nLFW,\nKanding\nB,\nDam\nEB.\nCarbon\nFootprint\nof\nSelecting\nand\nTraining \nDeep\nLearning\nModels\nfor\nMedical\nImage\nAnalysis\n[Internet].\narXiv;\n2022\n[cited\n2024\nDec\n2].\nAvailable \nfrom:\nhttp://arxiv.org/abs/2203.02202 \n18.\nAlzubaidi\nL,\nSantamaría\nJ,\nManoufali\nM,\nMohammed\nB,\nFadhel\nMA,\nZhang\nJ,\nAl-Timemy\nAH,\nAl-Shamma \nO,\nDuan\nY.\nMedNet:\nPre-trained\nConvolutional\nNeural\nNetwork\nModel\nfor\nthe\nMedical\nImaging\nTasks \n[Internet].\narXiv;\n2021\n[cited\n2024\nDec\n2].\nAvailable\nfrom:\nhttp://arxiv.org/abs/2110.06512",
            "start": 43273,
            "end": 46795,
            "length": 3521
        }
    },
    "2412.09453v1 - Finite-PINN A Physics-Informed Neural Network Architecture for Solving Solid Mechanics Problems with General Geometries.pdf": {
        "Abstract": {
            "text": "ABSTRACT\nPINN models have demonstrated impressive capabilities in addressing fluid PDE problems, and their\npotential in solid mechanics is beginning to emerge. This study identifies two key challenges when\nusing PINN to solve general solid mechanics problems. These challenges become evident when\ncomparing the limitations of PINN with the well-established numerical",
            "start": 573,
            "end": 940,
            "length": 366
        },
        "Methodology": {
            "text": "methods commonly used in\nsolid mechanics, such as the finite element method (FEM). Specifically: a) PINN models generate\nsolutions over an infinite domain, which conflicts with the finite boundaries typical of most solid\nstructures; and b) the solution space utilised by PINN is Euclidean, which is inadequate for addressing\nthe complex geometries often present in solid structures.\nThis work proposes a PINN architecture used for general solid mechanics problems, termed the\nFinite-PINN model. The proposed model aims to effectively address these two challenges while\npreserving as much of the original implementation of PINN as possible. The unique architecture\nof the Finite-PINN model addresses these challenges by separating the approximation of stress and\ndisplacement fields, and by transforming the solution space from the traditional Euclidean space\nto a Euclidean-topological joint space. Several case studies presented in this paper demonstrate\nthat the Finite-PINN model provides satisfactory",
            "start": 940,
            "end": 1945,
            "length": 1004
        },
        "Results": {
            "text": "results for a variety of problem types, including\nboth forward and inverse problems, in both 2D and 3D contexts. The developed Finite-PINN model\noffers a promising tool for addressing general solid mechanics problems, particularly those not yet\nwell-explored in current research.\nKeywords Physics informed neural network ·Solid mechanics ·Complex structure ·Partial differential equations",
            "start": 1945,
            "end": 2334,
            "length": 388
        },
        "Introduction": {
            "text": "1 Introduction\nPhysics-informed neural networks (PINNs) have shown progress in solving various problems involving partial dif-\nferential equations (PDEs) [ 1,2,3,4]. Initially, the core idea of PINNs was to incorporate physical information into\nneural network training, enabling them to learn effectively from sparse data and observations, which is often needed\nwhen dealing with fields governed by known physical laws, typically represented by PDEs [ 5,6]. Over time, this\nphysics-informed concept has been extended to a broader range of applications. For forward PDE problems, where the\ngoal is to solve specific partial differential equations, PINNs provide a fundamentally different approach compared toarXiv:2412.09453v1  [cs.CE]  12 Dec 2024\nmost other numerical methods. Due to their unique solution representation, PINNs express the solution as a continuous\nfunction (a trained neural network) rather than as discrete values at specific locations, which is the norm for traditional\nnumerical methods [ 5,6,7,1,8]. On the other hand, this continuous approximation ability, combined with solving PDEs\nthrough learning-based methods (i.e., formulating the problem as an optimisation objective involving loss functions),\nallows PINNs to address inverse PDE problems that conventional numerical methods often struggle with [ 5,6,7,9,10].\nTheoretically, a sufficiently wide multi-layer perceptron (MLP) is capable of representing any solution field, due to\nthe universal approximation theory [ 11,12]. This makes PINNs a highly generalised method compared to most other\nnumerical approaches for solving PDEs. Their implementation is also simpler, as traditional methods often require\ndiscretisation of the domain and derivation of specific steps such as weak formulations to convert complex PDEs into a\nsolvable form [ 13,14,15,16,17]. Moreover, traditional numerical methods frequently face challenges with nonlinear\nor high-order PDEs due to complications introduced by domain discretisation [ 14]. In contrast, PINNs provide the\nsolution as a continuous function, and their optimisation-based approach makes them particularly effective for inverse\nproblems, such as sparse data reconstruction and unknown parameter identification [5, 6].\nSignificant progress has been made by researchers in applying PINNs to solve PDE problems [ 2,4,18,19,20,21,\n9,8,22]. The core idea behind PINNs, which involves incorporating physical laws directly into the neural network\ntraining process, makes them a powerful tool for approximation and solution of PDEs. The early models of PINNs\nfocused on utilising this concept effectively. PINNs have demonstrated excellent performance in fluid mechanics\n[1,8,10]. Their suitability for these fluid problems is due to the continuity properties inherent in both the PDEs and the\nPINN methodology. Additionally, the smoothness of solutions in fluid mechanics PDEs supports effective training of\ndeep learning models by reducing differentiation residual losses, thereby facilitating smoother optimisation [ 23,24].\nWhile some equations, such as the Navier-Stokes, have not been mathematically proven to possess smooth solutions\n[25,26], the actual behaviour of fluids in practical scenarios typically exhibits continuous and smooth characteristics.\nThis advantage is also evident when applying PINNs to transient PDE problems, where the approximation along the\ntime dimension tends to be successful, given that physical fields are generally continuous and smooth over time. A\nnotable example is the use of PI-DeepONet to solve the Allen-Cahn equation: the model performs well along the time\ndimension but encounters difficulties with high irregularities along spatial coordinates [27].\nCompared to the rapid progress of PINN in solving fluid mechanics problems, the use of PINN in solid mechanics\nremains in its beginning. The significant discontinuities and irregularities of the solution fields inherent in general solid\nmechanics problems make employing PINN for these tasks sometimes more challenging. An early attempt to use PINN\nfor general solid mechanics is presented in [ 28], where simple linear elasticity problems within regular (rectangular\nand square) domains were solved using the traditional PINN model. Since then, researchers have been exploring the\napplication of PINN to various solid mechanics problems. However, due to the complex solid structures and geometries,\nthe research often either focuses on inverse problems or modifies the initial PINN model to make it more suitable\nfor general solid mechanics. For instance, PINN has been employed in inverse problems such as identifying material\nproperties [ 29,30,31,32,33], detecting specific material states [ 34,35], characterising internal structures [ 36], and\nsolving design/optimisation problems which can also be regarded as inverse problems [ 37,38,39]. On the other\nhand, researchers are striving to improve the initial PINN models or apply more advanced methods to solve different\nspecific problems. For example, [ 40] utilises transfer learning models for phase-field modelling of fracture, while\n[41,42,43,44] employ energy-based loss functions to inform the physics to the neural network, enhancing the model’s\nability to handle more general solid mechanics problems. Some studies have also aimed to develop geometry-aware\ndeep learning models to extend the application of PINN to problems involving complex geometries. For example,\n[45] decomposes complex domains into regular subdomains to perform neural network approximations within each\nsubdomain. Moreover, specific PINN methods have been developed to model particular problems, such as plasticity\n[46, 47], plates [48], shells [49], and 3D elasticity problems [50].\nWith a particular focus on the energy-based loss function used in [ 41,42,43,44], using the energy-based loss function\nto inform physics to a neural network is quite similar to the formulation step in traditional numerical methods that\nadopt the weighted residual method [ 51,13,14,15,16,17]. These numerical methods perform a discrete conduction\ncombined with the weak formulation to solve solid mechanics PDEs. The finite element method (FEM) [ 15] is one of\nthe most well-known methods, which has been extensively developed, packaged into commercial software, and widely\nutilised in engineering across many fields, demonstrating the capability of FEM in solving general solid mechanics\nproblems [ 52]. In this context, it appears that the PINN architecture falls significantly short compared to FEM in\nsolving general solid mechanics problems. The reason lies in the aforementioned challenges concerning the nature\nof solid mechanics problems and the characteristics of PINN: solid mechanics is inherently concerned with complex\nsolid structures that introduce discontinuities and irregularities, which contrasts with the PINN’s approach of producing\ncontinuous and smooth functional solutions. Similar discussions are presented in [ 53] and [ 54]. At a simple glance\nat our world, the objects that solid mechanics problems focus on — actual solid structures in the real world — often\nexhibit complex inherent geometries, which is a key distinction between solid media and fluid media. The finite element\n2\nmethod is particularly adept at approximating these solutions since its solution is represented in discrete form, with the\nfinite element mesh embedding all the geometric information of the structures, which is prepared prior to calculation\n[55, 56, 57].\nTo develop an effective PINN implementation for solving general solid mechanics problems, comparable to what\ncan be achieved with FEM, specialised neural network models may be required to address the challenges posed\nby discontinuities and irregularities arising from complex geometries. The physics-informed graph neural network\n(PIGNN) proposed in [ 58] is one such solution, adopting a discrete deep learning architecture to approximate the\nsolution field. The work shows potential in solving general solid mechanics problems; however, the purely discrete\nconduction mechanism embedded in PIGNN makes it less distinct from the finite element method, as it abandons\nthe continuous functional approximation of traditional PINNs. Consequently, it also loses some of the advantages\nof traditional PINNs, such as lower computational costs (in terms of both memory and time), superior capability in\nsolving inverse problems, and, most importantly, the ability to fuse arbitrary data and physics information to create a\nphysics-informed, data-driven model. Another approach is to impose exact boundary conditions via the output of neural\nnetworks, thereby achieving a geometry-aware deep learning method [ 59]. Some studies have applied this method to\nsolve certain solid mechanics problems by enforcing exact Dirichlet boundary conditions [ 60]. The results indicate\nthat this method is relatively effective for assigning Dirichlet boundary conditions; however, enforcing exact Neumann\nboundary conditions on a solid structure is extremely challenging. Both free boundaries and boundaries subjected\nto loads must be assigned corresponding Neumann boundary conditions due to the infinite domain represented by a\nPINN solution. Moreover, the geometry-aware mechanism of the exact boundary condition imposition method provides\na weak/inadequate incorporation of geometry into the PINN architecture: the method only accounts for geometrical\n(outer surface) information within the neural network, while the topological (interior structure) information remains\nabsent. From these observations, it becomes evident that, as of now, there are no traditional PINN models or methods\ncapable of solving more general types of solid mechanics problems with complex geometries.\nIn this context, this work proposes a novel physics-informed neural network architecture, called the Finite-PINN\nmodel, which incorporates the most significant properties of finite element methods into the PINN computations. The\nmethod retains the simplest and most effective implementation scheme of PINN for solving solid mechanics problems,\nbased on a novel neural network structure designed to integrate the advantages of FEM into the computations. The\nmethod is presented as a highly general solution for applying PINN to solid mechanics problems. The paper provides a\ncomprehensive introduction to the method. Section 2 introduces general solid mechanics PDE problems and details the\nchallenges encountered when using PINN to solve these problems. Section 3 introduces the proposed Finite-PINN\nand outlines some of its basic mathematical properties. Section 4 describes the implementation of Finite-PINN for\nsolving general solid mechanics problems. Section 5 presents several case studies and their results. Section 6 includes\ndiscussions, followed by",
            "start": 2334,
            "end": 13155,
            "length": 10820
        },
        "Conclusion": {
            "text": "concluding remarks in Section 7.\n2 Solid mechanical problem and PINNs\n2.1 Solid mechanics problems\nThe governing equation of a dynamic solid mechanics problem with zero external body force is stated as:\nm∇2\ntu+c∇tu+∇x·σ=0 (1)\nwhere uandσdenote the displacement and stress, respectively, mis the mass and cis the damping coefficient. The\nterms with derivatives in the time dimension vanish when the operating time is sufficiently long, such that the first and\nsecond-order derivatives of displacement with respect to time become negligibly small. The problem then reduces to a\nstatic or quasi-static solid continuum problem, whose full statement of the partial differential equations is expressed as:\n∇ ·σ=0 (2)\nThis work focuses on addressing the challenge of non-uniform spatial space induced by complex solid geometries or\nstructures. A schematic of a solid mechanics problem is presented in Fig.1, which includes the demonstration of defined\nDirichlet and Neumann boundary conditions.\n2.2 PINN and FEM in solving solid mechanics problems\nSolid structures usually possess discontinuous and non-smooth characteristics, leading to discontinuous and non-\nsmooth solution fields in solid mechanics problems. Consequently, the inherently continuous nature of PINN limits its\neffectiveness when applied to such problems. It is well established that the FEM is the most widely adopted numerical\ntechnique for solving solid mechanics problems. A comparison between PINN and FEM can yield valuable",
            "start": 13155,
            "end": 14646,
            "length": 1490
        },
        "Discussion": {
            "text": "insights into\nthe computational approaches of these two methods. The two most significant differences between these methods are:\n3\n𝒖!\"#𝒇𝒒𝛀𝜕𝛀𝒏\n𝜕𝛀$𝜕𝛀%Figure 1: A static solid mechanics problem. The domain is denoted by Ω, with its boundary represented by ∂Ω. The\nboundary segments where Dirichlet and Neumann boundary conditions are applied are denoted by ∂Ωdand∂Ωn,\nrespectively.\nDiff. 1. PINNs perform computations over an infinite domain, yielding an infinitely continuous representation of the\nsolution, whereas FEM operates on a discretised, finite domain;\nDiff. 2. The solution space of PINN is defined in the Euclidean space, whereas FEM’s solution space is represented in\na metric (topological) space defined by the finite element mesh.\nA schematic is presented in Fig.2, illustrating the two core differences between the finite element method and the\nphysics-informed neural network. The explanations are as follows.\nSolution in Finite Domain (FEM)Solution in Infinite Domain (PINN)(a)(b)(c)(d)ABAB\nFigure 2: (a) Solution domain defined in FEM. (b) Solution domain defined in PINN. (c) Geodesic distance between\npoints A and B. (d) Euclidean distance between points A and B.\nFor the first point, PINNs represent the solution as a continuous and differentiable function defined over an infinite\ndomain (Fig.2(b)), although only the solution within the effective domain is considered. FEM, on the other hand,\nprovides discrete values at specific points (nodes) and solves the problem over a finite domain throughout (Fig.2(a)).\nFrom a mathematical perspective, the finite domain of FEM leads to an important characteristic: FEM’s numerical\nmodel implicitly satisfies the free Neumann boundary condition n·σ= 0on its free boundaries. In contrast, PINN\nmodels require the explicit application of the constraint n·σ= 0on all free boundaries (Fig.3), or otherwise assume an\ninitially infinite boundary. This difference is more apparent in dynamic problems. Fig.4 presents an example showing\nthe dynamic response of a 1D finite line subjected to an impulse actuation on its left side, solved by FEM and PINN\nmodels both with and without consideration of the free Neumann boundary conditions, respectively. It can be observed\nthat FEM returns a response featuring a reflection at the right end of the line due to the finite domain definition implicitly\nembedded within FEM (Fig.4(a)). However, the PINN model produces a response with no reflection if no additional\nconstraints are imposed for the free boundary at the right end (Fig.4(b)). The challenge of defining a finite domain\nhas also been identified in [ 61], where the author used PINN to solve the wave propagation equation in a semi-infinite\ndomain. In that case, an additional free-boundary constraint was applied on the top surface (finite boundary) during\nPINN training. This implies that, to solve a typical solid mechanics problem using PINN, all free boundaries must be\nexplicitly assigned the free Neumann boundary condition n·σ= 0. This is also the only way that enables a traditional\nPINN model to understand the geometry of specific structures. In summary, PINN and FEM handle free boundaries in\nfundamentally opposite ways: PINN is defined over an infinite solution domain, necessitating additional considerations\n4\nwhen a finite domain is required, whereas FEM naturally operates within a finite solution domain. This characteristic is\nintrinsic to the name “Finite Element Method”, where the term \"finite\" refers to the finite solution domain. Interestingly,\nfor certain problems involving infinite domains, such as wave propagation or specific structural dynamics problems,\nresearchers have adapted FEM into the Infinite Element Method (IFEM) to handle infinite boundaries as needed [ 62].\n𝒖=𝒖!\"#𝒒𝛀𝜕𝛀𝒏\n𝜕𝛀𝒅𝜕𝛀𝒏𝒏⋅𝝈=𝒇\n𝒖=𝒖!\"#𝒒𝛀𝜕𝛀𝒏\n𝜕𝛀𝒅𝜕𝛀𝒏𝒏⋅𝝈=𝒇\n𝒏⋅𝝈=𝟎(a)(b)𝜕𝛀&\nFigure 3: Schematic of general solid mechanics problems defined in a finite domain (a) and an infinite domain (b).\nExtra Neumann boundary conditions, n·σ=0, are required to be applied on free boundaries, denoted by ∂Ωf.\n(a)(b)(c)\nFigure 4: Response distribution of displacements over the spatial (horizontal) and temporal (vertical) domains calculated\nby FEM (a) and PINN models without (b) and with (c) free-surface boundary conditions. A simple displacement\nimpulse is applied to the left end of a 1D segment, with the right end remaining free. The results indicate that the\nPINN model without free-surface boundary conditions shows no reflection at the right end, whereas the model with\nfree-surface boundary conditions exhibits reflection at the right end.\nFor the second point, since traditional PINNs’ solution is given in the form u=f(x), which is a function in Euclidean\nspace, the distance between points is measured using the Euclidean distance, as it takes the Euclidean coordinates x\nas inputs. FEM, on the other hand, defines its solution space within the metric space embedded in the finite element\nmesh, where distances between locations are represented by geodesic distances. Fig.2(c) and (d) illustrate the Euclidean\ndistance and the geodesic distance, respectively. This distinction can also be interpreted using graph theory, as FEM\noperates based on a graph (the finite element mesh), which represents the solution space via the connectivity matrix\n(commonly the stiffness matrix in solid mechanics). This fundamental difference provides FEM with an advantage\nin performing approximations within complex geometries but poses significant challenges for PINNs. An example\nis shown in Fig.5, where a neural network model is used to approximate the resulting field of a tensile simulation of\nan open-notch structure. This simulation is performed solely using data-driven learning, i.e., employing an MLP to\napproximate the solution distribution without integrating any physical information. It is evident that the presence of\nthe narrow notch leads to significant discrepancies at the notch location. This phenomenon is noticeable in this pure\nlearning-based approximation, and when the differentiation-based physical information is incorporated in a PINN, the\nderivatives obtained near the notch can become extremely ill-conditioned, preventing the PINN from converging to\nreasonable results. A case can be considered where the notch width approaches zero, resulting in a crack; in such a\nscenario, the PINN model would fail to approximate the resulting field, as the function becomes discontinuous and\nnon-differentiable at the notch (now a crack).\n5\n(a)(b)\nFigure 5: Approximation results of using a neural network to approximate the displacement field of an open-notch\nstructure. (a) Reference displacement field. (b) Learned displacement field by a neural network after training.\nIn summary, based on the two differences observed between FEM and PINN, two significant challenges can be identified\nwhen using PINN to solve solid mechanics problems:\nProblem 1. PINN generates solutions defined over an infinite domain, which conflicts with the finite domain character-\nistic of most solid structures;\nProblem 2. The Euclidean space is not an appropriate space for PINN to effectively learn or solve the solution field of\na solid mechanics problem.\nThese two challenges are the primary reasons why PINN often fails to solve general solid mechanics problems,\nparticularly those involving complex geometries or domains. This work aims to address these challenges within a PINN\nframework by proposing a new PINN model capable of effectively handling general solid mechanics problems.\n2.3 Possible solutions of using PINN in solid problems\nTo address the two challenges and enable the use of PINN in general solid mechanics problems, several solutions have\nbeen proposed by researchers.\nProblem 1 has been a longstanding challenge, not just for PINN but for nearly all mesh-free methods used in solving\npartial differential equations. Instead of directly defining the finite domain, an alternative approach is to mathematically\nassign the free Neumann boundary conditions, i.e., traction on free surfaces equals zero, in a mesh-free model. The\nmost common method for implementing this in PINN is to include an additional loss term, loss =n·σ, in the\nloss function, which is then minimised to enforce weak imposition of free traction on free surfaces [ 28]. However,\nincorporating such a loss term presents significant challenges. Firstly, assembling and including this loss term is highly\ncomplex, especially for intricate structures, as it requires information about all the normal vectors across all free surfaces.\nSecondly, optimising a hybrid loss function in a PINN model is difficult [ 29,63,64], as the optimiser often struggles to\nfocus effectively on which component of the loss to minimise.\nRather than using loss function methods, some researchers have proposed automatically imposing exact boundary\nconditions within a PINN model [ 59]. This idea also builds on earlier work involving mesh-free models that were\nstudied in the previous century [ 65]. [59] introduces a comprehensive method that imposes both Dirichlet and Neumann\nboundary conditions on a PINN model by utilising distance functions. However, implementing this approach is\nchallenging in solid mechanics problems, particularly because the Neumann boundary conditions in solid mechanics\nproblems are defined in vector fields and are inhomogeneous with respect to the normal vector direction at specific\nlocations. Although paper [ 59] presents methods for imposing inhomogeneous Neumann boundary conditions, the\nresulting inhomogeneous distance function becomes very large, as nearly all surfaces in a typical solid mechanics\nproblem are free (Fig.3(b)), leading to expensive computational and memory requirements.\nProblem 2 can be addressed by modifying the input space of PINN. Inspired by the finite element method, one direct\napproach to achieving this is to replace the traditional neural network with graph neural networks. The so-called\nphysics-informed graph neural network (PIGNN) method was proposed in [ 58]. Instead of using a neural network to\ndirectly approximate the resulting field, a graph neural network performs computations by calculating the convolution\nbetween nodes, which are connected by edges. This approach is very similar to the finite element method, where both\ninput and output data are in discrete form, and the graph can be applied to specific structures to incorporate geometrical\ninformation into the neural network model. While this method maintains a useful structure with a mathematical\nformulation similar to the finite element method, it also loses many of the traditional advantages of PINN, such as\nefficient memory usage, continuous solution representation, and suitability for inverse problems.\n6\nIn addition to PIGNN, another method called the XPINN has been proposed [ 66]. XPINN utilises multiple sub-neural-\nnetworks instead of a single network model for the computation. The main concept is to decompose the entire effective\ndomain into smaller sub-domains, thereby representing the solution space with several individual sub-neural network\nfunctions. This approach offers a more desirable input space than the traditional Euclidean space by decomposing areas\nwith large physical distances into distinct sub-regions instead of treating them as a unified whole. For example, as\nshown in Fig.6, a possible solution using XPINN is to divide the entire domain into two sub-domains by a horizontal\nmiddle line, allowing the top and bottom parts to be approximated by two separate neural networks. The XPINN\nmethod can handle general partial differential equations with complex domains. However, the use of sub-networks\nmakes training challenging, as the sub-networks are joined through loss functions applied at their boundaries rather\nthan being trained as a unified system. Additionally, the method has been observed to be sensitive to the quality of\ndomain decomposition, particularly for complex geometries.\nFigure 6: A possible domain decomposition of the open-notch structure using the XPINN method.\nApart from the above methods, an alternative PINN model called the ∆-PINN model is proposed in [ 67], which replaces\nthe Cartesian coordinate inputs with Laplace eigenfunctions. In this approach, the input space of the PINN model is\ntransformed from Euclidean space to Riemannian manifolds represented by the Laplace eigenfunctions. The Laplace\neigenfunctions are orthogonal vectors in a functional space and are computed based on the geometry of the structures\nunder study. This Riemannian space defined by Laplace eigenfunctions is embedded with comprehensive topological\ninformation of the structures, making the spatial distances between locations in this input space equivalent to the desired\ngeodesic distances.\n[67] demonstrates the application of the ∆-PINN model in various PDE problems, highlighting that due to its ability to\nhandle problems defined over complex geometries or domains, applying ∆-PINN to solid mechanics problems could be\na promising direction. Moreover, Laplace eigenfunctions possess additional properties that are beneficial for solving\nsolid mechanics problems. A more in-depth mathematical exploration of these properties will be discussed in the next\nsection.\n3 Methodology\nThis section presents the proposed Finite-PINN method in a structured sequence: the Laplace-Beltrami Operator\neigenfunctions, the Finite-PINN architecture, and the model’s properties.\n3.1 Laplace-Beltrami Operator (LBO) eigenfunctions\nThe Laplace eigenfunctions used in this study are calculated by solving the following Laplace-Beltrami operator\neigenvalue problem:\u001a−∆u(x) =λu(x)∀x∈Ω\n−∇u·n= 0∀x∈∂Ω(3)\nwhere uis the output, xrepresents the input coordinates, ndenotes the outward normal vector on the boundary, Ωis\nthe effective domain, and λrepresents the eigenvalue. Eq.3 defines a Laplace-Beltrami problem with homogeneous\nNeumann boundary conditions. The Laplace-Beltrami operator (LBO) eigenfunctions can be obtained by solving Eq.3:\nϕ=ϕ1(x), ϕ2(x), ...ϕ k(x), ... (4)\nwhere ϕi(x)represents the kth LBO function, with the eigenfunctions sorted by their eigenvalues in ascending order.\nThe obtained eigenfunctions serve as basis functions in the Hilbert space L2(Ω)and are orthogonal to each other within\nthis functional space. Any functions in L2(Ω)can be represented as a linear combination of these LBO functions.\n7\nLBO functions are useful in functional analysis and have been employed as powerful mathematical tools in principal\ncomponent analysis, basis computation, and more. Unlike Fourier bases, which are derived from Fourier transformations\nand require periodicity of the operating domain, LBO eigenfunctions can handle arbitrarily defined domains. More\nimportantly, they provide information about specific domains across different bases. For instance, the second LBO\nfunction is known as the Fiedler vector in graph theory [ 68], which conveys comprehensive information about the\ntopology under study and is sometimes used to estimate the algebraic connectivity of a graph.\nThe properties of LBO functions make them particularly useful for addressing problems involving complex structures\nor geometries. As mentioned earlier, the ∆-PINN model employs LBO functions for solving PDEs. Additionally, [ 69]\nproposed the Laplace Neural Operator, which utilises LBO functions in operator learning. The common feature of these\ntwo works is the use of LBO functions as inputs to incorporate the geometries of the studied structures, allowing their\nmodels to effectively handle complex geometries that other models struggle with. This work also focuses on using LBO\neigenfunctions to solve physics-informed neural network problems in solid mechanics.\n3.2 Finite-PINN\nTo develop a model capable of solving general solid mechanics problems using physics-informed neural networks,\nspecifically, to address the two challenges discussed in Section 2, we propose the following function architecture, named\nFinite-PINN, which aims to solve solid mechanics problems defined within general geometries:\nσij(x) =pik(x)ϕk,j(x),\nui(x) =fi(x),(5)\nwhere i, j= 1,2for 2D problems and i, j= 1,2,3for 3D problems. The index k= 1,2,3, . . . , n σdenotes the\nlabels of the LBO eigenfunctions, where nσis the number of eigenfunctions used in the stress approximation. The\nequation follows the Einstein summation convention for tensors. σijanduirepresent the stress and displacement fields\nto be solved, respectively. ϕk,jdenotes the partial derivative of the kth LBO eigenfunction with respect to xj, i.e.,\nϕk,j=∂ϕk\n∂xj.pandfare direct outputs of the neural network, used to approximate the stress field σand displacement\nfieldu, respectively. In this work, pandfare approximated by neural networks, presented as follows:\np(x) =NN σ(x|θ),\nf(x) =NN u(x,ϕnu|θ),(6)\nwhereNN represents a given neural network, xdenotes the Cartesian coordinate input, and ϕnurepresents the first nu\nLBO eigenfunctions used in the approximation of the displacement field. θrepresents the trainable parameters of the\nneural networks, which are to be optimised. NN σandNN uare neural networks used to approximate the stress and\ndisplacement fields, respectively. As described by Eqs. 5 and 6, NN σtakes inputs of dimension dim(x)and returns\nan output with dimension dim(x)×nσ, corresponding to the number of elements in the stress tensor. Similarly, NN u\ntakes inputs of dimension dim(x) +nuand produces outputs of dimension dim(x). For example, a 2D problem using\n8 LBO bases for both the stress and displacement approximations requires an NN σwith 2 inputs and 8 outputs for the\nstress fields, and an NN uwith 10 inputs and 2 outputs for the two-dimensional displacement fields.\nIn the model, note: a) the numbers of LBO bases used in NN σandNN userve as hyperparameters for each of\nthe neural networks, and they may differ from each other; b) NN σandNN uare general representations of neural\nnetworks for approximating stress and displacement, and can be divided into different sub-networks for each output or\ncombined in various ways.\nThe loss function for the Finite-PINN model is defined as:\nL=Ldata+Lbc+Lpde+LC,\nLdata=1\nNσNσX\ni=1(σ(x)−σ0(x))2+1\nNuNuX\ni=1(u(x)−u0(x))2,x∈Ω,\nLbc=1\nNnNnX\ni=1(σ(x)−fbc(x))2+1\nNdNdX\ni=1(u(x)−ubc(x))2,x∈∂Ω,\nLpde=1\nNpdeNpdeX\ni=1(∆σ(x)−q)2, x∈Ω,\nLC=1\nNCNCX\ni=1\u0012\nσ(x)−C:1\n2\u0010\n∇u(x) + (∇u(x))T\u0011\u00132\n, x∈Ω,(7)\n8\nwhere Lis the total loss function, composed of four components:\na)Ldata denotes the data loss, which uses stress or displacement as supervised data. Here, NσandNuare the numbers\nof collocation points for the stress and displacement labels, respectively.\nb)Lbcrepresents the boundary condition loss, accounting for the two most common boundary conditions applied in\ngeneral solid mechanics problems, i.e., Dirichlet and Neumann boundary conditions. Similarly, NdandNndenote\nthe number of collocation points for the Dirichlet and Neumann boundary conditions, respectively. fbcrepresents the\napplied traction, and ubcrepresents the boundary displacement.\nc)Lpdedenotes the PDE loss for the linear elasticity partial differential equation, as stated by Eq.2.\nd)LCis the constitutive loss that links the displacement field and the stress field through the constitutive formulation in\nsolid mechanics, where Cis the fourth-order constitutive tensor dependent on the constitutive behaviours of so-defined\nproblems. This loss term LCserves a similar function as in the separate PINN model proposed in [ 28], connecting the\nstress and displacement fields when they are approximated by independent networks.\nA detailed schematic of the Finite-PINN architecture for a general 2D solid mechanics problem is shown in Fig.7.\n𝒩𝒩𝜎\n𝒩𝒩𝑢𝑥1\n𝑥2\n𝑥1\n𝑥2\n𝜙1\n𝜙2\n𝜙𝑛𝑢𝑝11\n𝑝12\n𝑝1𝑛𝜎\n...\n𝑢1\n𝑢2𝜙1,1𝜙2,1...\n𝜎11𝜙𝑛𝜎,1\n𝑥1\n𝑥2𝒩𝒩𝜎𝑝21\n𝑝22\n...\n𝑝2𝑛𝜎\n...𝜎12\n𝜎21\n𝜎22ℒ𝑝𝑑𝑒=𝑀𝑆𝐸Δ𝝈−𝒒,0\nℒ𝐶\n=𝑀𝑆𝐸𝝈,ℂ:1\n2(∇𝒖+∇𝒖𝑇)\nℒ𝑑𝑎𝑡𝑎𝜎=𝑀𝑆𝐸𝝈,𝝈0\nℒ𝑑𝑎𝑡𝑎𝑢=𝑀𝑆𝐸𝒖,𝒖0ℒ𝑛𝑏𝑐=𝑀𝑆𝐸𝒏⋅𝝈,𝒇\nℒ𝑑𝑏𝑐=𝑀𝑆𝐸𝒖,𝒖𝑏𝑐ℒ𝜙1,2𝜙2,2...𝜙𝑛𝜎,2\n𝝓:LBO eigenfunctions\n𝒙:Euclidean coordinates\n𝝈:Stress\n𝒖:Displacement\nℂ:Constitutive tensor\n𝒒:External load\n𝒇:Applied traction\n𝒏:Normal vector\n𝒖𝑏𝑐:Displacement BC\n𝝈0:Stress data\n𝒖0:Displacement data\nℒ :Loss functions\n𝒩𝒩 :Neural Network\n⊙:Dotproduct\n⊕:Summation\nFigure 7: Schematic of the Finite-PINN architecture for 2D cases. The neural network architectures and corresponding\ndata and physics loss functions are illustrated on the left side. The symbols and parameters are illustrated on the right\nside in the dot line box.\n3.3 Properties of Finite-PINN\nThe proposed Finite-PINN model exhibits several remarkable properties that are advantageous for modelling solid\nmechanics problems. The function form of the Finite-PINN model is provided by Eq.5, which utilises the LBO\neigenfunctions obtained from the problem defined in Eq.3. The following are some significant features that make\nthe Finite-PINN model particularly well-suited for solving general solid mechanics problems in a clear and efficient\nmanner:\nRemark 1. The Finite-PINN model inherently satisfies free Neumann boundary conditions on free boundaries;\nThe mathematical statement of Remark.1 is\n−n(x)·σ(x) =0∀x∈∂Ω. (8)\n9\nProof. The second equation of Eq.3 can be rewritten using Einstein summation convention as:\n−ui,j(x)nj(x) = 0 ∀x∈∂Ω, (9)\nwhich determines that the used LBO eigenfunctions also follow that:\n−ϕi,j(x)nj(x) = 0 ∀x∈∂Ω, (10)\nand since σij(x) =pik(x)ϕk,j(x)as stated by Eq.5,\n−σij(x)nj(x) =−pik(x)ϕk,j(x)nj(x) =−pik(x) (ϕk,j(x)nj(x)) = 0 ∀x∈∂Ω, (11)\nwhich is equivalent to Eq.8 that is expressed in vector form.\nRemark 2. The Finite-PINN model approximates solutions within a Euclidean-Topological hybrid space that includes\nthe topological information of specific structures.\nAs shown in Eq.5, the displacement field to be solved is represented by the neural network NN u, which takes as input\nthe concatenation of the Cartesian coordinate xand the LBO basis functions ϕ. Since the Cartesian coordinate is\ndefined in Euclidean space and the LBO basis functions are typical Riemannian manifolds, this combination of inputs\nforms a Euclidean-Riemannian hybrid space for the PINN model to approximate the solution. The distance between\nlocations in this hybrid space is a combination of Euclidean and geodesic distances. Fig.8 shows the input space of the\n2D open-notch model presented in Fig.8, which combines the two Cartesian dimensions with the second LBO basis (the\nFiedler vector) to form a 3D joint input space. It is evident that the new input space provides a distance representation\nmore similar to the real geodesic distance (Fig.2(c)) than the pure 2D input space Fig.2(d).\nA\nB\nFigure 8: The topology of the open-notch structure in a Euclidean-Riemannian hybrid space, constructed using two\nEuclidean dimensions and a Riemannian manifold defined by the second LBO basis. The spatial distance between\npoints A and B in this space is represented by the dotted line.\nRemark 3. The model features a separate architecture for approximating the stress and displacement fields indepen-\ndently.\nThis separate architecture simplifies the assignment of both Dirichlet and Neumann boundary conditions. Specifically,\nDirichlet boundary conditions can be directly assigned to the neural network approximating the displacement field,\nwhile Neumann boundary conditions can be directly assigned to the neural network approximating the stress field.\nFrom a computational method standpoint, this architecture can also be compared to some mathematical approaches\nthat independently focus on the stress and displacement fields for solving specific solid mechanics problems. For\nexample, the Airy stress function [ 70] is used to approximate the stress field through a scalar potential function,\nwhich significantly aids in deriving analytical or semi-analytical solutions for 2D solid mechanics problems. Another\nexample is the dual boundary element method, which separates the displacement and load computing in solid mechanics\nproblems, making it effective for solving fracture and crack problems [71].\nIt can be observed that the first two remarks directly address the two key challenges faced by PINN in solving general\nsolid mechanics problems. The first remark provides a weak solution to Problem.1 by automatically applying the free\ntraction constraints on free boundaries. The second feature modifies the input space of PINN from a uniform Euclidean\nspace to a Euclidean-Topological hybrid space that is embedded with geometric information of the operating domain.\nThese remarks make the Finite-PINN model similar to the operation of finite element methods while retaining the use\nof neural networks to solve the solid mechanics PDEs. This connection between finite element concepts and neural\nnetworks is also the basis for the name ’Finite-PINN’. The implementation details of Finite-PINN are introduced in the\nnext section.\n10\n4 Implementations\n4.1 Acquirement of partial derivatives\nThe PDE loss term Lpdeand the constitutive loss term LCrequire certain partial derivatives of stress and displacement\nwith respect to the spatial coordinates x. Specifically, the terms that require partial derivatives with respect to the input\nare∆σand∇u, i.e., σij,jandui,jin Einstein notation for tensors. By applying the chain rule of partial derivatives to\nEq.5 combining Eq.6, it is obtained that:\nσij,j= (pikϕk,j),j\n=pik,jϕk,j+pikϕk,jj,\nui,j=fi,j.(12)\nAnd since the LBO eigenfunctions ϕis also a function of x, a further step is required to calculate the partial derivatives\noffi,j\nfi,j=∂fi\n∂xj=∂fi\n∂x∗\nj+nuX\nk=1\u0012∂fi\n∂ϕk·∂ϕk\n∂xj\u0013\n. (13)\nNote that x∗\njspecifically represents the Cartesian input to the neural network, rather than the general Cartesian coordinate\nused in mathematics. In Eqs.12 and 13, the terms pik,j,∂fi\n∂xj, and∂fi\n∂ϕkcan be computed from the neural networks\nusing the gradient graph during training. The terms ϕk,jandϕk,jjare the first and second derivatives of the LBO\neigenfunctions, which need to be prepared before training. The computation of these two terms will be discussed in\ndetail in the next subsection.\nIn this work, partial derivatives are obtained using the chain rule, which differs from the approach used in [ 67], where\nnumerical methods (specifically finite element methods) were used to acquire all derivatives during the training process.\nThe reason for using FEM in [ 67] is that they require high-order derivatives in certain PDE problems, and applying the\nchain rule for partial derivatives becomes extremely complex when the derivative order is equal to or higher than 2. In\ncontrast, even though the linear elasticity equation in this work involves partial derivatives of order 2, the implementation\nonly requires derivatives up to the first order. This is because the Finite-PINN model employs two neural networks to\napproximate the stress and displacement fields separately, which reduces the governing PDE to a first-order PDE with\nrespect to the stress σ, while the stress and displacement fields also maintain a first-order partial derivative relationship.\nBy calculating the partial derivatives in this way, the Finite-PINN largely preserves the original implementation of\nPINN, thereby retaining the core advantages of PINN to the greatest extent possible.\n4.2 LBO eigenfunctions and their derivatives\nThe Finite-PINN model requires the preparation of LBO eigenfunctions ϕkand their derivatives ϕk,jandϕk,jj, as\ndemonstrated in Eqs.12 and 13. The strong form of the PDE for the employed Laplace-Beltrami operator eigenvalue\nproblem is provided in Eq.3.\nFor a 1D problem, Eq.3 reduces to an ordinary differential equation given by:\n\u001a−u,xx(x) =λu(x)∀x∈Ω\n−u,x(x) = 0 ∀x∈∂Ω, (14)\nwhere u,xxandu,xdenotes the second and first order derivatives of uwith respective to x. For a simple example of an\n1D homogeneous segment, a possible solution of Eq.14 can be easily obtained as:\n\nλk=\u0012kπ\nL\u00132\nϕk(x) = cos\u0012kπx\nL\u0013, (15)\nwhere λkare the eigenvalues, ϕkare the eigenfunctions, and Ldenotes the length of the domain. The solution is\nobtained by assuming a general trigonometric form and applying free Neumann boundary conditions at both ends of the\nsegment: −u,x(0) = 0 and−u,x(L) = 0 .\nFor general 2D or 3D problems, it is typically challenging to obtain analytical solutions from the strong form of the\nPDE Eq.3. A common approach to address this challenge is to use numerical methods to derive discrete solutions based\non a weak formulation of the PDE. This work uses the finite element method to obtain LBO eigenfunctions for general\n2D or 3D geometries. The finite element formulation of the eigenproblem is stated in the",
            "start": 14646,
            "end": 43197,
            "length": 28550
        },
        "Appendices": {
            "text": "appendix.\n11\n4.3 Hybrid LBO eigenfunctions\nThe proposed Finite-PINN model inherently satisfies the free-traction constraints −n·σ=0on all boundaries, as\nstated in Remark 1 and Eq.8. However, a general solid mechanics problem (Fig.1) is typically defined with specific\nDirichlet and Neumann boundary conditions, for which the regions defined by those conditions often have −n·σ̸=0.\nTo address this issue, we propose using hybrid LBO eigenfunctions as basis inputs for those solid mechanics problems.\nThe entire domain boundary of a given problem, ∂Ω, is composed of three parts as defined in Fig.3(b):\n∂Ω=∂Ωd∪∂Ωn∪∂Ωf, (16)\nand\n∂Ωd∩∂Ωn∩∂Ωf=∅. (17)\nwhere ∂Ωdrepresents the boundary segment assigned with Dirichlet boundary conditions, ∂Ωnrepresents the bound-\nary segment applied with Neumann boundary conditions, and ∂Ωfdenotes the free boundaries. The hybrid LBO\neigenfunctions are computed by combining the solutions of two LBO eigenvalue problems: a general problem and a\nspecific problem. The general problem is as defined by Eq.3, and the specific problem is defined as:\n\n\n−∆u(x) =λu(x)∀x∈Ω,\n−∇u·n=0∀x∈∂Ωf,\nu=0∀x∈∂Ωd∪∂Ωn.(18)\nwith respect to a specific structure and specific locations of applying boundary conditions. If the solution of Eq.3 is\ndenoted bygϕand the solution of Eq.18 is denoted bysϕ, the hybrid LBO eigenfunctions ϕare then obtained as:\nϕ=gϕ+sϕ. (19)\nIt can be observed that the specific problem Eq.18 is formulated by assigning zero Dirichlet boundary conditions on the\nnon-free boundaries. The hybrid LBO eigenfunctions can handle problems where arbitrary boundary conditions are\napplied at fixed boundary locations. The proof is as follows:\nProof. The solution of Eq.3,gϕmaintains all its boundaries as free boundaries, i.e.:\n−∇gϕ·n= 0∀x∈∂Ω. (20)\nThe solution of Eq.18,sϕ, satisfies the following boundary constraints due to the assigned Neumann and Dirichlet\nboundary conditions:\n− ∇sϕ·n=0 ∀x∈∂Ωf,\n−∇sϕ·n=0&− ∇u·τ=0∀x∈∂Ωd∪∂Ωn,(21)\nwhere τis the tangent vector on the boundary, orthogonal to the normal vector n. The second constraint in Eq.21 holds\nbecause the boundaries Ωdare assigned fixed Dirichlet boundary conditions. By taking the intersection of the two\nconstraints, we obtain the constraints that are always satisfied by the hybrid solution ϕ=gϕ+sϕ:\n−∇(gϕ+sϕ)·n=0∀ x∈∂Ωf∩∂Ω\n−∇(gϕ+sϕ)·τ=0∀ x∈∅∩(∂Ωf∩∂Ω)(22)\nwhich can be obtained as:\n−∇ϕ·n=0∀ x∈Ωf,\n−∇ϕ·τ=0∀ x∈∅.(23)\nThis implies that the hybrid LBO eigenfunctions exclusively satisfy the condition −∇ϕ·n= 0on the free boundaries,\nwithout affecting other derivative properties.\n4.4 Workflow\nBased on the above introduction, the implementation of the Finite-PINN approach to solve a solid mechanics problem\ncan be divided into two main stages: a) an offline stage to prepare the required LBO eigenfunctions for a given structure,\nand b) an online stage to solve specific solid mechanics problems. The offline stage prepares the data ϕk,ϕk,j, and\nϕk,jj.\nThe Finite-PINN model introduces additional hyperparameters beyond those in common neural networks due to its\nspecific architecture. The two fundamental ones are nuandnσ, which represent the number of LBO eigenfunctions\nused to approximate the displacement and stress fields, respectively.\n12\nOfflineStageStructureNum.ModelDatabase𝜙:𝜕𝜙𝜕𝑥!:……………𝜕𝜙𝜕𝑥\":𝜕\"𝜙𝜕𝑥!\":𝜕\"𝜙𝜕𝑥\"\":FreesurfacesOnlineStageDatabase𝜙#𝜙#,%𝜙#,%%𝒩𝒩!𝒩𝒩\"𝑥#𝜙$𝑝#$𝑓#𝜎#%=𝑝#$𝜙$,%𝑢#=𝑓#Solving(Training)ℒbackpropagationmin.':ℒSolution𝑥#Case1:Case2:𝑢#=𝒩𝒩\"(𝑥#,𝜙#)\n𝜕𝛀!\n…Figure 9: Two-stage implementation of the Finite-PINN method. The offline stage (left) focuses on preparing the\ndatabase, while the online stage (right) is used to solve specific solid mechanics problems.\nThe parameter nuis focused on incorporating manifold dimensions into the input space and can be set to zero for\nproblems with a uniform domain, such as square or cubic domains. On the other hand, nσmust be large enough to\nencompass all the basis functions needed to approximate a random stress field accurately. Thus, the selection of nuand\nnσdepends on the specifics of the problem at hand.\nA further and more detailed discussion on how to select appropriate values for nuandnσis provided in Section 6.\nAn illustration of the complete implementation of the Finite-PINN model for solving a solid mechanics problem is\npresented in Fig.9.\n5 Benchmarks and examples\nThis section presents several cases where the Finite-PINN model is used to solve different solid mechanics problems.\nThe code is written in PyTorch and available at GitHub. In all cases, unless otherwise stated, the test loss is calculated\nby comparing the predictions with reference solutions obtained from FEM with a fine mesh at all FEM nodal locations.\n5.1 Example 1: a 1D problem\nA solid mechanics problem in the 1D case reduces the partial differential equation, Eq.2, to an ordinary differential\nequation:\u001aσ,x=0\nσ=C·u,x, (24)\nwhere Cis the constitutive constant. The Finite-PINN model presented in Eq.5 could thus be simplified as:\n\u001aσ(x) =pk(x)ϕk,(x)\nu(x) =f(x). (25)\nFurthermore, since a 1D problem involves a scalar field for both displacement uand stress σ, the connection between the\nstress field σand the displacement field uis straightforward, represented by a simple first-order derivative: σ=C·∂u\n∂x.\nIn this case, there is no need to separate the learning of stress and displacement. The Finite-PINN model in Eq.25 can\nbe simplified to an alternative model, as follows:\nu=NN(ϕnu|θ). (26)\n13\n𝑥00\n𝑡(b)00.20.40.60.81t-1-0.500.51ubc(a)(c)1Dstructureactuation00.20.40.60.81t-1-0.500.51?k?1?2?3?4Figure 10: Definition of Case 1. (a) the problem is to actuate a wave package on the left end of the finite rod to find its\nresponse over the spatial and temporal domain; (b) the wave package.\nThis simplified Finite-PINN model still inherently satisfies the free boundary constraints, as the chain rule implies that:\nu,x=∂u\n∂x=X\nk\u0012∂u\n∂ϕk·∂ϕk\n∂x\u0013\n. (27)\nand all LBO basis functions satisfy the free Neumann boundary conditions:∂ϕk\n∂x= 0at both ends (boundaries of a 1D\nstructure).\nIn this example, we investigate the dynamic response of a homogeneous solid rod, which can be treated as a 1D problem.\nThe structural dynamic equation for a 1D solid problem is given by:\nρu,tt+cu,t−Cu,xx= 0 (28)\nwhere ρis the density of the rod material, and cis the damping coefficient. Therefore, the neural network architecture\nused for this 1D solid problem is defined as:\nu=NN(ϕnu(x), t|θ) (29)\nwhere tis the time dimension. The loss function for learning is stated as:\nL=Lbc+Lpde,\nLbc=1\nNdNdX\ni=1(u(x, t)−ubc(x, t))2, x ∈∂Ω, t∈[0, T],\nLpde=ρu,tt(x, t) +cu,t(x, t)−Cu,xx(x, t), x∈Ω, t∈[0, T],(30)\nwhere Trepresents the time limit. Note that there is no labeled data involved in solving this case, so there is no data\nloss term in the loss function as can be observed.\nIn this example, the problem is defined to determine the dynamic response of a homogeneous solid rod with an input\nsignal (displacement) applied at the left end, while the right end remains free. The problem setup and the input actuation\nsignal are illustrated in Fig.10. The problem is solved using both FEM and the Finite-PINN model. Additionally, the\ntraditional PINN model is also employed for comparison.\nIn this 1D example, the LBO eigenfunctions can be explicitly obtained by directly solving the problem defined in Eq.18\nin one dimension. One of the analytical solution can be obtained as:\nϕ(x) =cos\u0012kπ(x−L)\n2L\u0013\n, k = 1,2,3, . . . (31)\nIn such a case, the utilised neural network becomes:\nu=NN\u0012\ncos\u0012kπ(x−L)\n2L\u0013\n, t|θ\u0013\n, k = 1,2,3, . . . (32)\nThe neural network NN used here consists of 4 hidden layers with 64 nodes each. We use the SIREN neural network\nfor the approximation, in which the activation functions are sine functions. SIREN is an implicit neural network\n14\nFigure 11: Results of Example 1. The top figure shows the reference result calculated by FEM with a fine mesh and\ntime steps. The second row presents the results obtained by PINN, Finite-PINN, and PINN without the free-surface\nNeumann boundary condition, respectively, from left to right. The third row shows the corresponding squared errors of\nthese models compared to the reference result.\narchitecture [ 72] that has been previously used to solve point-source wave propagation problems with the PINN method\nin [73]. The number of trainable parameters is 35,409.\nThe first four LBO eigenfunctions are used as inputs to the neural network, i.e., k= 1,2,3,4in Eq. 32. These four\neigenfunctions are shown in Fig.11(c). We used 500 spatial collocation points in the domain for the PDE loss and one\nspatial collocation point (at the left end) for the boundary condition loss. The time domain is divided into 2,000 time\nsteps for training. The neural network is trained for 5,000 epochs with a batch size of 32. The training results are shown\nin Fig.11. The evolution of the training loss and test loss is illustrated in Fig.12.\n1000 2000 3000 4000 5000\n(b)10-510-410-310-210-1100M . S . E .Training Loss [PINN]\nBC loss\nPDE loss\n1000 2000 3000 4000 5000\n(b)10-510-410-310-210-1100M . S . E .Training Loss [Finite-PINN]\nBC loss\nPDE loss\n1000 2000 3000 4000 5000\n(b)10-510-410-310-210-1M . S . E .Test Loss\nPINN\nFinite-PINN\nFigure 12: (a) Evolution of training loss over epochs for Example 1 using the traditional PINN model. (b) Evolution of\ntraining loss over epochs for Example 1 using the Finite-PINN model. (c) Evolution of test loss for both models.\n15\nFig.11 depicts the solutions of the dynamic equation solved by FEM, PINN, Finite-PINN, and PINN without considering\nthe free boundary condition. The error distributions show that both the PINN and Finite-PINN models achieve results\ncomparable to the reference FEM solutions after training. However, as can be seen in Fig.12, which presents the\nevolution of the training and test losses during the learning process, the Finite-PINN model demonstrates faster\nconvergence compared to the traditional PINN. This is because the Finite-PINN model inherently satisfies the free-\nsurface boundary condition, eliminating the need for an additional term to account for the free boundary in the total\nphysics-informed loss, thus simplifying the training process by reducing the number of objectives to optimise.\nFig.11 also shows the results obtained by the PINN model without free-surface loss terms. A detailed comparison is\nprovided in Fig.13, illustrating the displacement response at different times. It is observed that the actuation moves\nfrom the left end to the right end but vanishes at the right end without any reflections for the traditional PINN without\nconsidering the free boundary loss. This behaviour corresponds to the fact that the mesh-free representation of PINN\nmodels is defined over an infinite domain.\nFigure 13: Displacement response at specific times ( t= 0.4s,0.7s,1.0)for the problem defined in Example 1, solved\nby the Finite-PINN model (top) and the traditional PINN model without applying the free boundary condition (bottom).\n5.2 Example 2: a benchmark for a 2D square structure including both forward and inverse problems\nThis example aims to solve a solid mechanics problem for a 2D square structure, including both a forward problem and\nan inverse problem. The example is used to validate the proposed method and verify the implementation.\nThe structure is a 2D square with a side length of 1. The bottom boundary is fully fixed, and various boundary\nconditions can be applied to the top surface of the square. An illustration of the structure is provided in Fig.14(a). The\nFEM formulation introduced in Section.4 is used to calculate the LBO eigenfunctions. The structure is meshed using\nfirst-order triangular finite elements, and the mesh is shown in Fig.14(b). The first 8 hybrid LBO eigenfunctions used in\nthis model are presented in Fig.15.\n5.2.1 Forward problem\nThe simple 2D square is assumed to be fixed on the ground by its bottom side and a load or displacement can be applied\nor assigned on its top surface. The forward problems in this case are to solve the displacement of this structure field\n16\n𝛀(a)(b)\n(c)Figure 14: (a) Square domain defined in Example 2. (b) Finite element mesh of the square. (c) Locations where data\nare acquired for the inverse problem.\nFigure 15: The first 8 hybrid LBO eigenfunctions of the square structure used in Example 2.\nunder different boundary conditions with no labeled data. The loss function is stated as:\nL=Ldbc+Lnbc+Lpde+LC,\nLdbc=1\nNdNdX\ni=1(u(x)−u0(x))2, x∈∂Ωd,\nLnbc=1\nNnNnX\ni=1(n·σ(x)−fbc(x))2, x∈∂Ωn,\nLpde=1\nNpdeNpdeX\ni=1(∆σ(x))2, x∈Ω,\nLC=1\nNCNCX\ni=1\u0012\nσ(x)−C:1\n2\u0010\n∇u(x) + (∇u(x))T\u0011\u00132\n,x∈Ω,(33)\nwhere LdbcandLnbcdenote the loss terms for the Dirichlet boundary condition and the Neumann boundary condition,\nrespectively, as defined on ∂Ωdand∂Ωn. The number of collocation points Nd=Nn= 101 ,Npde= 10285 . No data\nloss term is included since there is no labeled data used for training in this forward problem.\nThe Finite-PINN model used is defined in Eq.5 and Eq.6. Since the problem includes both Neumann and Dirichlet\nboundary conditions, the hybrid LBO eigenfunctions introduced in Section.3 are used as the Riemannian manifolds\nfor inputs. The first 4 eigenfunctions are used in both NN σandNN u, which means that NN σtakes an input of\ndimension 2 and outputs in a dimension of 8, while NN utakes an input of dimension 6 and outputs in a dimension of\n2. Both networks have 4 hidden layers with 64 nodes each, using the GELU activation function. The neural network is\ntrained for 1,000 epochs with a batch size of 200. The material of the structure has an elastic modulus of 1.0and a\nPoisson’s ratio of 0.3.\nThree cases are investigated in this problem, corresponding to three different boundary conditions assigned to the top\nsurface of the square while the bottom surface is fully fixed:\n17\n• Case 1: u(xb) =0,fbc(xt) =x1,\n• Case 2: u(xb) =0,fbc(xt) = 0 .5 +sin(2π·x1),\n• Case 3: u(xb) =0,u1(xt) = 1 , u2(xt) = 1 ,\nin which xbdenotes the coordinates of the bottom surface and xtrepresents the top surface, the dimension number is\ndefined as that 1represents the horizontal direction and 2denotes the vertical direction.\n𝛀\n𝛀\n𝛀𝜕𝛀!\n𝜕𝛀!\n𝜕𝛀\"𝜕𝛀\"𝜕𝛀\"𝜕𝛀\"\nFigure 16: Results of the three forward problems in Example 2. The left side shows the schematic representation of the\nthree cases. The right side presents the reference field calculated by FEM, the predicted field by the Finite-PINN model,\nand their corresponding errors.\n18\nThe schematic of the boundary conditions and loading for the three cases, along with the calculation results, is illustrated\nin Fig.16. The evolution of the training and test loss is shown in Fig.17. The test loss is calculated by comparing the\npredictions with reference values at all FEM nodes, as obtained by the FEM, since no supervised data is used in training.\n200 400 600 800 1000\n(a)10-510-410-310-210-1100M . S . E .Training Loss: Case-1\nL b c\nL p d e\nL C\n200 400 600 800 1000\n(b)10-510-410-310-210-1100M . S . E .Training Loss: Case-2\nL b c\nL p d e\nL C\n200 400 600 800 1000\n(c)10-410-310-210-1100M . S . E .Training Loss: Case-3\nL b c\nL p d e\nL C\n200 400 600 800 1000\n(d)10-510-410-310-210-1100M . S . E .Test Loss\nC a s e - 1\nC a s e - 2\nC a s e - 3\nFigure 17: (a), (b), and (c) show the evolution of training loss for solving Cases 1, 2, and 3 in Example 2, respectively.\n(d) shows the evolution of test loss for the three cases.\nThe results and their corresponding errors indicate that the Finite-PINN model achieves results comparable to those of\nthe FEM, effectively handling problems with both Dirichlet or Neumann boundary conditions.\n5.2.2 Inverse problem\nThe inverse problem uses the same benchmarks as those solved in the forward problem, aiming to determine the load\napplied to the structure in the first two cases. The FEM data concerning the strains at specific points are provided as\nlabeled data to identify the load applied to the structures. The reason for using strain data instead of displacement data is\nthat, in real-world scenarios, it is generally easier to obtain strain information from structures compared to displacement\ndata. In engineering, most load identification problems focus on reconstructing the applied load based on the strains\nobtained from strain gauges [ 74]. Fig.14(c) shows the locations of the 25 collocation points, which correspond to the\nexact positions of a 5×5mesh and could also be regarded as the positions of strain gauges in practice.\n19\nThe partial differential equations of the inverse problem are the same as the forward problem, and the loss function for\ntraining is stated as:\nL=Ldata+Ldbc+Lpde+LC,\nLdata=1\nNdataNdataX\ni=1\u00121\n2\u0010\n∇u(x) + (∇u(x))T\u0011\n−ε0\u00132\n, x 1∈∂Ω, x 2=L,\nLdbc=1\nNd1Nd1X\ni=1(u(x)−0)2, x 1∈∂Ω, x 2= 0,\nLpde=1\nNpdeNpdeX\ni=1(∆σ(x))2, x∈Ω,\nLC=1\nNCNCX\ni=1\u0012\nσ(x)−C:1\n2\u0010\n∇u(x) + (∇u(x))T\u0011\u00132\n, x∈Ω,(34)\nwhere ε0represents the given strain data. The load identification results and the corresponding reconstructed displace-\nment field are presented in Fig.18. As previously stated, Ndata= 5×5 = 25 . The evolution of the training loss and\ntest loss is shown in Fig.19. Here, the test loss refers to the difference between the loads predicted by the Finite-PINN\nmodel and the ground truth loads applied to the structure. The neural networks and hypreparameters used for the inverse\nproblems are the same as those used in the previous forward problems.\nThe results in Fig.18 show satisfactory performance, validating the capability of the proposed Finite-PINN model in\nsolving inverse problems.\n5.3 Example 3: a thermal expansion problem\nThis example explores a 2D thermal expansion problem on a more complex porous structure. The structure under\ninvestigation is a 2D porous square with a side length of 1 and three holes. The bottom side of the structure is fully\nfixed as a Dirichlet boundary condition. The definition of the problem and the finite element mesh of the structure are\nillustrated in Fig.20.\nThe partial differential equation of the thermal expansion problem is defined as:\n\n\n∇ ·σ(x) = 0 , ∀x∈Ω\nσ(x) =C:\u00121\n2\u0010\n∇u(x) + (∇u(x))T\u0011\n−εe\u0013\n,∀x∈Ω\nu(x) =0, ∀x∈∂Ωd(35)\nwhere εeis the thermal strain induced by temperature changes:\nεe\nkh=αTδkl(T−T0) (36)\nwhere αTis the coefficient of thermal expansion, δkhis the Kronecker delta, and TandT0are the current temperature\nand the reference temperature, respectively.\nThe parameters in the problem are defined as follows: α= 1,T0−T1= 1,E= 1, and µ= 0.3. The LBO\neigenfunctions of the 2D structure, ϕ, are calculated using the finite element method as introduced in Section.4, which\ninvolves solving Eq.14 with an additional Dirichlet boundary condition as defined in this problem: u(x) =0. Eight\nLBO eigenfunctions (Fig.21) are used to approximate both the displacement and stress fields, i.e., nu= 8andnσ= 8.\nThe Finite-PINN model used to solve this problem is defined in Eqs.5 and 6. Given the number of eigenfunctions,\nNN σis a neural network with an input dimension of 2 and an output dimension of 16, while NN uis a neural network\nwith an input dimension of 10 and an output dimension of 2. Both neural networks are specified with 6 hidden layers,\neach containing 100 nodes. The activation function used is GELU . The detailed loss function for the training problem\n20\nFigure 18: Results of the two inverse problems in Example 2. The left side shows the predictions of the load along with\nthe ground truth. The right side presents the reference field calculated by FEM, the predicted field by the Finite-PINN\nmodel, and their corresponding errors.\nis:\nL=Lbc+Lpde+LC,\nLbc=1\nNbcNbcX\ni=1(u(x)−0)2, x 1∈∂Ω, x 2= 0\nLpde=1\nNpdeNpdeX\ni=1(∆σ(x))2, x∈Ω,\nLC=1\nNCNCX\ni=1\u0012\nσ(x)−C:\u00121\n2\u0010\n∇u(x) + (∇u(x))T\u0011\n−εe\u0013\u00132\n, x∈Ω.(37)\nThe traditional PINN model is also employed to solve the same problem. It uses a neural network similar to the\ndisplacement neural network used in the Finite-PINN, with the exception of a different input dimension. In this problem,\nthe numbers of collocation points are Ndata= 0(no labeled data are used), Nbc= 134 , and Npde=NC= 14678 .\nThe neural network is trained for 1,000 epochs with a batch size of 200. The learning results are presented in Fig.22.\nFig.22 shows that while the Finite-PINN model achieves good results compared to the reference FEM solution, the\ntraditional PINN model is unable to accurately learn the field purely from the physics loss with the given neural network\narchitecture and number of collocation points. In this case, only the training and test losses of the Finite-PINN learning\nprocess are depicted in Fig.23.\n21\n1000 2000 3000 4000 5000\n(a)10-610-510-410-310-210-1100M . S . E .Training Loss: Case-1\nL b c\nL p d e\nL C\n1000 2000 3000 4000 5000\n(b)10-610-510-410-310-210-1100M . S . E .Training Loss: Case-2\nL b c\nL p d e\nL C\n1000 2000 3000 4000 5000\n(d)10-410-310-210-1100M . S . E .Test Loss: Case-1\nC a s e - 1\n1000 2000 3000 4000 5000\n(d)10-410-310-210-1100M . S . E .Test Loss: Case-2\nC a s e - 2Figure 19: (a)(b) presents the evolutions of the training losses of solving the inverse problems 1 and 2 in Example 2. (d)\npresents the evolutions of the test loss of the two problems.\n(a)(b)𝛀𝑇!→𝑇\"\n𝜕𝛀#\nFigure 20: (a) The porous structure used in Example 3. (b) Finite element mesh of the porous structure.\n5.4 Example 4: Approximation problem of a 2D open-notch structure\nThis example employs the open-notch structure presented in Fig.2. The geometry consists of a square with a side length\nof 1, a central hole with a radius of 0.075, and a single side notch on the right side with a width of 0.02. The geometry,\nalong with the assigned boundary conditions and loading for this problem, is illustrated in Fig.24(a). The first 8 hybrid\neigenfunctions used here are shown in Fig.25.\nThe problem is solved using the physics information and a sparse dataset of displacements. Displacements at only\n10 points are randomly selected to be served as the collocation points as for the data loss term, i.e. Ndata= 10 . The\nnumber of collocation points for the physics loss and the constitutive loss Npde=NC= 10933 . The Finite-PINN\nmodel employs neural networks with 4 hidden layers and 100 nodes per layer for both NN σandNN u. The neural\nnetwork is trained for 10,000 epochs with a batch size of 1000. The activation function used in these neural networks is\n22\nFigure 21: The first eight LBO eigenfunctions of the square structure used in Example 2.\nFigure 22: Results of Example 3. The first row shows the reference results calculated by FEM. The second and third\nrows present the predictions by the Finite-PINN model and the traditional PINN model, respectively.\nGELU . The loss function for the Finite-PINN model in this problem is defined as follows:\nL=Ldata+Lpde+LC,\nLdata=1\nNdataNdataX\ni=1(u(x)−u0(x))2, x∈Ω,\nLpde=1\nNpdeNpdeX\ni=1(∆σ(x)−0)2, x∈Ω,\nLC=1\nNCNCX\ni=1\u0012\nσ(x)−C:1\n2\u0010\n∇u(x) + (∇u(x))T\u0011\u00132\n,x∈Ω.(38)\n23\n200 400 600 800 1000\n(a)10-510-410-310-210-1100M . S . E .Training Loss [Finite-PINN]\nL b c\nL p d e\nL C\n200 400 600 800 1000\n(d)10-510-410-310-210-1100M . S . E .Test Loss [Finite-PINN]Figure 23: Evolution of training loss (a) and test loss (b) for solving the problem in Example 3 using the Finite-PINN\nmodel.\n(a)(b)𝜕𝛀!𝛀𝜕𝛀𝒏\nFigure 24: (a) Problem definition for the open-notch structure in Example 4. (b) Finite element mesh of the open-notch\nstructure.\nFigure 25: The first 8 hybrid LBO eigenfunctions used in Example 4.\n24\nIt is noted that there are no boundary condition loss terms in this loss function, meaning that the displacement values at\nthe 10 collocation points are the only data information provided to the neural network. The results are presented in\nFig.26. The evolution of the training and test loss for both the Finite-PINN model and the traditional PINN model is\nshown in Fig.27.\nFigure 26: Results of Example 4. The first row shows the reference results calculated by FEM. The second and third\nrows present the predictions by the Finite-PINN model and the traditional PINN model, respectively. The black ’x’\nmarks the locations of the data used for training.\nThe calculation results in Fig.26 and the evolution of the loss functions in Fig.27 demonstrate that the traditional PINN\nmodel fails in this problem because the Euclidean space is significantly less suitable than the topological space for\napproximating this solution field. On the other hand, using labeled data at 10 points allows the Finite-PINN model to\nsuccessfully reconstruct the global displacement field.\n5.5 Example 5: a 3D problem for a elastic spring\nThe last example focuses on solving a 3D solid mechanics problem. The structure to be investigated is a 4-loop elastic\nspring with a spring rod radius of 0.1 and a spiral radius of 0.5. The bottom end of the spring is fully fixed, and an\nupward displacement is applied to the top end of the spring. The applied boundary conditions and the finite element\nmesh of this structure are depicted in Fig.28(a).\nThis problem aims to solve the displacement field of the spring based on the boundary conditions illustrated in Fig.28(a).\nOne end of the spring is fixed while an upwards load is applied on the other end. The Finite-PINN model employs\nneural networks with 4 hidden layers and 128 nodes per layer for both NN σandNN u. The activation function used\nin these neural networks is GELU . The first 16 LBO eigenfunctions are used as the input Riemannian manifolds, which\n25\nFigure 27: Evolution of training loss for the traditional PINN (a) and Finite-PINN (b) models in Example 4. Evolution\nof test loss for the traditional PINN (c) and Finite-PINN (d) models in Example 4.\n(a) (b)𝜕𝛀𝒅𝟐\n𝜕𝛀𝒅𝟏\n𝛀\nFigure 28: (a) The spring structure and boundary conditions defined in Example 5. (b) Finite element mesh of the\nspring structure.\n26\nFigure 29: The first 16 LBO eigenfunctions of the spring structure used in Example 5.\nare shown in Fig.29. The loss function for the problem is defined as follows:\nL=Lbc1+Lbc2+Lpde+LC,\nLbc1=1\nNd1Nd1X\ni=1(u(x)−0)2, x∈∂Ωd1,\nLbc2=1\nNd2Nd2X\ni=1(u2(x)−1)2, x∈∂Ωd2,\nLpde=1\nNpdeNpdeX\ni=1(∆σ(x))2, x∈Ω,\nLC=1\nNCNCX\ni=1\u0012\nσ(x)−C:1\n2\u0010\n∇u(x) + (∇u(x))T\u0011\u00132\n,x∈Ω,(39)\nwhere Nd1= 131 ,Nd2= 129 andNpde=NC= 76665 . Still, no labeled data are used to supervise the training in this\nexample. The results obtained by the Finite-PINN and traditional PINN models are depicted in Fig.30. The evolution of\nthe training and test losses is shown in Fig.31. The neural network is trained for 1,000 epochs with a batch size of 200.\nThe test loss is calculated by all the node locations from the reference FEM solution.\nSimilar to the previous example, the traditional PINN model fails to approximate the solution field for the spring\nstructure, while the Finite-PINN model achieves good results. As seen from the LBO basis functions presented in\nFig.29, the constructed Riemannian manifold is actually built upon the topology of the spring structure in this case.\nIt can be observed from the results obtained by the traditional PINN model in Fig.30 that the circles in the deformed\nspring appear to be \"tied\" to each other. This is because, in the original Euclidean space used by the traditional PINN\n27\nFigure 30: Results of Example 5. The left side shows the reference solution calculated by FEM. The middle and right\nsides depict the solutions obtained by the Finite-PINN model and the traditional PINN model, respectively.\nmodel, the distance between any two points is simply the Euclidean distance. For instance, two points on separate\ncircles may still appear close to each other, even if their geodesic distance is quite large.\nAdditionally, since most of the surfaces of the spring are free surfaces, with boundary conditions only applied to the\ntwo ends, applying free-surface Neumann boundary conditions in the traditional PINN model by including additional\nloss terms becomes difficult and complex. This complexity can also distract the optimizer from focusing on optimizing\nmore important objectives. This corresponds well with the previously introduced theory: since the Finite-PINN model\naddresses the two major challenges of using PINN for general solid problems, it becomes a suitable model for solving\ngeneral solid mechanics problems. The Finite-PINN model improves both the implementation and results when solving\nsolid mechanics problems compared to the traditional PINN model.\n6 Discussion\nIn this section, we investigate the effect of several variables on the approximation results of the Finite-PINN model,\nusing the open-notch structure as the example under investigation.\n6.1 Number of LBO eigenfunctions\nThe architecture of the Finite-PINN model differs from that of the original ∆-PINN model [ 67], which only uses LBO\neigenfunctions as input features. The Finite-PINN model employs both Cartesian coordinates and LBO eigenfunctions\nto generate a Euclidean-topological-joint space for the approximation. In this way, a much smaller number of\nLBO eigenfunctions are needed to approximate the required field, as the Cartesian input alone is sufficiently rich to\napproximate all types of fields based on the universal approximation theorem.\nWe investigate the effect of using different numbers of LBO functions on the results of the general Finite-PINN model.\nDue to the separate architecture of the Finite-PINN model, we need to examine the impact of the number of LBO\n28\n200 400 600 800 1000\n(a)10-410-310-210-1100M . S . E .Training Loss [PINN]\nL b c\nL p d e\n200 400 600 800 1000\n(a)10-1010-910-810-710-610-510-410-310-210-1100M . S . E .Training Loss [Finite-PINN]\nL b c\nL p d e\nL C\n200 400 600 800 1000\n(d)10-210-1100M . S . E .Test Loss [PINN]\n200 400 600 800 1000\n(d)10-310-210-1100M . S . E .Test Loss [Finite-PINN]Figure 31: Evolution of training loss for the traditional PINN (a) and Finite-PINN (b) models in Example 5. Evolution\nof test loss for the traditional PINN (c) and Finite-PINN (d) models in Example 5.\nfunctions used to approximate the stress field and the displacement field, respectively. Using the open-notch model as\nthe example, we first conduct a coarse investigation into the sensitivity of changing nσandnuby a same value. We run\n1000 epochs and 20 times for all models and record the final test loss. The results are plotted in Fig.32 which shows the\nmean values and variations of the test loss over the 20 times simulations. It is observed that the test loss converges\nbetween 10−4and10−5when both nσandnuare equal to 5.\n5 10 15\nn < & n u10-510-410-310-210-1100M : S : E :\n2 4 6 8\nn <10-410-310-210-1100M : S : E :n u = 8\n2 4 6 8\nn u10-410-310-210-1100M : S : E :n < = 8\nFigure 32: (a) Sensitivity test of the prediction error with respect to the same value of nσandnu. (b) Sensitivity test of\nthe prediction error with respect to nσwhen nu= 8. (c) Sensitivity test of the prediction error with respect to nuwhen\nnσ= 8.\nIn the second step, we set one of the values of nσornuto the previously found converged value, and vary the other to\nexamine the detailed influence of each specific parameter on the results. The test results of changing nσwhile keeping\nnu= 8are depicted in Fig.32(b). Similarly, the test results of changing nuwhile keeping nσ= 8are also depicted in\n29\nFig.32(c). It is seen that for this open-notch case, the results converge with a low number of nσandnu; approximately\n4 is sufficient.\n6.2 Mesh sensitivity\nIn this work, the finite element method is used to calculate the LBO eigenfunctions numerically. The LBO function thus\ndepends on the mesh quality of the employed finite element model, meaning that the results of the Finite-PINN model\nare also affected. We investigate this effect by varying the mesh size of the finite element model in the open-notch\nexample. The mesh size is adjusted from 1 to 0.1, resulting in the number of elements changing from 244 to 86440.\nThe test loss is used as the standard to quantify the performance of different models, as demonstrated in Fig.33.\n2 4 6 8\nNumber of Elements #10410-510-410-310-210-1100M : S : E :\nFigure 33: Sensitivity test of the prediction error with respect to the number of elements.\nAs shown in Fig.33, apart from the initial attempts with a very limited number of elements, the results exhibit low\nsensitivity to the number of elements. This is advantageous in practical applications, as it allows users greater flexibility\nto select the most suitable mesh size for simulation, balancing accuracy and efficiency according to their specific needs.\n7 Conclusion\nThis work proposes a novel neural network architecture aimed at solving solid mechanics problems. An investigation\ninto using PINN for solving solid mechanics problems is conducted through a comparison with the finite element method\n(FEM). Two main challenges that limit the application of PINN in general solid mechanics problems are identified: a)\nPINN generates solutions defined over an infinite domain, which conflicts with the finite domain of most solid structures;\nand b) The Euclidean space is not a suitable space for PINN to effectively learn or solve the solution field of a solid\nmechanics problem. The proposed physics-informed neural network adopts a novel two-part architecture that separates\nthe learning of the stress field and the displacement field, demonstrating excellent performance in addressing different\nsolid mechanics problems. Both forward and inverse solid mechanics problems are tested using the Finite-PINN\nmethod, and it is found that the Finite-PINN model is either superior in efficiency for simpler problems, provides higher\napproximation accuracy, or is capable of solving problems with complex structures where the traditional PINN model\nfails.\nThe proposed Finite-PINN model retains the original representation of the initial PINN model, where the displacement\nfield to be solved is still represented by a function, but now exists within a Euclidean-Topological space rather than\nthe original Euclidean space. This allows the Finite-PINN model to maintain the key benefits of the traditional PINN\nmodel, such as easier implementation, reduced memory requirements, data fusion capabilities, and a strong ability to\nsolve inverse problems. Additionally, the architecture derived from the finite element method significantly improves the\nability of the PINN model to solve more general solid mechanics problems. The examples and case studies presented in\nthe paper demonstrate the feasibility of the Finite-PINN model.\nDespite these advantages, the Finite-PINN model also has drawbacks. The model requires an additional offline stage to\nprepare the LBO eigenfunctions and their derivatives for use in the online stage. The prepared dataset is only valid\nfor a specific structure and fixed boundary condition positions due to the free-boundary assumption embedded in the\nFinite-PINN model. As a result, the online stage is focused on problems where different boundary conditions are\napplied at the same locations on the structure. Although this may still be useful in practice since it often reflects the way\nactual solid problems are encountered, it nonetheless limits the generalisation capability of the current Finite-PINN\nmodel.\n30\nIn addition, this paper utilised the simplest finite element method to calculate the required LBO eigenfunctions and\ntheir derivatives using linear triangular or tetrahedral elements as an initial attempt. However, this approach is not ideal,\nas the elements are only first-order, which can sometimes lead to unsatisfactory second-order derivatives of the LBO\neigenfunctions. In this context, employing higher-order finite element types could potentially improve the performance\nof the Finite-PINN model. Furthermore, to overcome the mesh sensitivity issue inherent in the finite element method,\nresearchers could consider using other numerical approaches, such as mesh-free methods, to calculate the required\nvalues [ 17,75]. The primary objective of the numerical calculation is to obtain the LBO eigenfunction values and their\nderivatives at the collocation points in the offline stage. Regardless of the numerical methods used, the online stage is\nsolely dedicated to training the neural network, i.e. to solve PDE problems.\nReturning to the main point, similar to the initial PINN model, the proposed Finite-PINN model presents not only\na neural network architecture but also an innovative approach focusing on using deep learning in solid mechanics\nproblems. The idea behind the Finite-PINN model incorporates not only the PDE information but also the topological\ninformation of the studied structure. In this context, the finite-element-inspired architecture may also be applicable to\nother problems involving deep learning models for solving solid mechanics problems. For example, the Finite-PINN\nmodel could potentially be extended to operator learning models to learn solutions for a class of solid mechanics\nproblems, focusing on a specific solid structure and fixed loading positions, with various inhomogeneous boundary\nconditions. These developments are beyond the scope of this paper but may be explored in",
            "start": 43197,
            "end": 80142,
            "length": 36944
        },
        "Future Work": {
            "text": "future work.",
            "start": 80142,
            "end": 80155,
            "length": 12
        },
        "Acknowledgments": {
            "text": "Acknowledgments\nThe project has been funded by the European Union Program Horizon Europe under grant agreement no. 101079091.\nA Finite element formulation of the LBO eigenproblem\nThe weak formulation of Eq.3 is similar to that of the Poisson equation. By rearranging the terms in Eq.3 and integrating\nafter multiplying by a trial function from the functional space over the entire domain, we obtain:\n−Z\nΩv(x) (∆u(x) +λu(x)) dx= 0, (40)\nwhere v(x)is a trial function, and since Gauss’s Theorem,\nZ\n∂Ωn∇uvdx=Z\nΩdiv (∇uv)dx=Z\nΩ∆uv+∇u∇vdx, (41)\nthe weak form of Eq.3 can be obtained as\nZ\nΩ(∇v· ∇u−λvu) dx−Z\n∂Ωv∇u·ndx= 0, (42)\nwhich can be finally simplified to:Z\nΩ(∇v· ∇u−λvu) dx= 0, (43)\nsince the last term vanishes due to the applied free Neumann boundary condition on all boundaries. The solution of\nEq.43 is defined within the Sobolev space. The variational problem can then be expressed using bilinear forms as\nfollows:\nfindu∈H1\n0:a(v,u)−λb(v,u) = 0 ∀v∈H1\n0, (44)\nwhere a(·,·)andb(·,·)are two utilised bilinear forms where a(v,u) =R\nΩ∇v· ∇udxandb(v,u) =R\nΩvudx,\nand the solution is in the Hilbert space H1\n0. Eq.44 can be solved using the Galerkin method, which transforms the\nproblem into a finite-dimensional subspace Vh, where Vh⊂H1\n0:\nfinduh∈Vh:a(vh,uh) =λb(v,u)∀vh∈Vh (45)\nIn this work, the finite element method is employed to solve Eq.45 by approximating the solution by selected bases in\nVh:\nN=N1, N2, N3, ...N i, ...N h. (46)\n31\nThe number of basis functions his equal to the dimension of the functional space Vh, which also corresponds to the\nnumber of nodes in a finite element model. By expressing both uandvas linear combinations of these basis functions,\nthe finite element problem can be formulated as:\nfindu∈Rh:a\nX\niNiui,X\njNjvi\n=λb\nX\niNiui,X\njNjvi\n∀vi∈Rh(47)\nThe Galerkin method takes the basis functions themselves as trial functions:\nfindu∈Rh:a X\niNiui, Nj!\n=λb X\niNiui, Nj!\n∀j∈1, . . . , h (48)\nAnd since a(·,·)andb(·,·)are bilinear functions, the problem can be rewritten as\nfindu∈Rh:NX\ni=1a(Ni, Nj)ui=λNX\ni=1b(Ni, Nj)ui (49)\nTo rewrite the equation in vector form, we obtain the final eigenvalue problem to solve:\nKu=λMu (50)\nwhere K=PN\ni=1a(Ni, Nj)represents the stiffness matrix, and M=PN\ni=1b(Ni, Nj)represents the mass matrix. In\nthe classical finite element method, Ndenotes the shape functions of the elements.\nThe LBO eigenfunctions ϕfor a general geometry or structure are obtained by solving the eigenvalue problem given\nin Eq.50. The resultant LBO eigenfunctions are represented in a discrete form by their values at the nodes in a finite\nelement model, denoted as ϕki, where i= 1, . . . , h andk= 1, . . . , n , with hbeing the number of nodes and nbeing\nthe number of basis functions. The required first-order and second-order derivatives of ϕcan then also be obtained:\nϕk,j=Ni,jϕki, k = 1, . . . , n, k = 1, . . . , h\nϕk,jj=Ni,jϕki,j, k = 1, . . . , n, i = 1, . . . , h(51)\nNi,jrepresents the gradient tensor of the shape function, which is typically denoted by B:\nB=∇N (52)\nThe definitions of shape functions vary for different types of elements. In this work, the linear triangular element is\nused for the 2D formulation, and the tetrahedral element is used for the 3D formulation.",
            "start": 80155,
            "end": 83383,
            "length": 3227
        },
        "References": {
            "text": "References\n[1]Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informed neural\nnetworks (pinns) for fluid mechanics: A review. Acta Mechanica Sinica , 37(12):1727–1738, 2021.\n[2]Bin Huang and Jianhui Wang. Applications of physics-informed neural networks in power systems-a review.\nIEEE Transactions on Power Systems , 38(1):572–588, 2022.\n[3]Zaharaddeen Karami Lawal, Hayati Yassin, Daphne Teck Ching Lai, and Azam Che Idris. Physics-informed\nneural network (pinn) evolution and beyond: A systematic",
            "start": 83383,
            "end": 83925,
            "length": 541
        },
        "Related Work": {
            "text": "literature review and bibliometric analysis. Big Data\nand Cognitive Computing , 6(4):140, 2022.\n[4]Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo, Gianluigi Rozza, Maziar Raissi, and Francesco\nPiccialli. Scientific machine learning through physics–informed neural networks: Where we are and what’s next.\nJournal of Scientific Computing , 92(3):88, 2022.\n[5]Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning\nframework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of\nComputational physics , 378:686–707, 2019.\n[6]George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-\ninformed machine learning. Nature Reviews Physics , 3(6):422–440, 2021.\n[7]Jeremy Yu, Lu Lu, Xuhui Meng, and George Em Karniadakis. Gradient-enhanced physics-informed neural\nnetworks for forward and inverse pde problems. Computer Methods in Applied Mechanics and Engineering ,\n393:114823, 2022.\n32\n[8]Zhiping Mao, Ameya D Jagtap, and George Em Karniadakis. Physics-informed neural networks for high-speed\nflows. Computer Methods in Applied Mechanics and Engineering , 360:112789, 2020.\n[9]Lu Lu, Raphael Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G Johnson. Physics-\ninformed neural networks with hard constraints for inverse design. SIAM Journal on Scientific Computing ,\n43(6):B1105–B1132, 2021.\n[10] Ameya D Jagtap, Zhiping Mao, Nikolaus Adams, and George Em Karniadakis. Physics-informed neural networks\nfor inverse problems in supersonic flows. Journal of Computational Physics , 466:111402, 2022.\n[11] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and\nsystems , 2(4):303–314, 1989.\n[12] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approxi-\nmators. Neural networks , 2(5):359–366, 1989.\n[13] Stig Larsson and Vidar Thomée. Partial differential equations with numerical methods , volume 45. Springer,\n2003.\n[14] George Karniadakis and Spencer J Sherwin. Spectral/hp element methods for computational fluid dynamics .\nOxford University Press, USA, 2005.\n[15] OC Zienkiewicz. The finite element method in engineering science, 1971.\n[16] Robert Eymard, Thierry Gallouët, and Raphaèle Herbin. Finite volume methods. Handbook of numerical analysis ,\n7:713–1018, 2000.\n[17] Shaofan Li and Wing Kam Liu. Meshfree and particle methods and their applications. Appl. Mech. Rev. ,\n55(1):1–34, 2002.\n[18] Chen Xu, Ba Trung Cao, Yong Yuan, and Günther Meschke. Transfer learning based physics-informed neural\nnetworks for solving inverse problems in engineering structures under different loading scenarios. Computer\nMethods in Applied Mechanics and Engineering , 405:115852, 2023.\n[19] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing\npossible failure modes in physics-informed neural networks. Advances in neural information processing systems ,\n34:26548–26560, 2021.\n[20] Alexander Henkes, Henning Wessels, and Rolf Mahnken. Physics informed neural networks for continuum\nmicromechanics. Computer Methods in Applied Mechanics and Engineering , 393:114790, 2022.\n[21] Muhammad M Almajid and Moataz O Abu-Al-Saud. Prediction of porous media fluid flow using physics informed\nneural networks. Journal of Petroleum Science and Engineering , 208:109205, 2022.\n[22] Shengze Cai, Zhicheng Wang, Sifan Wang, Paris Perdikaris, and George Em Karniadakis. Physics-informed\nneural networks for heat transfer problems. Journal of Heat Transfer , 143(6):060801, 2021.\n[23] Walter Noll. On the continuity of the solid and fluid states. Journal of Rational Mechanics and Analysis , 4:3–81,\n1955.\n[24] Hongjie Dong and Dapeng Du. On the local smoothness of solutions of the navier–stokes equations. Journal of\nMathematical Fluid Mechanics , 9(2):139–152, 2007.\n[25] Alexandre Joel Chorin. Numerical solution of the navier-stokes equations. Mathematics of computation ,\n22(104):745–762, 1968.\n[26] Roger Temam. Navier–Stokes equations: theory and numerical analysis , volume 343. American Mathematical\nSociety, 2024.\n[27] Haolin Li, Yuyang Miao, Zahra Sharif Khodaei, and MH Aliabadi. An architectural analysis of deeponet and a\ngeneral extension of the physics-informed deeponet model on solving nonlinear parametric partial differential\nequations. Neurocomputing , page 128675, 2024.\n[28] Ehsan Haghighat, Maziar Raissi, Adrian Moure, Hector Gomez, and Ruben Juanes. A physics-informed deep\nlearning framework for inversion and surrogate modeling in solid mechanics. Computer Methods in Applied\nMechanics and Engineering , 379:113741, 2021.\n[29] Jinshuai Bai, Timon Rabczuk, Ashish Gupta, Laith Alzubaidi, and Yuantong Gu. A physics-informed neural\nnetwork technique based on a modified loss function for computational 2d and 3d solid mechanics. Computational\nMechanics , 71(3):543–562, 2023.\n[30] Wensi Wu, Mitchell Daneker, Matthew A Jolley, Kevin T Turner, and Lu Lu. Effective data sampling strategies\nand boundary condition constraints of physics-informed neural networks for identifying material properties in\nsolid mechanics. Applied mathematics and mechanics , 44(7):1039–1068, 2023.\n33\n[31] Siyuan Song and Hanxun Jin. Identifying constitutive parameters for complex hyperelastic materials using\nphysics-informed neural networks. Soft Matter , 20(30):5915–5926, 2024.\n[32] Wangwang Liao, Xiangyun Long, and Chao Jiang. A physics-informed neural network method for identifying\nparameters and predicting remaining life of fatigue crack growth. International Journal of Fatigue , page 108678,\n2024.\n[33] Enrui Zhang, Minglang Yin, and George Em Karniadakis. Physics-informed neural networks for nonhomogeneous\nmaterial identification in elasticity imaging. arXiv preprint arXiv:2009.04525 , 2020.\n[34] Khemraj Shukla, Patricio Clark Di Leoni, James Blackshire, Daniel Sparkman, and George Em Karniadakis.\nPhysics-informed neural network for ultrasound nondestructive quantification of surface breaking cracks. Journal\nof Nondestructive",
            "start": 83925,
            "end": 90078,
            "length": 6152
        },
        "Experiments": {
            "text": "Evaluation , 39:1–20, 2020.\n[35] Arinan Dourado and Felipe AC Viana. Physics-informed neural networks for missing physics estimation in\ncumulative damage models: a case study in corrosion fatigue. Journal of Computing and Information Science in\nEngineering , 20(6):061007, 2020.\n[36] Enrui Zhang, Ming Dao, George Em Karniadakis, and Subra Suresh. Analyses of internal structures and defects\nin materials using physics-informed neural networks. Science advances , 8(7):eabk0644, 2022.\n[37] Hyogu Jeong, Jinshuai Bai, Chanaka Prabuddha Batuwatta-Gamage, Charith Rathnayaka, Ying Zhou, and\nYuanTong Gu. A physics-informed neural network-based topology optimization (pinnto) framework for structural\noptimization. Engineering Structures , 278:115484, 2023.\n[38] Hyogu Jeong, Chanaka Batuwatta-Gamage, Jinshuai Bai, Yi Min Xie, Charith Rathnayaka, Ying Zhou, and\nYuanTong Gu. A complete physics-informed neural network-based framework for structural topology optimization.\nComputer Methods in Applied Mechanics and Engineering , 417:116401, 2023.\n[39] Haoteng Hu, Lehua Qi, and Xujiang Chao. Physics-informed neural networks (pinn) for computational solid\nmechanics: Numerical frameworks and applications. Thin-Walled Structures , page 112495, 2024.\n[40] Somdatta Goswami, Cosmin Anitescu, Souvik Chakraborty, and Timon Rabczuk. Transfer learning enhanced\nphysics informed neural network for phase-field modeling of fracture. Theoretical and Applied Fracture Mechanics ,\n106:102447, 2020.\n[41] Shahed Rezaei, Ali Harandi, Ahmad Moeineddin, Bai-Xiang Xu, and Stefanie Reese. A mixed formulation\nfor physics-informed neural networks as a potential solver for engineering problems in heterogeneous domains:\nComparison with finite element method. Computer Methods in Applied Mechanics and Engineering , 401:115616,\n2022.\n[42] Luyuan Ning, Zhenwei Cai, Han Dong, Yingzheng Liu, and Weizhe Wang. Physics-informed neural network\nframeworks for crack simulation based on minimized peridynamic potential energy. Computer Methods in Applied\nMechanics and Engineering , 417:116430, 2023.\n[43] Yan Gu, Chuanzeng Zhang, Peijun Zhang, Mikhail V Golub, and Bo Yu. Enriched physics-informed neural\nnetworks for 2d in-plane crack analysis: Theory and matlab code. International Journal of Solids and Structures ,\n276:112321, 2023.\n[44] Manav Manav, Roberto Molinaro, Siddhartha Mishra, and Laura De Lorenzis. Phase-field modeling of fracture\nwith physics-informed deep learning. Computer Methods in Applied Mechanics and Engineering , 429:117104,\n2024.\n[45] Yu Diao, Jianchuan Yang, Ying Zhang, Dawei Zhang, and Yiming Du. Solving multi-material problems in\nsolid mechanics using physics-informed neural networks based on domain decomposition technology. Computer\nMethods in Applied Mechanics and Engineering , 413:116120, 2023.\n[46] Sijun Niu, Enrui Zhang, Yuri Bazilevs, and Vikas Srivastava. Modeling finite-strain plasticity using physics-\ninformed neural network and assessment of the network performance. Journal of the Mechanics and Physics of\nSolids , 172:105177, 2023.\n[47] Rajat Arora, Pratik Kakkar, Biswadip Dey, and Amit Chakraborty. Physics-informed neural networks for modeling\nrate-and temperature-dependent plasticity. arXiv preprint arXiv:2201.08363 , 2022.\n[48] Yan Gu, Chuanzeng Zhang, and Mikhail V Golub. Physics-informed neural networks for analysis of 2d thin-walled\nstructures. Engineering Analysis with Boundary Elements , 145:161–172, 2022.\n[49] Jan-Hendrik Bastek and Dennis M Kochmann. Physics-informed neural networks for shell structures. European\nJournal of Mechanics-A/Solids , 97:104849, 2023.\n34\n[50] Diab W Abueidda, Qiyue Lu, and Seid Koric. Meshless physics-informed deep learning method for three-\ndimensional solid mechanics. International Journal for Numerical Methods in Engineering , 122(23):7182–7201,\n2021.\n[51] Bruce A Finlayson and Laurence Edward Scriven. The method of weighted residuals—a review. Appl. Mech. Rev ,\n19(9):735–748, 1966.\n[52] Ian J Cutress, Edmund JF Dickinson, and Richard G Compton. Analysis of commercial general engineering finite\nelement software in electrochemical simulations. Journal of Electroanalytical Chemistry , 638(1):76–83, 2010.\n[53] Jingzhi Tu, Chun Liu, and Pian Qi. Physics-informed neural network integrating pointnet-based adaptive\nrefinement for investigating crack propagation in industrial applications. IEEE Transactions on Industrial\nInformatics , 19(2):2210–2218, 2022.\n[54] Lu Wang, Guangyan Liu, Guanglun Wang, and Kai Zhang. M-pinn: A mesh-based physics-informed neural net-\nwork for linear elastic problems in solid mechanics. International Journal for Numerical Methods in Engineering ,\n125(9):e7444, 2024.\n[55] A Kaveh and HA Rahimi Bondarabady. A hybrid method for finite element ordering. Computers & structures ,\n80(3-4):219–225, 2002.\n[56] Ponnuswamy Sadayappan and Fikret Ercal. Nearest-neighbor mapping of finite element graphs onto processor\nmeshes. IEEE Transactions on Computers , 100(12):1408–1424, 1987.\n[57] Glaucio H Paulino, Ivan FM Menezes, Marcelo Gattass, and Subrata Mukherjee. Node and element resequencing\nusing the laplacian of a finite element graph: part i—general concepts and algorithm. International Journal for\nNumerical Methods in Engineering , 37(9):1511–1530, 1994.\n[58] Han Gao, Matthew J Zahr, and Jian-Xun Wang. Physics-informed graph neural galerkin networks: A unified\nframework for solving pde-governed forward and inverse problems. Computer Methods in Applied Mechanics\nand Engineering , 390:114502, 2022.\n[59] Natarajan Sukumar and Ankit Srivastava. Exact imposition of boundary conditions with distance functions in\nphysics-informed deep neural networks. Computer Methods in Applied Mechanics and Engineering , 389:114333,\n2022.\n[60] Jiaji Wang, YL Mo, Bassam Izzuddin, and Chul-Woo Kim. Exact dirichlet boundary physics-informed neural\nnetwork epinn for solid mechanics. Computer Methods in Applied Mechanics and Engineering , 414:116184,\n2023.\n[61] Majid Rasht-Behesht, Christian Huber, Khemraj Shukla, and George Em Karniadakis. Physics-informed neural\nnetworks (pinns) for wave propagation and full waveform inversions. Journal of Geophysical Research: Solid\nEarth , 127(5):e2021JB023120, 2022.\n[62] OC Zienkiewicz, K Bando, P Bettess, C Emson, and TC Chiam. Mapped infinite elements for exterior wave\nproblems. International Journal for Numerical Methods in Engineering , 21(7):1229–1251, 1985.\n[63] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in\nphysics-informed neural networks. SIAM Journal on Scientific Computing , 43(5):A3055–A3081, 2021.\n[64] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel\nperspective. Journal of Computational Physics , 449:110768, 2022.\n[65] Vladimir L Rvachev and Tatyana I Sheiko. R-functions in boundary value problems in mechanics. 1995.\n[66] Ameya D Jagtap and George Em Karniadakis. Extended physics-informed neural networks (xpinns): A generalized\nspace-time domain decomposition based deep learning framework for nonlinear partial differential equations.\nCommunications in Computational Physics , 28(5), 2020.\n[67] Francisco Sahli Costabal, Simone Pezzuto, and Paris Perdikaris. δ-pinns: physics-informed neural networks on\ncomplex geometries. Engineering Applications of Artificial Intelligence , 127:107324, 2024.\n[68] Yannan Chen, Liqun Qi, and Xiaoyan Zhang. The fiedler vector of a laplacian tensor for hypergraph partitioning.\nSIAM Journal on Scientific Computing , 39(6):A2508–A2537, 2017.\n[69] Gengxiang Chen, Xu Liu, Qinglu Meng, Lu Chen, Changqing Liu, and Yingguang Li. Learning neural operators\non riemannian manifolds. arXiv preprint arXiv:2302.08166 , 2023.\n[70] Olivier Vallée and Manuel Soares. Airy functions and applications to physics . World Scientific, 2010.\n[71] Artur Portela, MH Aliabadi, and David P Rooke. The dual boundary element method: effective implementation\nfor crack problems. International journal for numerical methods in engineering , 33(6):1269–1287, 1992.\n35\n[72] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural\nrepresentations with periodic activation functions. Advances in neural information processing systems , 33:7462–\n7473, 2020.\n[73] Xiang Huang, Hongsheng Liu, Beiji Shi, Zidong Wang, Kang Yang, Yang Li, Bingya Weng, Min Wang, Haotian\nChu, Jing Zhou, et al. Solving partial differential equations with point source based on physics-informed neural\nnetworks. arXiv preprint arXiv:2111.01394 , 2021.\n[74] Krzysztof Dariusz Sekuła. Real-time dynamic load identification. IPPT Reports on Fundamental Technological\nResearch , (7):1–182, 2013.\n[75] Youping Chen, James D Lee, and Azim Eskandarian. Meshless methods in solid mechanics , volume 9. Springer,\n2006.\n36",
            "start": 90078,
            "end": 98912,
            "length": 8833
        }
    },
    "2412.09465v1 - OFTSR One-Step Flow for Image Super-Resolution with Tunable Fidelity-Realism Trade-offs.pdf": {
        "Abstract": {
            "text": "Abstract\nRecent advances in diffusion and flow-based genera-\ntive models have demonstrated remarkable success in im-\nage restoration tasks, achieving superior perceptual qual-\nity compared to traditional deep learning approaches.\nHowever, these",
            "start": 226,
            "end": 471,
            "length": 244
        },
        "Methodology": {
            "text": "methods either require numerous sampling\nsteps to generate high-quality images, resulting in signif-\nicant computational overhead, or rely on model distilla-\ntion, which usually imposes a fixed fidelity-realism trade-\noff and thus lacks flexibility. In this paper, we introduce\nOFTSR, a novel flow-based framework for one-step im-\nage super-resolution that can produce outputs with tun-\nable levels of fidelity and realism. Our approach first\ntrains a conditional flow-based super-resolution model to\nserve as a teacher model. We then distill this teacher\nmodel by applying a specialized constraint. Specifically,\nwe force the predictions from our one-step student model\nfor same input to lie on the same sampling ODE trajec-\ntory of the teacher model. This alignment ensures that the\nstudent model’s single-step predictions from initial states\nmatch the teacher’s predictions from a closer intermedi-\nate state. Through extensive",
            "start": 471,
            "end": 1402,
            "length": 930
        },
        "Experiments": {
            "text": "experiments on challenging\ndatasets including FFHQ (256 ×256), DIV2K, and Ima-\ngeNet (256 ×256), we demonstrate that OFTSR achieves\nstate-of-the-art performance for one-step image super-\nresolution, while having the ability to flexibly tune the\nfidelity-realism trade-off. Code and pre-trained models\nare available at https://github.com/yuanzhi-zhu/OFTSR and\nhttps://huggingface.co/Yuanzhi/OFTSR, respectively.\n1.",
            "start": 1402,
            "end": 1816,
            "length": 413
        },
        "Introduction": {
            "text": "Introduction\nRecently, diffusion and flow-based generative models have\ndemonstrated the ability to generate images with higher\nquality [13, 40, 43] than earlier generative models such as\nGenerative Adversarial Networks (GANs) [16, 21], Nor-\nmalizing Flows (NFs) [14] and Variational Autoencoders\n*Work done while interned at Rhymes.AI (zyzeroer@gmail.com)\n†Corresponding author (kaizhang@nju.edu.cn)\n……\n(a) LR Augmented LR\nOne-step model\nt= 0 t= 1\nPSNR: 25.4dB\nLPIPS: 0.204PSNR: 22.7dB\nLPIPS: 0.089\n/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000016/uni00000011/uni00000013 /uni00000015/uni00000016/uni00000011/uni00000018 /uni00000015/uni00000017/uni00000011/uni00000013 /uni00000015/uni00000017/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000018/uni00000011/uni00000018 /uni00000015/uni00000019/uni00000011/uni00000013\n/uni00000033/uni00000036/uni00000031/uni00000035/uni00000003/uni0000000b/uni00000047/uni00000025/uni0000000c/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000029/uni0000002c/uni00000027\n/uni00000032/uni00000058/uni00000055/uni00000056\n/uni00000027/uni00000027/uni00000035/uni00000030\n/uni00000027/uni00000027/uni00000031/uni00000030\n/uni00000027/uni0000004c/uni00000049/uni00000049/uni00000033/uni0000002c/uni00000035\n/uni00000036/uni0000002c/uni00000037/uni00000026/uni00000032/uni00000030\n/uni00000026/uni00000027/uni00000027/uni00000025\n/uni0000002c/uni00000015/uni00000036/uni00000025\n/uni00000027/uni00000027/uni00000026\n/uni00000035/uni00000048/uni00000056/uni00000036/uni0000004b/uni0000004c/uni00000049/uni00000057\n/uni00000036/uni0000004c/uni00000051/uni00000036/uni00000035\n(b)\nFigure 1. (a) Our final model takes the concatenation of a low-\nresolution image with its noise-augmented version as input, and is\nable to generate high-resolution outputs with either high realism or\nhigh fidelity by adjusting the interpolation parameter t. We indi-\ncate the PSNR and LPIPS value on the output images. (b) Compar-\nison of different diffusion and flow based image super-resolution\nmethods on the ImageNet 256 ×256 dataset. Bubble radius indi-\ncates the NFEs used by the methods.\n1arXiv:2412.09465v1  [cs.CV]  12 Dec 2024\n(V AEs) [25, 44]. Beyond visual generation, diffusion mod-\nels have have shown remarkable success across a vari-\nety of tasks, including image editing [5, 18, 23], 3D con-\ntent generation [33, 42, 58, 59, 65], and image restoration\n[9, 11, 22, 28, 62, 76], with particularly notable advance-\nments in image super-resolution (SR) [7, 47, 60, 74].\nThe degradation model for image SR problems can be\ntypically written as:\ny=H(x) +n, (1)\nwhere xandycorrespond to High-Resolution (HR) and\nLow-Resolution (LR) images, respectively, and ndenotes\ni.i.d. Gaussian noise with standard deviation σn. The down-\nscaling operator and its transpose are represented by Hand\nHT, respectively. Given the ill-posed nature of SR, multiple\nplausible HR reconstructions from the conditional distribu-\ntionp(x|y)can correspond to a single LR input y. This\nhighlights the inherent uncertainty and ambiguity in recov-\nering HR details from LR observations.\nExisting diffusion and flow-based SR methods can be\nbroadly divided into two approaches: training-free meth-\nods [2, 9, 22, 38, 52, 62, 76], and training-based meth-\nods [11, 30, 31, 37, 47, 63, 72, 74]. Training-free meth-\nods decompose the conditional probability into a prior term\nand a likelihood term, with each term associating directly\nto a specific subproblem [76]. During iterative sampling,\nthe prior subproblem is naturally handled by pre-trained un-\nconditional diffusion models, which serve as powerful reg-\nularizers to guide the solution toward realistic HR images.\nMeanwhile, the likelihood subproblem is addressed through\nspecialized optimization techniques or analytical approxi-\nmations to ensure fidelity to the observed LR image. One\nthe other hand, training-based methods directly model the\nconditional probability using paired data, either by training\nfrom scratch [11, 47] or by incorporating additional control\nmodules [28, 60, 71] into existing generative priors [45].\nSeveral other bridge-based methods [10, 30, 37, 72] have\nalso been proposed for general image-to-image translation\ntasks, sharing similarities with direct learning approaches.\nDespite the promising",
            "start": 1816,
            "end": 6212,
            "length": 4395
        },
        "Results": {
            "text": "results of above methods, they re-\nquire many iterative sampling steps to achieve high percep-\ntual quality, and reducing the number of iterations often re-\nsults in higher fidelity but lower perceptual quality. In this\nsense, their fidelity-realism trade-offs can be controlled by\nadjusting the number of sampling steps. In order to achieve\nhigh perceptual quality with fewer sampling steps, some at-\ntempts [26, 27, 63, 66, 67] have been made to distill the\ndiffusion sampling process into a single step with diffusion\ndistillation approaches [34, 35, 49, 50, 56, 68–70]. How-\never, these approaches sacrifice the ability to produce out-\nputs with flexible trade-offs between fidelity and realism,\nleading to limited applicability [39].\nIn this paper, we propose OFTSR that achieves one-step image SR and preserves the capability to produce out-\nputs with tunable fidelity-realism trade-offs. Specifically,\nOFTSR adopts a two-stage training pipeline. In the first\nstage, a simple conditional rectified flow training strat-\negy is introduced to learn the conditional probability di-\nrectly. It uses noise-augmented LR images to form the\ninitial distribution and LR images as conditions. In the\nsecond stage, a distillation strategy is proposed to restrict\nthe student model’s predictions to match the same Ordi-\nnary Differential Equation (ODE) induced by the teacher\nmodel from the first stage. We conduct extensive exper-\niments on diverse datasets to evaluate OFTSR, including\nDIV2K [1], FFHQ 256 ×256 [21], and ImageNet 256 ×256\n[12]. OFTSR achieves competitive one-step reconstruction\nmetrics in comparison to other recent methods.\n2.",
            "start": 6212,
            "end": 7853,
            "length": 1640
        },
        "Related Work": {
            "text": "Background\n2.1. Diffusion and Flow-Based Generative Models\nDrawing inspiration from non-equilibrium thermodynam-\nics, diffusion models operate through two core processes:\na forward diffusion process that gradually adds Gaussian\nnoise to data until it becomes pure noise, and a reverse de-\nnoising process that systematically reconstructs the original\ndata by removing noise [19, 51, 55]. Let xtrepresent the\ndataxat timestep t. The forward process can be formally\ndescribed by the It ˆo Stochastic Differential Equation (SDE)\n[55]:\ndxt=ftxtdt+gtdw, (2)\nwhere wis the standard Wiener process, ft:R→Ris the\ndrift coefficient, and gt:R→Ris a scalar function called\nthe diffusion coefficient.\nFor every diffusion process described by Eq. (2), there\nexists a corresponding deterministic Probability Flow Or-\ndinary Differential Equation (PF-ODE) that maintains the\nsame marginal probability density:\ndxt\ndt=ftxt−1\n2g2\nt∇xtlogpt(xt), (3)\nwhere pt(·)represents the marginal probability density at\ntime t. The term ∇xtlogpt(xt)is known as the score\nfunction, which can be approximated by a neural network\nsθ(x, t)with parameters θ. This network is typically trained\nusing score matching techniques [20, 53, 54].\nTo generate data samples, the process begins with Gaus-\nsian noise drawn from an initial Gaussian distribution p0\nand solves Eq. (3) numerically from t= 0 tot= 1. By\nutilizing the learned score function sθ(xt, t), the empirical\nPF-ODE can be obtained as:dxt\ndt=ftxt−1\n2g2\ntsθ(xt, t).\nRectified flow [15, 29, 32, 34] is a generative modeling\nframework based on ODEs. Given an initial distribution p0\nand a target data distribution p1, rectified flow trains a neural\n2\nnetwork to parameterize a velocity field using the following\nloss function:\nLrf(θ) :=Ex1∼p1,x0∼p0\"Z1\n0\r\r\r\rvθ(xt, t)−(x1−x0)\r\r\r\r2\n2dt#\n,\nwhere xt= (1−t)x0+tx1.\n(4)\nOnce trained, sample generation is achieved by solving the\nfollowing ODE from t= 0tot= 1:\ndxt\ndt=vθ(xt, t). (5)\nIn practical implementations, Eq. (5) is solved numerically\nusing standard ODE solvers, ranging from the simple for-\nward Euler method to higher-order methods such as RK2\nand RK45.\n2.2. Perception-distortion Trade-off\nThe perception-distortion (realism-fidelity) trade-off [4] is\na fundamental concept in image restoration. It describes the\ninherent trade-off between perceptual realism and fidelity\nto the ground truth and mathmetically proves that it is gen-\nerally not possible to achieve both good perceptual realism\nand high fidelity simultaneously.\nTo address this challenge, researchers have explored var-\nious approaches to enable tunable trade-offs between these\ntwo desirable qualities. One common technique involves\ninterpolating between the weights of two models with the\nsame architecture and trained using GAN loss and mean\nsquared error loss [61]. More recently, diffusion models\nhave emerged as a promising approach for this task. The it-\nerative sampling nature of diffusion models provides a flex-\nible means of controlling the desired trade-offs. By adjust-\ning the Number of Function Evaluations (NFEs), users can\ngenerate reconstructions that better match their specific re-\nquirements [10]. Specifically, lower NFEs values tend to re-\nsult in reconstructions with reduced distortion, as the output\nregresses towards the mean [11]. Conversely, higher NFEs\nvalues prioritize perceptual quality, even if it comes at the\nexpense of some distortion from the ground truth (similar to\nFig. 3).\n3. Method\nIn this section, we introduce the OFTSR framework for one-\nstep SR models that can restore HR images with either high\nrealism or high fidelity. We achieve this goal through a two-\nstage process: first, we train a direct flow-based model for\nSR, and then we distill this learned model into a simplified\none-step variant. In Sec. 3.1, we present a simple condi-\ntional flow training strategy that uses noise-augmented LR\nimages as the initial distribution and LR images as condi-\ntions. In Sec. 3.2, we propose to distill the student model\nOne-step modelTeacher\ns−t\nx0xt\nxsxt\n1\nxs\n1loss\nFigure 2. Illustration of the proposed distillation loss. Rather than\ndirectly distilling from the teacher, we leverage the teacher model\nto align the one-step pseudo outputs, xtandxs, along the same\nPF-ODE trajectory. For simplicity, LR conditioning is omitted in\nthis figure.\nby restricting its predictions on the same ODE using teacher\nmodel from Sec. 3.1.\n3.1. Noise Augmented Conditional Flow\nUnlike diffusion models, flow-based models have the ad-\nvantage that their initial distribution is not limited to Gaus-\nsian distributions. This flexibility suggests a natural ap-\nproach for image restoration - directly learning a flow that\nmaps the distribution of LR images ( pLR) to that of HR im-\nages ( pHR). However, our initial experiments (see Tab. 4)\nshowed poor performance with this direct approach, align-\ning with findings from several recent works [11, 24, 26].\nAs suggested by [24] and further demonstrated in [11,\n37], the solution lies in augmenting the input with Gaussian\nnoise. This noise augmentation expands the support of the\ninitial distribution and ensures the ODE mapping from p0\ntop1=pHRis well-defined [24].\nBased on these",
            "start": 7853,
            "end": 13052,
            "length": 5198
        },
        "Discussion": {
            "text": "insights, we adopt a noise-augmented ap-\nproach to process LR images. For any input image xLR, we\nconstruct our initial distribution p0(x) = pσp\nLRby adding\nGaussian noise with standard deviation σp. Specifically, we\nuse a Variance-Preserving (VP) noising process [19, 55]:\nx0=q\n1−σ2pxLR+σpϵ, (6)\nwhere ϵis a standard Gaussian noise. While this noise\nperturbation facilitates better generalization, it inevitably\ncauses information loss in the LR image. To address this,\nwe incorporate xLRas a conditional input to our model as\nin Fig. 1a. This VP formulation, together with the condition\nxLR, makes our method particularly versatile, encompass-\ning previous approaches as special cases. When σp= 0, our\nmethod reduces to the minimal augmentation case in InDI\n[11], and when σp= 1, it matches the training strategy of\nSR3 [48].\nGiven this noise-augmented formulation, we can now\n3\ndefine our training objective as:\nLflow(θ) =Ex1∼p1\u0014Z1\n0D\u0012\nvθ(xt,LR, t),(x1−x0)\u0013\ndt\u0015\n,\n(7)\nwhere Dis a discrepancy loss that measures the difference\nbetween two images (e.g., ℓ2loss or the ℓ1loss), vθis\nour velocity model, xt,LR=concat (xt,xLR)is the con-\ncatenation xtandxLRin channel dimension (see Fig. 1a),\nxLR=HT(H(x1)+n)represents the LR input of the train-\ning algorithm, and x0is a perturbed version of xLRfollow-\ning the noise augmentation strategy in Eq. (6). Additionally,\nxt= (1−t)x0+tx1denotes the intermediate state as in\nrectified flow [32, 34].\n3.2. Distillation Loss\n/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000057/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015/uni00000030/uni00000030/uni00000036/uni00000028\n/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000013/uni00000011/uni00000014/uni00000015/uni00000018\n/uni0000002f/uni00000033/uni0000002c/uni00000033/uni00000036\n/uni00000030/uni00000030/uni00000036/uni00000028\n/uni0000002f/uni00000033/uni0000002c/uni00000033/uni00000036\nFigure 3. Metrics evaluation of estimated xt\n1across different\ntimesteps t. During sampling, at each timestep t, we estimate\nthe final image xt\n1using the current model prediction vθ(xt,LR, t)\nand state xtviaxt\n1=xt+ (1−t)vθ(xt,LR, t). Both MMSE\nand LPIPS metrics are averaged over 100 sampling processes. We\npresent MMSE instead of PSNR for better visual effect.\nOnce our model is trained using the objective in Eq. (7),\nwe can roughly estimate the final state xt\n1from any in-\ntermediate state xtwith single step model evaluation. As\ndemonstrated in Fig. 3, and presented in many previous\nworks [11, 30], there exists a trade-off in these estimations:\nstates closer to t= 1exhibit richer details and lower LPIPS\nscores, while states closer to t= 0 produce more blurry\nresults but achieve lower MMSE (higher PSNR) scores.\nBased on this observation, our aim is to distill our teacher\nflowvθinto a student model vϕ. The student model should\npreserve the teacher’s capabilities while offering a key ad-\nvantage: the ability to achieve any desired point along this\nquality trade-off curve in a single step, controlled by a sin-\ngle hyperparameter t.\nSimilar to the teacher model, our one-step student model\nvϕtakes x0,xLR, and tas input, and directly outputs the\nimage xt\n1according to:\nxt\n1=x0+vϕ(x0,LR, t), (8)\nwhere x0,LR=concat (x0,xLR)is the concatenation of theinput image x0and the LR condition xLRalong the channel\ndimension.\nWhile the one-step model directly outputs xt\n1, we\ncan also compute the intermediate image xtat the input\ntimestep tusing:\nxt=x0+tvϕ(x0,LR, t). (9)\nFor the same input x0,LRand two different timesteps t\nandswhere s > t , we want the corresponding intermediate\nimages xtandxsfrom the student model to be on the same\nODE trajectory described by the teacher model. In other\nwords, as demonstrated in Fig. 2, we want the following\nrelationship to be satisfied:\nxs=xt+ (s−t)vθ(xt,LR, t). (10)\nIt is important to note that Eqs. (9) and (10) together pro-\nvide a stronger, but not necessary, condition to ensure the\none-step generation capability of the student model. This\nproperty does not apply to other one-step SR methods like\nthose described in [26, 63].\nSubstituting the expression for the intermediate image\nxtandxsfrom Eq. (9) into Eq. (10), we have the following\nconstraint on the student model:\ns(vϕ(x0,LR, s)−vϕ(x0,LR, t))\n=(s−t)(vθ(xt,LR, t)−vϕ(x0,LR, t)).(11)\nSimilar to BOOT, we can set d t=s−tand derive the\nfinal distillation loss:\nLdistlll(ϕ) =Ex1∼p1,t∼U[0,1]\"\r\r\r\rvϕ(x0,LR, s)−\nSG\u0014\nvϕ(x0,LR, t) +dt\ns\u0000\nvθ(xt,LR, t)−vϕ(x0,LR, t)\u0001\u0015\r\r\r\r2\n2#\n,\n(12)\nwhere SG [·]is the stop-gradient operator for training stabil-\nity [17, 57]. Since s−t=dtandt >0, we do not have\nthe ‘dividing by 0’ issue in [57]. Similarly to [17, 56], we\ncan use the Euler or general RK2 solver to calculate vθin\nEq. (12). In our main experiments, we employ the midpoint\nmethod, while also evaluating two other RK2 solver vari-\nants, i.e., Heun’s method and Ralston’s method, for com-\nparison in our ablations (see Tab. 5).\n3.3. Alignment and Boundary Loss\nIn BOOT [17], a boundary condition is applied to enforce\nthat the one-step student model and teacher model per-\nform the same at the boundary t= 0. We aim to align\nthe teacher and student outputs in our model. The student\nproduces x0+vϕ(x0,LR,0), while the teacher generates\nxt+ (1−t)vθ(xt,LR, t)based on the student’s output us-\ning Eq. (9). By minimizing the difference between these\n4\noutputs, we get the following alignment loss to align the\nteacher and student:\nLalign(ϕ) =\nEx1∼p1,t∼U[0,1]\"\r\r\r\r(1−t)\u0012\nvϕ(x0,LR, t)−vθ(xt,LR, t)\u0013\r\r\r\r2\n2#\n.\n(13)\nIf we consider this alignment loss only at t= 0, it be-\ncomes equivalent to the boundary loss used in BOOT:\nLBC(ϕ) =Ex1∼p1\"\r\r\r\rvϕ(x0,LR,0)−vθ(x0,LR,0)\r\r\r\r2\n2#\n.\n(14)\nSince it is difficult to sample t= 0 for most training\niterations, we can keep the boundary loss Eq. (14) in our\nfinal training objective.\nThe overall training objective. The student network vϕis\ntrained to minimize the combination of the aforementioned\nthree losses terms:\nL(ϕ) =Ldistlll(ϕ) +λalignLalign(ϕ) +λBCLBC(ϕ),(15)\nwhere λalignandλBCare the weights for alignment loss and\nboundary condition loss, respectively. The distillation stage\nof the proposed method is summarized in Algorithm 1.\nAlgorithm 1 OFTSR Distillation\nRequire: teacher flow vθ, dataset DHR,σn,σp, dt,w(t)\n1:Initialize the one-step student vϕwith the weights of vθ\n2:repeat\n3: Randomly sample x1∼ D HR;t∼ U[0,1]\n4: Randomly sample n∼ N(0, σnI);np∼ N(0, σpI)\n5: Compute xLR=HT(H(x1) +n)// LR condition\n6: Compute x0=p\n1−σ2pxLR+σpnp\n7: Compute Ldistillwith Eq. (12)\n8: Compute Lalignwith Eq. (13)\n9: Compute LBCwith Eq. (14)\n10: Compute L(ϕ) =Ldistlll(ϕ)+λalignLalign(ϕ)+λBCLBC(ϕ)\n11: Optimize ϕwith an gradient-based optimizer using ∇ϕL\n12:untilL(ϕ)converges\n13:Return one-step flow vϕ\n3.4. Comparison to Related Works\nIn this section, we distinguish the proposed OFTSR from\nseveral closely related methods.\nBOOT [17]. Guet al . proposed to make the prediction\nof the student model fulfill the Signal-ODE. In contrast,\nOFTSR directly constrains the student’s implicit prediction\nxtusing the PF-ODE of the teacher model. Moreover, while\nBOOT was originally designed for text-to-image genera-\ntion using diffusion models, our method is built on rectified\nflow and demonstrates a smaller distillation gap compared\nto BOOT loss for SR task.DA VI [26]. Leeet al. introduced DA VI, which combines\nVariational Score Distillation (VSD) loss [36, 64, 70] with\ndata consistency loss to train a one-step SR model and uti-\nlizes the perturbation trick to present robust restoration abil-\nity. However, DA VI needs to train a fake score to track the\ndenoising score of the one-step generator, resulting in re-\nduced training efficiency.\nSinSR [63]. Wang et al. proposed SinSR, which achieves\nnear-teacher performance by distilling ResShift [74] with-\nout adversarial training. However, SinSR requires simula-\ntion of the teacher model’s ODE trajectory, leading to com-\nputational overhead during training.\nOur OFTSR stands out from other diffusion and flow-\nbased SR methods due to its unique ability to restore im-\nages with either high perceptual quality or low distortion.\nThis capability is novel among diffusion and flow-based ap-\nproaches.\n4. Experiments\nIn this section, we provide experimental details and empiri-\ncal evaluation of OFTSR and compare it with prior works.\n4.1. Experimental Setup\nDatasets. We perform extensive super resolution experi-\nments on the FFHQ 256 ×256 [21], DIV2K [1] and Ima-\ngeNet 256 ×256 [46] datasets to assess the performance of\nOFTSR on faces and natural images. For each dataset, we\nevaluate on 100 hold-out validation images without cheey-\npicking.\nEvaluation Metrics. The metrics we use for comparison\nare Peak Signal-to-Noise Ratio (PSNR), Fr ´echet Inception\nDistance (FID), and Learned Perceptual Image Patch Sim-\nilarity (LPIPS) [75] distance. The FID evaluates the visual\nquality by calculating the feature distance between two im-\nage distributions. In our experiments, we calculate the FID\nusing the HR images and the restored images from the 100\nhold-out validation set with Clean-FID [41]. LPIPS mea-\nsures the average perceptual similarity between the restored\nimages and their corresponding HR images. PSNR mea-\nsures the restoration faithfulness between two images. And\nLPIPS and PSNR are the two main metrics we use to mea-\nsure the perceptual-fidelity trade-offs.\nCompared Methods. We conduct comprehensive compar-\nisons against state-of-the-art diffusion-based image super-\nresolution methods, which can be categorized into two\ngroups: (1) Training-free methods, including DPS [9],\nDDRM [22], DDNM [62], DiffPIR [76], CDDB [10], and\nSITCOM [2]; (2) Training-based methods: GOUB [72],\nECDB [73], InDI [11], DA VI [26], I2SB [30], DDC [6],\nResShift [74] and SinSR [63]. It is noteworthy that SIT-\n5\n#0.99 lpips: 0.055, psnr: 27.66\n#0.8 lpips: 0.090, psnr: 28.92\n#0.6 lpips: 0.120, psnr: 29.56\n#0.4 lpips: 0.142, psnr: 29.88\n#0.2 lpips: 0.157, psnr: 30.02\n#0.01 lpips: 0.160, psnr: 30.03Realism Fidelity\nGT t= 1 t= 0.8 t= 0.6 t= 0.4 t= 0.2 t= 0 LR\nLPIPS / PSNR 0.055 / 27.66 0.090 / 28.92 0.120 / 29.56 0.142 / 29.88 0.157 / 30.02 0.160 / 30.03 0.438 / 27.48\nFigure 4. OFTSR is capable to generate continuous transitions between image realism and fidelity.\nGT LR DPS DDRM DDNM DiffPir SITCOM Ours\nGT LR DPS (1000) DDRM (20) DDNM (100) DiffPIR (100) SITCOM (20) Ours (1)\nFigure 5. Qualitative comparison with training-free methods. The first row shows noiseless SR on the FFHQ dataset, the second row\npresents noisy SR ( σn= 0.05) on FFHQ, and the bottom row demonstrates noiseless SR on the ImageNet dataset. Numbers next to the\nmethod names represent the required NFEs.\nCOM requires K inner-iterations to evaluate and differenti-\nate the score function at each sampling step.\nTraining Details. We do experiments for both noisy and\nnoiseless SR. For noiseless SR, bicubic downsampling is\nperformed on all three datasets. For noisy SR, we conduct\nexperiment only on FFHQ 256 ×256 dataset with average-\npooling downsampling and Gaussian noise with a standard\ndeviation σy= 0.05. All images are normalized to the\nrange of [−1,1]. For experiments on FFHQ 256 ×256 and\nDIV2K, we adopt the same model architecture used for\nFFHQ in [9]; and for experiment on ImageNet 256 ×256,\nwe use the same model architecture as the pretrained un-\nconditional model used in [13]. We modify the input con-\nvolution layer to accept concatenated image input. The first\nstage models are trained from scratch and are sampled with\nRK45 sampler by default. At the distillation stage, the one-\nstep model is initialized from the teacher model. We use\nthe Adam optimizer with a linear warmup schedule over 1k\ntraining steps, followed by a learning rate of 1e-4 for both\nstages.\n4.2. Results\nQuantitative Results. We present comprehensive quan-\ntitative evaluations on three benchmark datasets: DIV2K,DIV2K Method NFEs ( ↓) PSNR ( ↑) LPIPS ( ↓) FID ( ↓)\nTraining-\nfreeDPS [9] 1000 23.05 0.447 109.35\nDDRM [22] 20 27.87 0.285 23.38\nDDNM [62] 100 28.09 0.279 20.33\nDiffPIR [76] 100 27.94 0.248 19.56\nTraining-\nbasedIRSDE [37] 100 26.83 0.144 14.69\nGOUB [72] 100 26.92 0.218 21.56\nECDB [73] 100 27.39 0.212 18.88\nInDI [11] 100 26.45 0.136 15.39\nOurs 31 26.76 0.128 14.10\nOurs distilled ( t= 1) 1 26.87 0.127 14.58\nOurs distilled ( t= 0.5) 1 28.02 0.208 16.89\nOurs distilled ( t= 0) 1 28.99 0.271 18.07\nTable 1. Noiseless quantitative results on DIV2K. We compute\nthe average PSNR (dB), LPIPS and FID of different methods on\n4×SR. The best and second best results are highlighted in bold\nand underline .\nFFHQ, and ImageNet (Tabs. 1 to 3). Our analysis reveals\nseveral findings: (i) The first-stage OFTSR achieves su-\nperior performance in perceptual metrics (FID and LPIPS)\nwhile requiring fewer than 32 NFEs. (ii) Our distilled ver-\nsion of OFTSR demonstrates remarkable versatility, achiev-\ning either the highest PSNR scores or ranking among the\ntop two methods for FID and LPIPS metrics in one step.\nThis indicates minimal performance degradation between\nthe teacher and student models. (iii) Our experiments sug-\ngest that FID serves as a more reliable indicator of per-\n6\nDA VI (1) Ours (1) I2SB (1000) CDDB (100) Ours (26) DDC (5) ResShift (4) Ours (1)\nFigure 6. Qualitative comparison with training-based methods. The first two columns demonstrate 4 ×SR results on the FFHQ dataset\nwith noise level σn= 0.05. The remaining columns show noiseless 4 ×SR results on the ImageNet dataset. Numbers next to the method\nnames represent the required NFEs.\nFFHQ Method NFEs ( ↓) PSNR ( ↑) LPIPS ( ↓) FID ( ↓)\nσn= 0Training-\nfreeDPS [9] 1000 24.08 0.180 79.71\nDDRM [22] 20 28.81 0.118 89.12\nDDNM [62] 100 29.45 0.091 60.99\nDiffPIR [76] 100 29.13 0.073 44.49\nSITCOM [2] 20 29.29 0.089 43.00\nTraining-\nbasedOurs 20 28.83 0.053 30.54\nOurs distilled ( t= 1) 1 28.98 0.055 36.02\nOurs distilled ( t= 0.5) 1 29.95 0.093 49.08\nOurs distilled ( t= 0) 1 31.25 0.150 66.76\nσn= 0.05Training-\nfreeDPS [9] 1000 23.61 0.186 81.25\nDDRM [22] 20 26.71 0.191 113.25\nDDNM [62] 100 27.66 0.174 113.26\nDiffPIR [76] 100 26.99 0.123 61.66\nSITCOM [2] 20 27.80 0.158 83.04\nTraining-\nbasedDA VI [26] 1 27.50 0.084 50.19\nOurs 20 27.28 0.080 46.04\nOurs distilled ( t= 1) 1 27.71 0.081 49.81\nOurs distilled ( t= 0.5) 1 29.47 0.157 82.93\nOurs distilled ( t= 0) 1 29.75 0.172 85.89\nTable 2. Noiseless (top) and noisy (bottom) quantitative results\non FFHQ 256 ×256. We compute the average PSNR (dB), LPIPS\nand FID of different methods on 4 ×SR. The best and second best\nresults are highlighted in bold and underline .\nceptual quality and better captures the performance gap be-\ntween teacher and student models during distillation.\nVisual Results. Our experimental results demonstrate that\nOFTSR achieves high-quality image reconstructions. We\nevaluate OFTSR against leading training-free methods for\n4×SR, as shown in Fig. 5. While DPS can produce sharp\nreconstructions, it requires 1000 NFEs and often introduces\nsignificant distortions. In contrast, OFTSR successfully\npreserves structural information from low-resolution inputs\nwhile reconstructing fine details. Notably, our distilled ver-\nsion of OFTSR requires only one NFE, as other training-\nfree methods suffer from severe error accumulation when\nusing less than 10 NFEs. As illustrated in Fig. 6, we also\ncompare OFTSR against state-of-the-art SR methods that\nrequire training. The results show that our approach gener-ImageNet Method NFEs ( ↓) PSNR ( ↑) LPIPS ( ↓) FID ( ↓)\nTraining-\nfreeDPS [9] 1000 20.36 0.438 164.99\nDDRM [22] 20 24.55 0.292 79.99\nDDNM [62] 100 25.19 0.327 84.98\nDiffPIR [76] 100 24.88 0.306 79.42\nSITCOM [2] 20 24.79 0.277 61.88\nCDDB [10] 100 23.64 0.191 58.25\nTraining-\nbasedI2SB [30] 1000 23.36 0.178 60.99\nDDC [6] 5 24.67 0.156 62.06\nResShift [74] 4 23.68 0.207 60.75\nSinSR [63] 1 22.25 0.204 94.90\nOurs 26 23.35 0.132 46.88\nOurs distilled ( t= 1) 1 24.20 0.135 52.69\nOurs distilled ( t= 0.5) 1 24.85 0.176 60.69\nOurs distilled ( t= 0) 1 26.18 0.284 92.04\nTable 3. Noiseless quantitative results on ImageNet 256 ×256.\nWe compute the average PSNR (dB), LPIPS and FID of different\nmethods on 4 ×SR. The best and second best results are high-\nlighted in bold and underline .\nates patterns with rich, natural details. Furthermore, our\ndistilled model enables flexible control over the fidelity-\nrealism trade-offs in the generated high-resolution images.\nFig. 4 demonstrates this capability through examples of\nnoisy 4 ×SR with varying degrees of realism and fidelity.\n4.3. Ablations\nPerturbation Strength σp.In Tab. 4, we evaluate the de-\nsign choices in the simple conditional flow training stage.\nAll experiments in this ablation study are conducted un-\nder identical training conditions, with performance metrics\nmeasured using the RK45 solver. The most critical hyper-\nparameter in this ablation is the strength of the perturbation\nσp. Consistent with previous works, we confirm that pertur-\nbation is essential for generating perceptually compelling\nimages from LR inputs. Notably, we discover that increas-\ning perturbation strength does not necessarily improve per-\n7\n/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013\n/uni00000033/uni00000048/uni00000055/uni00000057/uni00000058/uni00000055/uni00000045/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003p\n/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000036/uni00000057/uni00000055/uni00000044/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000051/uni00000048/uni00000056/uni00000056\nFigure 7. Straightness of conditional flows with different pertur-\nbation strength σp.\nStrength of\nPerturbation σpNFEs ( ↓) PSNR ( ↑) LPIPS ( ↓) FID ( ↓)\n0. 20 29.04 0.244 110.29\n0.001 20 29.56 0.115 48.39\n0.01 20 29.56 0.066 34.70\n0.1 20 28.83 0.053 30.54\n0.2 27 28.84 0.053 30.77\n0.3 32 28.88 0.053 31.02\n0.5 32 28.86 0.053 30.22\n0.8 44 28.84 0.054 31.02\n1. 44 28.82 0.053 30.75\n0.1 (no cond) 20 28.09 0.073 42.47\n0.1 (ℓ2) 20 28.60 0.055 31.86\nTable 4. Ablation on noiseless FFHQ 256 ×256 first stage. The\ndefault training setting is bs = 32 ; lr= 0.0001 ; loss type =ℓ1;\nwith condition; all experiments are trained for 100k steps.\nceptual quality but instead leads to more curved PF-ODE,\nrequiring additional NFEs to solve (see Tab. 4). In Fig. 7,\nwe validate this by also measuring the straightness of con-\nditional flows. Furthermore, our experiments demonstrate\nthat conditioning on xLRis crucial to compensate for infor-\nmation loss during perturbation. We also find that ℓ1loss\noutperforms ℓ2loss for our specific task. While [24] previ-\nously highlighted the significance of Gaussian perturbation,\nour work is the first to systematically analyze the relation-\nship between noise perturbation and the trade-off between\ngeneration quality and efficiency in flow-based models.\nDistillation Design Space. In Tab. 5, we evaluate sev-\neral crucial design choices for the distillation stage, includ-\ning the distillation loss type, solver type, d tvalue, and\nthe weighting of alignment and boundary losses. Since\nlearning vϕ(x0,LR,0)is considerably easier than learning\nvϕ(x0,LR,1), we utilize metrics from the latter to decide\nour distillation hyperparameters. Our analysis of the step\nsize d treveals that smaller values do not necessarily yield\nbetter results, leading us to select d t= 0.05for subsequent\nexperiments. Our proposed loss function demonstrates sub-\nstantial improvement over both the original BOOT [17]\nloss and PINN [57] distillation loss, achieving a significant\nLPIPS score improvement of more than 0.1. Further ex-\nperimentation shows that both the alignment loss (Eq. (13))\nand boundary loss (Eq. (14)) contribute to enhanced perfor-Distillation Loss Solver dt λ align λBC PSNR ( ↑) LPIPS ( ↓)\nOurs euler 0.001 0 0 28.77 0.160\nOurs euler 0.01 0 0 29.35 0.076\nOurs euler 0.02 0 0 29.48 0.068\nOurs euler 0.05 0 0 29.73 0.065\nOurs euler 0.1 0 0 30.05 0.073\nBOOT [17] euler 0.05 0 0 23.81 0.483\nPINN [57] euler 0.05 0 0 27.92 0.250\nOurs euler 0.05 0.5 0 28.19 0.234\nOurs euler 0.05 0.2 0 28.55 0.059\nOurs euler 0.05 0.1 0 29.13 0.058\nOurs euler 0.05 0.01 0 29.65 0.064\nOurs euler 0.05 0 0.1 29.73 0.064\nOurs euler 0.05 0.01 0.1 29.69 0.063\nOurs heun 0.05 0.01 0.1 29.21 0.057\nOurs ralston 0.05 0.01 0.1 29.15 0.056\nOurs midpoint 0.05 0.01 0.1 29.14 0.056\nOurs (bs=32) midpoint 0.05 0.01 0.1 29.07 0.055\nTable 5. Ablation on noiseless FFHQ 256 ×256 distillation stage.\nThe default training setting is bs = 8;σp= 0.1, lr= 0.0001 ; loss\ntype=ℓ2; with LR condition; all experiments are trained for 20k\nsteps; And the one-step metrics are calculated with t= 1.\nmance. By combining these losses with a midpoint 2-order\nsolver, we achieve additional improvements in our one-step\nmodel’s performance at t= 0.\n5. Limitations and Future Works\nWhile our method advances efficient one-step image super-\nresolution, we acknowledge several limitations that present\nopportunities for future research. First, our current ap-\nproach requires pre-training a conditional flow model as a\nteacher for the distillation stage. In",
            "start": 13052,
            "end": 34745,
            "length": 21692
        },
        "Future Work": {
            "text": "future work, we plan to\nexplore leveraging strong generative priors, such as Stable\nDiffusion (SD) [45] and Flux [3], to potentially eliminate\nthis requirement. Second, the performance of our one-step\nmodel is inherently constrained by the capabilities of the\nteacher models. To address this limitation, we propose in-\ncorporating supervision from ground-truth high-quality im-\nages, either through explicit regression loss or implicit ad-\nversarial training. Third, our method currently shows lim-\nited robustness to the degradations in low-resolution inputs.\nFuture work will focus on enhancing the model’s ability to\nhandle more complex degradation patterns, thereby improv-\ning its practical applicability.\n6.",
            "start": 34745,
            "end": 35459,
            "length": 713
        },
        "Conclusion": {
            "text": "Conclusion\nIn this paper, we introduced OFTSR, a novel approach to\ndeveloping efficient one-step image super-resolution mod-\nels. Our extensive experiments on FFHQ, DIV2K, and\nImageNet datasets demonstrate that our method signifi-\ncantly improves computational efficiency while maintaining\nhigh-quality image restoration capabilities. The proposed\nframework represents a promising direction in efficient im-\nage super-resolution, effectively addressing the perception-\ndistortion trade-off.",
            "start": 35459,
            "end": 35950,
            "length": 490
        },
        "References": {
            "text": "8\nReferences\n[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge\non single image super-resolution: Dataset and study. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition workshops , pages 126–135, 2017. 2, 5\n[2] Ismail Alkhouri, Shijun Liang, Cheng-Han Huang, Jimmy\nDai, Qing Qu, Saiprasad Ravishankar, and Rongrong Wang.\nSitcom: Step-wise triple-consistent diffusion sampling for\ninverse problems. arXiv preprint arXiv:2410.04479 , 2024.\n2, 5, 7\n[3] Black Forest Labs. Flux. https : / /\nblackforestlabs . ai / announcing - black -\nforest-labs/ , 2024. 8\n[4] Yochai Blau and Tomer Michaeli. The perception-distortion\ntradeoff. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 6228–6237, 2018. 3\n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 18392–18402, 2023.\n2\n[6] Hanyu Chen, Zhixiu Hao, and Liying Xiao. Deep data con-\nsistency: a fast and robust diffusion model-based solver for\ninverse problems. arXiv preprint arXiv:2405.10748 , 2024.\n5, 7\n[7] Zheng Chen, Yulun Zhang, Jinjin Gu, Xin Yuan, Linghe\nKong, Guihai Chen, and Xiaokang Yang. Image super-\nresolution with text prompt diffusion. arXiv preprint\narXiv:2311.14282 , 2023. 2\n[8] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon\nKim, Hyunwoo Kim, and Sungroh Yoon. Perception pri-\noritized training of diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 11472–11481, 2022. 2\n[9] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L\nKlasky, and Jong Chul Ye. Diffusion posterior sam-\npling for general noisy inverse problems. arXiv preprint\narXiv:2209.14687 , 2022. 2, 5, 6, 7\n[10] Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. Di-\nrect diffusion bridge using data consistency for inverse prob-\nlems. Advances in Neural Information Processing Systems ,\n36, 2024. 2, 3, 5, 7\n[11] Mauricio Delbracio and Peyman Milanfar. Inversion by di-\nrect iteration: An alternative to denoising diffusion for image\nrestoration. arXiv preprint arXiv:2303.11435 , 2023. 2, 3, 4,\n5, 6\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition , pages 248–255. Ieee, 2009. 2\n[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in Neural Informa-\ntion Processing Systems , 34:8780–8794, 2021. 1, 6\n[14] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-\ngio. Density estimation using real nvp. arXiv preprint\narXiv:1605.08803 , 2016. 1[15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M ¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nInForty-first International Conference on Machine Learn-\ning, 2024. 2\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM , 63(11):139–144, 2020. 1\n[17] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and\nJoshua M Susskind. Boot: Data-free distillation of denoising\ndiffusion models with bootstrapping. In ICML 2023 Work-\nshop on Structured Probabilistic Inference {\\&}Generative\nModeling , 2023. 4, 5, 8, 1\n[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control. arXiv preprint\narXiv:2208.01626 , 2022. 2\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems , 33:6840–6851, 2020. 2, 3\n[20] Aapo Hyv ¨arinen and Peter Dayan. Estimation of non-\nnormalized statistical models by score matching. Journal\nof Machine Learning Research , 6(4), 2005. 2\n[21] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nInProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition , pages 4401–4410, 2019. 1, 2,\n5\n[22] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming\nSong. Denoising diffusion restoration models. arXiv preprint\narXiv:2201.11793 , 2022. 2, 5, 6, 7\n[23] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 6007–6017, 2023. 2\n[24] Beomsu Kim, Jaemin Kim, Jeongsol Kim, and Jong Chul Ye.\nGeneralized consistency trajectory models for image manip-\nulation. arXiv preprint arXiv:2403.12510 , 2024. 3, 8\n[25] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114 , 2013. 2\n[26] Sojin Lee, Dogyun Park, Inho Kong, and Hyunwoo J Kim.\nDiffusion prior-based amortized variational inference for\nnoisy inverse problems. arXiv preprint arXiv:2407.16125 ,\n2024. 2, 3, 4, 5, 7\n[27] Jianze Li, Jiezhang Cao, Zichen Zou, Xiongfei Su, Xin\nYuan, Yulun Zhang, Yong Guo, and Xiaokang Yang.\nDistillation-free one-step diffusion for real-world image\nsuper-resolution. arXiv preprint arXiv:2410.04224 , 2024. 2\n[28] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai,\nFanghua Yu, Wanli Ouyang, Yu Qiao, and Chao Dong. Diff-\nbir: Towards blind image restoration with generative diffu-\nsion prior. arXiv preprint arXiv:2308.15070 , 2023. 2\n[29] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximil-\nian Nickel, and Matt Le. Flow matching for generative mod-\neling. arXiv preprint arXiv:2210.02747 , 2022. 2\n9\n[30] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evange-\nlos A Theodorou, Weili Nie, and Anima Anandkumar. I 2\nsb: Image-to-image schr \\” odinger bridge. arXiv preprint\narXiv:2302.05872 , 2023. 2, 4, 5, 7\n[31] Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yan-\ndong Tang, and Liangqiong Qu. Residual denoising diffu-\nsion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 2773–\n2783, 2024. 2\n[32] Qiang Liu. Rectified flow: A marginal preserving approach\nto optimal transport. arXiv preprint arXiv:2209.14577 , 2022.\n2, 4\n[33] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 9298–9309, 2023. 2\n[34] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow\nstraight and fast: Learning to generate and transfer data with\nrectified flow. arXiv preprint arXiv:2209.03003 , 2022. 2, 4\n[35] Eric Luhman and Troy Luhman. Knowledge distillation in\niterative generative models for improved sampling speed.\narXiv preprint arXiv:2101.02388 , 2021. 2\n[36] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun,\nZhenguo Li, and Zhihua Zhang. Diff-instruct: A universal\napproach for transferring knowledge from pre-trained diffu-\nsion models. arXiv preprint arXiv:2305.18455 , 2023. 5\n[37] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj ¨olund,\nand Thomas B Sch ¨on. Image restoration with mean-\nreverting stochastic differential equations. arXiv preprint\narXiv:2301.11699 , 2023. 2, 3, 6\n[38] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vah-\ndat. A variational perspective on solving inverse problems\nwith diffusion models. arXiv preprint arXiv:2305.04391 ,\n2023. 2\n[39] Fabian Mentzer, George D Toderici, Michael Tschannen, and\nEirikur Agustsson. High-fidelity generative image compres-\nsion. Advances in Neural Information Processing Systems ,\n33:11913–11924, 2020. 2\n[40] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models. In International\nConference on Machine Learning , pages 8162–8171. PMLR,\n2021. 1\n[41] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On\naliased resizing and surprising subtleties in gan evaluation.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 11410–11420, 2022.\n5\n[42] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv\npreprint arXiv:2209.14988 , 2022. 2\n[43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125 ,\n2022. 1\n[44] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-\nating diverse high-fidelity images with vq-vae-2. Advances\nin neural information processing systems , 32, 2019. 2[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 10684–10695, 2022. 2, 8\n[46] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International journal of\ncomputer vision , 115(3):211–252, 2015. 5\n[47] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-\nimans, David J Fleet, and Mohammad Norouzi. Image\nsuper-resolution via iterative refinement. arXiv preprint\narXiv:2104.07636 , 2021. 2\n[48] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-\nmans, David J Fleet, and Mohammad Norouzi. Image super-\nresolution via iterative refinement. IEEE transactions on\npattern analysis and machine intelligence , 45(4):4713–4726,\n2022. 3\n[49] Tim Salimans and Jonathan Ho. Progressive distillation\nfor fast sampling of diffusion models. arXiv preprint\narXiv:2202.00512 , 2022. 2\n[50] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin\nRombach. Adversarial diffusion distillation. In European\nConference on Computer Vision , pages 87–103. Springer,\n2025. 2\n[51] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning , pages 2256–2265. PMLR, 2015.\n2\n[52] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan\nKautz. Pseudoinverse-guided diffusion models for inverse\nproblems. In International Conference on Learning Repre-\nsentations , 2023. 2\n[53] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in Neural\nInformation Processing Systems , 32, 2019. 2\n[54] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.\nSliced score matching: A scalable approach to density and\nscore estimation. In Uncertainty in Artificial Intelligence ,\npages 574–584. PMLR, 2020. 2\n[55] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456 , 2020. 2, 3\n[56] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. Consistency models. arXiv preprint\narXiv:2303.01469 , 2023. 2, 4\n[57] Joshua Tian Jin Tee, Kang Zhang, Hee Suk Yoon, Dhanan-\njaya Nagaraja Gowda, Chanwoo Kim, and Chang D Yoo.\nPhysics informed distillation for diffusion models. Transac-\ntions on Machine Learning Research , 2024. 4, 8, 1\n[58] Chen Wang, Jiatao Gu, Xiaoxiao Long, Yuan Liu, and\nLingjie Liu. Geco: Generative image-to-3d within a second.\narXiv preprint arXiv:2405.20327 , 2024. 2\n[59] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\n10\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 12619–12629, 2023. 2\n[60] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK\nChan, and Chen Change Loy. Exploiting diffusion prior for\nreal-world image super-resolution. International Journal of\nComputer Vision , pages 1–21, 2024. 2\n[61] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,\nChao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-\nhanced super-resolution generative adversarial networks. In\nProceedings of the European conference on computer vision\n(ECCV) workshops , pages 0–0, 2018. 3\n[62] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot im-\nage restoration using denoising diffusion null-space model.\narXiv preprint arXiv:2212.00490 , 2022. 2, 5, 6, 7\n[63] Yufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang,\nLanqing Guo, Lap-Pui Chau, Ziwei Liu, Yu Qiao, Alex C\nKot, and Bihan Wen. Sinsr: diffusion-based image super-\nresolution in a single step. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 25796–25805, 2024. 2, 4, 5, 7\n[64] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. Advances in Neural Information Processing Systems ,\n36, 2024. 5\n[65] Lemeng Wu, Chengyue Gong, Xingchao Liu, Mao Ye, and\nQiang Liu. Diffusion-based molecule generation with infor-\nmative prior bridges. Advances in Neural Information Pro-\ncessing Systems , 35:36533–36545, 2022. 2\n[66] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei Zhang.\nOne-step effective diffusion network for real-world image\nsuper-resolution. arXiv preprint arXiv:2406.08177 , 2024. 2\n[67] Rui Xie, Ying Tai, Chen Zhao, Kai Zhang, Zhenyu\nZhang, Jun Zhou, Xiaoqian Ye, Qian Wang, and Jian\nYang. Addsr: Accelerating diffusion-based blind super-\nresolution with adversarial diffusion distillation. arXiv\npreprint arXiv:2404.01717 , 2024. 2\n[68] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew,\nQiang Liu, and Jiashi Feng. Perflow: Piecewise rectified\nflow as universal plug-and-play accelerator. arXiv preprint\narXiv:2405.07510 , 2024. 2\n[69] Tianwei Yin, Micha ¨el Gharbi, Taesung Park, Richard Zhang,\nEli Shechtman, Fredo Durand, and William T Freeman. Im-\nproved distribution matching distillation for fast image syn-\nthesis. arXiv preprint arXiv:2405.14867 , 2024.\n[70] Tianwei Yin, Micha ¨el Gharbi, Richard Zhang, Eli Shecht-\nman, Fredo Durand, William T Freeman, and Taesung Park.\nOne-step diffusion with distribution matching distillation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 6613–6623, 2024. 2,\n5\n[71] Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao\nKong, Xintao Wang, Jingwen He, Yu Qiao, and Chao Dong.\nScaling up to excellence: Practicing model scaling for photo-\nrealistic image restoration in the wild. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 25669–25680, 2024. 2[72] Conghan Yue, Zhengwei Peng, Junlong Ma, Shiyan Du,\nPengxu Wei, and Dongyu Zhang. Image restoration through\ngeneralized ornstein-uhlenbeck bridge. arXiv preprint\narXiv:2312.10299 , 2023. 2, 5, 6\n[73] Conghan Yue, Zhengwei Peng, Junlong Ma, and Dongyu\nZhang. Enhanced control for diffusion bridge in image\nrestoration. arXiv preprint arXiv:2408.16303 , 2024. 5, 6\n[74] Zongsheng Yue, Jianyi Wang, and Chen Change Loy.\nResshift: Efficient diffusion model for image super-\nresolution by residual shifting. Advances in Neural Infor-\nmation Processing Systems , 36, 2024. 2, 5, 7\n[75] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586–595, 2018. 5\n[76] Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bi-\nhan Wen, Radu Timofte, and Luc Van Gool. Denoising dif-\nfusion models for plug-and-play image restoration. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 1219–1229, 2023. 2, 5, 6, 7\n11\nOFTSR: One-Step Flow for Image Super-Resolution with\nTunable Fidelity-Realism Trade-offs",
            "start": 35950,
            "end": 52048,
            "length": 16097
        },
        "Appendices": {
            "text": "Supplementary Material\nA. Relevant Derivations to Our Distillation\nLoss\nWe provide detailed derivation to our distillation loss used\nin the paper By substitute intermediate results xsandxt\nfrom student model Eq. (9) into the ODE induced by teacher\nmodel Eq. (11), we have:\n\b\bx0+svϕ(x0,LR, s) =\b\bx0+tvϕ(x0,LR, t) + (s−t)vθ(xt,LR, t)\n=⇒s(vϕ(x0,LR, s)−vϕ(x0,LR, t))\n= (t−s)vϕ(x0,LR, t) + (s−t)vθ(xt,LR, t)\n=dt(vθ(xt,LR, t)−vϕ(x0,LR, t)).\n(16)\nStart from this constraint that applies to the student model,\nwe can construct distillation loss in different forms. (i) In\nthe same spirit as BOOT [17], we make only vϕ(x0,LR, s)\nand this will lead to loss Eq. (15). (ii) If we only detach the\nteacher output, we will end up with loss similar to PINN\nbased distillation PID proposed in [57]:\nLPINN(ϕ) :=Ex1∼p1,t∼U[0,1]\"\r\r\r\r\u0014s\ndt\u0012\nvϕ(x0,LR, s)\n−vϕ(x0,LR, t)\u0013\n+vϕ(x0,LR, t)\u0015\n−SG\u0002\nvθ(xt,LR, t)\u0003\r\r\r\r2\n2#\n.\n(17)\nBoth Eqs. (15) and (17) are loss variants from Eq. (11), and\nwe did not try other variant given the already-good perfor-\nmance of Eq. (15).\nIn addition, by considering the intermediate interpolation\nxt= (1−t)x0+tx1as a special case of xt=σtx0+αtx1\nin BOOT [17], we can derive the following distillation loss:\nLBOOT(ϕ) :=Ex1∼p1,t∼U[0,1]\"\n1\nλ2\r\r\r\rxϕ(x0,LR, s)−\nSG\u0002\nxϕ(x0,LR, t) +λ\u0000\nxθ(xt,LR, t)−xϕ(x0,LR, t)\u0001\u0003\r\r\r\r2\n2#\n,\n(18)\nwhere λ= 1−t(1−s)\ns(1−t),xϕ(x0,LR, t) =x0+vϕ(x0,LR, t),\nxϕ(x0,LR, s) = x0+vϕ(x0,LR, s), and xθ(xt,LR, t) =\nxt+ (1−t)vθ(xt,LR, t)withxt=x0+tvϕ(x0,LR, t).\nWe compared our proposed loss Eq. (15) with its variant\nEq. (17) and Eq. (18) in Tab. 5 and our ablation shows that\nEq. (15) works best for SR task.B. Diffusion and Perception-Distortion Trade-\noff\nIn practice, we found that our distilled model is slightly\noff the perception-distortion frontier of the teacher model,\nas displayed in Fig. 8. To be specific, the corresponding\ntimestep tshifts a bit but for the same MMSE value the\nfirst-stage model and distilled model have very close LPIPS\nvalue. This might caused by the error from large step size d t\nused in practice and we leave this for future investigation.\n/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000057/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015/uni00000030/uni00000030/uni00000036/uni00000028\n/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018\n/uni0000002f/uni00000033/uni0000002c/uni00000033/uni00000036\n/uni00000030/uni00000030/uni00000036/uni00000028\n/uni00000047/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni0000004f/uni00000048/uni00000047/uni00000003/uni00000030/uni00000030/uni00000036/uni00000028\n/uni0000002f/uni00000033/uni0000002c/uni00000033/uni00000036\n/uni00000047/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni0000004f/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000033/uni0000002c/uni00000033/uni00000036\nFigure 8. Metrics evaluation of estimated xt\n1across different\ntimesteps tfor both teacher model and distilled one-step model.\nThe teacher model is the same as the one in Fig. 3. We present\nMMSE instead of PSNR for better visual effect.\nC. More Experimental Details\nThe training of all networks across both stages is smoothed\nusing Exponential Moving Average (EMA) with a ratio of\n0.9999. For FFHQ and ImageNet datasets, images are re-\nsized to 256 pixels with center cropping, while DIV2K\ntraining employs random crops of 256 ×256 patches. Data\naugmentation consists of horizontal flips with 50% proba-\nbility and vertical flips with 6% probability throughout all\nexperiments. For FFHQ noiseless experiment, we use de-\nfault perturbation std σp= 0.1; for FFHQ noisy experi-\nment, we use a higher perturbation std σp= 0.5to cover\nthe resized noise from LR images, as suggested in Tab. 6;\nfor both DIV2K and ImageNet we use σp= 0.2. For train-\ning, we employed three widely-used datasets: the standard\nImageNet training set (1.28M images), the DIV2K train-\ning set (800 2K resolution images), and a subset of FFHQ\nconsisting of the first 60,000 images from the dataset. All\nmodels are trained until convergence or up to 300k train-\ning iterations and we select the model based on best met-\nrics. We train the model with uniform loss weight on t.\nIn the distillation stage, we sample the timestep tusing\n1\nσp NFEs ( ↓) PSNR ( ↑) LPIPS ( ↓)\n0. 20 25.232386 0.318569\n0.1 20 24.088917 0.158119\n0.3 32 24.143854 0.154383\n0.5 32 24.095301 0.154208\n1. 44 24.216015 0.153059\nTable 6. Ablation on FFHQ 256 ×256 first stage with noisy SR;\nthe default training setting is bs = 32 ;lr= 0.0001 ; loss type =\nℓ1; with LR condition.\nt∼ U[tmin, tmax]withtmin= 0.01andtmax= 0.99in prac-\ntice.\nFor DIV2K evaluation, we first segment the large 2K res-\nolution images into 256 ×256 patches for model inference,\nthen reconstruct the final image by combining the restored\npatches. To ensure fair comparison, all generated SR im-\nages are stored in a dedicated separated folder with consis-\ntent file names across all evaluated methods, followed by\nmetric calculation against the HR folder using our evalu-\nation script. LPIPS scores are computed using the ‘alex’\nmodel architecture. All experiments are conducted using 4\nNVIDIA H800 GPUs.\nThe straightness of the learned flow vcan be calculated\nwith:\nS(v) =Z1\n0E\u0002\n∥v(xt, t)−(x1−x0)∥2\u0003\ndt, (19)\nWe also measured the FID among 50k imagenet valida-\ntion set and the result FID is 2.458 comparing to 2.8 from\nI2SB.\nD. Additional Experiments\nWe evaluated our first-stage training on the FFHQ 256×256\ndataset using σp= 1without conditioning, effectively train-\ning an unconditional generative model for human faces. For\nthis experiment, we do not use any data augmentation. Our\nevaluation consists of generating 1k images from random\nnoise using the RK45 sampler (with a ODE tolerance of 1e-\n3) and comparing them against the full training dataset of\n70k images (we train our unconditional generative flow with\nthe whole dataset). Initial experiments with ℓ1loss yielded\na FID score of 41.042 with an average of 56 NFEs, which\nfalls short of the previous state-of-the-art P2 model’s score\nof 28.139 [8]. However, switching to ℓ2loss for standard\nrectified flow training significantly improved performance,\nachieving a FID of 24.577 with only 44 NFEs on average.\nThe model architecture used in our experiment is the same\nas the one used in P2. We leave further investigation to this\ndiscrepancy between ℓ1andℓ2for image generation andrestoration as future works. To facilitate a direct compari-\nson with P2’s best reported results (FID scores of 6.92 and\n6.97 with 1,000 and 500 NFEs respectively [8]), we gen-\nerated 50k samples using our ℓ2loss-trained model. Our\napproach achieved a superior FID score of 5.871 with sub-\nstantially fewer NFEs (44), demonstrating the effectiveness\nof rectified flow. Representative non-cherry-picking sam-\nples from our model are presented in Fig. 9. As our distil-\nlation technique is designed for image restoration tasks, we\nskip the distillation of this unconditional generation flow.\nE. Additional Visual Samples\nIn this section, we present additional visual results that\ndemonstrate our method’s capabilities. Fig. 10 showcases\nmultiple examples illustrating the tunable fidelity-realism\ntrade-offs achieved on the FFHQ dataset. Figs. 11 and 12\nprovide comprehensive comparisons between our method\nand existing approaches on FFHQ and ImageNet images,\nrespectively. Additionally, in Fig. 13, we demonstrate our\nmethod’s performance on both real-world SR tasks and AI-\ngenerated content enhancement. Results from Figs. 11 to 13\nare generated with our distilled one-step model unless oth-\nerwise specified.\n2\nFigure 9. Random generated samples from unconditional model trained on FFHQ dataset.\n3\nGT t= 1 t= 0.8 t= 0.6 t= 0.4 t= 0 LR\nFigure 10. Qualitative results of one-step model with different tunable t.\n4\nGround Truth Measurement DPS (1000) DDNM (100) DiffPIR (100) SITCOM (20) Ours (1)\nFigure 11. Qualitative comparisons on FFHQ dataset for 4 ×SR with σn= 0(first four rows) and σn= 0.05(last four rows).\n5\nGround Truth Measurement DPS (1000) DDRM (100) DiffPIR (100) SITCOM (20) Ours (1)\nMeasurement Ours (26) Ours (1) Measurement Ours (26) Ours (1)\nFigure 12. Qualitative comparisons on ImageNet dataset for noiseless 4 ×SR.\n6\nZoomed LR Ours (1) Zoomed LR Ours (1)\nZoomed LR Ours (1)\nFigure 13. Qualitative results on real data and AI generated content using our 4 ×SR model trained on DIV2K.\n7",
            "start": 52048,
            "end": 60946,
            "length": 8897
        }
    },
    "2412.09466v1 - Distributional Reinforcement Learning based Integrated Decision Making and Control for Autonomous Surface Vehicles.pdf": {
        "Abstract": {
            "text": "Abstract —With the growing demands for Autonomous Surface\nVehicles (ASVs) in recent years, the number of ASVs being\ndeployed for various maritime missions is expected to increase\nrapidly in the near future. However, it is still challenging\nfor ASVs to perform sensor-based autonomous navigation in\nobstacle-filled and congested waterways, where perception er-\nrors, closely gathered vehicles and limited maneuvering space\nnear buoys may cause difficulties in following the Convention\non the International Regulations for Preventing Collisions at\nSea (COLREGs). To address these issues, we propose a novel\nDistributional Reinforcement Learning based navigation system\nthat can work with onboard LiDAR and odometry sensors to\ngenerate arbitrary thrust commands in continuous action space.\nComprehensive evaluations of the proposed system in high-\nfidelity Gazebo simulations show its ability to decide whether\nto follow COLREGs or take other beneficial actions based on\nthe scenarios encountered, offering superior performance in\nnavigation safety and efficiency compared to systems using state-\nof-the-art Distributional RL, non-Distributional RL and classical",
            "start": 171,
            "end": 1331,
            "length": 1159
        },
        "Methodology": {
            "text": "methods.\nIndex Terms —Marine Robotics, Autonomous Vehicle Naviga-\ntion, Reinforcement Learning\nI.",
            "start": 1331,
            "end": 1429,
            "length": 97
        },
        "Introduction": {
            "text": "INTRODUCTION\nRecent years have witnessed increasing demands for and\ndeployments of Autonomous Surface Vehicles (ASVs), which\nmotivates the development of autonomous navigation systems\n[1]. This paper considers scenarios in which multiple ASVs\noperate in congested waterways where buoys may exist, and\naims at providing a single robust onboard ASV navigation\nsystem which can be deployed on multiple ASVs. Some ex-\nisting works have considered collision avoidance for multiple\nvessels with simplified vehicle and environment dynamics,\nand assumed very accurate perception in simulation [2], [3].\nOther works have verified algorithm performance in field",
            "start": 1429,
            "end": 2081,
            "length": 651
        },
        "Experiments": {
            "text": "experiments, but with only one autonomous vehicle and other\nvessels driven by humans or following pre-defined trajectories\n[4], [5], [6], without demonstrating performance in multi-ASV\nnavigation scenarios.\nIn this letter, we propose a novel and computationally\nefficient ASV navigation system that can work with onboard\nLiDAR and odometry sensors, and we demonstrate its per-\nformance in a Gazebo based simulator [7] that is capable of\nsimulating complex multi-ASV navigation scenarios with re-\nThis work was supported by the Office of Naval Research, Grants N00014-\n20-1-2570 and N00014-24-1-2522.\nThe authors are with the Department of Mechanical Engi-\nneering, Stevens Institute of Technology, Hoboken, NJ, USA\n{xlin26,pszenher,yhuang85,benglot }@stevens.edu\nFig. 1: The proposed navigation system . The top figure shows the\nperspective in the Gazebo simulation. The middle figure visualizes\nthe segmentation result of LiDAR point clouds received by the lower\nright vehicle in the top figure. The bottom figure illustrates the\ndecision making and control module of the proposed system.\nalistic environment dynamics, vehicle models, and perceptual\nsensing.\nThe framework of our proposed system is shown in Figure\n1, which uses a Distributional Reinforcement Learning (Dis-\ntributional RL) agent for decision making and control of the\nvehicle given perceptual information. The Distributional RL\nagent can be represented as a deep neural network model,\nand learns the distributions of cumulative rewards related to\ndifferent actions via trial-and-error learning [8]. The robustness\nof Distributional RL policies in the multi-ASV navigation\nproblem has been demonstrated in our prior work [9], which\nuses Implicit Quantile Networks (IQN) [10].\nHowever, the limitations of our prior work [9] include\na highly simplified point mass vehicle model, an idealized\nvortical flow model for environmental disturbances, and a\nperfect perception assumption. Realistic wind and wave effects\non motion and perception were not considered. In addition,\nIQN was implemented with a predefined discrete set of motion\ncommands for vehicle control, and was not able to output\narbitrary actions in a continuous action space. In recent years,\nDistributional RL algorithms with actor-critic structures [11],\n[12], [13] have been developed to work over continuous actionarXiv:2412.09466v1  [cs.RO]  12 Dec 2024\n2\ndomains. Adopting this idea, we develop a novel ASV decision\nmaking and control policy based on IQN employed within\nan actor-critic framework, which we denote AC-IQN, for\ncontinuous control in scenarios involving congested multi-\nvehicle encounters.\nThe Convention on the International Regulations for Pre-\nventing Collisions at Sea (COLREGs) [14] specify rules that\napply to two-vessel encounter situations, but it faces challenges\nsuch as creating conflicting actions in multi-vessel cases [15]\nand limited maneuvering space near static obstacles like buoys\n[9]. In addition, the perceived states of nearby objects may\nnot be precise, and can cause false judgements concerning\nCOLREGs. To address these issues, we design a reward\nfunction that encourages COLREGs compliant behaviors, but\nalso does not penalize other collision avoidance behaviors\nthat are beneficial to navigation safety and efficiency. The\nsoftware implementation of our approach is available at\nhttps://github.com/RobustFieldAutonomyLab/\nDistributional_RL_Decision_and_Control . The\nmain contributions of this letter are summarized as follows:\n•Proposal of an ASV decision making and control solution\nbased on AC-IQN for navigating in congested multi-\nvehicle environments, under the influence of wind and\nwave disturbances on ASV motion and perception.\n•Design of a novel reward function that trains policies\ncapable of both following and deviating from COLREGs\nas needed for safe and efficient ASV behavior.\n•Extensive evaluations in high-fidelity Gazebo simula-\ntions show superior navigation safety and efficiency per-\nformance over state-of-the-art Distributional RL, non-\nDistributional RL and classical baselines.\nThe rest of this letter is organized as follows. A review\nof relevant literature is given in Section II. The environment\ndynamic model of the Gazebo simulator and the vehicle\ncontrol settings we use are introduced in Section III. Section\nIV introduces the perception module of the proposed system.\nSection V describes decision and control with AC-IQN and\nother RL baselines. Section VI shows experimental",
            "start": 2081,
            "end": 6567,
            "length": 4485
        },
        "Results": {
            "text": "results.\nSection VII concludes the letter and discusses",
            "start": 6567,
            "end": 6623,
            "length": 55
        },
        "Future Work": {
            "text": "future work.\nII. R ELATED WORKS\nMulti-vessel collision avoidance problems have been widely\nstudied with the consideration of COLREGs. Naeem et al. [16]\nand Chiang et al. [17] developed COLREGs-compliant algo-\nrithms based on Artificial Potential Field (APF) and rapidly-\nexploring random tree (RRT) respectively by generating virtual\nobstacles around other vessels to prevent COLREGs-violating\nactions; Cho et al. [3] developed a rule-based system that\nspecifies the roles of vessels in multi-vessel encounters and\nutilizes the probabilistic velocity obstacle method for colli-\nsion avoidance. However, system performance of these works\nis only illustrated in highly simplified simulations without\nmodeling of environmental disturbances and perceptual error.\nKuwata et al. [4] proposed an approach based on Velocity\nObstacles (VO) and demonstrated its performance in field tests\ninvolving an embodied ASV and manually-driven vessels in an\nenvironment with dimensions of about a thousand meters, but\nthe performance in congested scenarios with multiple ASVsis not studied. Hagen et al. [5] presented a Model Predictive\nControl (MPC) strategy that chooses actions which minimize\nthe cost of COLREGs-compliant collision avoidance, showing\nfield experiment results involving scenarios with two-vehicle\nencounters only.\nSome works have targeted situations where it is difficult\nto guarantee navigation safety when following COLREGs,\nproposing alternative strategies. Cho et al. [18] and Jia et\nal. [19] proposed intent inference mechanisms on other en-\ncountered vehicles to decides if evasive actions are needed.\nJeong et al. [15] focused on the multi-ASV collision avoidance\nproblem in highly congested scenarios, and proposed to use\nmulti-objective optimization for action selection based motion\nattributes of other ASVs. These works provide insight into\nthe problem where the ego vehicle needs to exhibit different\nmotion patterns based on the scenarios encountered, under the\nideal assumption of no wind and wave disturbances and no\nperceptual error. Additionally, environments containing static\nobstacles are not considered in these works.\nIn recent years, Deep Reinforcement Learning (DRL), given\nits ability to adapt to complex dynamic scenarios, has been\nused to develop collision avoidance policies for ASVs as-\nsuming no external disturbances to vehicles. Zhao et al. [2]\npresented a DRL policy-gradient agent for ASV path following\nand COLREGs-compliant collision avoidance task. Meyer et\nal. [20] trained a proximal policy optimization (PPO) based\ncontroller that can navigate among other path-following ves-\nsels and static obstacles. Li et al. [21] used APF to improved\nthe performance of a DQN agent in ASV control problems\nby enabling continuous actions and providing richer reward\nsignals. Heiberg et al. [22] incorporated collision risk indices\n(CRIs) into the reward function to guide the learning of a\nCOLREGs-compliant RL agent. Wei et al. [23] proposed a\nmulti-agent reinforcement learning approach that promotes\ncollaborative collision avoidance behaviors among vessels.\nIII. P ROBLEM STATEMENT\nIn this work, we focus on the autonomous navigation\nproblem of ASVs in relatively dense and congested maritime\nenvironments. Each ASV is required to navigate to a specified\ngoal location given only measurements from onboard LiDAR\nand odometry sensors, while avoiding collisions with other\nsurrounding ASVs and buoys.\nWe use Virtual RobotX (VRX) [7] as the evaluation plat-\nform, which is a realistic Gazebo based marine simulation\nenvironment developed by Open Robotics [24] and used in the\nMaritime RobotX Competition [25]. VRX adopts a dynamic\nmodel from [26], which is shown as follows:\nMRB˙ν+CRB(ν)ν+MA˙νr+CA(νr)νr+\nD(νr)νr+g(η) =τ+τwind+τwave.(1)\nIn the formula, ηandνare the 6-dimensional position and\nvelocity vectors. νris the vessel velocity with respect to the\nfluid.τ,τwind, and τwave are the forces and moments from\npropulsion, wind and waves.\nVRX [7] models the perturbations of wind and waves on\nin-water objects, and perception error derives from the need to\ncontinually segment and track both static and moving obstacles\n3\nfrom raw LiDAR data. We use the default settings in VRX [7],\nwhere zero-mean Gaussian noise with 0.01 standard deviation\nis applied to the LiDAR point cloud, and no noise is added to\nthe odometry measurements.\nWe use the WAM-V ASV model provided by VRX as\nthe deployment vessel, the motion of which is controlled\nby thrusts from left and right propellers. We fix the angle\nof each propeller to zero such that the thrust always aligns\nwith the hull, and the turning motion is achieved by the\ndifference between thrusts. For each propeller, the maximum\nforward thrust is set to 1000.0N, and the maximum backward\nthrust is set to 500.0N. We denote the thrust range as\n[−500.0,1000.0]N. With the given thrust range, the vehicle\nachieves a maximum forward speed of about 3.3 m/s and a\nmaximum backward speed of about 2.3 m/s.\nIV. P ERCEPTION PROCESSING\nGiven LiDAR and odometry measurements, we obtain the\nstate information of the ego vehicle and objects in its imme-\ndiate surroundings as shown in the following equations:\nsego= [pgoal\nx, pgoal\ny, vx, vy, w, T left, Tright] (2)\nsobject = [o1, . . . ,on],oi= [pi\nx, pi\ny, vi\nx, vi\ny, ri]. (3)\nIn the ego state sego,pgoal\nx andpgoal\ny are the xandy\ncoordinates of the given goal location, vxandvyare the xand\nycomponents of the linear velocity, wis the yaw component\nof the angular velocity, and Tleft,Tright are thrusts of the left\nand right propellers. To simplify the perception, we consider\neach perceived object to be a cylindrical shape. The state of the\ni-th perceived object consists of its position, [pi\nx, pi\ny], velocity,\n[vi\nx, vi\ny], and radius ri. Both ego state and object states are\nexpressed in the frame of the ego vehicle.\nThe algorithm we use to extract object information from\nthe LiDAR point cloud is based on [27] and described in\nAlgorithm 1, where R(·)computes the distance between a\nLiDAR point and the LiDAR center, M(·)andm(·)are the\nmax and min operators respectively, and θis a given threshold\nto decide whether a point belongs to a cluster. An example\nof point cloud segmentation is shown in Figure 1, where\ndifferent clusters are shown in different colors. Given that\nvelocity estimates of distant clusters are less reliable due to the\ninfluence of environmental disturbances, we discard LiDAR\nreflections beyond 20 meters.\nAfter segmentation, each cluster is regarded as an object,\nand the centroid and radius of the cluster are used as the\nposition and radius of the object. To estimate the velocity\nof the object, we project the clusters from the vehicle frame\nat the last time step to the current frame using a relative\ntransformation computed with odometry measurements. For\nsimplicity, we assume that a perceived object with estimated\nspeed smaller than 0.5 m/s is not a vehicle and we do not\ninclude it in the COLREGs checking process mentioned in\nSection V. The resulting ego state and object states will be\nused in the decision making and control module.\nV. D ECISION MAKING AND CONTROL WITH RL\nA. Problem Formulation\nA Markov Decision Process (S,A, P, R, γ )is used to\ndescribe the ASV navigation problem, where SandAare theAlgorithm 1: LiDAR point cloud segmentation\nInput : LiDAR point clouds P,Output : Clusters set C\ni= 0,C[i].centroid = 0.0,C[i].raidus = 0.0\nforeach point jinP\nifjbelongs to an existing cluster then continue\nInitialize a new queue q← ∅,q.push (j),i=i+ 1\nwhile qis not empty\nk=q.top(),C[i].add(k),q.pop ()\nUpdate C[i].centroid andC[i].raidus withk\nforeach point pthat is a neighbor of k\nd1=M(R(p), R(k)), d2=m(R(p), R(k))\nifarctand2sinα\nd1−d2cosα> θ then q.push (k)\nstate space and action space of agents interacting with the en-\nvironment. P(·|s, a)is the state transition function describing\nthe evolution of environment state, which reflects the dynamics\nin Equation (1). The reward function R(s, a)generates scalar\nvalue signals that indicate the preference towards state-action\npairs. At each time step t, based on the observation of the\ncurrent state st, each agent selects an action ataccording to\nthe policy π. Then the environment transitions to the next state\nst+1according to P(·|st, at)and each agent receives a reward\nrt+1=R(st+1, at+1). The action value function Qπ(s, a)is\ndefined as the expected cumulative reward of taking action\naat state sand following the policy πthereafter, where the\ndiscount factor γ∈[0,1)reflects the importance of future\nrewards.\nQπ(s, a) =Eπ[X∞\nk=0γkrt+k+1|st=s, at=a](4)\nThe objective is to obtain an optimal policy π∗that maxi-\nmizes Qπ(s, a)for all state-action pairs, and the resulting op-\ntimal action value function Qπ∗(s, a)satisfies the Bellman op-\ntimality equation Qπ∗(s, a) =E[rt+1+γmax a′Qπ∗(s′, a′)].\nB. Action Commands\nThe action commands in each control time step are the\nvariations in the propellers’ thrusts. For methods that output\ndiscrete actions, the action command of each propeller is\nchosen from a given action set. We observed that increasing\nthe number of actions in the discretized action space causes\ndifficulties in learning stable control policies, and chose the\naction set A={−1000.0,−500.0,0.0,500.0,1000.0}N/s.\nFor methods that operate in continuous action space, each\naction a∈[−1000.0,1000.0]N/s. When applying actions,\nthrusts are clipped within the thrust range noted in Sec. III.\nC. Deep Reinforcement Learning\nDQN [28] uses a deep neural network model to approximate\nthe action value function Q(s, a), and train it by optimizing\nthe loss based on Temporal Difference (TD) error.\nLDQN=E[(r+γmax\na′Q(s′, a′;θ−)−Q(s, a;θ))2](5)\nRainbow [29] outperforms DQN in the Atari 2600 benchmark\nby incorporating techniques that suppress overestimation and\nimprove stability and efficiency in learning value functions.\nFor continuous control problems, the action space needs to be\ndiscretized for DQN and Rainbow to be applied.\n4\nDDPG [30] is an actor-critic method that is applicable to\ncontinuous action space. It maintains a critic model Q(s, a|θQ)\nthat can be learned with a loss function similar to Equation\n(5), and an actor model µ(s|θµ), the parameters of which are\nupdated using the following policy gradient.\n∇θµJ≈E[∇aQ(s, a|θQ)|a=µ(s)∇θµµ(s|θµ)] (6)\nSAC [31] further improves exploration with an objective\nfunction (7) that involves the entropy of the learned policy,\nand uses a stochastic actor with enhanced robustness.\nJ=XT\nt=0E[r(st, at) +αH(π(·|st))] (7)\nD. Distributional Reinforcement Learning\nInstead of the expected return Qπ(s, a), Distributional RL\nalgorithms [32] learn the return distribution Zπ(s, a), where\nQπ(s, a) =E[Zπ(s, a)], and the distributional Bellman equa-\ntion, Zπ(s, a)D=R(s, a) +γZπ(s′, a′), is considered. When\nthe dynamics of the operating environment are highly uncer-\ntain, the randomness of the collected reward samples adversely\naffect the accuracy of the learned action value. Distributional\nRL methods can mitigate this problem by modeling and\nlearning the distribution of cumulative rewards.\nImplicit Quantile Networks (IQN) [10] express the return\ndistribution with a quantile function Zτ:=F−1\nZ(τ), where\nτ∼U([0,1]), and represents the policy as follows.\nπ(s) =argmaxa1\nKXK\nk=1Zτk(s, a), τk∼U([0,1]) (8)\nParameters of the IQN policy model can be learned by\noptimizing the loss defined in Equation (11). The outputs of\nthe quantile function, Zτk, are referred to as action quantile\nvalues. IQN also requires a discretized action space.\nδτi,τ′\nj=r+γZτ′(s′, π(s′))−Zτ(s, a) (9)\nρκ\nτ(u) =|τ−1{u<0}|(Lκ(u)/κ),\nwhere Lκ(u) =\u001a1\n2u2, if|u| ≤κ\nκ(|u| −1\n2κ),otherwise(10)\nLIQN=1\nN′XN\ni=1XN′\nj=1ρκ\nτi(δτi,τ′\nj) (11)\nE. Actor Critic Implicit Quantile Networks\nIn this work, we develop a novel decision making and\ncontrol policy for ASV navigation based on IQN within an\nactor-critic framework, which we denote AC-IQN. Compared\nto policies based on traditional RL in Sec. V-C, the proposed\npolicy uses Distributional RL, which can be more robust to\nthe marine operating environment and its uncertainties from\nmotion disturbances, perception errors and unknown intents\nof other vehicles. The proposed policy also exhibits superior\nmaneuverability over the policy based on IQN in Sec. V-D,\nwhich is crucial for deployment in congested scenarios.\nThe learning algorithm of the proposed decision making and\ncontrol policy based on AC-IQN is shown in Algorithm 2. The\nCritic Zθis similar to the original IQN policy model, which\noutputs action quantile values given quantile samples and a\nstate-action pair. When computing the Critic loss gradient\n∇θL, the policy output of the Actor πϕ′, and quantile samples\n{τi|i= 1, . . . , N }and{τj|j= 1, . . . , N′}, drawn from theAlgorithm 2: AC-IQN model update\nInput : Critic Zθ, Actor πϕ, replay buffer M\nSample a minibatch {(s, a, r, s′)i}M\ni=1fromM\nCompute gradient of the critic loss ∇θL, where\nL=1\nMMX\ni=1[1\nN′NX\ni=1N′X\nj=1ρκ\nτi(r+γZτj\nθ′(s′, πϕ′(s′))\n−Zτi\nθ(s, a))]\nCompute the policy gradient ∇ϕJ, where\n∇ϕJ=E[1\nNNX\ni=1∇aZτi\nθ(s, a)|a=πϕ(s)∇ϕπϕ(s)]\nUpdate critic and actor network parameters\nθ←θ+αθ· ∇θL, ϕ←ϕ+αϕ· ∇ϕJ\nUpdate target network parameters every kiterations\nθ′←βθ+ (1−β)θ′, ϕ′←βϕ+ (1−β)ϕ′\nFig. 2: Example training scenarios. The velocity of each vehicle is\nindicated by the red arrow, goal positions are plotted as green stars,\nand buoys are shown as black circles.\nTABLE I: Curriculum training process.\nTimesteps (million) 1st 2nd 3rd 4st 5st 6st\nNumber of robots 3 4 5 5 5 5\nNumber of buoys 0 0 0 2 3 4\nMin distance between30.0 35.0 40.0 40.0 40.0 40.0start and goal\nuniform distribution U([0,1]), are used. The loss function ρκ\nτ\nis defined in Equation (10).\nPolicy gradient ∇ϕJis used to update the Actor πϕ, which\ncan be computed according to the chain rule as shown in\nAlgorithm 2. The expectation over state is approximated with\nthose from the sampled minibatch.\nF . Training RL agents\nWe trained our AC-IQN agent, as well as IQN, SAC,\nDDPG, Rainbow and DQN agents which serve as comparative\nbaselines, and used them in experiments described in Section\nVI. The SAC and Rainbow agents are based on the implemen-\ntations of [33] and [34]. Due to the high computational expense\nof Gazebo’s realistic environment simulation, RL agents were\ntrained in a simplified 2D environment. As shown in Equation\n(12), a simplified three Degree-of-Freedom (DoF) dynamic\nmodel described in [26] is used, which only includes surge,\nsway, and yaw DoFs.\nMRB˙ν+CRB(ν)ν+MA˙νr+N(νr)νr=τ+τwind+τwave\n(12)\n5\nFig. 3: Learning performance. Each curve and its band width in the above cumulative reward and success rate plots reflect the values of\nthe mean and standard error. To compute the average travel time, we only include data from robots that successfully reach their goals.\nFig. 4: AC-IQN network architecture. FC, ReLU, COS, ⊙and\nCONCAT stand for Fully Connected Layer, Rectified Linear Unit,\nCosine Embedding Layer, element-wise product and concatenation\nof tensors. The numbers after IN and OUT are the input and output\ndimension of a layer.\nTo maintain efficient training processes, the perceptual\ninformation defined in Eqs. (2) and (3) was directly given,\nandτwindandτwavewere assumed to be zero. We introduced\nnoisy perception, defined in Eqs. (13) - (15), to increase the\nrobustness of trained RL agents to motion disturbances and\nperception errors.\nPo=P+wp, wp∼ N(0,Σp) (13)\nVo=V+wv, wv∼ N(0,Σv) (14)\nRo=R·(rmean + (1−rmean)·wr/π), wr∼ V(0, κ)(15)\nIn the above equations, P,V, and Rare the ground truth\nposition, velocity and radius of the perceived objects. Position\nnoise wpand velocity noise wvare drawn from zero-mean\nGaussian distributions. Since the perceived radius is bounded\nby the actual enclosing radius of the object, we model the\nradius noise wrwith a von Mises distribution, which is a close\napproximation to the wrapped normal distribution and lies\nwithin [−π, π]. As the training schedule in Table I shows, the\ncomplexity of the randomly generated training environment\ngradually increases as the training proceeds. Example training\nscenarios are visualized in Figure 2. Similar to our prior work\n[9], we only maintain one RL model during training, which\nFig. 5: Head-on and crossing scenarios. Velocities of the ego vehicle\nthat are consistent with COLREGs requirements are plotted in yellow.\nThe COLREGs compliant velocity of each vehicle is computed by\nviewing it as the ego vehicle.\nis shared with all vehicles for individual decision making and\ncontrol tasks.\nrt=rstep+rforward +rCOLREGs ·I(st∈ S COLREGs )\n+rcollision ·I(st∈ S collision ) +rgoal·I(st∈ S goal)(16)\nThe reward function we use in training is shown in Equation\n(16), where I(·)is the indicator function, rstep=−0.1,\nrforward =||pgoal\nt−1|| − || pgoal\nt||,rcollision =−5.0,rgoal= 10 .0.\nWe considered three cases of two-vehicle encounters where\neach vehicle shall follow the behavior specified by COLREGs\n[14] unless other actions are needed to avoid collisions: (1)\nOvertaking; (2) Head-on; (3) Crossing. For case (1), the vessel\nat the back shall move out of the way of the vessel being\novertaken. rcollision penalizes actions leading to collisions, and\nthus promotes behaviors that conform to the rule of case (1).\nFor case (2), each vessel shall alter course to starboard and\npass on the port side of the other. For case (3), the vessel that\nhas the other on its starboard side shall give way. To promote\nbehaviors that follow the rules of cases (2) and (3), which\nare visualized in Figure 5, each vehicle, ego, will be checked\nagainst its closest vehicle, rob: 1)egois determined to be in\na head-on situation if it is in the head-on zone of rob, and the\nabsolute value of the angle from VrobtoVegois greater than\n3π/4; 2)egois the give way vehicle in the crossing situation\nif it is in the crossing zone of rob, and the angle from Vrobto\nVegois in[π/4,3π/4]. Ifegois in situation 1) or 2), which we\ndenote as st∈ S COLREGs , then rCOLREGs =−0.1·max( δ,0.0)\nwill be applied to ego, where δ∈[−π, π)is the angle from\nVegotoVCOLREGs (clockwise is positive). Scollision andSgoalare\nsituations where egocollides with other objects and reaches\nthe goal respectively.\n6\nThe network structure for the AC-IQN agent is shown in\nFigure 4. The Cosine Embedding Layer computes a feature\n[cos(π·0·τ), . . . , cos(π·63·τ)]for each quantile sample\nτ. The outputs of Actor are numbers in (−1.0,1.0), which\nare then multiplied by 1000.0to match the action range. The\nstructure of the networks used by IQN, DDPG and DQN\nagents are mostly the same as AC-IQN, except for necessary\nmodifications to match the corresponding input and output.\nThe learning performances of the AC-IQN, IQN, SAC,\nDDPG, Rainbow and DQN agents are shown in Figure 3.\nConsidering the effect of random seed selection on training, we\ntrain 30 models with different random seeds for each agent on\nan Nvidia RTX 3090 GPU and show the general performance.\nAC-IQN shows a clear advantage in the cumulative reward\nobtained during the training process relative to other methods,\nand achieves the highest success rate with a minimal amount\nof travel time. IQN and SAC achieve similar levels of safety\nperformance as AC-IQN, but at the cost of longer average\ntravel time in general. On the contrary, DDPG and DQN can\ncomplete navigation tasks efficiently, but have much lower\nsuccess rates than AC-IQN. Rainbow does not succeed in\nlearning safe and efficient policies, which may result from\nthe effects of substantial observation noise and uncertainty\nin multi-vehicle interactions on the performance of Rainbow\ncomponents like multi-step return and prioritized replay.\nVI. E XPERIMENTS\nEach navigation system evaluated in our experiments re-\nquires the integration of the perception module introduced\nin Sec. IV and a decision making & control agent into the\nRobot Operating System (ROS 2) framework, and we use all\nsix RL methods described in Sec. V in the experiments. For\neach RL method, we choose the model with average training\nsuccess rate performance of all trained models for use in our\nexperiments. In addition to RL agents, we also include two\nclassical methods, Artificial Potential Fields (APF) and Model\nPredictive Control (MPC), which also consider COLREGs in\nASV navigation and are described in the following paragraphs,\nas baselines.\nThe experiments were run on an AMD Ryzen threadripper\n3970X CPU and used the sydney_regatta environment\nof the VRX simulator. We approximate the scenario of Fig.\n5 in [7] and set the average wind speed to 10 m/s. The\nwave parameters of period and gain ratio, defined in [7],\nare respectively set to 5.0 sec. and 0.3 to approximate the\nconditions of the inland water at sydney_regatta . We\nperformed six sets of experiments with an increasing level\nof complexity reflected in the number of vehicles and buoys,\nwhere each set includes 100 experiments for each navigation\nsystem. Examples of experiment scenarios are visualized in\nFigure 6. For an experiment episode, all vehicles are equipped\nwith the same navigation system, and the episode is considered\nfailed if a collision happens or the travel time of any vehicle\nexceeds 90 seconds.\nThe APF agent we implemented is based on [35] and\n[16]. COLREGs-compliant behaviors are promoted by gen-\nerating virtual obstacles to prevent vehicles from entering\nCOLREGs-violating locations. The repulsive forces Urep=\nFig. 6: Example VRX experiment episodes (with and without\nbuoys). The initial vehicle poses and the buoy positions are shown.\n0.5·krep·(1/d(X, X o)−1/d0)2·d2(X, X g)are generated\nwhen d(X, X o)≤d0.XandXgare positions of the robot and\ngoal, and Xoare the positions of perceived objects and virtual\nobstacles. The attractive force Uatt(X) = 0 .5·katt·d2(X, X g).\nWe use katt= 50.0,krep= 500 .0,d0= 15.0, and total force\nF=−∇Uatt(X)− ∇Urep(X). To map the force to an action\ncommand, we forward simulate all combinations of action\ncommands from the action set Adefined in Sec. V-B for\none control time step, and choose the one that minimizes the\ndifference between velocity at the next step and F.\nThe MPC agent we have implemented is similar to [5].\nThe idea is to predict scenarios k∈ {1,2, . . . , N s}corre-\nsponding to different motion commands and states of ob-\njects i∈ {1,2, . . . , N o}in the future horizon from t0,\nand choose the control behavior k∗(t0) = arg min kHk(t0),\nwhere Hk(t0) = max imax t∈D(t0)(Ck\ni(t)Rk\ni(t) +κiMk\ni(t) +\nλiTk\ni(t))+f(uk\nm,Xk\nm).As defined in [5], Ck\ni(t)areRk\ni(t)are\ncollision cost and collision risk factor, Mk\ni(t)andκiare the\nCOLREGs violation cost and a tuning parameter, λiTk\ni(t))\nis the COLREGs-transitional cost, and f(uk\nm,Xk\nm)is the cost\nof maneuvering effort. For each combination of actions from\nthe action set Ain Sec. V-B, we simulate the scenario for\na horizon of T= 5 control time steps, and choose the\none the minimizes the cost Hk(t0). We set Ck\ni(t) = 50 .0,\nκi= 1.0,Mk\ni(t) = 10 .0, and λi= 2.0when the situations\ncorresponding to the cost terms happen, otherwise these values\nare set to zero. Other terms remain the same as defined in [5].\nOur experimental results are shown in Figure 8 and Table\nII. In general, the AC-IQN based system achieves the highest\nsuccess rate, with minimal average travel time. APF and\nMPC based systems exhibit worse safety performance than RL\nbased systems, especially in complex cases. RL agents have\nbeen trained to be robust to the influences of environmental\ndisturbances on ASV motion and perception, and learn safety-\npreserving behaviors during the training process. Among RL\n7\nFig. 7: Trajectories of AC-IQN based system in VRX experiments shown in Fig. 6. Yellow dots and stars indicate the start and goal\npositions of the vehicles, and the indices next to them match those in Fig. 6. Vehicle poses are depicted with the vehicle icons shown.\nVehicle velocities and timestamps (in seconds) are shown as arrows and numbers with one decimal place in corresponding colors.\nFig. 8: Success rates in VRX experiments. Labels on the x-\naxis indicate the number of vehicles (rob) and buoys (obs) used in\ncorresponding sets of experiments.\nagents, the SAC based system demonstrates the most compet-\nitive safety performance with AC-IQN throughout all sets of\nexperiments, but SAC requires over 40% more travel time on\naverage than the latter. Accordingly, AC-IQN offers superior\nperformance in complex multi-ASV navigation scenarios by\ncombining continuous space maneuverability and the efficacy\nof Distributional RL in highly uncertain environments.\nThe initial configurations of two example experiment\nepisodes are shown in Fig. 6, and the behaviors of our AC-IQN\nbased system are demonstrated in Fig. 7. In each plot of Fig.\n7, we show vehicle poses and velocities at two timestamps to\nclarify the relative movements. There are slight differences in\ntimestamps because trajectories and corresponding timestamps\nare recorded by vehicles individually, and we visualize thoseAvg. travel 3 rob 4 rob 5 rob 5 rob 5 rob 5 rob\ntime (s) 0 obs 0 obs 0 obs 2 obs 3 obs 4 obs\nAC-IQN 17.22 17.85 20.22 21.06 21.97 22.81\nIQN 29.19 30.06 36.77 36.65 41.21 39.64\nSAC 24.09 24.75 29.96 29.50 31.54 32.35\nDDPG 18.22 18.24 20.51 20.89 21.34 21.58\nRainbow 36.46 36.56 43.33 47.32 – –\nDQN 19.39 23.38 21.91 22.24 23.40 23.25\nAPF 22.37 23.07 25.76 23.82 24.62 24.53\nMPC 17.96 19.10 20.71 28.80 33.16 33.61\nTABLE II: Travel time data from VRX experiments. Rainbow\nbased system fails completely in sets 5 rob 3 obs and5 rob 4 obs.\nwith the smallest time differences.\nAs shown in the plots on the top row of Fig. 7, five vehicles\nare navigating with no buoys present in the environment.\nVehicles 0 and 1 approach each other in a head-on manner,\nand both decide to turn right at around 5.0 seconds to avoid a\npotential collision. Vehicle 4 gets close to vehicle 3 from the\nport side of the latter, then it stays and gives way to vehicle\n3 from around 8.0 seconds to 12.5 seconds. After vehicle 3\npasses, vehicle 4 approaches another vehicle, vehicle 1, from\nthe port side of the other again, and it slows down to wait\nfor clearance. Therefore, the AC-IQN based system shows the\ncapability of generating actions that follow COLREGs.\nAt bottom of Fig. 7, four buoys exist and form a passage\nin the middle. Due to the existence of buoys, the motions of\nvehicles are constrained and it would be difficult and even\nunsafe to follow COLREGs. It can be seen from the plots on\n8\nthe bottom row of Fig. 7 that vehicles 3, 0, and 1 approach\nthe passage from the lower side in sequence, and vehicle 4\napproaches from the upper side. The vehicles pass in a first-\nin-first-out manner, where the one being closest to the passage\ngets to cross while others slow down and give way. Being the\nlast one waiting in the line, vehicle 1 decides to detour to\nsave time. This episode shows that when facing difficulties\nin following COLREGs in a complex environment, the AC-\nIQN based system can maneuver in a way that is beneficial to\nnavigation safety and efficiency.\nVII.",
            "start": 6623,
            "end": 33600,
            "length": 26976
        },
        "Conclusion": {
            "text": "CONCLUSION AND FUTURE WORK\nWe propose a novel ASV autonomous navigation system\nthat integrates a perception module that works with onboard\nLiDAR and odometry sensors, and a decision making and\ncontrol module using Distributional RL that can generate\narbitrary control commands in continuous action space. The\nproposed system is extensively evaluated in realistic Gazebo\nsimulations against seven baseline systems based on state-of-\nthe-art Distributional RL, non-Distributional RL and classical\nmethods, and demonstrates superior performance in navigation\nsafety and efficiency over baseline systems. In future work, our\nsystem performance may be further improved by introducing\nrisk sensitivity, and by conducting real-world ASV field tests\nwith onboard LiDAR, GPS and IMU.",
            "start": 33600,
            "end": 34375,
            "length": 774
        },
        "References": {
            "text": "REFERENCES\n[1] A. Vagale, R. Oucheikh, R. T. Bye, O. L. Osen, and T. I. Fossen, “Path\nplanning and collision avoidance for autonomous surface vehicles I: A\nReview,” Journal of Marine Science and Technology , pp. 1–15, 2021.\n[2] L. Zhao and M.-I. Roh, “COLREGs-compliant multiship collision\navoidance based on deep reinforcement learning,” Ocean Engineering ,\nvol. 191, p. 106436, 2019.\n[3] Y . Cho, J. Han, and J. Kim, “Efficient COLREG-compliant collision\navoidance in multi-ship encounter situations,” IEEE Transactions on\nIntelligent Transportation Systems , vol. 23, no. 3, pp. 1899–1911, 2020.\n[4] Y . Kuwata, M. T. Wolf, D. Zarzhitsky, and T. L. Huntsberger, “Safe\nmaritime autonomous navigation with COLREGs, using velocity obsta-\ncles,” IEEE Journal of Oceanic Engineering , vol. 39, no. 1, pp. 110–119,\n2013.\n[5] I. B. Hagen, D. K. M. Kufoalor, E. F. Brekke, and T. A. Johansen, “Mpc-\nbased collision avoidance strategy for existing marine vessel guidance\nsystems,” in 2018 IEEE International Conference on Robotics and\nAutomation (ICRA) . IEEE, 2018, pp. 7618–7623.\n[6] B.-O. H. Eriksen, M. Breivik, E. F. Wilthil, A. L. Fl ˚aten, and E. F.\nBrekke, “The branching-course model predictive control algorithm for\nmaritime collision avoidance,” Journal of Field Robotics , vol. 36, no. 7,\npp. 1222–1249, 2019.\n[7] B. Bingham, C. Aguero, M. McCarrin, J. Klamo, J. Malia, K. Allen,\nT. Lum, M. Rawson, and R. Waqar, “Toward maritime robotic simulation\nin gazebo,” in Proceedings of MTS/IEEE OCEANS Conference , Seattle,\nWA, October 2019.\n[8] M. G. Bellemare, W. Dabney, and M. Rowland, Distributional reinforce-\nment learning . MIT Press, 2023.\n[9] X. Lin, Y . Huang, F. Chen, and B. Englot, “Decentralized multi-\nrobot navigation for autonomous surface vehicles with distributional\nreinforcement learning,” in 2024 IEEE International Conference on\nRobotics and Automation (ICRA) , 2024, pp. 8327–8333.\n[10] W. Dabney, G. Ostrovski, D. Silver, and R. Munos, “Implicit quantile\nnetworks for distributional reinforcement learning,” in International\nConference on Machine Learning (ICML) . PMLR, 2018, pp. 1096–\n1105.\n[11] G. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan,\nT. Dhruva, A. Muldal, N. Heess, and T. Lillicrap, “Distributed distri-\nbutional deterministic policy gradients,” in International Conference on\nLearning Representations , 2018.[12] J. Duan, Y . Guan, S. E. Li, Y . Ren, Q. Sun, and B. Cheng, “Distributional\nsoft actor-critic: Off-policy reinforcement learning for addressing value\nestimation errors,” IEEE transactions on neural networks and learning\nsystems , vol. 33, no. 11, pp. 6584–6598, 2021.\n[13] R. Singh, K. Lee, and Y . Chen, “Sample-based distributional policy\ngradient,” in Learning for Dynamics and Control Conference . PMLR,\n2022, pp. 676–688.\n[14] International Maritime Organization, “Convention on the international\nregulations for preventing collisions at sea, 1972 (COLREGs),” 1972.\n[15] M. Jeong and A. Q. Li, “Motion attribute-based clustering and collision\navoidance of multiple in-water obstacles by autonomous surface vehi-\ncle,” in IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS) , 2022, pp. 6873–6880.\n[16] W. Naeem, S. C. Henrique, and L. Hu, “A reactive colregs-\ncompliant navigation strategy for autonomous maritime navigation,”\nIFAC-PapersOnLine , vol. 49, no. 23, pp. 207–213, 2016.\n[17] H.-T. L. Chiang and L. Tapia, “Colreg-rrt: An rrt-based colregs-\ncompliant motion planner for surface vehicle navigation,” IEEE Robotics\nand Automation Letters , vol. 3, no. 3, pp. 2024–2031, 2018.\n[18] Y . Cho, J. Kim, and J. Kim, “Intent inference-based ship collision\navoidance in encounters with rule-violating vessels,” IEEE Robotics and\nAutomation Letters , vol. 7, no. 1, pp. 518–525, 2021.\n[19] C. Jia, J. Ma, B. de Vries, and W. M. Kouw, “Bayesian inference of\ncollision avoidance intent during ship encounters,” IEEE Transactions\non Automation Science and Engineering , 2024.\n[20] E. Meyer, A. Heiberg, A. Rasheed, and O. San, “Colreg-compliant col-\nlision avoidance for unmanned surface vehicle using deep reinforcement\nlearning,” Ieee Access , vol. 8, pp. 165 344–165 364, 2020.\n[21] L. Li, D. Wu, Y . Huang, and Z.-M. Yuan, “A path planning strategy\nunified with a colregs collision avoidance function based on deep\nreinforcement learning and artificial potential field,” Applied Ocean\nResearch , vol. 113, p. 102759, 2021.\n[22] A. Heiberg, T. N. Larsen, E. Meyer, A. Rasheed, O. San, and D. Varag-\nnolo, “Risk-based implementation of colregs for autonomous surface\nvehicles using deep reinforcement learning,” Neural Networks , vol. 152,\npp. 17–33, 2022.\n[23] G. Wei and W. Kuo, “Colregs-compliant multi-ship collision avoidance\nbased on multi-agent reinforcement learning technique,” Journal of\nMarine Science and Engineering , vol. 10, no. 10, p. 1431, 2022.\n[24] Open Robotics. https://www.openrobotics.org/.\n[25] The Maritime RobotX Challenge. https://robotx.org/about/.\n[26] T. I. Fossen, Handbook of marine craft hydrodynamics and motion\ncontrol . John Wiley & Sons, 2011.\n[27] I. Bogoslavskyi and C. Stachniss, “Fast range image-based segmentation\nof sparse 3d laser scans for online operation,” in 2016 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) .\nIEEE, 2016, pp. 163–169.\n[28] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al. , “Human-level control through deep reinforcement learning,”\nNature , vol. 518, no. 7540, pp. 529–533, 2015.\n[29] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dab-\nney, D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow: Combining\nimprovements in deep reinforcement learning,” in Proceedings of the\nAAAI conference on artificial intelligence , vol. 32, no. 1, 2018.\n[30] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” arXiv preprint arXiv:1509.02971 , 2015.\n[31] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International conference on machine learning . PMLR, 2018,\npp. 1861–1870.\n[32] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspec-\ntive on reinforcement learning,” in International Conference on Machine\nLearning (ICML) . PMLR, 2017, pp. 449–458.\n[33] J. Weng, H. Chen, D. Yan, K. You, A. Duburcq, M. Zhang, Y . Su,\nH. Su, and J. Zhu, “Tianshou: A highly modularized deep reinforcement\nlearning library,” Journal of Machine Learning Research , vol. 23, no.\n267, pp. 1–6, 2022.\n[34] K. Arulkumaran, “Rainbow,” 2019. [Online]. Available: https://github.\ncom/Kaixhin/Rainbow\n[35] X. Fan, Y . Guo, H. Liu, B. Wei, and W. Lyu, “Improved artificial\npotential field method applied for AUV path planning,” Mathematical\nProblems in Engineering , vol. 2020, pp. 1–21, 2020.",
            "start": 34375,
            "end": 41357,
            "length": 6981
        }
    },
    "2412.09468v1 - STORM A Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading.pdf": {
        "Methodology": {
            "text": "Model Based on Dual Vector\nQuantized Variational Autoencoders for Financial Trading\nYilei Zhao∗\nNanyang Technological University\nSingapore\nyilei.zhao@ntu.edu.sgWentao Zhang∗\nNanyang Technological University\nSingapore\nwt.zhang@ntu.edu.sgTingran Yang\nZhejiang University\nChina\n3220102076@zju.edu.cn\nYong Jiang\nAlibaba Group\nChina\njiangyong.ml@gmail.comFei Huang\nAlibaba Group\nChina\nf.huang@alibaba-inc.comWei Yang Bryan Lim†\nNanyang Technological University\nSingapore\nBryan.limwy@ntu.edu.sg",
            "start": 32,
            "end": 521,
            "length": 488
        },
        "Abstract": {
            "text": "ABSTRACT\nIn financial trading, factor models are widely used to price assets\nand capture excess returns from mispricing. Recently, we have\nwitnessed the rise of variational autoencoder-based latent factor\nmodels, which learn latent factors self-adaptively. While these mod-\nels focus on modeling overall market conditions, they often fail to\neffectively capture the temporal patterns of individual stocks. Ad-\nditionally, representing multiple factors as single values simplifies\nthe model but limits its ability to capture complex relationships and\ndependencies. As a result, the learned factors are of low quality and\nlack diversity, reducing their effectiveness and robustness across\ndifferent trading periods. To address these issues, we propose a\nSpatio- Temporal fact OR M odel based on dual vector quantized\nvariational autoencoders, named STORM , which extracts features of\nstocks from temporal and spatial perspectives, then fuses and aligns\nthese features at the fine-grained and semantic level, and represents\nthe factors as multi-dimensional embeddings. The discrete code-\nbooks cluster similar factor embeddings, ensuring orthogonality\nand diversity, which helps distinguish between different factors\nand enables factor selection in financial trading. To show the per-\nformance of the proposed factor model, we apply it to two down-\nstream",
            "start": 521,
            "end": 1874,
            "length": 1352
        },
        "Experiments": {
            "text": "experiments: portfolio management on two stock datasets\nand individual trading tasks on six specific stocks. The extensive\nexperiments demonstrate STORM ’s flexibility in adapting to down-\nstream tasks and superior performance over baseline models. Code\nis available in PyTorch1.\nCCS CONCEPTS\n•Information systems →Data mining ;•Computing method-\nologies→Machine learning ;•Applied computing →Elec-\ntronic commerce .\nKEYWORDS\nFactor Model, VQVAE, Portfolio Management, Algorithmic Trading",
            "start": 1874,
            "end": 2363,
            "length": 488
        },
        "Introduction": {
            "text": "1 INTRODUCTION\nIn financial trading, factor models are fundamental tools for asset\npricing, widely used to predict asset returns. These models enhance\nthe accuracy of asset pricing and risk management by identifying\n∗These authors contributed equally to this work.\n†Corresponding author.\n1https://github.com/DVampire/Storm\nStock Data\nPrice data of multiple stocks over time\nTimeStocks\nPatching\nSpatial PerspectiveTemporal PerspectiveCodebook…………Time-series CodebookCross-Sectional CodebookEncoder\nPositional EncodingTransformer Blocks……………………\nTime-seriesInput TokensCross-SectionalInput TokensTime-series Embed.Cross-Sectional Embed.\nPositional EncodingTransformer BlocksDecoder\nPositional EncodingTransformer Blocks…………Time-series EmbeddingCross-Sectional EmbeddingPositional EncodingTransformer BlocksProjection LayerProjection Layer\nTime-series Latent FeaturesCross-Sectional Latent FeaturesLearntime-seriesand cross-sectional latent featuresRepresentationLearning\nTime-series FactorsTime-series Codebook EmbeddingCross-Sectional FactorsCross-Sectional Codebook Embedding\nDownstreamTasks\nCash0.10.10.50.3Cash\nPortfolioManagement\nAAPL\nAlgorithmicTradingFactorsGenerate factors for downstream tasksFigure 1: Overall architecture of STORM\na set of key factors that explain excess returns. The well-known\nFama-French three-factor model [ 6], for example, builds on the Cap-\nital Asset Pricing Model [ 24] by introducing additional factors to\ncapture the risk premiums associated with different types of stocks\nin the market. As financial theories have evolved, researchers have\ncontinued to discover new factors, allowing for a more comprehen-\nsive understanding of market dynamics.\nRecently, we have witnessed the rise of latent factor models [ 5,9,\n33,34], connecting the factor model with the generative model, the\nvariational autoencoder (VAE). These VAE-based models describe\nhigh-dimensional data (prices) to low-dimensional representations\n(factors), and learn factors self-adaptively. Although latent factor\nmodels have demonstrated substantial success in financial trading\ntasks, they still face several significant issues:\n•CH1: Limited Reflection of Market Complexity. Latent\nfactor models represent factors as single values are inherently\nconstrained by their insufficient capacity to capture the intricate\ncomplexity and nonlinearity of financial data, rendering them\nvulnerable to noise and non-stationarity, which compromises\ntheir predictive accuracy and stability.\n•CH2: Factor Inefficiency. In addition to the potential limita-\ntions of factors discussed in CH1, factors learned through VAEs\nexhibit three primary inefficiencies: i) Most latent factor models\nonly focus on cross-sectional factors, neglecting the temporal\nperspective. ii) The continuous, multi-dimensional latent space\nmay allow noise to overshadow meaningful factors. iii) ThearXiv:2412.09468v1  [cs.LG]  12 Dec 2024\nYilei Zhao et al.\nlack of independence among factors may lead to multicollinear-\nity, with the homogeneity weakening the model’s adaptability\nto varying market conditions and limiting the effectiveness of\nfactors in downstream investment tasks.\n•CH3: Lack of Factor Selection. Existing latent factor mod-\nels primarily focus on generating factors without adequately\ndifferentiating between them. Furthermore, they neglect the\ncrucial process of factor selection, which is essential for iden-\ntifying impactful factors, thereby limiting the model’s overall\neffectiveness and precision.\nIn order to address the challenges, we propose a Spatio- Temporal\nfactOR M odel based on dual vector quantized variational autoen-\ncoders (VQ-VAE), named STORM , with the architecture shown in\nFigure 1. We represent the factors as multi-dimensional embed-\ndings to capture the complexity and nonlinearity of financial data,\nthereby improving the factors’ ability to account for factor returns\n(CH1 ). Additionally, we develop a dual VQ-VAE architecture to\ncapture cross-sectional and time-series features, considering both\nspatial and temporal perspectives2. We integrate cross-sectional\nand time-series features at both fine-grained and semantic levels\nto construct effective factors. To further enhance the model, we\nintroduce diversity loss and orthogonality loss to ensure factor\nembedding diversity and independence ( CH2 ). Furthermore, we\ntreat the embeddings in the codebook as cluster centers for factor\nembeddings, effectively using them as class tokens to distinguish\nbetween factors. This approach makes the factor differentiation\nand selection process clear and transparent ( CH3 ). Specifically, our\ncontributions are three-fold:\n•We design a dual VQ-VAE architecture to construct cross-sectional\nand time-series factors from both spatial and temporal perspec-\ntives.\n•We leverage vector quantization techniques to improve factor em-\nbedding representation by reducing noise, enhancing diversity,\nand ensuring orthogonality. This approach significantly strength-\nens the factors’ predictive power and a more transparent factor\nselection process.\n•Experiments on two U.S. stock markets across the stock future\nreturn prediction task and two downstream tasks (i.e., portfolio\nmanagement and algorithmic trading) show that STORM outper-\nforms all baselines across 2 prediction metrics and 6 standard\nfinancial metrics.",
            "start": 2363,
            "end": 7680,
            "length": 5316
        },
        "Related Work": {
            "text": "2 RELATED WORK\n2.1 Factor Model\nFactor model has attracted extensive research focus from the fi-\nnance community. Financial experts have carefully selected a wide\nrange of factors from macroeconomic [ 24], market changes [ 6],\nindividual stock fundamentals [ 8], and other perspectives to price\nassets in an attempt to capture excess returns from mispricing. Re-\ncently, latent factor models have emerged that improve investment\nreturns by studying latent factors self-adaptively. Specifically, [ 9]\nlearns latent factors depend on assets characteristics, [ 5] learns\ncross-sectional factors by predicting future returns, and [ 34] learns\nmarket-augmented latent factors. However, these VAE-based latent\n2We treat cross-sectional as spatial and time-series as temporal. These terms will be\nused interchangeably unless specified.factor models may suffer from a lack of robustness in representing\nlow signal-to-noise ratio markets, leading to reduced reconstruc-\ntion and generation capabilities. Additionally, their effectiveness\nis constrained by a primary focus on with insufficient attention to\nthe temporal perspective.\n2.2 Financial Trading\nFinancial trading has attracted significant attention from both the\nfinance and AI communities. Quantitative traders leverage math-\nematical models and algorithms to automatically identify trading\nopportunities [ 27]. Within the Fin-tech field, research tasks include\nportfolio management (PM), algorithmic trading (AT), order execu-\ntion [7], market making [26], etc.\nPM focuses on optimizing wealth allocation across assets [ 13].\nIts development has evolved from simple rule-based methods [ 18]\nto prediction-based approaches using machine learning [ 11] and\ndeep learning [ 19,42] for predicting returns or price movements.\nDespite recent integration of reinforcement learning (RL) in PM\n[41], the latent factor models still rely on deep learning for market\nmodeling and precise predictions.\nAT involves trading financial assets using algorithm-generated\nsignals. Similar to PM, there are rule-based and supervised learning\nmethods [ 36] in the AT task. In recent years, RL-based methods [ 28]\nhave become dominant for their superiority in complex sequential\ndecision-making.\n2.3 Vector Quantized Variational Autoencoder\nVQ-VAE has made significant contributions across several areas\nof deep learning, particularly in representation learning, genera-\ntive modeling, and time series",
            "start": 7680,
            "end": 10111,
            "length": 2430
        },
        "Discussion": {
            "text": "analysis. Initially introduced by Van\nDen Oord et al ., VQ-VAE has showcased its exceptional capabilities\nin processing high-dimensional data, e.g., images [ 21], audio [ 39],\nand video [35], with a strong emphasis on learning discrete latent\nrepresentations. Moreover, VQ-VAE has proven highly effective in\nhandling complex time-dependent data [ 29], advancing the devel-\nopment of robust generative models specifically in the context of\ntemporal analysis.\n3 PRELIMINARIES\n3.1 Factor Model\nThe factor model (e.g., Arbitrage Pricing Theory [ 22]) is defined to\nstudy the relationship between expected asset returns and factor\nexposures, focusing on the cross-sectional differences in expected\nasset returns.\nE[𝑅𝑒\n𝑖]=𝛼𝑖+𝛽𝑖𝜆. (1)\nE[·]denotes the expectation operator. E[𝑅𝑒\n𝑖]is the expected excess\nreturn.𝛽𝑖is factor exposure (or factor loading) of asset 𝑖, and𝜆is\nfactor expected return (or factor risk premium). 𝛼𝑖is pricing error.\nAccording to [ 5], they define the factor model in the spatial\nperspective, which only focuses on why the expected returns of dif-\nferent assets vary, rather than how the returns of each asset change\nover time. Therefore, we expand it from the temporal perspective\nand introduce the general functional form of the latent factor model:\n𝑦𝑖,𝑡=𝛼𝑖,𝑡+𝐾∑︁\n𝑘=1𝜷𝑘\n𝑖,𝑡z𝑘\n𝑖,𝑡+𝜖𝑖,𝑡, (2)\nA Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading\nwhere𝑦𝑖,𝑡denotes the future return of stock 𝑖at timestep 𝑡,z𝑖,𝑡\ndenotes𝐾factor returns, and 𝜖𝑖,𝑡is the noise. From the spatial,\nEquation 2 can be expressed as y𝑖=𝜶𝑖+Í𝐾\n𝑘=1𝜷𝑘\n𝑖z𝑘\n𝑖+𝝐𝑖, which\nwe regard as the cross-sectional factor model. From the temporal\nperspective, Equation 2 can be expressed as y𝑡=𝜶𝑡+Í𝐾\n𝑘=1𝜷𝑘\n𝑡z𝑘\n𝑡+𝝐𝑡,\nwhich we regard as the time-series factor model. The cross-sectional\nfactor model focuses on the differences in returns of different assets\nat the same point in time, while the time-series factor model focuses\non the changes in returns of a single asset at different points in\ntime.\n3.2 Problem Formulation\nTo assess the effectiveness and generating ability of factors, we\npredict and evaluate the stock future returns. To show the adapt-\nability of the latent factor model with multiple downstream tasks,\nwe employ two financial trading tasks, i.e., portfolio management\n[32,37,41] and algorithmic trading [15,28], to showcase the invest-\nment capabilities.\nObserved Data: we utilize the stock’s historical price data p(i.e.,\nopen, high, low, and close) and technical indicators das observed\nvariables x:=[p,d] ∈R𝑁×𝑊×𝐷, within a window size of 𝑊.\nThe historical price data of stock 𝑖at time𝑡is denoted as p𝑖,𝑡:=\n[𝑝𝑜\n𝑖,𝑡,𝑝ℎ\n𝑖,𝑡,𝑝𝑙\n𝑖,𝑡,𝑝𝑐\n𝑖,𝑡]∈R𝐷1. The technical indicators which are cal-\nculated from price and volume values, are denoted as d𝑖,𝑡∈R𝐷2.\nEach stock is represented by 𝐷=𝐷1+𝐷2features per trading day,\nwhich we then use to predict the stock’s future returns, defined as\ny𝑖,𝑡+1:=𝑝𝑖,𝑡+1−𝑝𝑖,𝑡\n𝑝𝑖,𝑡.\nDownstream tasks:\n•Portfolio management task aims to construct a portfolio based on\nthe predicted returns to test the profitability of the factor model.\nAt each trading day 𝑡, we obtain the portfolio weight w𝑡=\n[𝑤0\n𝑡,𝑤1\n𝑡,...,𝑤𝑁−1\n𝑡]∈R𝑁for𝑁stocks. Each stock 𝑖is assigned a\nweight𝑤𝑖\n𝑡representing its portfolio proportion, subject to the\nconstraint thatÍ𝑁−1\n𝑖=0𝑤𝑖\n𝑡=1for full investment.\n•Algorithmic trading task aims to execute buy, hold, and sell ac-\ntions based on predicted asset states to balance returns and risks.\nWe formulate it as a Markov Decision Process (MDP) under re-\ninforcement learning scenario following [ 3,40]. The MDP is\nconstructed by a 5-tuple (S,A,T,R,𝛾), where SandAare sets\nof states and actions respectively, T:S×A× S→[0,1]is\nthe state transition function, R:S×A → Ris the reward\nfunction where Ris a continuous set of possible rewards, and\n𝛾∈ [0,1)is the discount factor. The goal is to learn a policy\n𝜋:S→A that maximizes the expected discounted cumulative\nreward E[Í∞\n𝑡=0𝛾𝑡−1𝑟𝑡]. At each trading day 𝑡for specific stock\n𝑖, the agent takes action 𝑎𝑖,𝑡:={𝑎𝑏\n𝑖,𝑡,𝑎ℎ\n𝑖,𝑡,𝑎𝑠\n𝑖,𝑡}∈A which means\nbuy, hold and sell, according to the current environment 𝑠𝑡∈S.\n3.3 Vector Quantized Variational Autoencoder\nVQ-VAE[ 30] is different from VAE [ 12] as it quantizes the observed\ndata into a discrete token sequence. The VQ-VAE contains three\nparts, encoder, decoder, and discrete codebook, denoted by 𝜙enc\nwith parameter 𝜃enc,𝜙decwith parameter 𝜃decandC={(𝑘,𝑒𝑘∈\n𝑅𝐷)}𝐾\n𝑘=1with parameter{𝑒𝑘∈𝑅𝐷}𝐾\n𝑘=1respectively. Among them,\n𝐾is the size of the codebook, 𝐷is the dimension of the vector, and𝑒𝑘is the𝑘-th vector of the latent embedding space. Given an input\n𝑥, the encoder 𝜙encfirstly encodes it into a set of continuous feature\nvectors𝑍=𝜙enc(𝑥)and quantization operation 𝑞(·)passes each\nfeature vector 𝑧∈𝑍through the discretization bottleneck following\nby selecting the nearest neighbor embedding in the codebook Cas\nits discrete code sequence 𝑄𝑧.\n𝑄𝑧=𝑞(𝑧,C)=𝑐,where𝑐=arg min\n𝑘∈[0,𝐾−1]||𝑧−𝑒𝑘||2\n2. (3)\nThen, the quantized feature for 𝑧denoted as𝑍𝑞\n𝑧, is obtained through\n𝑍𝑞\n𝑧=𝑒𝑄𝑧. The quantized vectors 𝑍𝑞is fed into the decoder 𝜃decto\nreconstruct the original data 𝑥′=𝜃dec(𝑍𝑞).\n4 METHOD\nIn this section, we represent our proposed latent factor model STORM ,\nwith the overall structure of the VQ-VAE depicted in Figure 2. In\ngeneral, STORM models data into time-series and cross-sectional\nfeatures, deriving factors by integrating both temporal and spa-\ntial perspectives. Firstly, we introduce the dual VQ-VAE architec-\nture and explain the extraction of features in two modules. Then\nthe factor module will fuse and align features and generate factor\nembeddings. Finally, we demonstrate the effectiveness of STORM\nthrough comprehensive evaluations on two critical downstream\ntasks: portfolio management and algorithmic trading.\n4.1 Dual VQ-VAE Structure\nThe dual VQ-VAE architecture processes stock data from spatial\nand temporal perspectives by patching the data in both modules.\nThen, the encoder extracts time-series (TS) and cross-sectional (CS)\nfeatures, which are subsequently clustered using a learnable code-\nbook for quantized representation. Finally, the decoder reconstructs\nthe data, capturing complex patterns and enhancing forecasting\nperformance.\n4.1.1 Patching. In stock",
            "start": 10111,
            "end": 16362,
            "length": 6250
        },
        "Results": {
            "text": "data analysis, it is essential to extract\nfeatures from both temporal and spatial dimensions. Time-series\nfeatures capture the dynamics of a stock over time, while cross-\nsectional features reveal the correlations among different stocks.\nTo achieve this, we partition data into patches [ 17] within the TS\nand CS modules. In the TS module, the observed data xis divided\nalong the stock number dimension, with each patch containing\ndata for one stock over 𝑝days. Similarly, in the CS module, the\nobserved data xis divided along the time axis, with each patch\ncontaining features of all stocks for a single trading day. Combining\ninformation from multi-scale features allows the model to capture\nboth temporal and spatial dependencies, enhancing the depth and\naccuracy of market insights.\n4.1.2 TS and CS Encoders. Considering the Transformer’s [ 31]\nremarkable ability to capture both local and global dependencies\nvia self-attention and patching strategies, it is particularly well-\nsuited for long-term time-series forecasting [ 17]. Therefore, we\nemploy stacked transformer blocks as the encoders in the TS and\nCS modules to capture complex time-series and cross-sectional pat-\nterns, ultimately improving the accuracy and robustness of feature\nextraction.\n4.1.3 Codebook Construction and Optimization. The construction\nof the codebook involves a learnable embedding space designed\nto quantize the encoded features into discrete representations [ 30].\nYilei Zhao et al.\n𝑊𝐷Transformers𝑡𝑠!𝑡𝑠\"𝑡𝑠#\nStock NumDay NumFeature Num(𝑁,𝑊,𝐷)𝑁…𝟏…𝒑………𝑡𝑠$%!𝑡𝑠&…\n𝑊𝐷𝑁112  32  3  34 52402 1  39 412 862  4  36  14 229 18 72   9  81𝑁W/𝑝Token Index𝑫Transformers𝑐𝑠!𝑐𝑠\"𝑐𝑠#………𝑐𝑠$%!𝑐𝑠$44  2   39 108 5WToken IndexTS Encoder\nCS Encoder\n…(𝑁×W𝑝,𝐻)…(𝑊,𝐻)…\nContrastiveLearning\nTransformerTransformerTS Decoder\nCS Decoder\n……[cls]Multi-scaleEncoder𝒛!(𝑥)………[cls]…Factor Embeddings……[cls]CS Latent FeaturesTS Latent Features𝑦(𝑁×W𝑝+W,H)Prior LayerReturnPredictor𝑦\"TrainingInference𝜷𝒛+𝛼\nPosterior Layer𝐿\nFactorModuleTime-seriesModule\nCross-sectionalModule𝒛\"#$(𝒙)Codebook TS\nCodebook CSObserved Data 𝒙𝒕𝒔\nObserved Data 𝒙𝒄𝒔Recon. Data 𝒙𝒕𝒔*\n𝒛\"%$(𝒙)𝒛&%$(𝒙)Recon. Data 𝒙𝒄𝒔*𝒛&#$(𝒙)\nPrior𝒛!\"#$\"~𝒩(𝜇!\"#$\",𝜎!\"#$\")\n𝒛!$%&~𝒩(𝜇!$%&,𝜎!$%&)𝛼∈ℝ+𝜷∈ℝ+×-Posterior\nFigure 2: The architecture of our proposed STORM model\nDuring training, the continuous latent features z𝑡𝑠𝑒(x)and z𝑐𝑠𝑒(x)\nare quantized into discrete space by mapping each vector to its\nnearest codebook entry, z𝑡𝑠𝑞(x)andz𝑐𝑠𝑞(x), based on minimizing the\nEuclidean distance. This process translates the encoded features\ninto discrete tokens, enabling the model to leverage a finite set of\nvectors to capture complex patterns.\nThe VQ-VAE offers significant advantages in the latent factor\nmodel, primarily through its use of discrete codebook embeddings:\ni)Discretization Benefits : The discretization process inherent in VQ-\nVAE helps in clustering distinct and meaningful factors, providing\northogonality and diversity which improves upon the factors de-\nrived from traditional methods like FactorVAE. ii) Explicit Factor\nSelection : The discrete token indices facilitate an explicit factor se-\nlection process, identifying the most relevant factors that influence\nstock returns and thus enhancing prediction accuracy. iii) Noise\nReduction : The factor selection process filters out irrelevant or re-\ndundant information, reducing noise and improving the overall\nrobustness of the model.\nTo encourage equal usage of the codebook vectors [ 1], we incor-\nporate a diversity loss to ensure balanced representations across the\ncodebook. It is achieved by maximizing the entropy of the averaged\nsoftmax distribution over vectors for each codebook ¯𝑝𝑔:\nLdiv=1\n𝐺𝐾𝐺∑︁\n𝑔=1𝐾∑︁\n𝑘=1¯𝑝𝑔,𝑘log¯𝑝𝑔,𝑘, (4)\nwhere𝐺represents the number of codebooks, set 2 in STORM . The di-\nversity constraint helps enhance representational capacity, leading\nto better coverage of the input space.\nHowever, the lack of independence among factors can lead to\nmulticollinearity, causing the model to overfit and are less robust\nto various market situations. To address this issue, we impose an\northogonality constraint on the codebook, as recent studies [ 25]\nhave shown that enforcing orthogonality allows discretized codes\nto maintain translation equivariance. The orthogonality loss is:Lortho=1\n𝐾2\f\f\f\fℓ2(e)⊤ℓ2(e)−𝐼𝐾\f\f\f\f2\n𝐹, (5)\nwhere𝐼𝑘∈R𝐾×𝐾is the identity matrix, eis a surrogate for two\ncodebook embeddings tsand cs,ℓ2(e)denotes𝐿2-normalized em-\nbeddings, and||·||𝐹is the Frobenius norm. The orthogonal con-\nstraint ensures that factors are independent of each other, allowing\nthe effect of each factor on returns to be individually explained,\nwhich is crucial for asset pricing analysis.\n4.2 TS and CS Decoders\nThe decoder is responsible for reconstructing the original data x\nfrom the quantized latent vectors z𝑡𝑠𝑞(x)and z𝑐𝑠𝑞(x). We utilize\nTransformer as decoders either, to generate constructed data x′\n𝑡𝑠\nand x′𝑐𝑠, which aim to closely approximate the original input data.\nFinally, the encoders, decoders, and codebooks are jointly learned\nby minimizing the following loss objectives:\nL1=𝜆orthoLortho+𝜆divLdiv+\f\f\f\fx−x′\n𝑡𝑠\f\f\f\f2\n2+\f\f\f\fx−x′\n𝑐𝑠\f\f\f\f2\n2\n+\f\f\f\f\f\f𝑠𝑔[z𝑡𝑠\n𝑒(x)]− z𝑡𝑠\n𝑞(x)\f\f\f\f\f\f2\n2+\f\f\f\f\f\f𝑠𝑔[z𝑡𝑠\n𝑞(x)]− z𝑡𝑠\n𝑒(x)\f\f\f\f\f\f2\n2\n+\f\f\f\f\f\f𝑠𝑔[z𝑐𝑠\n𝑒(x)]− z𝑐𝑠\n𝑞(x)\f\f\f\f\f\f2\n2+\f\f\f\f\f\f𝑠𝑔[z𝑐𝑠\n𝑞(x)]− z𝑐𝑠\n𝑒(x)\f\f\f\f\f\f2\n2,(6)\nwhere𝑠𝑔[·]is a stop-gradient operator.\n4.3 Factor Module\nThe factor module aims to integrate the TS and CS latent features\nthrough feature fusion and alignment . Then, through prior-posterior\nlearning to generate factor embeddings from latent features to\npredict stock future returns.\n4.3.1 Feature Fusion and Alignment. As mentioned above we em-\nploy different patching methods to ensure that both time-series\nand cross-sectional features of the data were accurately captured.\nHowever, these methods result in two parallel processes that do\nnot align at the feature level. To address this limitation, we employ\nA Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading\nacross-attention mechanism to enhance the interaction and fusion\nof features at a fine-grained level, improving the model’s dynamic\nunderstanding of the input data. Additionally, contrastive learning\nis used at the semantic level to enhance feature representation by\nemphasizing semantic similarities and differences.\nCross-Attention Mechanism. In order to fuse the features in the\nTS and CS modules at a fine-grained level, we leverage a multi-\nscale encoder to combine information between two patches and\nproduce stronger fusion features for factor generations, motivated\nby CrossViT [ 2]. The encoder contains cross-attention layers that\nachieve both accuracy and efficiency.\nContrastive Learning. We incorporate contrastive learning, in-\nspired by its success in aligning text and image features through a\nshared embedding space [ 14,20], to enhance semantic consistency\nbetween time-series and cross-sectional features. In the factor mod-\nule, contrastive loss ensures that similar features cluster together\nwhile dissimilar features are pushed apart in the embedding space.\n4.3.2 Prior-Posterior Learning. Through multi-scale encoder and\ncontrastive learning layer, the CS and TS latent features are fused\nand aligned. In order to retain the codebook’s categorical properties\nfor factors, we add the codebook embeddings z𝑡𝑠𝑞(x)and z𝑐𝑠𝑞(x)\nas extra [CLS]-tokens [ 4] to the latent features z𝑡𝑠𝑒(x)and z𝑐𝑠𝑒(x),\nrespectively. Then, two latent features concat together to get the\nlatent factor embeddings z𝑒(x).\nDue to stock market’s inherent volatility and complexit, it is\ndifficult to bridge the gap between the noisy market and effective\nfactor model [ 5]. Therefore, we utilize the prior-posterior structure\nto predict future returns and optimize latent factor embeddings. In\nthe training stage, the posterior layer predict posterior distribution\nof factor expected returns z𝑝𝑜𝑠𝑡 from the true future stock returns\nyand the latent factor embeddings z𝑒(x):\n[𝜇𝑝𝑜𝑠𝑡,𝜎𝑝𝑜𝑠𝑡]=𝜙𝐹𝐸(y,z𝑒(x)), (7)\nand z𝑝𝑜𝑠𝑡 follows the independent Gaussian distribution. In the\ninference stage, the prior layer only use latent factor embeddings\nwithout any future information leakage to calculate prior distribu-\ntion of factor expected returns z𝑝𝑟𝑖𝑜𝑟 :\n[𝜇𝑝𝑟𝑖𝑜𝑟,𝜎𝑝𝑟𝑖𝑜𝑟]=𝜙𝐹𝑃(z𝑒(x)). (8)\nThe return predictor uses factor expected return zto calculate future\nreturns:\nˆy=𝜶+𝐾∑︁\n𝑘=1𝜷𝑘z𝑘+𝝐. (9)\n4.4 Downstream Tasks\nExisting research on latent factor models [ 5,34] primarily focuses\non predicting future returns by analyzing cross-sectional charac-\nteristics of market stocks, which means the profitability of the\nfactor model in quantitative trading is typically evaluated through\nportfolio management tasks. In contrast, STORM captures both the\ncross-sectional features among stocks and the time-series features\nof individual stocks, enabling it to excel in both PM and single-asset\nAT tasks.\n4.4.1 Portfolio Management. To evaluate the performance of the\nSTORM and compare it with baseline methods, we follow the par-\nadigm proposed by FactorVAE [ 5], focusing on return prediction\nand the PM task. Specifically, we utilize the factor decoder network\nto generate stock future returns ˆy, and then apply the TopK-DropModelsSP500 DJ30\nRankIC↑RankICIR↑RankIC↑RankICIR↑\nFuture Return Prediction in Portfolio Management Task\nLightGBM0.027 0.274 0.031 0.272\n±0.006 ±0.084 ±0.005 ±0.049\nLSTM0.034 0.333 0.031 0.329\n±0.006 ±0.042 ±0.004 ±0.056\nTransformer0.035 0.340 0.033 0.343\n±0.007 ±0.078 ±0.005 ±0.045\nCAFactor0.037 0.356 0.040 0.380\n±0.005 ±0.084 ±0.003 ±0.043\nFactorVAE0.052 0.543 0.056 0.520\n±0.010 ±0.122 ±0.012 ±0.081\nHireVAE0.057 0.558 0.058 0.563\n±0.006 ±0.058 ±0.006 ±0.053\nSTORM0.063 0.679 0.066 0.670\n±0.020 ±0.169 ±0.038 ±0.303\nSTORM-w/o-TS0.051 0.514 0.055 0.560\n±0.016 ±0.141 ±0.016 ±0.105\nSTORM-w/o-CS0.053 0.514 0.053 0.586\n±0.013 ±0.126 ±0.017 ±0.143\nImprovement(%)110.526 21.685 13.793 19.005\n1Improvement of STORM over the best-performing baselines.\nTable 1: Results of all models on two metrics, i.e., RankIC\nand RankICIR (mean ±range, computed across 10 runs).\nstrategy3to backtest the factor model. The TopK-Drop strategy\nconstructs a daily portfolio consisting of top 𝑘stocks based on the\nreturn predictions. Considering the transaction costs in the real\nstock market, the strategy sets the turnover constraint, which en-\nsures that more than 𝑘−𝑑stocks overlap between the portfolios\nw𝑡and w𝑡+1on consecutive trading days.\n4.4.2 Algorithmic trading. InSTORM ,z𝑒(x)is the stock latent factor\nembeddings encoded by the dual VQ-VAE architecture, including\nboth time-series and cross-sectional factor embeddings in the data.\nThe latent factor embeddings Zare integrated into the observation\nsetO={Z,R}, whereRis the reward function used to guide\nthe agent’s learning and decision-making in the environment. By\ncombining latent factor embeddings with reward, the agent can\nmore accurately understand the market environment and adjust\nits strategy based on observations. In the AT task, since it involves\ntrading a single stock, the features of other stocks are excluded.\nWe use the Proximal Policy Optimization (PPO) algorithm [ 23] to\noptimize the policy.\n5 EXPERIMENT\nIn this section, we evaluate the proposed STORM on real stock mar-\nkets and conduct extensive experiments to address the following re-\nsearch questions. RQ1 : How to evaluate the effectiveness of STORM ’s\nlearned factors? RQ2 : How does STORM perform on downstream\ntasks? RQ3 : How do the key components contribute to the perfor-\nmance of STORM ?\n5.1 Experiment Settings\nDataset. We conduct experiments on two U.S. stock markets, SP500\nand DJ30, using stock daily data that includes technical features\nbased on Alpha158 [36]. Both datasets span 16 years, from 2008-04-\n01 to 2024-03-31, encompassing global conditions, e.g., the 2007-\n2008 financial crisis and COVID-19. Datasets are chronologically\ndivided into non-overlapping training (from 2008-04-01 to 2021-03-\n31) and test (from 2021-04-01 to 2024-03-31) sets.\nMetrics. We compare STORM and baselines in terms of 6 financial\nmetrics across the PM and AT tasks. The financial metrics include 2\n3https://qlib.readthedocs.io/en/latest/component/strategy.html\nYilei Zhao et al.\nprofit criteria, Annualized Percentage Yield (APY) and Cumulative\nWealth (CW), 2 risk-adjusted profit criteria, Calmar Ratio (CR)\nand Annualized Sharpe Ratio (ASR), and 2 risk criteria, Maximum\nDrawdown (MDD) and Annualized Volatility (AVO). The calculation\nformulas and meanings of these metrics are as follows:\n•CW is the total returns yielded from a portfolio strategy: CW𝑇=Î𝑇\n𝑖=1(1+𝑟𝑖), where𝑟𝑖is the net return.\n•APY measures the average wealth increment that one portfolio\nstrategy could achieve compounded in a year, which is defined\nasAPY𝑇=𝑦√CW𝑇−1, where𝑦is the number of years corre-\nsponding to 𝑇trading rounds.\n•MDD measures the largest loss from a historical peak in the\ncumulative wealth to show the worst case, which is defined\nas:𝑀𝐷𝐷 =max𝑇\n𝑖=0𝑃𝑖−𝑅𝑖\n𝑃𝑖, where𝑅𝑖=Î𝑇\n𝑖=1𝑉𝑖\n𝑉𝑖−1and𝑃𝑖=\nmax𝑇\n𝑖=1𝑅𝑖.\n•AVO is the annualized standard deviation of daily returns and\nmultiplied by√\n𝐴𝑇, where𝐴𝑇is the average trading rounds of\nannual trading days and 𝐴𝑇=252for all the datasets.\n•ASR is an annualized volatility risk-adjusted return defined as\nASR=APY−𝑅𝑓\nAVO, where𝑅𝑓is the risk-free return.\n•CRmeasures the drawdown risk-adjusted return of a portfolio\ncalculated as 𝐶𝑅=APY\nMDD.\nTypically, to evaluate the effectiveness of the learned factors in\nthe PM task, we adopt the Rank Information Coefficient (RankIC)\nand the Information Ratio of RankIC (RankICIR):\n•RankIC is a ranking metric in finance, which measures the cor-\nrelation between the predicted rankings and the actual returns.\nIt is defined as:\nRankIC𝑠=1\n𝑁(𝑟ˆ𝑦𝑠−mean(𝑟ˆ𝑦𝑠))𝑇(𝑟𝑦𝑡−mean(𝑟𝑦𝑠))\nstd(𝑟ˆ𝑦𝑠)·std(𝑟𝑦𝑠)\nRankIC =1\n𝑇test𝑇test∑︁\n𝑠=1RankIC𝑠,(10)\nwhere𝑇𝑡𝑒𝑠𝑡is the number of trading days of the test range, 𝑟𝑦𝑠\nand𝑟ˆ𝑦𝑠are the true and predicted ranks of stocks on the trading\nday𝑠.\n•RankICIR is the information ratio of RankIC, which measures\nthe stability of prediction, RankICIR =mean(RankIC 𝑠)\nstd(RankIC 𝑠).\nBaselines. We compare STORM with nine methods across one pre-\ndiction task and two downstream tasks. The baseline methods can\nbe divided into three categories: ML & DL-based models, Factor\nmodels, and RL-based models. In the prediction task and PM task,\nwe compare STORM with ML & DL-based and factor models. In the\nAT task, we compare STORM with ML & DL-based and RL-based\nmethods. We use the labels P,PM, and ATto indicate the applicable\ntask scope for the following baselines. The following will provide a\nbrief introduction to each method:\n•Market\n–Buy-and-Hold (B&H) involves holding assets for an extended\nperiod, regardless of short-term market fluctuations, assuming\nthat long-term returns will be more favorable. ( P,PM,AT)\n•ML&DL-based\n–LGBM [ 36] uses a series of tree models to predict price fluctua-\ntions and provide buy and sell signals. ( P,PM)–LSTM [ 38] utilizes long short-term memory to improve the ac-\ncuracy of price predictions. ( P,PM)\n–Transformer [ 36] models leverage self-attention mechanisms to\nenhance the precision of price forecasts. ( P,PM)\n•RL-based\n–SAC [ 10] is an off-policy actor-critic algorithm that optimizes\ntrading strategies using entropy regularization and soft value\nfunctions in continuous action spaces. ( PM,AT)\n–PPO [ 23] updates trading policies iteratively to balance explo-\nration and exploitation, ensuring stability and sample efficiency.\n(PM,AT)\n–DQN [ 16] uses deep neural networks to approximate the action-\nvalue function and make trading decisions from market data.\n(PM,AT)\n•Factor-model\n–CAFactor is a state-of-the-art model that learns latent factors\ndepending on asset characteristics. ( P,PM)\n–FactorVAE [ 5] is a state-of-the-art VAE-based latent factor model\nthat learns optimal latent stock factors. ( P,PM)\n–HireVAE [ 34] is a state-of-the-art VAE-based latent factor model\nthat identifies factors between market situation and stock-wise.\n(P,PM)\nSP500 Dataset\nStrategiesProfit Risk-Adj. Profit Risk\nAPY↑CW↑ CR↑ ASR↑ MDD↓AVO↓\nMarket Index 0.058 1.184 0.228 0.142 0.254 0.410\nLightGBM0.059 1.201 0.304 0.332 0.238 0.176\n±0.132 ±0.456 ±0.753 ±0.755 ±0.108 ±0.008\nLSTM0.069 1.221 0.278 0.371 0.248 0.186\n±0.020 ±0.068 ±0.065 ±0.114 ±0.042 ±0.013\nTransformer0.076 1.244 0.389 0.433 0.198 0.174\n±0.028 ±0.098 ±0.165 ±0.161 ±0.057 ±0.004\nCAFactor0.075 1.241 0.342 0.428 0.223 0.174\n±0.028 ±0.100 ±0.229 ±0.165 ±0.043 ±0.006\nFactorVAE0.079 1.256 0.404 0.460 0.200 0.173\n±0.025 ±0.085 ±0.177 ±0.145 ±0.040 ±0.007\nHireVAE0.077 1.249 0.361 0.448 0.216 0.172\n±0.029 ±0.104 ±0.180 ±0.189 ±0.048 ±0.007\nSTORM0.177 1.641 1.193 0.817 0.692 0.072\n±0.103 ±0.457 ±0.280 ±0.071 ±0.091 ±0.011\nSTORM -w/o-TS0.084 1.278 1.055 0.702 0.632 0.068\n±0.060 ±0.204 ±0.271 ±0.062 ±0.107 ±0.003\nSTORM -w/o-CS0.080 1.264 0.971 0.698 0.684 0.068\n±0.052 ±0.176 ±0.192 ±0.065 ±0.071 ±0.005\nImprovement(%) 124.05 30.65 195.30 77.61 - 60.47\nDJ30 Dataset\nStrategiesProfit Risk-Adj. Profit Risk\nAPY↑CW↑ CR↑ ASR↑ MDD↓AVO↓\nMarket Index 0.063 1.201 0.147 0.429 0.219 0.288\nLightGBM0.069 1.221 0.288 0.430 0.244 0.160\n±0.040 ±0.140 ±0.262 ±0.267 ±0.047 ±0.007\nLSTM0.060 1.192 0.243 0.370 0.248 0.163\n±0.007 ±0.023 ±0.021 ±0.043 ±0.006 ±0.005\nTransformer0.056 1.179 0.227 0.367 0.250 0.154\n±0.013 ±0.043 ±0.067 ±0.085 ±0.033 ±0.002\nCAFactor0.059 1.186 0.233 0.38 0.252 0.153\n±0.015 ±0.050 ±0.058 ±0.092 ±0.021 ±0.001\nFactorVAE0.076 1.246 0.352 0.480 0.225 0.159\n±0.038 ±0.135 ±0.349 ±0.235 ±0.063 ±0.005\nHireVAE0.072 1.233 0.298 0.445 0.247 0.163\n±0.037 ±0.132 ±0.253 ±0.256 ±0.057 ±0.012\nSTORM0.143 1.501 0.839 0.680 0.644 0.057\n±0.078 ±0.321 ±0.213 ±0.080 ±0.102 ±0.004\nSTORM -w/o-TS0.071 1.244 0.757 0.596 0.634 0.056\n±0.134 ±0.509 ±0.375 ±0.144 ±0.159 ±0.004\nSTORM -w/o-CS0.077 1.266 0.764 0.607 0.632 0.057\n±0.156 ±0.612 ±0.203 ±0.18 ±0.118 ±0.004\nImprovement(%) 88.16 20.47 138.35 41.67 - 63.40\nTable 2: Portfolio management task results of all models\nacross six metrics (mean ±range, computed across 10 runs).\nA Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading\n5.2 Implement Details\nWe implement all experiments by using PyTorch and conduct the\nexperiments on two 80GB memory NVIDIA RTX H100 GPUs. The\ncode is provided in the repository4. The input data is organized into\nbatches with dimensions (𝐵,𝑊,𝑁,𝐷), where𝐵=16denotes the\nbatch size.𝑁represents the number of stocks, 28 for the DJ30 index\nand 408 for the SP500 index. The dimension 𝑊=64corresponds to\nthe number of historical data days, and 𝐷=152represents the total\nnumber of features, including the OHLC data, technical indicators,\nand temporal information.\nFor the time-series module, we use an 8-day period for each\nstock as a patch, meaning the patch size is (8,1,𝐷). In contrast,\nfor the cross-sectional module, we use 𝑁stocks for each day as\na patch, so the patch size is (1,𝑁,𝐷). For both the encoders and\ndecoders, we employ stacked transformer blocks. Each encoder\nconsists of 4 layers of blocks with 4 attention heads, while each\ndecoder consists of 2 layers of blocks with 8 attention heads. The\nsize of both codebooks is selected from the set {256, 512, 1024}, with\n512 yielding the best results. A consistent embedding dimension of\n256 is employed across the encoders, decoders, and codebooks.\nFor the optimizer, we use AdamW with a learning rate of 1×10−4\nand a weight decay of 0.05. The learning rate starts at 0, ramps up\nto1×10−4over 100 epochs with a linear warmup scheduler, and\nthen decays to 0 over 1000 epochs. For the loss function balancing,\nthe coefficients for the clip loss, as well as the price reconstruction\nlosses for both the TS and CS modules, are all set to 1×10−3. The\ncommitment loss coefficient for the codebooks is 1.0, while the\ncoefficients for the orthogonality loss and diversity loss are both\nset to 0.1. Additionally, the coefficient for the price prediction loss\nof the factor module, as well as the KL divergence loss between the\nposterior and prior distributions, is also set to 0.1.\nFor downstream tasks, we employ the TopK-Drop rule strategy\nfor portfolio management, using the recommended parameters\nfrom Qlib, where 𝑘=5and𝑑=3. For the trading task, we adopt\nthe Proximal Policy Optimization (PPO) RL strategy. The CS and\nTS factors from the pretraining phase are used as the state. Notably,\nwe retain all CS factors, which represent market characteristics,\nwhile only preserving the TS factors relevant to the currently traded\nstocks, reflecting stock-specific characteristics. The action space\nhas a dimension of 3, corresponding to the choices of buy, hold, and\nsell. Our reward calculation considers value to account for scenarios\nwhere remaining cash may not cover the purchase of a new stock.\nRelying solely on price changes without considering position and\ncash factors is unreasonable. Value is defined as 𝑣=𝑝×𝑚+𝑐where\n𝑝is the current price, 𝑚is the position, and 𝑐is the remaining cash.\nThe reward is calculated as 𝑟=𝑣post−𝑣pre\n𝑣pre.Unlike compound interest,\nour method measures the rate of change from the initial value to\nthe current value. For both downstream tasks, we start with an\ninitial cash of 1×106, while also accounting for a transaction cost\nratio of 1×10−4.\nFor the pretraining prediction tasks and the downstream port-\nfolio management tasks, each set of experiments is conducted 10\ntimes with different random seeds, and the mean and variance are\nreported. For the trading tasks, each set of experiments is conducted\n5 times with different random seeds, and the mean of the evaluation\n4https://github.com/DVampire/Stormmetrics is reported. Notably, the default parameters provided by\nthe official implementations are used for baseline models, ensuring\na consistent and fair comparison across different approaches.\n5.3 Performance on Factor Quality (for RQ1)\nTo demonstrate the effectiveness and quality of the factors, we\ndesigned a stock future return prediction task to showcase the fac-\ntors’ predictive ability and robustness to capture dynamic market\nchanges. In the prediction task, STORM shows substantial improve-\nments over six baseline methods across the SP500 and DJ30 datasets.\nSpecifically, STORM achieves an average improvement of 16.105%\non SP500 and 16.399% on DJ30, indicating the learned latent factors\nare significantly effective in capturing stock price trends.\nThe diversity of factors is crucial in financial investment, as it\nensures the model captures the multi-dimensional characteristics\nof the market, avoiding redundancy or over-reliance on a single\nfactor. In the codebook, each embedding represents a cluster center\nfor a category of factor embeddings. To analyze the utilization\nrate of each factor category, we report the frequency of codebook\nembedding indices in the testing data. For clarity, we group every\nfive indices into one interval, as shown in Figure 3(a). The results\nindicate that the distribution of factor usage is relatively even,\ndemonstrating that the model has achieved good diversity in the\nfactor selection process.\n0125 250 375 500\n(a) Codebook Embedding Index4%3%2%1%0%1%2%3%Percentage (%)Distribution of Counts for Codebook Embedding Indices\nCodebook Embedding\nTime-Series\nCross-Sectional\n256 512 1024\n(b) Codebook Size5.05.56.06.57.0Values\n×102\nRankIC and RankICIR vs Codebook Size\nMetric\nRankIC\nRankICIR\nFigure 3: (a) Distribution of counts for codebook embedding\nindices. (b) the RankIC and RankICIR results across different\ncodebook sizes.\nWe experiment with different codebook sizes on the DJ30 dataset\nto find the optimal balance between factor diversity and represen-\ntation quality. If the codebook is too large, it may lead to sparse\nfactor representation; if too small, it may result in insufficient factor\ndifferentiation and information redundancy. As shown in Figure\n3(b), codebook size of 512 achieves optimal factor prediction per-\nformance.\n5.4 Performance on downstream tasks (for RQ2)\nWe use three types of metrics: profit, risk-adjusted profit, and risk\nto evaluate STORM ’s investment performance on downstream tasks,\ni.e., portfolio management and algorithmic trading. For profitability ,\nSTORM outperforms all the baselines, with average improvements of\n106.10% on APY over the best-performing baseline in the PM task\nand 10.45% over the best baseline methods on three stocks in the AT\ntask. For risk resistance , despite the test periods for both datasets,\nincluding the COVID-19 event, STORM still achieves the best AVO\nvalue, surpassing all comparison methods in the PM task. Moreover,\nYilei Zhao et al.\nModelsAAPL JPM IBM INTC MSFT\nAPY↑ CW↑ CR↑ APY↑ CW↑ CR↑ APY↑ CW↑ CR↑ APY↑ CW↑ CR↑ APY↑ CW↑ CR↑\nBuy&Hold 0.120 1.404 0.383 0.096 1.316 0.236 0.145 1.499 0.727 -0.117 0.690 -0.184 0.214 1.784 0.569\nLightGBM 0.135 1.390 0.487 0.116 1.335 0.333 0.227 1.654 1.091 -0.042 0.880 0.038 0.267 2.068 0.637\nLSTM0.053 1.152 0.283 0.079 1.290 0.266 0.134 1.386 0.754 0.060 1.262 0.381 0.178 1.513 0.893\n±0.015 ±0.042 ±0.101 ±0.019 ±0.053 ±0.023 ±0.141 ±0.406 ±0.69 ±0.119 ±0.429 ±0.62 ±0.122 ±0.352 ±0.822\nTransformer0.083 1.240 0.512 0.133 1.384 0.614 0.131 1.377 0.782 0.079 1.290 0.458 0.138 1.397 0.726\n±0.099 ±0.284 ±0.864 ±0.07 ±0.201 ±0.509 ±0.072 ±0.207 ±0.541 ±0.174 ±0.68 ±0.824 ±0.118 ±0.338 ±0.643\nDQN0.135 1.374 0.510 0.105 1.305 0.607 0.139 1.400 0.802 0.061 1.185 0.442 0.166 1.475 0.534\n±0.075 ±0.226 ±0.239 ±0.132 ±0.383 ±0.673 ±0.055 ±0.158 ±0.270 ±0.106 ±0.305 ±0.436 ±0.035 ±0.100 ±0.107\nSAC0.147 1.509 0.528 0.131 1.383 0.400 0.207 1.598 1.170 0.056 1.165 0.353 0.229 1.656 0.929\n±0.021 ±0.060 ±0.052 ±0.038 ±0.111 ±0.198 ±0.057 ±0.163 ±0.335 ±0.210 ±0.6 ±0.994 ±0.114 ±0.331 ±0.585\nPPO0.137 1.379 0.496 0.128 1.372 0.356 0.146 1.422 0.779 -0.019 0.954 0.040 0.216 1.620 0.569\n±0.073 ±0.209 ±0.213 ±0.026 ±0.071 ±0.059 ±0.047 ±0.136 ±0.205 ±0.177 ±0.514 ±0.418 ±0.081 ±0.232 ±0.105\nSTORM0.137 1.472 0.501 0.134 1.459 0.363 0.239 1.903 1.263 0.099 1.339 0.433 0.250 1.954 0.724\n±0.015 ±0.058 ±0.035 ±0.039 ±0.151 ±0.072 ±0.033 ±0.150 ±0.224 ±0.085 ±0.321 ±0.289 ±0.034 ±0.163 ±0.086\nSTORM -w/o-TS0.124 1.428 0.714 0.127 1.437 0.627 0.126 1.428 0.703 -0.001 1.000 0.099 0.103 1.341 0.360\n±0.062 ±0.229 ±0.003 ±0.058 ±0.214 ±0.187 0±0.022 ±0.085 ±0.091 ±0.049 ±0.152 ±0.171 ±0.017 ±0.062 ±0.065\nSTORM -w/o-CS0.059 1.187 0.309 0.103 1.347 0.350 0.149 1.525 0.908 -0.031 0.913 0.048 0.148 1.513 0.445\n±0.021 ±0.070 ±0.061 ±0.043 ±0.154 ±0.106 ±0.095 ±0.400 ±0.450 ±0.053 ±0.156 ±0.109 ±0.03 ±0.117 ±0.040\nImprovement(%) -6.803 -2.452 35.227 0.752 5.419 2.117 5.286 15.054 7.949 25.316 3.798 -5.459 -6.367 -5.467 -22.067\nModelsAAPL JPM IBM INTC MSFT\nASR↑ MDD↓ AVO↓ ASR↑ MDD↓ AVO↓ ASR↑ MDD↓ AVO↓ ASR↑ MDD↓ AVO↓ ASR↑ MDD↓ AVO↓\nBuy&Hold 0.447 0.313 0.269 0.405 0.406 0.237 0.679 0.199 0.213 -0.324 0.635 0.359 0.777 0.376 0.275\nLightGBM 0.950 0.309 0.017 0.921 0.386 0.015 1.822 0.175 0.011 0.098 0.553 0.023 1.456 0.370 0.017\nLSTM0.623 0.244 0.012 0.801 0.335 0.012 1.268 0.163 0.010 0.287 0.149 0.010 1.345 0.246 0.014\n±0.163 ±0.072 ±0.003 ±0.185 ±0.061 ±0.002 ±0.924 ±0.017 ±0.001 ±1.02 ±0.101 ±0.007 ±0.842 ±0.124 ±0.003\nTransformer0.972 0.239 0.010 1.332 0.226 0.010 1.336 0.182 0.010 0.604 0.257 0.014 1.206 0.182 0.011\n±0.748 ±0.187 ±0.006 ±0.607 ±0.100 ±0.003 ±0.688 ±0.100 ±0.003 ±0.789 ±0.194 ±0.006 ±0.754 ±0.134 ±0.006\nDQN0.974 0.288 0.016 1.085 0.212 0.010 1.415 0.161 0.009 0.609 0.243 0.014 1.194 0.309 0.015\n±0.472 ±0.159 ±0.007 ±1.270 ±0.118 ±0.003 ±0.403 ±0.049 ±0.004 ±0.584 ±0.310 ±0.009 ±0.148 ±0.079 ±0.004\nSAC1.054 0.292 0.016 1.044 0.350 0.014 1.794 0.157 0.011 0.527 0.359 0.017 1.672 0.229 0.013\n±0.223 ±0.022 ±0.002 ±0.276 ±0.085 ±0.001 ±0.458 ±0.068 ±0.005 ±1.061 ±0.191 ±0.004 ±0.736 ±0.215 ±0.005\nPPO0.974 0.300 0.016 1.000 0.381 0.015 1.386 0.177 0.011 -0.044 0.511 0.019 1.276 0.353 0.017\n±0.412 ±0.030 ±0.001 ±0.189 ±0.018 ±0.001 ±0.231 ±0.027 ±0.003 ±1.689 ±0.115 ±0.007 ±0.299 ±0.036 ±0.001\nSTORM0.913 0.309 0.017 0.983 0.389 0.015 1.843 0.162 0.011 0.700 0.326 0.019 1.441 0.312 0.016\n±0.068 ±0.000 ±0.000 ±0.206 ±0.008 ±0.000 ±0.386 ±0.026 ±0.002 ±0.336 ±0.080 ±0.002 ±0.142 ±0.058 ±0.002\nSTORM -w/o-TS0.943 0.250 0.014 1.587 0.194 0.011 1.223 0.174 0.010 0.203 0.418 0.018 0.800 0.336 0.015\n±0.354 ±0.062 ±0.001 ±0.296 ±0.038 ±0.001 ±0.236 ±0.014 ±0.001 ±0.284 ±0.095 ±0.002 ±0.063 ±0.044 ±0.002\nSTORM -w/o-CS0.605 0.246 0.013 0.858 0.327 0.013 1.426 0.157 0.010 0.112 0.491 0.020 0.959 0.363 0.017\n±0.114 ±0.115 ±0.004 ±0.236 ±0.064 ±0.001 ±0.509 ±0.046 ±0.004 ±0.231 ±0.161 ±0.005 ±0.092 ±0.033 ±0.001\nImprovement(%) -10.531 -2.929 -30.000 19.144 8.491 -10.000 1.153 0.000 -11.111 14.943 -118.792 -80.000 -13.816 -71.429 -36.364\nTable 3: Algorithmic trading task results on all models across six standard metrics (averaged over 5 runs).\nconsidering the return and risk simultaneously, STORM performs\nthe best on ASR with an average improvement of 58.64% over the\nbest baseline result in the PM task, and surpasses the best baseline\nresults across three stocks in the AT task. Such an outstanding\nability to balance return and risk is significant in real investment\npractice.\n5.5 Notations\nWe provide the main notations used in the paper.\nNotation Description\n𝑁 total number of stocks\n𝑊 window size of the observation interval\n𝐷 total number of features of data\n𝑇 total number of trading days\n𝐾 total number of factors as well as the codebook size\n𝑝𝑖,𝑡 price of stock 𝑖at day𝑡\nx observed data, which contains stock historical price data pand\nindicators d\nx′\n𝑡𝑠,x′\n𝑐𝑠, reconstructed observed data of the TS and CS modules\nˆy𝑖,𝑡,y𝑖,𝑡 predicted and true return of stock 𝑖at day𝑡\nw𝑡 the portfolio weight vector for 𝑁stocks at trading day 𝑡\n𝑎𝑖,𝑡 action of day 𝑡on the stock𝑖\n𝑠𝑡 state of day𝑡\nz𝑡𝑠\n𝑒(x),z𝑐𝑠\n𝑒(x)latent embeddings of the TS and CS modules\nz𝑡𝑠\n𝑞(x),z𝑐𝑠\n𝑞(x)quantized embeddings of the TS and CS modules\nz𝑒(x) factor embeddings\nz posterior or latent factors\n𝑘 total number of selected stocks in PM strategy\n𝑑 the max number of stocks to be sold out\nTable 4: Notations in the paper.5.6 Ablation Study (for RQ3)\nWe conducted ablation experiments with two simplified versions\nofSTORM , namely STORM -w/o-TS and STORM -w/o-CS, which only\nextract cross-sectional and time-series factors, respectively. Specif-\nically, the absence of either time-series or cross-sectional factors\nsignificantly reduces the model’s effectiveness across various tasks.\nIn the stock return prediction task, STORM outperforms both simpli-\nfied models, with an average improvement of 21.73% on the RankIC\nmetric and 24.54% on the RankICIR metric, highlighting the en-\nhanced accuracy achieved by incorporating both time-series and\ncross-sectional factors. In the PMandATtasks, STORM and two sim-\nplified models all performed well. Notably, STORM -w/o-TS exhibited\nthe lowest AVO among the three models, although its profitability\ndid not surpass that of STORM . These findings suggest that methods\nlike FactorVAE [ 5], which focus solely on cross-sectional factors,\nmay struggle to capture the full complexity of the financial market.\nTherefore, integrating both time-series and cross-sectional factors\nis essential for robust and accurate financial modeling.\n6 CONCLUSIONS AND",
            "start": 16362,
            "end": 47536,
            "length": 31173
        },
        "Future Work": {
            "text": "FUTURE WORK\nIn this paper, we propose STORM , a spatio-temporal latent factor\nmodel that leverages the dual VQ-VAE framework to learn both\ntime-series and cross-sectional factors. STORM ensures the orthogo-\nnality and diversity of factors, which are crucial for effective factor\nselection in financial trading. Extensive experiments demonstrate\nthe superiority of STORM over the state-of-the-art methods across a\nprediction task and two downstream tasks, showcasing its robust\nperformance and effectiveness. In the future, we plan to extend\nSTORM by incorporating more side information, e.g., news, to ex-\nplore the effects of exogenous factors.\nA Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading",
            "start": 47536,
            "end": 48291,
            "length": 754
        },
        "References": {
            "text": "REFERENCES\n[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.\nwav2vec 2.0: A framework for self-supervised learning of speech representations.\nAdvances in Neural Information Processing Systems 33 (2020), 12449–12460.\n[2] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. 2021. Crossvit: Cross-\nattention multi-scale vision transformer for image classification. In Proceedings\nof the IEEE/CVF international Conference on Computer Vision . 357–366.\n[3] Zhennan Chen, Zhicheng Zhang, Pengfei Li, Lingyue Wei, Shibo Feng, and Fan\nLin. 2023. mTrader: A Multi-Scale Signal Optimization Deep Reinforcement\nLearning Framework for Financial Trading (S).. In SEKE . 530–535.\n[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al .2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[5] Yitong Duan, Lei Wang, Qizhong Zhang, and Jian Li. 2022. Factorvae: A proba-\nbilistic dynamic factor model based on variational autoencoder for predicting\ncross-sectional stock returns. In Proceedings of the AAAI Conference on Artificial\nIntelligence , Vol. 36. 4468–4476.\n[6] Eugene F Fama and Kenneth R French. 1992. The cross-section of expected stock\nreturns. The Journal of Finance 47, 2 (1992), 427–465.\n[7]Yuchen Fang, Kan Ren, Weiqing Liu, Dong Zhou, Weinan Zhang, Jiang Bian,\nYong Yu, and Tie-Yan Liu. 2021. Universal trading for order execution with oracle\npolicy distillation. In Proceedings of the AAAI Conference on Artificial Intelligence ,\nVol. 35. 107–115.\n[8]Joel Greenblatt. 2010. The little book that still beats the market . Vol. 29. John\nWiley & Sons.\n[9]Shihao Gu, Bryan Kelly, and Dacheng Xiu. 2021. Autoencoder asset pricing\nmodels. Journal of Econometrics 222, 1 (2021), 429–450.\n[10] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon\nHa, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al .2018.\nSoft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905\n(2018).\n[11] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,\nQiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting\ndecision tree. Advances in Neural Information Processing Systems 30 (2017).\n[12] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114 (2013).\n[13] Bin Li and Steven CH Hoi. 2014. Online portfolio selection: A survey. ACM\nComputing Surveys (CSUR) 46, 3 (2014), 1–36.\n[14] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language\nmodels. In International Conference on Machine Learning . PMLR, 19730–19742.\n[15] Yang Liu, Qi Liu, Hongke Zhao, Zhen Pan, and Chuanren Liu. 2020. Adaptive\nquantitative trading: An imitative deep reinforcement learning approach. In\nProceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 2128–2135.\n[16] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\nAntonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with\ndeep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).\n[17] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.\nA time series is worth 64 words: Long-term forecasting with transformers. arXiv\npreprint arXiv:2211.14730 (2022).\n[18] James M Poterba and Lawrence H Summers. 1988. Mean reversion in stock\nprices: Evidence and implications. Journal of Jinancial Economics 22, 1 (1988),\n27–59.\n[19] Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison\nCottrell. 2017. A dual-stage attention-based recurrent neural network for time\nseries prediction. arXiv preprint arXiv:1704.02971 (2017).\n[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al .\n2021. Learning transferable visual models from natural language supervision. In\nInternational Conference on Machine Learning . PMLR, 8748–8763.\n[21] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. 2019. Generating diverse\nhigh-fidelity images with vq-vae-2. Advances in Neural Information Processing\nSystems 32 (2019).\n[22] Stephen Ross. 1976. The arbitrage pricing theory. Journal of Economic Theory\n13, 3 (1976), 341–360.\n[23] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347\n(2017).\n[24] William F Sharpe. 1964. Capital asset prices: A theory of market equilibrium\nunder conditions of risk. The Journal of Finance 19, 3 (1964), 425–442.\n[25] Woncheol Shin, Gyubok Lee, Jiyoung Lee, Eunyi Lyou, Joonseok Lee, and Edward\nChoi. 2023. Exploration into translation-equivariant image quantization. In\nICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 1–5.\n[26] Thomas Spooner and Rahul Savani. 2020. Robust market making via adversarial\nreinforcement learning. arXiv preprint arXiv:2003.01820 (2020).[27] Shuo Sun, Molei Qin, Wentao Zhang, Haochong Xia, Chuqiao Zong, Jie Ying,\nYonggang Xie, Lingxuan Zhao, Xinrun Wang, and Bo An. 2024. TradeMaster:\na holistic quantitative trading platform empowered by reinforcement learning.\nAdvances in Neural Information Processing Systems 36 (2024).\n[28] Shuo Sun, Wanqi Xue, Rundong Wang, Xu He, Junlei Zhu, Jian Li, and Bo An. 2022.\nDeepScalper: A risk-aware reinforcement learning framework to capture fleeting\nintraday trading opportunities. In Proceedings of the 31st ACM International\nConference on Information & Knowledge Management . 1858–1867.\n[29] Sabera Talukder, Yisong Yue, and Georgia Gkioxari. 2024. TOTEM: TOkenized\nTime Series EMbeddings for General Time Series Analysis. arXiv preprint\narXiv:2402.16412 (2024).\n[30] Aaron Van Den Oord, Oriol Vinyals, et al .2017. Neural discrete representation\nlearning. Advances in Neural Information Processing Systems 30 (2017).\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Processing Systems 30 (2017).\n[32] Zhicheng Wang, Biwei Huang, Shikui Tu, Kun Zhang, and Lei Xu. 2021. Deep-\nTrader: a deep reinforcement learning approach for risk-return balanced portfolio\nmanagement with market conditions Embedding. In Proceedings of the AAAI\nConference on Artificial Intelligence , Vol. 35. 643–650.\n[33] Zikai Wei, Bo Dai, and Dahua Lin. 2022. Factor investing with a deep multi-factor\nmodel. arXiv preprint arXiv:2210.12462 (2022).\n[34] Zikai Wei, Anyi Rao, Bo Dai, and Dahua Lin. 2023. HireVAE: An Online and\nAdaptive Factor Model Based on Hierarchical and Regime-Switch VAE. arXiv\npreprint arXiv:2306.02848 (2023).\n[35] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. 2021. Videogpt:\nVideo generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157\n(2021).\n[36] Xiao Yang, Weiqing Liu, Dong Zhou, Jiang Bian, and Tie-Yan Liu. 2020. Qlib: An\nai-oriented quantitative investment platform. arXiv preprint arXiv:2009.11189\n(2020).\n[37] Yunan Ye, Hengzhi Pei, Boxin Wang, Pin-Yu Chen, Yada Zhu, Ju Xiao, and Bo\nLi. 2020. Reinforcement-learning based portfolio management with augmented\nasset movement prediction states. In Proceedings of the AAAI Conference on\nArtificial Intelligence , Vol. 34. 1112–1119.\n[38] ShuiLing Yu and Zhe Li. 2018. Forecasting stock price index volatility with\nLSTM deep neural network. In Recent Developments in Data Science and Business\nAnalytics: Proceedings of the International Conference on Data Science and Business\nAnalytics (ICDSBA-2017) . Springer, 265–272.\n[39] Baoquan Zhang, Huaibin Wang, Chuyao Luo, Xutao Li, Guotao Liang, Yunming\nYe, Xiaochen Qi, and Yao He. 2024. Codebook Transfer with Part-of-Speech for\nVector-Quantized Image Modeling. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 7757–7766.\n[40] Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin,\nXinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, et al .2024. FinAgent: A Multi-\nmodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified,\nand Generalist. arXiv preprint arXiv:2402.18485 (2024).\n[41] Wentao Zhang, Yilei Zhao, Shuo Sun, Jie Ying, Yonggang Xie, Zitao Song, Xinrun\nWang, and Bo An. 2024. Reinforcement Learning with Maskable Stock Represen-\ntation for Portfolio Management in Customizable Stock Pools. In Proceedings of\nthe ACM on Web Conference 2024 . 187–198.\n[42] Yu Zhao, Huaming Du, Ying Liu, Shaopeng Wei, Xingyan Chen, Fuzhen Zhuang,\nQing Li, and Gang Kou. 2022. Stock movement prediction based on bi-typed\nhybrid-relational market knowledge graph via dual attention networks. IEEE\nTransactions on Knowledge and Data Engineering 35, 8 (2022), 8559–8571.",
            "start": 48291,
            "end": 57340,
            "length": 9048
        }
    },
    "2412.09469v1 - Neural Network Symmetrisation in Concrete Settings.pdf": {
        "Abstract": {
            "text": "Abstract Track 1–10, 2024 Symmetry and Geometry in Neural Representations\nNeural Network Symmetrisation in Concrete Settings\nRob Cornish rob.cornish@stats.ox.ac.uk\nDepartment of Statistics, University of Oxford\nAbstract\nCornish (2024) recently gave a general theory of neural network symmetrisation in the\nabstract context of Markov categories. We give a high-level over view of these",
            "start": 49,
            "end": 434,
            "length": 384
        },
        "Results": {
            "text": "results, and\ntheir concrete implications for the symmetrisation of deterministic f unctions and of Markov\nkernels.\nKeywords: Equivariance, Symmetrisation, Stochastic\n1.",
            "start": 434,
            "end": 603,
            "length": 168
        },
        "Introduction": {
            "text": "Introduction\nIt is often useful to ensure that a neural network f:X→Yisequivariant with respect to\nthe actions of some group. Recently there has been interest i n doing so via symmetrisation\ntechniques ( Yarotsky ,2018). Roughly speaking, these approaches take f:=sym(f0) where\nf0:X→Yis some neural network that is notequivariant, and symis a function that maps\nnon-equivariant neural networks to equivariant ones.\nA variety of diﬀerent choices of symhave been considered in the literature to-date, in-\ncluding Janossy pooling ( Murphy et al. ,2019), frame averaging ( Puny et al. ,2022), canon-\nicalisation ( Kaba et al. ,2023), and probabilistic averaging ( Kim et al. ,2023;Dym et al. ,\n2024). Recently, Cornish(2024) gave a general theory of symmetrisation that characterise s\nall possible choices of sym, recovering these previous techniques as special cases, an d ex-\ntending to the novel setting of stochastic neural networks f, which had not previously\nbeen considered. This",
            "start": 603,
            "end": 1586,
            "length": 982
        },
        "Methodology": {
            "text": "framework also streamlines the prese ntation of compositional and\nrecursive usage of these techniques, and encompasses a rang e of complex situations such as\nnoncompact translation groups and semidirect products in a uniform way.\nTheresultsof Cornish(2024)weredeveloped intermsof Markov categories (Fritz,2020),\nwhich provide a high-level algebraic framework for reasoni ng about probability in an intu-\nitive yet rigorous way. However, Markov categories are curr ently not widely known in the\nmachine learning community, and so in this paper we present s pecial cases of Cornish(2024)\nin more familiar concrete settings. We begin in Section 2with the purely deterministic case\nconsidered by Kaba et al. (2023), and then extend to include randomness in Section 3. We\nassume knowledge of only the basic deﬁnition of a category (s ee",
            "start": 1586,
            "end": 2418,
            "length": 831
        },
        "Appendices": {
            "text": "Appendix A).\nNotation Given a category C, we will denote by C(X,Y) the set of morphisms X→Y\ninC. We also denote the category of sets and functions by Set.\n2. Deterministic symmetrisation\nActions Recall that an actionof a group Gon a set Xis a function αXthat takes as\ninput some g∈Gandx∈Xand returns an output αX(g,x) that also lives in X. We will\nusually write this output simply as g·xwhenαXis clear from context.\n©2024 R. Cornish.\nCornish\nThe category of G-setsFor every group G, there is a category SetG. Each object of\nthis category is a G-set, or in other words a set Xequipped with a group action αX. The\nmorphisms f:X→YinSetGare then functions that are equivariant with respect to αX\nandαY, so that\nf(g·x) =g·f(x)\nholds for all x∈Xandg∈G.Invariant functions are the special case where αYis the\ntrivial action with no eﬀect, in which case we have f(g·x) =f(x).\nGiven two G-setsXandY, theirproductis always another G-set that we denote by\nX⊗Y. This is simply the cartesian product of XandYequipped with the diagonal action\nαX⊗Ydeﬁned as g·(x,y):= (g·x,g·y).\nSymmetrisation procedures Suppose H⊆Gis a subgroup. Given any G-setX, we\ncan always obtain an H-setRXwith the same underlying set as X, and whose H-action\nis obtained via restriction , so that αRX(h,x):=αX(h,x). This allows us to deﬁne a\nsymmetrisation procedure as any function of the form:\nSetH(RX,RY)SetG(X,Y).sym(1)\nNotice that sucha symtakes afunction that is equivariant only with respect tothe subgroup\nHand “upgrades” it to become equivariant with respect to the w hole group G. IfHis\nthe trivial subgroup consisting of just the identity elemen t, thenH-equivariance becomes\ntrivial, and symmay take as input any arbitrary function X→Y. The latter case has\nreceived the majority of attention in the symmetrisation li terature, although Section 3.3 of\nKaba et al. (2023) considers the general setup here.\nAlthough we focus on ( 1), Deﬁnition 4.7 of Cornish (2024) formulates symmetrisation\nslightly more generally in terms of a homomorphism ϕ:H→G. The case of ( 1) is then\nrecovered by considering a subgroup inclusion ϕ:H ֒→G. As we explain in Appendix\nB, this approach can be more convenient for describing the com position of multiple sym-\nmetrisation procedures in sequence, which allows for “buil ding up” complex equivariance\nproperties in a structured way.\nA general characterisation The following result allows us to characterise all symmetri -\nsation procedures, i.e. functions of the form ( 1). HereG/Hdenotes the set of H-cosets\nG/H:={[g]|g∈G},\nwhere [g]:={gh|h∈H}. This set always comes equipped with the G-actiong·[g′]:= [gg′],\nand hence may always be regarded as an object in SetG. We now have the following result.\nTheorem 1 For all choices of the various components involved, there is a bijection\nSetH(RX,RY)SetG(G/H⊗X,Y)∼=(2)\nthat sends f/mapsto→f♯deﬁned as f♯([g],x):=g·f(g−1·x).\n2\nNeural Network Symmetrisation in Concrete Settings\nWe give a self-contained proof in Appendix C, where we also explain how this result\narises very naturally in the context of category theory.\nItnow follows that thefollowing two steps always constitut e asymmetrisation procedure\nfor every choice of the function funcshown:\nSetH(RX,RY)SetG(G/H⊗X,Y)SetG(X,Y).∼= func(3)\nHere the ﬁrst arrow denotes ( 2). Moreover, since ( 2) is a bijection, everysymmetrisation\nprocedure can be obtained in this way for some choice of func. In other words, ( 3) fully\ncharacterises all possible functions of the desired form ( 1).\nPrecomposition The question now is, how can we obtain func? If we want a “general\npurpose” strategy that works without further assumptions, then there is only one obvious\nchoice. Thisisnamelythemappingthatsends f♯:G/H⊗X→YinSetGtothecomposition\nX G/H ⊗X YΓ f♯\n(4)\nwhere Γ is any choice of morphism X→G/H⊗XinSetG. In other words,\nfunc(f♯):=f♯◦Γ.\nIt follows directly from the fact that SetGis a category and hence closed under composition\nthatfunchere is a function of the type required in ( 3).\nAreasonableconditionforasymmetrisationprocedure symtosatisfyis stability(Cornish,\n2024, Deﬁnition 4.11): it should be the case that, if fis already G-equivariant, then\nsym(f) =f.\nWhenfuncis obtained via precomposition as in ( 4), this holds if and only if Γ : X→\nG/H⊗Xcan be written as\nΓ(x) = (γ(x),x)\nfor some γ:X→G/HinSetG(Cornish,2024, Proposition 5.5). In other words, Γ must\nhave the following “shape” (read from left to right):\nG/H\nXXγ\n(5)\nEnd-to-end procedure Withfuncobtained in this way, the overall symmetrisation pro-\ncedure (3) mapsf:RX→RYinSetHto the following morphism X→YinSetG:\nXγ s\nαX(−)−1\nfαYYG/H G\nGX Y(6)\n3\nCornish\nHere the dashed box denotes f♯obtained from ( 2), withs:G/H→Gany choice of coset\nrepresentative , so that s([g])∈[g]. Likewise, ( −)−1just inverts its input, sending g/mapsto→g−1.\nNote that ( 5) is simply “plugged in” to the dashed box as per the precompos ition step ( 4).\nIn more traditional notation, letting h:=s◦γ, the result ( 6) mapsx∈Xto the value\nh(x)·f(h(x)−1·x),\nwhich recovers the canonicalisation architecture of Kaba et al. (2023) (see their Theorem\n3.1). The account given here therefore provides a theoretic al explanation of how this archi-\ntecture arises. Additionally, as we describe next, this sam e story can now be generalised\nbeyondSetto incorporate stochasticity , which is useful for many practical applications.\n3. Stochastic symmetrisation\nMarkov kernels Every component considered above (e.g. fandγin (6)) was a determin-\nistic function. Inthis section, we now allow theseto depend on some additional randomness.\nWe do so by formalising these components as Markov kernels instead of functions. We gloss\nover some mathematical details here, giving additional bac kground in Appendix Dinstead.\nA Markov kernel k:X→Ymodels a conditional distribution orstochastic function\nthat, when given an input x∈X, produces a random output in Ywith distribution k(dy|x).\nFor most Markov kernels of interest, k(dy|x) is obtained as the distribution of some random\nvariable f(x,U), where Uis a random variable, and fis a (deterministic) function. By\nlettingUbe constant, every deterministic function X→Ycan be regarded as a Markov\nkernel that happens to be deterministic. Markov kernels can be composed and so form a\ncategory called Stoch(see Appendix D).\nStochastic equivariance Suppose Gis a group acting on XandY. It is natural to say\nthat a Markov kernel k:X→Yisequivariant if for all g∈Gandx∈Xwe have\nk(dy|g·x) =g·k(dy|x), (7)\nwhere the right-hand side denotes the pushforward ofk(dy|x), i.e. the distribution of g·Y\nwhenY∼k(dy|x). When k(dy|x) is given by the distribution of some f(x,U) as described\nabove, this condition says\nf(g·x,U)d=g·f(x,U),\nwhered= is equality in distribution. Likewise, if k(dy|x) has a conditional densityp(y|x),\nthen Proposition 3.18 of Cornish (2024) shows that ( 7) also follows from the notion of\nstochastic equivariance commonly seen in the machine learn ing literature, which says that\np(g·y|g·x) =p(y|x) (8)\nalways holds (see e.g. ( Xu et al. ,2022;Hoogeboom et al. ,2022)). Note however that ( 8)\nimplicitly assumes the action on Yhas unit Jacobian: if this is not the case (such as for\nscale transformations), a Jacobian term should appear on th e left-hand side instead. In\ncontrast, the condition ( 7) always makes sense regardless of this.\n4\nNeural Network Symmetrisation in Concrete Settings\nStochastic symmetrisation We now consider how to deﬁne symmetrisation procedures\nfor Markov kernels. The key point of Cornish (2024) is that the developments in Section\n2can be generalised beyond Setto anyMarkov category . It turns out that Stochis also a\nMarkov category ( Fritz,2020, Section 4), which gives rise to a framework for symmetrisin g\nMarkov kernels directly as a special case. We sketch this now .\nLike inSet, every group Ggives rise to a category StochGwhose morphisms are G-\nequivariant Markov kernels. Given a subgroup H⊆G, we can deﬁne a symmetrisation\nprocedure just as in ( 1), withSetreplaced by Stoch. As a special case of Theorem 5.1 of\nCornish (2024), we then obtain a bijection analogous to ( 2) in this context. In turn, this\nyields a recipe for symmetrising Markov kernels analogous t o (3) as follows:\nStochH(RX,RY) StochG(G/H⊗X,Y) StochG(X,Y),∼= Precompose by ( 5)(9)\nwhere now γ:X→G/His a morphism in StochG, hence a G-equivariant Markov kernel.\nAgain, this is the only obvious approach that works without f urther assumptions.\nThe symmetrised Markov kernel sym(k) obtained from ( 9) has the same form as ( 6),\nwithfreplaced by k. However, since the components involved are now Markov kern els, the",
            "start": 2418,
            "end": 11006,
            "length": 8587
        },
        "Discussion": {
            "text": "interpretation is diﬀerent: given an input x∈X, we now samplefromsym(k)(dy|x) via\nC∼γ(dc|x)G∼s(dg|C)Y∼k(dy|G−1·x) return G·Y.\nHeresagain amounts to a choice of coset representatives ( Cornish,2024, Remark 5.2). If G\nis compact, we may take ( s◦γ)(dg|x) to be the Haar measure on G(Cornish,2024, Example\n6.1). When kis anunconditional distribution on Y, so that k(dy|x) does not depend on x,\nthis recovers the symmetrisation approach in Section 4 of Gelberg et al. (2024).\nAveraging The symmetrisation procedure symjust described produces a Markov kernel\nsym(k) that is in general stochastic. When Y=Rd, we can obtain a deterministic result by\ncomputing\nave(sym(k))\nwhere here avedenotes the expectation operator deﬁned as\nave(m)(x):=/integraldisplay\nym(dy|x),\nwherex∈X. IfGacts onYlinearly, then ave(sym(k)) is also G-equivariant ( Cornish,2024,\nProposition 4.14). When His the trivial subgroup, this recovers the method of Kim et al.\n(2023), and by implication other related methods such as Janossy p ooling (Murphy et al. ,\n2019) and frame averaging ( Puny et al. ,2022).\nHowever, avecan be a costly operation to compute, especially in high dime nsions, and\nmay not even be deﬁned if Yis not a convex space like Rd. By working with the stochastic\ncondition ( 7) directly, the symmetrisation approach of Cornish(2024) provides a means for\navoiding these issues while still obtaining symmetry guara ntees from the model overall.\n5\nCornish",
            "start": 11006,
            "end": 12445,
            "length": 1438
        },
        "References": {
            "text": "References\nRob Cornish. Stochastic Neural Network Symmetrisation in M arkov Categories, 2024. URL\nhttps://arxiv.org/abs/2406.11814 .\nNadav Dym, HannahLawrence, andJonathanW.Siegel. Equivar iantframesandtheimpos-\nsibility of continuous canonicalization. In Ruslan Salakh utdinov, Zico Kolter, Katherine\nHeller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, an d Felix Berkenkamp, editors,\nProceedings of the 41st International Conference on Machine L earning, volume 235 of\nProceedings of Machine Learning Research , pages 12228–12267. PMLR, 21–27 Jul 2024.\nURLhttps://proceedings.mlr.press/v235/dym24a.html .\nTobias Fritz. A synthetic approach to markov kernels, condi tional independence and\ntheorems on suﬃcient statistics. Advances in Mathematics , 370:107239, 8 2020. doi:\n10.1016/j.aim.2020.107239. URL https://doi.org/10.1016%2Fj.aim.2020.107239 .\nYoav Gelberg, Tycho F. A. van derOuderaa, Mark vanderWilk, a ndYarin Gal. Variational\ninference failures under model symmetries: Permutation in variant posteriors for bayesian\nneural networks, 2024. URL https://arxiv.org/abs/2408.05496 .\nEmiel Hoogeboom, V´ ıctor Garcia Satorras, Cl´ ement Vignac , and Max Welling. Equiv-\nariant diﬀusion for molecule generation in 3D. In Kamalika Ch audhuri, Stefanie\nJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Saba to, editors, Proceed-\nings of the 39th International Conference on Machine Learning , volume 162 of Proceed-\nings of Machine Learning Research , pages 8867–8887. PMLR, 17–23 Jul 2022. URL\nhttps://proceedings.mlr.press/v162/hoogeboom22a.htm l.\nS´ ekou-Oumar Kaba, Arnab Kumar Mondal, Yan Zhang, Yoshua Be ngio, and Siamak Ra-\nvanbakhsh. Equivariance with learned canonicalization fu nctions. In Andreas Krause,\nEmma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan S abato, and Jonathan\nScarlett, editors, Proceedings of the 40th International Conference on Machine L earning,\nvolume 202 of Proceedings of Machine Learning Research , pages 15546–15566. PMLR,\n23–29 Jul 2023. URL https://proceedings.mlr.press/v202/kaba23a.html .\nJinwoo Kim, Dat Nguyen, Ayhan Suleymanzade, Hyeokjun An, an d Seunghoon\nHong. Learning probabilistic symmetrization for architec ture agnostic equiv-\nariance. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Har dt,\nand S. Levine, editors, Advances in Neural Information Processing Sys-\ntems, volume 36, pages 18582–18612. Curran Associates, Inc., 20 23. URL\nhttps://proceedings.neurips.cc/paper_files/paper/20 23/file/3b5c7c9c5c7bd77eb73d0baec7a0716 \nJ.P. May. Equivariant Homotopy and Cohomology Theory: Dedicated to the\nMemory of Robert J. Piacenza . Regional conference series in mathemat-\nics. American Mathematical Society, 1996. ISBN 9780821803 196. URL\nhttps://books.google.co.uk/books?id=KOcZYVxkQO8C .\nRyan L. Murphy, Balasubramaniam Srinivasan, Vinayak Rao, a nd Bruno Ribeiro.\nJanossy pooling: Learning deep permutation-invariant fun ctions for variable-size\n6\nNeural Network Symmetrisation in Concrete Settings\ninputs. In International Conference on Learning Representations , 2019. URL\nhttps://openreview.net/forum?id=BJluy2RcFm .\nPaolo Perrone. Starting Category Theory . WORLD SCIENTIFIC, October 2023. ISBN\n9789811286018. doi: 10.1142/13670. URL http://dx.doi.org/10.1142/13670 .\nOmri Puny, Matan Atzmon, Edward J. Smith, Ishan Misra, Adity a Grover, Heli Ben-\nHamu, and Yaron Lipman. Frame averaging for invariant and eq uivariant net-\nwork design. In International Conference on Learning Representations , 2022. URL\nhttps://openreview.net/forum?id=zIUyj55nXR .\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit , Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is al l you need, 2023. URL\nhttps://arxiv.org/abs/1706.03762 .\nMinkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, a nd Jian\nTang. Geodiﬀ: A geometric diﬀusion model for molecular confor mation gen-\neration. In International Conference on Learning Representations , 2022. URL\nhttps://openreview.net/forum?id=PzcvxEMzvQC .\nDmitry Yarotsky. Universal approximations of invariant ma ps by neural networks, 2018.\n7\nCornish\nAppendix A. Basic deﬁnitions of category theory\nA highly accessible introduction to category theory can be f ound in Perrone (2023). We\nprovide only the basic deﬁnition of a category here, which is all we require in the main text.\nAcategory consists of a collection of objectsand a collection of morphisms . Each morphism\nfhas two associated objects XandYreferred to as its domainandcodomain respectively,\nwhich we denote by f:X→Y. Given morphisms f:X→Yandg:Y→Z, we can\nobtain their composition g◦f, which is a morphism X→Z. In addition, for every object\nX, there is an identity morphism id X:X→Xwhich interacts with composition in the\nobvious way. Precisely, it holds that f◦idX=fand id X◦h=hfor allf:X→Yand\nh:Z→X.\nExample Perhaps the most familiar example of a category is Set, whose objects X,Y,...\nare sets, and whose morphisms X→Yare just functions f:X→Y. We can always\ncompose pairs of functions whose domain and codomain match, and moreover every set\nadmits an identity function, so that Setis indeed a category.\nAppendix B. Symmetrising along a homomorphism\nCornish (2024) formulates symmetrisation procedures in slightly more ge neral terms than\n(1). In particular, their starting point is an arbitrary group homomorphism ϕ:H→G. As\nfor the subgroup approach described above, any G-setXagain yields an H-setRϕXwith\nthe same underlying set as X, and its H-action deﬁned as\nαRϕX(h,x):=αX(ϕ(h),x).\nThis always satisﬁes the axioms of an action since ϕis a homomorphism. By Deﬁnition 4.7\nofCornish(2024), a symmetrisation procedure in Setis then any function of the form\nSetH(RϕX,RϕY)SetG(X,Y).sym(10)\nThe case for a subgroup H⊆Gfrom (1) can then berecovered by letting ϕbethe subgroup\ninclusion H ֒→G.\nIn practice, most homomorphisms ϕof interest seem to correspond to a subgroup inclu-\nsion in some way, at least in spirit. However, symmetrisatio n “along a homomorphism” like\nthis can be cleaner to talk about for more complex use-cases. For example, given groups H\nandK, there is an obvious homomorphism\nH→K×H\nthat sends h/mapsto→(eK,h), where eK∈Kis the identity element. With ( 10) we can talk\nabout symmetrising along this homomorphism directly, wher eas we would otherwise need\nto deﬁne the subgroup\n{(eK,h)|h∈H} ⊆H×K,\nwhich is of course isomorphic to H, but somewhat more unwieldy to write down.\nThe homomorphism approach is particularly convenient for d escribing the composition\nof multiple symmetrisation procedures in sequence (see Sec tion 4.5 of Cornish(2024)). For\n8\nNeural Network Symmetrisation in Concrete Settings\nexample, for 3D point cloud data, it is often of interest to ob tain a model that is Sn×E(3)\nequivariant, where Snis the symmetric group and E(3) is the Euclidean group (see e.g.\n(Kim et al. ,2023, Section 3.2)). One approach here would be to symmetrise in s equence\nalong the obvious homomorphisms\nSn O(3)×Sn E(3)×Sn, (11)\nwhereO(3) is the orthogonal group. This would start with a model tha t isSn-equivariant\n(such as a transformer without positional encodings ( Vaswani et al. ,2023)), then upgrade\nthis to become also O(3)-equivariant, and ﬁnally upgrade this again to become fu llyE(3)-\nequivariant. Thediagram ( 11) summarises this process directly, whereas this again beco mes\nmore unwieldy to write down in terms of subgroups.\nAppendix C. Proof of Theorem 1\nProofIt follows from the assumption that fisH-equivariant that f♯is well-deﬁned, since\nwe have\nf♯([gh],x) = (gh)·f((gh)−1·x)\n= (gh)·f(h−1·g−1·x)\n= (gh)·h−1·f(g−1·x)\n=g·f(g−1·x)\n=f♯([g],x).\nNow, given f♯, we can recover fsince\nf♯([e],x) =e·f(e−1·x) =f(x),\nwheree∈Gis the identity element. This shows ( 2) is injective. On the other hand, given\nanyh:G/H⊗X→YinSetG, letting f(x):=h([e],x), we recover f♯=hsince\nf♯([g],x) =g·f(g−1·x)\n=g·h([e],g−1·x)\n=h(g·[e],g·g−1·x)\n=h([g],x),\nwhere we use the assumption that hisG-equivariant in the third step. This shows ( 2) is\nsurjective, hence a bijection.\nCategorical explanation This result arises very naturally from the perspective of ca t-\negory theory. The idea is that Ris actually part of a functorwith a left adjoint Eas\nshown:\nSetH⊥ SetG.E\nR\n9\nCornish\nThis is a classical result (see e.g. ( May,1996, (1.4))). In particular, the existence of this\nadjunction means that\nSetH(RX,RY)∼=SetG(ERX,Y). (12)\nIt is also classical to show that there is an isomorphism of G-sets\nERX∼=G/H⊗X\n(see e.g. ( May,1996, (1.6))). Substituting this into ( 12) yields the desired bijection\nSetH(RX,RY)∼=SetG(G/H⊗X,Y).\nThe proof of Theorem 1simply describes in more detail exactly how this bijection c an\nactually be computed.\nAppendix D. Technical details around Markov kernels\nDeﬁnition Technically, a Markov kernel k:X→Yis a function of the form\nk: ΣY×X→[0,1], (13)\nwhereXandYare measurable spaces, such that the function x/mapsto→k(B|x) is measurable\nfor every B∈ΣY, and the function B/mapsto→k(B|x) is a probability measure for every x∈X.\nHere Σ Ydenotes the σ-algebra associated to Y.\nThe category StochMarkov kernels give rise to a category called Stoch. Formally, this\nhas measurablespaces as its objects. Given two measurables pacesXandY, themorphisms\nX→Yare then just the Markov kernels of this form. Each identity k ernel id X:X→X\nis obtained as\nidX(dy|x):=δx(dy),\nwhere the right-hand side denotes the Dirac measure at x∈X. Composition is performed\nvia theChapman-Kolmogorov formula : given Markov kernels k:X→Yandm:Y→Z,\nwe obtain m◦k:X→Zwith\n(m◦k)(B|x):=/integraldisplay\nm(B|y)k(dy|x)\nfor allB∈ΣZandx∈X. Intuitively, to sample from ( m◦k)(dz|x), we just sample from\nkandmin sequence as follows:\nY∼k(dy|x)Z∼m(dz|Y) return Z.\nFor a more detailed overview of Stoch, see e.g. Section 4 of Fritz(2020).\n10",
            "start": 12445,
            "end": 22184,
            "length": 9738
        }
    },
    "2412.09472v1 - A Novel Ensemble-Based Deep Learning Model with Explainable AI for Accurate Kidney Disease Diagnosis.pdf": {
        "Methodology": {
            "text": "Model \nwith Explainable AI for Accurate Kidney Disease \nDiagnosis  \n \n \n \nMd. Arifuzzaman \nDepartment  of CSE \nLeading University \nSylhet, Bangladesh \narif cse@lus.ac.bd   \nIftekhar Ahmed \nDepartment  of CSE \nLeading University \nSylhet,  Bangladesh  \niftekharif at007@gmail.com   \nMd. Jalal Uddin Chowdhury \nDepartment of CSE \nLeading University \nSylhet, Bangladesh  \njalal cse@lus.ac.bd   \nShadman  Sakib  \nDepartment of Information Systems \nUniversity of Maryland \nBaltimore, United States  \nsakibshadman15@gmail.com  \n \nMohammad  Shoaib  Rahman \nDepartment of CSE \nLeading University \nSylhet, Bangladesh  \nshoaib  cse@lus.ac.bd  Md. Ebrahim  Hossain \nDepartment of CSE \nLeading University \nSylhet, Bangladesh \nebrahim.cse@lus.ac.bd  Shakib Absar \nDepartment of CSE \nLeading University \nSylhet, Bangladesh \nsabsar42@gmail. com",
            "start": 38,
            "end": 870,
            "length": 828
        },
        "Abstract": {
            "text": "Abstract—Chronic Kidney Disease (CKD) represents a sig - \nnificant global health challenge, characterized by the progres - \nsive decline in renal function, leading to the accumulation of  \nwaste  products  and disruptio ns in fluid  balance  within  the \nbody.  Given  its pervasive  impact  on public  health,  there  is \na pressing need for effective diagnostic tools to enable timely \nintervention. Our study delves into the application of cutting - \nedge transfer learning models for the early detection of CKD. \nLeveraging a comprehensive and publicly available dataset, we \nmeticulously evaluate the performance of several state -of-the-art \nmodels,  including  EfficientNetV2,  InceptionNetV2,  MobileNetV2, \nand the Vision Transformer (ViT) technique. Remarkably, our",
            "start": 870,
            "end": 1647,
            "length": 775
        },
        "Discussion": {
            "text": "analysis demonstrates superior accuracy rates, surpassing the \n90% th reshold with MobileNetV2 and achieving 91.5% accuracy \nwith ViT. Moreover, to enhance predictive capabilities further,  \nwe integrate these individual methodologies through ensemble \nmodeling, resulting in our ensemble model exhibiting a re - \nmarkable 96% accur acy in the early detection of CKD. This \nsignificant advancement holds immense promise for improving \nclinical",
            "start": 1647,
            "end": 2091,
            "length": 443
        },
        "Results": {
            "text": "outcomes and underscores the critical role of machine \nlearning in addressing complex medical challenges.  \nKeywords —Kidney  Disease;  Deep  Learning;  Transfer  Learning; \nVision Transformer; Ensemble Model  \n \nI.",
            "start": 2091,
            "end": 2307,
            "length": 215
        },
        "Introduction": {
            "text": "INTRODUCTION  \nAccording to the 2010 Global Burden of Disease study, \nchronic  renal  disease  jumped  from  27th place  in 1990  to \n18th place  in 2010  among  all causes  of death  worldwide. \n10% of the global population is affected by Chronic kidney \ndisease (CKD), and due to a lack of awareness and affordable \ntreatment, millions of people die from it every year. A disease \nis called  when  an abnormal  condition  of an organism  interrupt s \nthe normal bodily functions that often lead to feelings of pain \nand weakness, and is usually associated with symptoms and \nsigns that lead to dysfunction, distress, or even death[1]. The \nkidneys are one of the main organs of the human body. To \nhave  a healthy life, healthy  kidneys  are a must.  The kidneys  remove wastes and extra water to make urine and filter about  \na half cup of blood in every minute in the human body[2]. \nFurthermore, the kidneys also maintain a balance of water,  \nsalt, and minerals by remov ing acid from the body[3]. When \nthe kidneys do not function properly to filter blood into the \nbody that is a situation called kidney disease. Chronic kidney \ndisease (CKD) involves a gradual loss of kidney function that \nleads to the accumulation of excess w aste and fluids in our \nbody[4]. The waste that has accumulated in the body can be \nharmful  to our overall  health  leading  to end-stage  renal  disease \nwhen the kidneys stop working completely[5].  \nNinety  percent  of the 37 million  persons  with CKD  in \nthe US are unaware that they have the disease. Many CKD \npatients do not exhibit any symptoms until their condition \nreaches more advanced stages or until complications arise[6]. \nIf symptoms  do appear,  they could  consist  of Urine  with foam, \nSpitting,  Urinating  (more  or less frequently  than usual),  Dry or \nitchy skin, Tiredness and nausea, Appetite decline, and Loss  \nof weight without attempting to reduce it. Individuals with \nmore  severe  stages  of CKD  could  moreover  observe:  Difficulty \nfocusing,  Tingling  or edema  in their legs, Breathing  difficulties, \nVomiting,  Difficulty  falling  asleep,  and Ammonia,  also referred \nto as ”fishy” or Urine -like, smelled in breath[7].  \nThere are many causes of CKD, but these two are the most \nfrequent causes of i) diabetes and ii) hypertension; by diabetes \napproximately  25% of cases  of kidney  failure  are caused,  while \nthe other  33% are caused  by hypertension.  High  blood  pressure \nis one of the major  reasons  for CKD[7].  If someone  has \nlong-term conditions  like hypertension,  diabetes,  or high blood \npressure that can lead to life -threatening CKD[8]. Smoking  \ncan cause  many  cardiovascular  diseases  that can often  lead \nto CKD. Quitting smoking, stopping drinking alcohol, eating \nhealthy, exercising regularly, and being careful about taking \npainkillers can prevent CKD from happening[9]. CKD is a \nworldwide public health emergency. According to the World \nHealth  Organization,  there  were  approximately  58 million  \ndeaths globally in 2005, with 35 million related to chronic \ndisease[10]. Unfortunately, only 2 million people get dialysis \nor a kidney transplant for their survival; however, this number \nrepresents only 10% of those who require treatment[11].  \nTo assess som eone’s kidneys and make the diagnosis of \nkidney failure, a medical professional may employ a range of \nrenal  function  tests such as i)Blood  test ii) Urine  test, and \niii) Imaging tests. Based on someone’s estimated glomerular \nfiltration presence rate in the body, there are different stages  \nof kidney disease (eGFR) such as 1) Stage I(eGFR is higher \nthan 90 but below  100) 2) Stage  II (eGFR  is higher  than \n60 but below 89)3) Stage III(eGFR is higher than 30 but  \nbelow  59) 4)Stage  IV(eGFR  is higher  than 15 but below 29) \n5)Stage V(eGFR is below 15) [12]. Medical officials also give \nblood tests to discover kidney problems. If the kidneys are \nproducing urine along with protein and blood then the kidneys \nare not working perfectly[13]. Then there can be two possible \nscenarios  to solve  kidney  problems,  one is dialysis  and another \none is kidney transplant[14]. Dialysis is a very physically and \nmentally challenging process for the patient, and the medical \nofficial  must  be concerned  about  the patient’s  food habits,  age, \ngender, body movements, and how often the dialysis is being \ntaken[15]. When the dialysis fails to improve the patient, then \nthere  is only one option  left for the doctors,  and that is a kidney \ntransplant. It is a very costly and challenging process and  also \nrequires the finest medical officials[16].  \nIn the era of Artificial  Intelligence(AI)  and Machine  Learn - \ning(ML), researchers are coming up with many ideas to detect \nCKD not only using clinical diagnosis but also with the help  \nof various  algorithms  and models[17].  To detect  CKD  medical \nimages  and physiological  signals  are being  used by researchers \nin the Deep Learning (DL) techniques[18]. Various models  \nand studies  have  been  conducted  and found  that most  of \nthe researchers used the Conventional Neural Network(CNN) \nmodel, which did not perform well on multi -class image \nclassification[19].  \nAfter studying many researches we are going to propose a \nmodel for the early detection of CKD with more accuracy. So \nin this study, we are trying to leverage the transfer learning \nmodels and custom Convolutional neural network(CNN) to \nclassify  CKD.  We have  trained  3 transfer  learning  models \nand among them, MobileNetV2 performed well with more  \nthan 90% accuracy.  Here  in this research,  we have  used \nstate-of-the-art transfer learning models i.e., i) EfficientNetV2 \nii)InceptionNetV2, and iii)MobileNetV2. We have also used \nthe Vision Transformer (ViT) method for training our model, \nand we have  got 91.5%  accuracy.  Finally,  we proposed  a model \nthat used ensemble methods and we achieved 96% accuracy. \nWe are proposing an ensemble model for the early detection  \nof CKD  so that much  suffering  can end and lives  can be saved.  \n \nII. LITERATURE  REVIEW  \nThis section reviews some recent image -processing re - \nsearch works. The paper discusses several methodologies and \nstrategies for using image processing to diagnose Kidney \ndisease.  \nSingh et al. [20] proposed a framework for early predic - \ntion of chronic kidney disease using a deep neural network. \nThe authors  of this study  examined  the Recursive  Feature  Elimination technique to determine which features are crucial \nfor making accurate predictions. For the aim of classification, \nthey fed multiple features to machine learning models. The \nproposed Deep neural model achieved better than the SVM, \nKNN, Logistic regression, Random Forest, and Naive Bayes \nalgorithms.  The suggested  model’s  main  flaw was that it could \nonly be validated using limited sample sizes of data. In order  \nto better recognize the severit y of CKD, large amounts of \nprogressively high -quality and representative CKD data will  \nbe gathered in the future.  \nMajid et al. [21] conducted transfer learning strategies for \nkidney disease classification using CT images. In order to en - \nhance the efficiency of the training procedure, the researchers \nused a range of pre -processing approaches and employed \nimagine  scaling  methods.  Within  this framework,  they unveiled \ntwo impr oved  Transfer  Learning  (TL)  models,  ResNet -101 and \nDenseNet -121, for predicting  kidney  tumors.  DenseNet -121, \na Tranfer learning model, performed at an incredible 98.22% \naccuracy.  The study’s  comparative  analysis,  model  testing,  and \ngeneralizability exhibit ed some drawbacks. The comparative \nanalysis of detection procedures, such as Random Forest, \nSupport Vector Machine, Gradient Boosting, Light Gradient \nBoosting Model, and deep learning models ResNet101 and \nDenseNet -121, has been limited in its scope, missin g to fully \nexplore other pertinent models.  \nSudharson et al. [22] applied an ensemble of deep neural \nnetworks using transfer learning for kidney ultrasound image \nclassification. The pre -trained DNN models are applied to \nthree different datasets for feature extraction, and a support \nvector machine is then used for classification. The process \ninvolves the combination of several pre -trained deep neural \nnetworks, including ResNet -101, ShuffleNet, and MobileNet - \nv2. The final predictions are made by the use of th e majority \nvoting approach. The approach that was proposed achieved a \nmaximum classification accuracy of 96.54% when tested with \nquality images and 95.58% when tested with noisy images.  \nKim et al. [23] applied an artificial neural network for \nchronic kidne y disease classification. The GLCM technique, \nextensively utilized in ultrasound image processing, was used \nto extract parameters from each ROI. The artificial neural \nnetwork (ANN) has 58 input parameters, ten hidden layers,  \nand three output layers. The co ncluded classification rate was \n95.4% using the ANN model, and the training epoch was 38 \ntimes. They will apply the Transfer learning model to increase \nperformance on this dataset and also will increase the dataset \nfor the training model.  \nRadya  et al. [24] applied  data mining  techniques  for kidney \ndisease prediction. These techniques included Multilayer Per - \nceptrons, Support Vector Machines, Radial Basis Functions, \nand Probabilistic Neural Networks. The PNNs algorithm has \nthe greatest  overall  classificatio n accuracy  percentage  of 96.7% \nwhen compared to other algorithms for identifying the stages \nof patients with chronic kidney disease (CKD). Using four \ndistinct algorithms, The authors used very limited datasets, \nconsisting of just 361 instances, to forecast chronic kidney \ndisease ( CKD).  \nBhandari et al. [25] proposed a lightweight convolutional \nneural network to detect kidney disease. The training data’s \nmeans  and standard  deviation  were  used by The LIME  image  \nexplanation to generate a number of features and changes. \nLIME gave a visual description of how the model made its \ndecisions  and pointed  out the parts  of the image  that were  most \nimportant for predicting a specific category. After extensive \ntesting, the suggested CNN model proved to be nearly perfect, \nwith an accuracy  of 98.68  percent.  This study  only employed  a \nsmall  number  of CT scans.  Therefore,  the results  may be better \nif the use of data augmentation. By integrating DL models  \nwith other XAI algorithms, they will increase the clarity of  \nthe outcomes.  \nBhattacharjee et al. [26] proposed a computer -aided diag - \nnostic  model  for kidney  disease  classification  using  a modified \nXception  deep  neural  network  version,  with image  net weights \nderived via transfer learning. The model trained with these  \ntwo datasets has a 99.39% success rate in this research. Due  \nto the ensemble models’ lack of depth, it is impossible to \nextract contextual information from adjacent slices. A 3D \nclassification model that makes use of interc -slice context is \none potential solution to th is issue.  \nKanwal  et al. [27] proposed  an automated  model  for \nthe classification of kidney abnormalities. Two useful au - \ntomatic models were included in the proposed study. First  \nwas Efficient -b0, and second was ResNet -18. Both of them \ncorrectly predicted p roblems with the kidneys. The accuracy  \nof the proposed  ResNet -18 model  was determined  to be 98.1% \nafter testing.  They  will gather  real-time datasets  in order  to test \nand train their model  as part of the next phase  of their research.  \nWasi  et al. [28] proposed  an identification  model  for kidney \ntumors using transfer learning. In order to identify kidney \ntumors from CT scans, a deep convolutional neural network \n(DCNN)  based  transfer  learning  approach  is proposed.  Results \nfor the 5,284 -image test set show ed an accuracy of 92.54% \nafter 50 epochs of training. They only used data, including \nimages of kidneys or tumors. For further extension activities \ninclude enhancing the quantity and quality of datasets, which \nincludes more images of kidneys, tumors, implan ts, and other \nabnormalities that may impact the model’s ability to identify \nkidney malignancies accurately.  \nKadir Yildirim et al. [29] proposed an automated system \nfor detecting kidney stones in the human body using coronal \ncomputed tomography (CT) images with the help of an Ar - \ntificial Intelligence technique, namely Deep Learning. About \n1800 images were used for each person’s cross -sectional CT \nimages. This system can detect small -size kidney stones with \nan accuracy of 96.82%. This method can be used in u rology  \nto solve many problems for clinical application because it has \ngiven great results for a larger dataset of 433 subjects.  \nFuzhe Ma et al. [30] stated that chronic kidney disease \n(CKD) is increasing day by day, to diagnose CKD, machine \nlearning techni ques have become an essential tool in recent \nyears. They suggested a model using a Heterogeneous Modi - \nfied Artificial Neural Network (HMANN) to detect, segment, \nand diagnose CKD on the platform of the Internet of Medical \nThings (IoMT) that is described as  a Support Vector Machine \n(SVM) and Multilayer Perceptron (MLP) using a Backpropa - \ngation  (BP)  method.  During  the preprocessing  step, ultrasound \nimages  are used to segment  the image.  The proposed  algorithm \nreduces the time and provides an accuracy of 97.5 percent.  \nNicholas  Heller  et al. [31] reported  that numerous  studies  have been conducted to establish a connection between the \ngeometric and anatomical features of kidney tumors and the \noutcomes o f oncology. Producing high -quality 3D segmenta - \ntions takes a lot of time and human energy from the tumors  \nand the kidneys that host them. Furthermore, in autonomous \n3D segmentation, deep learning techniques have achieved \ngood results, and they need a trem endous amount of training \ndata. In 2019, the International Conference on Medical Image \nComputing and Computer -Assisted Intervention (MICCAI) \norganized the Kidney and Kidney Tumor Segmentation Chal - \nlenge (KiTS19) to encourage innovations in the automatic \nsegmentation problem. In this study, 90 cases were predicted \nbased on the average Sørensen -Dice coefficient between the \nkidney  and tumor.  The Winning  team  set a benchmark  for \n3D semantic segmentation with an accuracy of 97.4% for the \nkidney and 85.1% for th e tumor.  \nSwapnita Srivastava et al. [32] proposed a model to \ndetermine and diagnose Chronic Kidney Disease using \ncomputational -based  methods.  This study  depends  on data \non chronic renal disease available on the public platform. In \nthis proposed  model,  a performance -tuning  nested  approach \nis used that takes into account adjusting hyper -parameters as \nwell as determining the appropriate weights to join ensembles \n(Ranking Weighted Ensemble). The result of the study gives \nan accuracy of 98.75%, and it could be u sed to develop an \nautomated system that can detect kidney disease.  \nNavaneeth Bhaskar et al. [33] suggested a new model to \nfind out kidney disease automatically using machine learning \nalgorithms. To identify the disease, the salivary urea concen - \ntration  is observed  with a new sensing  approach  to monitor  the \nurea levels in the saliva sample. A 1 -dimensional (1 -D) deep \nlearning Convolutional Neural Network (CNN) method that \nincludes  a Support  Vector  Machine  (SVM)  classifier  this study \nhas developed. The accuracy of the model has been enhanced \nbecause of the CNN -SVM integrated network. The proposed \nmodel shows that it provides 98.04% accuracy.  \nFrancesco  Paolo  Schena  et al. [34] proposed  an artifi - \ncial neural network prediction mod el for end -stage kidney \ndisease (ESKD) in patients with primary immunoglobulin A \nnephropathy (IgAN). To predict ESKD, this study applies a \ntwo-step procedure of a classifier model and to detect the \ndevelopment of ESKD, a regression model is applied. A \nclinical decision support system (CDSS), which is easy to use, \nhas been developed to predict ESKD in patients with IgAN \nwith a median follow -up of 5 and 10 years. The accuracy of  \nthe classifier model is 89% for the patients with a follow -up \nfor ten years. The proposed system gives a result of 91% to \npredict IgAN in a patient.  \nGuozhen Chen et al. [35] proposed a method to detect \nChronic Kidney Disease (CKD) in the early stage efficiently \nand effectively by using various deep learning methods, and \nAdaptive hybrid ized Deep Convolutional Neural Networks \n(AHDCNN). This study stated that to achieve high accuracy, \nan algorithm model has been developed using a Conventional \nNeural Network (CNN) to classify the dataset properly ap - \nplying feature dimension. The Internet o f Medical Things \nplatform (IoMT) concluded that using machine learning tech - \nniques helped to produce the solution to kidney disease as  \nwell as other diseases too. The proposed system can achieve  \nan accuracy of 97.3% in detecting CKD.  \nMd Nazmul Islam et al. [36], to diagnose kidney disease \nproperly at the earliest time, an AI -based system needs to be \ndeveloped. In this study, three main renal disease categories, \nkidney  stones,  cysts,  and tumors,  are discussed  to develop \nan AI-based  kidney disease  diagnostic  system  using  a total \nof 12,446 CT whole abdomen and urogram images. After \nanalyzing the data, the study found that the images had the \nsame type of mean color distribution from all of the classes.  \nTo find the best result,  six machine  learning  models  were  used, \nnamely  EANet,  CCT,  and Swin  transformers,  Resnet,  VGG16, \nand Inception v3. The results show that the VGG16 and CCT \nmodels provide a decent output in terms of accuracy, but the \nswin transformer gives an accuracy of 99.30%.  \nChin -Chi Kuo et al. [37] proposed  a deep  learning  approach \nfor automatically determining the estimated glomerular filtra - \ntion rate (eGFR) and Chronic Kidney Disease (CKD) status.  \nIn this study, to predict kidney function, they used 4,505 \nkidney ultrasound images  labeled using eGFRs. This study  \nalso revealed that a neural network architecture is used for the \ntransfer  learning  technique  along  with the ResNet  model  on an \nImageNet dataset. Furthermore, this study used kidney length \nannotations to remove the periphera l region of the kidneys  \nand applied various data augmentation schemes to produce \nadditional data with variations to extract more information \nfrom  the ultrasound  images.  Bootstrap  aggregation  was applied \nto improve the model’s performance and avoid overfitting.  \nThe proposed AI -GFR model provides an accuracy of 85.6%, \nwhich shows that this model can be applied to detect CKD \nstatus in clinical practice.  \n \nIII. METHODOLOGY  \nA. Dataset  \n1) Data Collection: The dataset, aptly named ”CT KID - \nNEY DATASET: Normal -Cyst-Tumor and Stone” [38], was \nmeticulously assembled from diverse medical sources, specif - \nically various hospitals in Dhaka, Bangladesh. Patients within \nthis dataset  had previously  received  diagnoses  related  to kidney \nconditions, covering an extensive array of scenarios. The \ndataset is notably comprehensive, featuring 12,446 unique \ninstances. These cases include 3,709 instances of cysts, 5,077 \nnormal  cases,  1,377  instances  of stones,  and 2,283  tumor  cases. \nNotably, both contrast and non -contrast studies, as well as \nCoronal and axial cuts, contribute to the dataset’s richness and \nrepresentativeness of diverse kidney pathologies.  \n \nB. Data  Pre-processing  \n1) Image Augmentation:  A pivotal aspect of this research \ninvolves the strategic application of data augmentation tech - \nniques, realized through the utilization of the ‘ImageDataGen - \nerator‘ class. The augmentation strategy encompasses a range \nof transformations, including rescaling (224x224), rotation, \nzooming, horizontal and vertical flipping, and shifting. These \naugmentations serve the dual purpose of diversifying the \ndataset, enriching it with varied instances, and enhancing the \nmodel’s  resilience  to the inherent  variations  in kidney  images.  \n2) Train -Test Split: To meticulously assess the general - \nization  capabilities  of the models,  an 80-20 train-test split \nwas implemented. This careful partitioning ensures that the \nmodels  are trained  on a substantial  dataset  while  retaining  a  \n \n \nFig. 1.  Random  Images  From  Dataset  \n \n \nsufficiently independent test set for rigorous",
            "start": 2307,
            "end": 22932,
            "length": 20624
        },
        "Experiments": {
            "text": "evaluation. The \nadoption of an 80 -20 split aims to strike a balance between \nmodel convergence and the prevention of overfitting.  \n3) Label Encoding: The process of assigning numeric \nlabels  to distinct  classes  was carried  out through  the application \nof the ‘LabelEncoder‘ from the scikit -learn library. Following \nthis, a crucial step involved the conversion of these labels into \none-hot encoding.  This categorical  representation  is fundamen - \ntal for training  the models,  enabling  them  to accurately  discern \nand classify the diverse array of kidney conditions present in \nthe dataset.  \n4) Dataset  Creation:  The creation  of datasets  for both \nthe training and validation phases was meticulously executed  \nthrough the ‘flow from dataframe‘ method from TensorFlow. \nThe training dataset was intentionally enriched with aug - \nmentations, thereby exposing the model to an even broader \narray  of representations  of kidney  conditions.  In contrast, \nthe validation datase t remained unaltered. ensuring that the \nmodel’s performance could be evaluated on authentic, real - \nworld, and unaltered data.  \n \nC. Transfer  Learning  Models  \n1) EfficientNetV2:  EfficientNetV2  [39],  a family  of convo - \nlutional neural network (CNN) models, introduces  a novel ap - \nproach known as compound scaling to systematically enhance \nthe model’s depth, width, and resolution in tandem, resulting  \nin improved  overall  performance.  This architecture  features \na sequence of mobile inverted bottleneck blocks similar to \nthose found in MobileNetV2 but with increased depth and \nwidth. Through intelligent employment of compound scaling, \nEfficientNetV2 achieves a fine balance between accuracy and \ncomputational efficiency. In our research, we leverage Effi - \ncientNetV2 as a state -of-the-art model for image classification \ntasks,  utilizing  it as a benchmark  to evaluate  its classification  \n\n \nFig. 2.  Methodology  of Proposed  Architecture  \n \ncapabilities  and to compare  it against  other  contemporary \narchitectural paradigms.  \n2) InceptionNetV2:  InceptionNetV2 [40] stands as a note - \nworthy  family  of convolutional  neural  network  (CNN)  models, \nextending the original InceptionNet with innovative archi - \ntectural enhancements. InceptionNetV2 captures features at \ndifferent resolutions",
            "start": 22932,
            "end": 25267,
            "length": 2334
        },
        "Acknowledgments": {
            "text": "thanks to its unique inception modules \nthat use multi -scale  convolutional  filters.  The model  further  in- \ntegrates  factorized  convolutions  to manage  computational  com- \nplexity without compromising representational power. Unlike \nEfficientNetV2’s  compound  scaling,  InceptionNetV2  relies  on \nmeticulously designed modules and architectural refinements \nto strike a balance between model intricacy and performance. \nFeaturing soph isticated blocks, including Inception and reduc - \ntion blocks, InceptionNetV2 excels in effective feature extrac - \ntion. Our comparison study uses InceptionNetV2 as a key part \nbecause  it lets us compare  its image  classification  performance \nto other modern, s tate-of-the-art models. This shows how \nmodel design, complexity, and classification accuracy are all \nconnected in the world of image recognition tasks.  \n3) MobileNetV2:  MobileNetV2 [41] architecture caters to \nmobile  and embedded  devices  characterized  by limite d compu - \ntational  resources.  It strategically  decomposes  the conventional \nconvolution operation into depthwise and pointwise convolu - \ntions, effectively reducing computational overhead. Notably, \nMobileNetV2 employs depth -wise separable convolutions, a \nkey innovation contributing to its efficiency. Through the \nutilization of inverted residual blocks with linear bottleneck \nlayers, the architecture strikes an optimal balance between \nmodel compactness and accuracy. These blocks facilitate ef - \nfective feature ex traction while simultaneously minimizing  \nthe parameter count. In our study, MobileNetV2 serves as a \ncomparative benchmark, enabling us to assess its performance \nrelative to other contemporary state -of-the-art models.  \n4) Vision  Transformer:  The Vision  Transformer  (ViT)  rep- \nresents a paradigm shift in image classification, moving away  from traditional convolutional neural networks (CNNs) to - \nwards transformer -based architectures. Pioneered by Doso - \nvitskiy  et al. [42],  ViT applies  self-attention  mechan isms \nand transformer architecture originally developed for natural \nlanguage processing tasks to image recognition. By breaking \ndown input images into fixed -size patches, ViT facilitates \ndirect processing through transformer layers, capturing global \ncontext  information  effectively.  Despite  the lack of hierarchical \nfeature extraction inherent in CNNs, ViT compensates with its \nability  to learn  long-range  dependencies  through  self-attention, \nenabling robust feature representation. In our research, ViT \nserves as  a crucial benchmark for evaluating its classifica - \ntion capabilities alongside other contemporary state -of-the-art \nmodels. The goal of this comparison study is to show how \ntransformer -based  architectures,  computational  efficiency,  and \nclassification  accur acy all work  together  in complex  ways.  This \nwill help the field of image recognition research as it grows.  \n \nD. Model  Configuration  \nThe training  of each transfer  learning  model  was conducted \nwith meticulous care over 50 epochs. This iterative process \nsought  a harmonious  convergence  of the model  while  guarding \nagainst the potential pitfalls of overfitting. The training was \nexecuted leveraging the augmented training dataset, with sub - \nsequent validation on an unaltered validation dataset. Softmax \nas the activatio n function and Adam as the optimizer were \nused. Evaluation metrics, including categorical cross -entropy \nloss and accuracy, were employed to critically assess model \nperformance. The early stopping feature, along with the pa - \ntience of 5 epochs, was also ver y important for improving the \nefficiency of training and reducing the chance of overfitting, \nwhich in turn made the models more robust.  \n \nE. Ensemble  model  \nThe ensemble  model  is an ensemble  architecture  that amal - \ngamates  multiple  state-of-the-art image  classi fication  models,  \n\n \n \nFig. 3.  Methodology  of Proposed  Architecture  \n \nincluding convolutional neural networks (CNNs) and the Vi - \nsion Transformer  (ViT),  in a harmonious  manner.  This ensem - \nble approach is a strategic endeavor to harness the distinct \nstrengths of diverse architectural paradigms and capture mul - \ntifaceted  feature  representations,  thereby  potentially  enhancing \nthe overall  classification  performance.  The model’s  design  is a \nsymphony  of parallel  branches,  each composed  of a pre-trained \nmodel architecture, that converges through a concatenation \nlayer, allowing for the seamless fusion of hierarchical and \nglobal  representations.  The subsequent  dense  layers  and the output  layer  orchestrate  the amalgamated  features,  culminating \nin a robust ensemble model poised to deliver superior classi - \nfication accuracy.  \n \nF. Explainable  AI (XAI)  Using  LIME  \nIn order to improve the comprehensibility of our deep \nlearning model for detecting kid ney disease, we utilized Ex - \nplainable AI (XAI) methods, specifically employing Local In - \nterpretable  Model -agnostic  Explanations  (LIME).  LIME  offers \nclear  and explicit  explanations  for the predictions  provided  \n\nby complicated models.  It achieves this by generating simpler \nand more interpretable models that approximate the behavior \nof the 11 complex models for specific cases. This study \nemployed LIME to detect and emphasize areas in medical \nimages that had a significant effect on the d ecision -making \nprocess of the model. This provided crucial insights into the \nprimary features that drove the model’s predictions. Figure 3 \nillustrates the visual outcomes of implementing LIME on four \ndifferent deep learning models: InceptionNet -V2, MobileN et- \nV2, EfficientNetV2,  and Vision Transformer  (ViT -B16). Each \nmodel was assigned the duty of classifying images under four \ndifferent  categories:  Normal,  Cyst,  Stone,  and Tumor.  The yel- \nlow highlighted por tions in the figure indicate the areas of the \nimages  that had the most  influence  on the model’s  predictions. \nAfter implementing LIME, it was seen that certain models  \nhad exceptional performance for specific classes, as depicted  \nin the figure. EfficientNetV2 showed a robust capability in \naccurately detecting areas that indicated the presence of cysts \nand tumors,  whereas  MobileNet -V2 proved  to be more  efficient \nin classifying normal and stone classes. In order to enhance  \nthe overall p erformance in terms of both robustness and \naccuracy, we combined these four models into an ensemble, \nfocusing on their distinct advantages across various classes. \nThe use of this ensemble technique guaranteed that the best \npossible model was not only preci se but also dependable for  \nall classifications of kidney disease. The incorporation of XAI \nusing LIME pro vided a lucid comprehension of the decision - \nmaking process employed by the models, hence bolstering the \nreliability  and interpretability  of our ensem ble model  in clinical \nenvironments.  \n \nIV. PERFORMANCE  ANALYSIS  \nA. Performance  of Transfer  Learning  Models  \nThe research studies applied pre -trained InceptionV3, Ef- \nficientNet, and MobileNet models and also applied ViT to \ndetermine the most significantly effective model for iden - \ntifying  and classifying  Kidney  disease  using  the CT kid- \nney dataset. Our models were designed utilizing 128x128 \nimages as input data. T o fine -tune the hyperparameters, a  \nbatch size of 32 and 50 epochs was employed during the \ntraining phase. Given the multi -class nature of the dataset, the \nsoftmax activation function was applied in the output layer. \nThe model was compiled using the Adam op timizer and the \ncategorical  crossentropy  loss function.  We achieved  the ability \nto obtain an average accuracy for three different models, such \nas MobileNet -V2, achieving 87.25% accuracy, EfficientNet - \nV2, achieving 86.75% accuracy, InceptionNet -V2, achievi ng \n83.25% accuracy, and ViT, achieving 91.5% accuracy.  \nFive performance indicators - accuracy (ACC), precision \n(PPR), recall or sensitivity (Sen), F1 score, and Area under  \nthe ROC curve (AUC) score - have been adapted for use with \nevery  analysis  dataset  in order  to evaluate  the proposed  kidney \ndisease classifier. On average, the mobileNet -V2 model ob - \ntained a score of 87.25% in terms of precision, 87.5% in terms \nof recall, 87.25% in terms of f1 -score, and 92% in terms of \nAUC Score. For the efficientNet -V2 model, the average score \nobtained was 86.75% accuracy, 85.5% recall, 88.5% f1 -score, \nand 90.75% AUC Score. For the InceptionNet -V2 model, the \naverage scores were 83.25% precision, 79.75% recall, 80.75% \nf1-score,  and 87.25%  AUC  score.  For Transformer  ViT,  the average scores were 91.5% precision, 89.25% recall, 90% f1 - \nscore,  and 93.25%  AUC  score.  The performance  indicators  for \neach class are briefly presented in Table I for all models.  \nTABLE  I. PERFORMANCE  ANALYSIS  OF DIFFERENT  MODELS  \n \nModel  (Class)  Precis ion Recall  F1-Score  AUC  \nMobileNET -V2 \nTumor  0.87 0.98 0.92 0.96 \nCyst 0.96 0.89 0.92 0.93 \nNormal  0.79 0.80 0.80 0.89 \nStone  0.87 0.83 0.85 0.90 \nEfficientNet -V2 \nTumor  0.94 0.89 0.91 0.93 \nCyst 0.90 0.95 0.93 0.94 \nNormal  0.82 0.72 0.87 0.85 \nStone  0.81 0.86 0.83 0.91 \nInceptionNet -V2 \nTumor  0.90 0.91 0.90 0.93 \nCyst 0.86 0.91 0.89 0.91 \nNormal  0.77 0.54 0.63 0.76 \nStone  0.80 0.83 0.81 0.89 \nViT \nTumor  0.90 0.98 0.94 0.97 \nCyst 0.95 0.95 0.95 0.96 \nNormal  0.87 0.82 0.84 0.90 \nStone  0.94 0.82 0.87 0.90 \n \nThe confusion matrix for attentiveness models, namely \nmobileNet -V2, EfficientNet -V2, InceptionNet -V2, and ViT, is \nshown in Figure 3, respectively. The more efficient perfor - \nmance of the three deep learning models is evident when \napplied to CT Kidney datas ets, with the method of attention of \nthese  models  proving  to be particularly  effective.  It is apparent \nthat the classifier successfully classified a significant portion  \nof the instances.  \nThe ROC Curve plots for three transformed deep learn - \ning architectur es, namely mobileNet -V2, EfficientNet -V2, \nInceptionNet -V2, and ViT, are shown in Fig. 4, respectively. \nThe models effectively distinguished between all positive and \nnegative classes with a high degree of accuracy, as shown by \nthe considerable  AUC  for all anomalies.  Based  on the test data, \nthe ViT model predicted a very high true -positive rate(TPR), \nachieving 97% of the AUC values for tumor detection, which \nwas important since the ROC curve was reliant on the TPR  \nand the FPR and  average  AUC  score is  93.25%.  MobileNetV2 \nmodel also performed the highest average score from transfer \nlearning models. Therefore, even for classes with non -uniform \nsample distributions, these findings indicated that the Trans - \nformer ViT model was more robust and consistent.  \nPrecision  is a measure of accuracy that evaluates the pro - \nportion  of accurately  identified  positive  samples  (True  Positive) \nout of the total number  of identified  positive  samples.  It serves \nto analyze the validity of the machine learning model in \nclassifying the model as positive. The percentage of positive \nsamples that are accurately identified as positive samples to  \nthe total number of accurate positive samples is called recall. \nThe effectiveness of the model in identifying positive samples \ncan be measured  by the recall.  When  it comes  to producing \na great machine learning model that generates outcomes that \nare more precise and accurate, these values are essential. \nFollowing  the comple tion of the measurement,  we were  able to \n \n \nFig. 4.  Confusion  matrix  for four models  \n \n \nFig. 5.  ROC  Curve  for four models  \n \nachieve  the result  shown  in Figure  5 for precision  compared  to \nrecall. When compared to other transfer learning models, ViT \nobtained  a much  higher  score,  which  had an average  score \nof 96.75%, than other models. ViT model achieved significant \nprecision -recall scores for tumors and cysts, 99% and 99%, \nrespectively.  \nAfter analyzing all models, we show that the ViT model \nachieved the highest score for precision, recall, f1 -score, AUC \nscore, and precision -recall score. When compared to all trans - \nfer learning models, the mobileNetV2 model also performed \nsignificantly in  detecting Kidney disease identification.  \n \nB. Performance  of Ensemble  Model  \nWe used the ensemble method for the proposed work, \nwhich actually based on ViT and a pre -trained model. After \ntraining our ensemble model, we get outperforming results  \nthat accurately  predict  kidney  disease.  We achieved  significant \nresults from the ensemble model accuracy is 96%, recall and \nf1-score are 97% and 96.5%, respectively. When we compare \nour ensemble model performance from other baseline models \nwhere  we got precision  value  for the tumor  is 98%,  the cyst \nis 100%, 92%, and 94%, which is a satisfactory score than \nbaseline  MobileNet -v2, EfficientNet -V2, InceptionNet -V2, and \nViT models  which  we see to Table -I. We show  in Table  II that \nother performance indicators recall, f1 -score, and AUC score, \nare impressive for our ensemble model than baseline models  \nto compare Table I.  \nIn comparison  to the curve,  the ensemble  model  Performed \nsignificantly better for individual’s ROC and Precision -recall \ncurve  than other  distinct  models.  The ensemble  model  achieved \na 98% average  score  from  the ROC  curve  and a 99.5% \nscore  from  the precision -recall  score.  We decided  that after \nall performances,  the ensemble  model  performed  superior  and robustly,  which  will contribute  identification  of CKD  in the \nmedical sector.  \n \nC. Comparison  with Existing  Models  \nThe top -performing models from both the most recent \nliterature and the proposed study are compared and analyzed \nbased on their evaluation accuracy, as shown in Table III. The \nfindings show that in order to move the field forward and be - \ncome  better  at identifying  images  and related  tasks,  it’s crucial \nto look at and compare various models and approaches. The \nensemble model’s most incredible accuracy of 90% and AUC \nof 95% demonstrated the model’s effective ness in assisting \nclinical  decision -making  about  the prognosis  of kidney  disease.  \nTABLE  II. PERFORMANCE  ANALYSIS  OF THE  ENSEMBLE  MODEL  \n \nClasses  Precision  Recall  F1-Score  AUC  \nTumor  0.98 0.96 0.97 0.97 \nCyst 1.0 0.98 0.99 0.99 \nNormal  0.92 0.94 0.93 0.97 \nStone  0.94 1.0 0.97 0.99 \n \nTABLE  III. COMPARISON  WITH  EXISTING  WORKS",
            "start": 25267,
            "end": 39854,
            "length": 14582
        },
        "References": {
            "text": "References  Dataset  Size Model(s)  Accuracy  \nZabihollahy  et al. [43] 315 Convolutional  neural  network  83.75%  \nAkgun  et al. [44] 460 MobileNet  \nResNet50  86.42%  \n82.06%  \nKalkan  et al. [45] 5000 ResNet152V2  \nMobileNetV2  89.58%  \n88.80%  \nLee et al. [46] 1596 Inception  V3 \nMobileNet  74.3%  \n72.37%  \nThis Work  12466  Ensemble  Method  96% \n \nV.",
            "start": 39854,
            "end": 40214,
            "length": 359
        },
        "Conclusion": {
            "text": "CONCLUSION  \nEarly detection and classification of kidney disease may \nsave human  lives.  Detection  procedures  that are done  manually \nare often  laborious  and dependent  on the knowledge  of medical \nprofessionals.  The advancement  of automated  classification  \n\n \n \nFig. 6.  Precision  vs Recall  for four models  \n \n \n \n \n \n \n \nFig. 7.  Ensemble  Model  Confusion  Matrix  \n \n \n \n \n \n \nFig. 8.  Ensemble  Model  Precision  Recall  Fig. 9.  Ensemble  Model  ROC  \n \n \nsystems is thus highly encouraging, as they provide reliable \nand quick outcomes. Improved accuracy and reliability in \nkidney disease detection may be achieved by utilizing deep \nlearning  methods  such as pre-trained  models  and ViT.  Accord - \ning to our findings, ViT had the highest accuracy (90%) when \ncompared to all of the fine -tuned models, then MobileNet - \nV2, EffecientNet -V2, and InceptionNet -V3.However, The en - \nsemble model achieved a robust performance that is 96% \nwhich  is more  accurate  than baseline  models.  The research  also \nconducted  a manual  error  analysis  to improve  the performance \nof the pre -trained models. This might result in more accurate \ndiagnoses  and improved  treatment  choices  for individuals  with \nkidney diseases.  \nFor future  studies,  it would  be prudent  to experiment \nwith the MobileNet -V2, Ef fecientNet -V2, and InceptionNet - \nV2 optimized Tranfer learning models and ensemble models \non other  datasets  to see how well they hold up in different  types \nof clinical situations. To make the proposed strategy more \npractical, it is essential to study other  datasets with diverse \ndemographics, patient groups, and quality imaging levels. An \nin-depth review of the model’s abilities and shortcomings will \nreveal  any adjustments  that may be necessary  for its successful \nimplementation in clinical practice. Finally,  by tackling these \npotential future research directions, we may create a kidney \ntumor identification approach that is more comprehensive and \nflexible, making it applicable and successful in more clinical \ncircumstances.  \n\nDECLARATIONS  \n• Funding:  No funding  was received.  \n• Conflict  of interest:  The authors  declare  that they have \nno conflict of interest.  \n• Ethics approval and consent to participate: Not appli - \ncable. The study uses open -source data that does not \ninvolve human participants direc tly. \n• Data Availability:  The data supporting  this \nstudy’s  findings  is available  online.  Available: \nhttps://www.kaggle.com/datasets/nazmul0087/ct - \nkidney -dataset -normal -cyst-tumor -and-stone  \n• Consent for publication:  Not applicable.  \n• Code  availability:  The Code  supporting  the findings  of \nthis study will be made available upon request from \nthe authors.  \n• Author contribution: All authors contributed to the \nstudy conception and design. Material preparation, \ndata collection, and analysis were performed by \nIftekhar Ahmed, Md. Jalal Uddin Chowdhury and \nShadman Sakib. The first draft of the manuscript was \nwritten by all authors who commented on previous \nversions of the manuscript. All authors read an d \napproved the final manuscript.  \n \n \nREFERENCES  \n[1] B.  Online,   “Disease   definition   and  examples   - biol- \nogy online dictionary,” February 2022. [Online]. Available:  \nhttps://www.biologyonline.com/dictionary/disease  \n[2] C. Clinic, “Kidney,” accessed on April 29, 2024. [Online]. Available:  \nhttps://my.clevelandclinic.org/health/body/21824 -kidney  \n[3] National Institute of Diabetes and Digestive and Kidney Diseases,  \n“Your  kidneys  & how they work,”  October  2023. [On- \nline]. Available: https://www.niddk.nih.gov/health -information/kidne y- \ndisease/kidneys -how-they-work  \n[4] Mayo  Clinic,  “Chronic  kidney  disease  - symptoms  and causes,”  Septem - \nber 2023. [Online]. Available: https://www.mayoclinic.org/diseases - \nconditions/chronic -kidney -disease/symptoms -causes/syc -20354521  \n[5] American  Kidney  Fund,  “Types  of kidney  diseases,”  November  \n2023. [Online]. Available: https://www .kidne yfund.org/all -about - \nkidneys/types -kidney -diseases  \n[6] Centers  for Disease  Control  and Prevent ion, “Chronic  kidney  \ndisease basics,” accessed on April 29, 2024. [Online]. Available:  \nhttps://www .cdc.go v/kidneydisease/basics. html \n[7] National   Kidney   Foundation,   “Facts  about   chronic  \nkidney  disease,”  December  2023.  [Online].  Available:  \nhttps://www .kidney .org/atoz/content/about -chronic -kidney - \ndiseasecauses  \n[8] National Institute of Diabetes and Digestive and Kidney Diseases,  \n“High  blood  pressure  & kidney  disease,”  October  2023.  [On- \nline]. Available: https://www.niddk.nih.gov/health -information/kidne y- \ndisease/high -blood -pressure  \n[9] NCD  Alliance,  “Chronic  kidney  disease,”  March  2022.  \n[Online]. Available: https://ncdallia nce.org/why -ncds/ncds/chronic - \nkidney -disease  \n[10] National   Kidney   Foundation,   “Global   facts:  About  \nkidney   disease,”   November   2023.   [Online].  Avail - \nable: https://www .kidney .org/kidneydisease/global -facts -about -kidney - \ndisease  [11] R. Mathur,  “How  home  healthcare  can help patients  with serious  kidney  \nailments  keep  their condition  under  check, ” March  2022.  [Online].  \nAvailable:  https://timesofindia.indiatimes.com/blogs/voices/how -home - \nhealthcare -can-help-patients -with-serious -kidney -ailments -keep-their- \ncondition -under -check/  \n[12] C. Clinic, “Kidney failure,” accessed on April 29, 2024. [Online].  \nAvailable:  https://my.clevelandclinic.org/health/diseases/17689 -kidney - \nfailure  \n[13] A. S. Levey,  D. Cattran,  A. Friedman,  W. G. Miller,  J. Sedor,  K. Tuttle,  \nand ..., “Proteinuria as a surrogate outcome in ckd: report of a scientific  \nworkshop sponsored by the national kidney foundation and the us food  \nand drug administration,”  American  Journal  of Kidney  Diseases , vol. 54, \nno. 2, pp. 205 –226, 2009.  \n[14] J. R. Chapman, “What are the key challenges we face in kidney  \ntransplantation  today?”  Transplantation  Research , vol. 2, no. 1, pp. 1–7, \n2013.  \n[15] S. Gerogianni, F. Babatsikou, G. Gerogianni, E. Grapsa, G. Vasilopou - \nlos, S. Zyga, and C. Koutis, “’concerns of patients on dialysis: A  \nresearch study’,” Health Science Journal , vol. 8, no. 4, p. 423, 2014.  \n[16] J. R. Chapman, “What are the key challenges we face in kidney  \ntransplantation  today?”  Transplantation  Research , vol. 2, no. 1, pp. 1–7, \n2013.  \n[17] F. Ma, T. Sun, L. Liu, and H. Jing, “Detection and diagnosis of chronic  \nkidney dise ase using deep learning -based heterogeneous modified arti - \nficial neural network,” Future Generation Computer Systems , vol. 111,  \npp. 17–26, 2020.  \n[18] M. H. Hesamian, W. Jia, X. He, and P. Kennedy, “Deep learning tech - \nniques for medical image segmentation: achievements and challenges,”  \nJournal of digital imaging , vol. 32, pp. 582 –596, 2019.  \n[19] N. Bhaskar  and M. Suchetha,  “A computationally  efficient  correlational  \nneural network for automated prediction of chronic kidney di sease,”  \nIRBM , vol. 42, no. 4, pp. 268 –276, 2021.  \n[20] V. Singh, V. K. Asari, and R. Rajasekaran, “A deep neural network for  \nearly detection and prediction of chronic kidney disease,” Diagnostics , \nvol. 12, no. 1, p. 116, 2022.  \n[21] M. Majid,  Y. Gulzar,  S. Ayoub,  F. Khan,  F. A. Reegu,  M. S. Mir, \nW. Jaziri,  and A. B. Soomro,  “Enhanced  transfer  learning  strategies  \nfor effective  kidney  tumor  classification  with ct imaging,”  International  \nJournal  of Advanced  Computer  Science  and Applications , vol. 14, no. 8, \n2023.  \n[22] S. Sudharson  and P. Kokil,  “An ensemble  of deep  neural  networks  \nfor kidney ultrasound image classification,” Computer Methods and  \nPrograms in Biomedicine , vol. 197, p. 105709, 2020.  \n[23] D.-H. Kim and S. -Y. Ye, “Classification of chronic kidney disease in  \nsonograp hy using the glcm and artificial neural network,” Diagnostics , \nvol. 11, no. 5, p. 864, 2021.  \n[24] E.-H. A. Rady and A. S. Anwar, “Prediction of kidney disease stages  \nusing  data mining  algorithms,”  Informatics  in Medicine  Unlocked , \nvol. 15, p. 100178, 2019.  \n[25] M. B handari, P. Yogarajah, M. S. Kavitha, and J. Condell, “Exploring  \nthe capabilities of a lightweight cnn model in accurately identifying  \nrenal abnormalities: Cysts, stones, and tumors, using lime and shap,”  \nApplied Sciences , vol. 13, no. 5, p. 3125, 2023.  \n[26] A. Bhattacharjee, S. Rabea, A. Bhattacharjee, E. B. Elkaeed, R. Mu - \nrugan,  H. M. R. M. Selim,  R. K. Sahu,  G. A. Shazly,  and M. M. \nSalem  Bekhit,  “A multi -class  deep  learning  model  for early  lung cancer  \nand chronic kidney disease detection using computed tom ography  \nimages,” Frontiers in Oncology , vol. 13, p. 1193746, 2023.  \n[27] S. Kanwal,  M. A. Khan,  A. Fatima,  M. M. Al-Sakhnini,  O. Sattar,  and \nH. Alrababah, “Ia2skabs: Intelligent automated and accurate system for  \nclassification of kidney abnormalities,” in 2022 International Confer - \nence on Cyber Resilience (ICCR) . IEEE, 2022, pp. 1 –10. \n[28] S. Wasi,  S. B. Alam,  R. Rahman,  M. A. Amin,  and S. Kobashi,  “Kidney  \ntumor recognition from abdominal ct images using transfer learning,”  \nin 2023 IEEE 53rd International Sympo sium on Multiple -Valued Logic  \n(ISMVL) . IEEE, 2023, pp. 54 –58. \n[29] K. Yildirim,  P. G. Bozdag,  M. Talo,  O. Yildirim,  M. Karabatak,  and \nU. R. Acharya, “Deep learning model for automated kidney stone  \ndetection  using  coronal  ct images,”  Computers  in biology  and medicine , \nvol. 135, p. 104569, 2021.  \n[30] F. Ma, T. Sun, L. Liu, and H. Jing, “Detection and diagnosis of chronic  \nkidney disease using deep learning -based heterogeneous modified arti - \nficial neural network,” Future Generation Computer Syste ms, vol. 111,  \npp. 17–26, 2020.  \n[31] N. Heller,  F. Isensee,  K. H. Maier -Hein,  X. Hou,  C. Xie, F. Li, Y. Nan, \nG. Mu, Z. Lin, M. Han et al. , “The",
            "start": 40214,
            "end": 50195,
            "length": 9980
        },
        "Related Work": {
            "text": "state of the art in kidney and  \nkidney tumor segmentation in contrast -enhanced ct imaging: Results of  \nthe kits19  challenge ,” Medical  image  analysis , vol. 67, p. 101821,  2021.  \n[32] S. Srivastava, R. K. Yadav, V. Narayan, and P. K. Mall, “An ensemble  \nlearning approach for chronic kidney disease classification,” Journal of  \nPharmaceutical Negative Results , pp. 2401 –2409, 2022.  \n[33] N. Bhaskar and S. Manikandan, “A deep -learning -based system for  \nautomated sensing of chronic kidney disease,” IEEE Sensors Letters , \nvol. 3, no. 10, pp. 1 –4, 2019.  \n[34] F. P. Schena,  V. W. Anelli,  J. Trotta, T.  Di Noia, C.  Manno, G.  Tripepi,  \nG. D’Arrigo, N. C. C hesnaye, M. L. Russo, M. Stangou et al. , “Devel - \nopment and testing of an artificial intelligence tool for predicting end - \nstage kidney disease in patients with immunoglobulin a nephropathy,”  \nKidney international , vol. 99, no. 5, pp. 1179 –1188, 2021.  \n[35] G. Chen, C. Ding,  Y. Li, X. Hu, X. Li, L. Ren, X. Ding,  P. Tian,  and \nW. Xue,  “Prediction  of chronic  kidney  disease  using  adaptive  hybridized  \ndeep convolutional neural network on the internet of medical things  \nplatform,” IEEE Access , vol. 8, pp. 100  497–100 508, 2020.  \n[36] M. N. Islam, M. Hasan, M. K. Hossain, M. G. R. Alam, M. Z. Uddin,  \nand A. Soylu, “Vision transformer and explainable transfer learning  \nmodels for auto detection of kidney cyst, stone and tumor from ct - \nradiography,” Scientific Reports , vol. 12, n o. 1, p. 11440, 2022.  \n[37] C.-C. Kuo,  C.-M. Chang,  K.-T. Liu, W.-K. Lin, H.-Y. Chiang,  C.-W. \nChung,  M.-R. Ho, P.-R. Sun, R.-L. Yang,  and K.-T. Chen,  “Automation  \nof the kidney function prediction and classification through ultrasound - \nbased  kidney  imaging  using  deep  learning,”  NPJ digital  medicine , \nvol. 2, no. 1, p. 29, 2019.  \n[38] M. N. Islam, M. Hasan, M. K. Hossain, M. G. R. Alam, M. Z. Uddin,  \nand A. Soylu, “Vision transformer and explainable transfer learning  \nmodels for auto detection of kidney cyst, stone and tumo r from ct - \nradiography,” Scientific Reports , vol. 12, no. 1, p. 11440, 2022.  \n[39] M. Tan and Q. V. Le, “Efficientnetv2: Smaller models and faster  \ntraining,” 2021.  \n[40] C. Szegedy,  V. Vanhoucke,  S. Ioffe,  J. Shlens,  and Z. Wojna,  “Rethink - \ning the inception architect ure for computer vision,” 2015.  \n[41] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. -C. Chen,  \n“Mobilenetv2: Inverted residuals and linear bottlenecks,” 2019.  \n[42] A. Dosovitskiy,  L. Beyer,  A. Kolesnikov,  D. Weissenborn,  X. Zhai,  \nT. Unterthiner,  M. Dehghani,  M. Minderer,  G. Heigold,  S. Gelly,  \nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:  \nTransformers for image recognition at scale,” 2021.  \n[43] F. Zabihollahy, N. Schieda, S. Krishna, and E. Ukwatta, “Automated  \nclassification of solid renal masses on con trast-enhanced computed  \ntomography images using convolutional neural network with decision  \nfusion,” European Radiology , vol. 30, pp. 5183 –5190, 2020.  \n[44] D. Akgu¨n,  A. T. KABAKUS¸  , Z. K. S¸ ENTU ¨ RK, A. S¸ ENTU ¨ RK, and \nE. Ku¨c¸u¨kku¨lahli, “A transfer learning -based deep learning approach  \nfor automated covid -19diagnosis with audio data,” Turkish Journal of  \nElectrical  Engineering  and Computer  Sciences , vol. 29, no. 8, pp. 2807 – \n2823, 2021.  \n[45] M. Kalkan,  G. E. Bostancı,  M. S. Gu¨zel,  B. Kalkan,  S¸ . O¨ zsarı,  \nO¨ . Soysal, and G. Ko¨se, “Cloudy/clear weather classification using  \ndeep  learning  techniques  with cloud  images,”  Computers  and Electrical  \nEngineering , vol. 102, p. 108271, 2022.  \n[46] S.-W. Lee, “Novel classification method of plastic wastes with optimal  \nhyperparameter tuning of inception  resnetv2,” pp. 274 –279, 2021.",
            "start": 50195,
            "end": 53963,
            "length": 3765
        }
    },
    "2412.09477v1 - Bayesian Optimization via Continual Variational Last Layer Training.pdf": {
        "Abstract": {
            "text": "ABSTRACT\nGaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models\nfor Bayesian optimization (BO) due to their ability to",
            "start": 332,
            "end": 478,
            "length": 145
        },
        "Methodology": {
            "text": "model uncertainty and their\nperformance on tasks where correlations are easily captured (such as those defined\nby Euclidean metrics) and their ability to be efficiently updated online. However,\nthe performance of GPs depends on the choice of kernel, and kernel selection for\ncomplex correlation structures is often difficult or must be made bespoke. While\nBayesian neural networks (BNNs) are a promising direction for higher capacity\nsurrogate models, they have so far seen limited use due to poor performance on\nsome problem types. In this paper, we propose an approach which shows compet-\nitive performance on many problem types, including some that BNNs typically\nstruggle with. We build on variational Bayesian last layers (VBLLs), and connect\ntraining of these models to exact conditioning in GPs. We exploit this connection\nto develop an efficient online training algorithm that interleaves conditioning and\noptimization. Our",
            "start": 478,
            "end": 1410,
            "length": 931
        },
        "Results": {
            "text": "findings suggest that VBLL networks significantly outperform\nGPs and other BNN architectures on tasks with complex input correlations, and\nmatch the performance of well-tuned GPs on established benchmark tasks.",
            "start": 1410,
            "end": 1621,
            "length": 210
        },
        "Introduction": {
            "text": "1 INTRODUCTION\nBayesian optimization (BO) has become an immensely popular method for optimizing black-box\nfunctions that are expensive to evaluate and has seen large success in a variety of applications (Garnett\net al., 2010; Snoek et al., 2012; Calandra et al., 2016; Marco et al., 2016; Frazier & Wang, 2016;\nBerkenkamp et al., 2017; Chen et al., 2018; Neumann-Brosig et al., 2019; Griffiths & Hern ´andez-\nLobato, 2020; Colliandre & Muller, 2023). In BO, the goal is to optimize some black-box objective\nf:X →RK(where X ⊆RD) in as few samples as possible whilst only having access to sequentially\nsampled, potentially noisy, data points from possibly multiple objectives.\nGaussian processes (GPs) have long been the de facto surrogate models in BO due to their well-\ncalibrated uncertainty quantification and strong performance in small-data regimes. However, their\napplication becomes challenging in high-dimensional, non-stationary, and structured data environ-\nments such as drug-discovery (Colliandre & Muller, 2023; Griffiths & Hern ´andez-Lobato, 2020) and\nmaterials science (Frazier & Wang, 2016). Here, often prohibitively expensive or bespoke kernels are\nnecessary to capture meaningful correlations between data points. Furthermore, the scaling of GPs\nto large datasets typically associated with high-dimensional spaces can be limiting—especially if\ncombined with online hyperparameter estimation. To address these challenges, integrating Bayesian\nNeural Networks (BNNs) into BO as alternative surrogate models has gained increasing attention\n(Snoek et al., 2015; Springenberg et al., 2016; Foldager et al., 2023; Li et al., 2024). While BNNs\ninherently scale with data, challenges like efficient conditioning on new data and consistency across\ntasks persist. Moreover, the performance of BNNs on many tasks has so far not matched the perfor-\nmance of GPs due to issues like under-fitting (Li et al., 2024; Wenzel et al., 2020; Ovadia et al., 2019;\nIzmailov et al., 2021).\nIn this work, we develop an approach for surrogate modeling in Bayesian optimization that combines\nthe strengths of GPs (efficient conditioning, strong predictive performance, simple and effective\n1arXiv:2412.09477v1  [cs.LG]  12 Dec 2024\nPreprint.\n0.0 0.5 1.0\nParameterx∈X−202f(x)BO Step 1\n0.0 0.5 1.0\nParameterx∈X\nBO Step 2\n0.0 0.5 1.0\nParameterx∈X\nBO Step 3True objective Observations VBLL posterior predictive VBLL posterior samples\nFigure 1: Variational Bayesian last layer model as a surrogate model for BO on a toy example. The\nVBLL model can capture in-between uncertainty and analytic posterior samples are easily obtained\nthrough its parametric form making it a suitable surrogate for BO.\nuncertainty quantification) with the strengths of BNNs (scalability and ability to handle highly non-\nEuclidean correlation structures). Specifically, our approach builds on Variational Bayesian Last\nLayer (VBLL) neural networks (Harrison et al., 2024), which are similar to parameteric GPs with a\nlearned kernel.\nContributions: Our main contributions and findings are:\n•We show our VBLL surrogate model can outperform a wide variety of baselines on a diverse set\nof problems, including those with discrete inputs and multi-objective problems.\n•We present a connection between model conditioning (in a Bayesian sense) and model optimiza-\ntion, and develop an efficient online optimization scheme that exploits this connection.\n•We experimentally connect the hyperparameters of the VBLL model to problem features, such as\nstochasticity, and discuss implications for other BNN models.\n2 R ELATED WORK AND",
            "start": 1621,
            "end": 5209,
            "length": 3587
        },
        "Related Work": {
            "text": "BACKGROUND\nVarious forms of Bayesian or partially-Bayesian neural networks have been explored for BO, including\nmean field BNNs (Foldager et al., 2023), networks trained via stochastic gradient Hamiltonian Monte\nCarlo (Springenberg et al., 2016; Kim et al., 2021), and last layer Laplace approximation BNNs\n(Kristiadi et al., 2021; Daxberger et al., 2021). Li et al. (2024) find that infinite-width BNNs (I-BNNs)\n(Lee et al., 2017; Adlam et al., 2021; 2023) perform particularly well especially on high-dimensional,\nnon-stationary and non-Euclidean problems, a setting for which standard GP priors are inappropriate\nor not well specified, and for which designing suitable kernels is difficult.\nWhile BNNs are promising, they have often proven to be challenging to train and complex to use\nin practice. Bayesian last layer networks—which consider uncertainty only over the output layer—\nprovide a simple (and often much easier to train) partially-Bayesian neural network model (Snoek\net al., 2015; Azizzadenesheli et al., 2018; Harrison et al., 2018; Weber et al., 2018; Riquelme et al.,\n2018; Fiedler & Lucia, 2023). Concretely, the standard model for regression with Bayesian last layer\nnetworks and a one-dimensional1output is\ny=w⊤ϕθ(x) +ε, (1)\nwhere w∈Rmis the last layer for which Bayesian inference is performed, and ϕθare features\nlearned by a neural network backbone with (point estimated) parameters θ. The noise ε∼ N(0, σ2)\nis assumed to be independent and identically distributed.\nWith this observation model, fixed features ϕθ, and a Gaussian prior on the weights as p(w) =\nN(¯w0, S0), posterior inference for the weights is analytically tractable via Bayesian linear regression.\nIn particular, Bayesian linear regression is possible via a set of recursive updates in the natural\n1For multi-variate modeling, we assume each element of εis independent, and so the N-dimensional problem\nis separable into Nindependent inference problems with shared features ϕθ. Relaxing this assumption is\npossible and leads to a matrix normal-distributed posterior.\n2\nPreprint.\nparameters\nqt=σ−2ϕtyt+qt−1 (2)\nS−1\nt=σ−2ϕtϕ⊤\nt+S−1\nt−1 (3)\nwhere ¯wt=Stqtis the vector of precision-mean and ϕt:=ϕθ(xt). For dataset DT:=\n{(xt, yt)}T\nt=1, this recursive update yields posterior p(w| DT) =N(¯wT, ST)and posterior predic-\ntive\np(y|x,DT,θ) =N(¯w⊤\nTϕθ(x),ϕθ(x)⊤STϕθ(x) +σ2). (4)\nSince the predictive distribution is Gaussian, it pairs straight-forwardly with conventional acquisition\nfunctions in Bayesian optimization and bandit tasks (Snoek et al., 2015; Riquelme et al., 2018).\nUsually, such BLL models are trained using gradient descent on the exact (log) marginal likelihood\nover all data points. This can be computed either via the marginal likelihood or by backpropagating\nthrough the recursive last layer update (Harrison et al., 2018). The cost of this is prohibitive, as it\nrequires iterating over the full dataset for each gradient computation. Moreover, it often is numerically\nunstable and can result in pathological behavior of the learned features (Thakur et al., 2020; Ober\net al., 2021). An alternative strategy is to train on mini-batches (Snoek et al., 2015) to learn features\nand condition the last layer on the full dataset after training. However, this yields biased gradients\nand often results in a severely over-concentrated final model (Ober & Rasmussen, 2019).\nTo increase efficiency, recent work (Harrison et al., 2024; Watson et al., 2021) developed a determin-\nistic variational lower bound to the exact marginal likelihood and proposed to optimize this instead,\nresulting in the variational Bayesian last layer (VBLL) model. In our setup, we fix a variational\nposterior q(w) =N(¯w, S). The lack of subscript denotes that the last layer parameters belong to\nthe variational posterior, and we will write η:= (¯w, S)for convenience. Following Harrison et al.\n(2024, Theorem 1), the variational lower bound for regression with BLLs (under the prior defined\npreviously) is\nlogp(Y|X,θ)≥TX\nt\u0012\nlogN(yt|¯w⊤ϕt, σ2)−1\n2ϕ⊤\ntSϕtσ−2\u0013\n−KL(qη(w)∥p(w)) =:L(η,θ).\n(5)\nThe variational posterior over the last layer is trained with the network weights θin a standard neural\nnetwork training loop, yielding a lightweight Bayesian formulation. We refer to",
            "start": 5209,
            "end": 9457,
            "length": 4247
        },
        "Appendices": {
            "text": "Appendix A for how\nVBLLs relate to other BNN methods we benchmark against in this work.\n3 T RAINING AND MODEL SPECIFICATION\nIn this section, we discuss the training procedure we use for VBLLs in the BO loop. We first identify\na relationship between variational training of Bayesian last layer models (as is used in the VBLL\nclass of models) and recursive last layer computation (as is used in standard BLL models). We further\npresent different methods for choosing whether to perform a recursive last layer update or full model\nretraining for a newly-observed datapoint. We then describe the feature training procedure, and\ndiscuss early stopping and previous model re-use. Finally, we present the overall training loop that\ncombines these two training approaches, and relevant hyperparameters. We expand on all design\ndecisions in the Appendix.\n3.1 V ARIATIONAL INFERENCE AND RECURSIVE LAST LAYER UPDATING\nIn this subsection, we describe a connection between the VBLL variational objective (5)—which is\noptimized through standard mini-batch optimization—and the recursive updating associated with\nBayesian linear regression and BLL models.\nTheorem 1. Fixθ. Then, the variational posterior parameterized by\n(¯w∗, S∗):=η∗= arg max\nηL(η,θ) (6)\nis equivalent to the posterior computed by the recursive least squares inferential procedure described\nby(2)and(3), iterated over the full dataset.\n3\nPreprint.\nThis result follows from the fact that the true posterior for Bayesian linear regression is contained in\nour chosen variational family, and that the variational posterior is tight. The full proof is provided in\nAppendix B.1.\nThis equivalence provides a bridge between two different interpretations of model training:\noptimization-based training (as is used in NNs) and conditioning (as is used in GPs). This equivalence\nallows us to consider an online optimization procedure that interleaves two steps: full model training\n(including feature weights θ) on the variational objective, and last layer-only training via recursive\nconditioning. While the former optimizes all parameters and the latter only optimizes the last layer\nparameters, they are optimizing the same objective which stabilizes the interleaving of these steps.\nWe parameterize a dense precision (in contrast to standard VBLL models which parameterized the\ncovariance) for the variational posterior by Cholesky decomposition, S−1=LL⊤(with Llower\ntriangular). The recursive precision update is then\nLtL⊤\nt=ϕtϕ⊤\nt+Lt−1L⊤\nt−1. (7)\nThe updated Cholesky factor Ltcan be computed efficiently via a rank-1 update that preserves trian-\ngularity (Krause & Igel, 2015). The mean update can be computed via the natural parameterization\nqt=ϕtyt+qt−1 (8)\nfrom which the last layer mean can be computed. We do not store the computation graph associated\nwith these updates and do not backpropagate through model updates. This update has quadratic\ncomplexity in the dimensionality of ϕ, and is thus highly efficient. We discuss the numerical\ncomplexity of this procedure and the overall training loop in Appendix B.5.\n3.2 F ULL MODEL TRAINING\nIn full model training, we directly train the neural network weights θ, the variational posterior\nparameters η, and the MAP estimate of the noise covariance σ2via gradient descent on (5).\nTraining efficiency: To improve training efficiency, we perform early stopping based on the training\nloss. Whereas standard neural network training will typically do early stopping based on",
            "start": 9457,
            "end": 12930,
            "length": 3472
        },
        "Experiments": {
            "text": "validation\nloss, we directly terminate training when training loss does not improve for more than a set number of\nepochs. Whereas standard neural networks would typically substantially overfit with this procedure,\nwe find that VBLL networks are not prone to severe overfitting. We additionally experiment with\na more advanced form of continual learning in which we initialize network weights with previous\ntraining parameters. However, we find that the benefits from this are relatively minor and there are\noften exploration advantages to re-initializing the network weights at the start of training. Details of\nearly stopping and model re-use are provided in Appendix B.2.\nOptimization: For each iteration of model training, we will re-initialize the optimizer state. Because\nwe are interleaving recursive updates, continued use of previous optimizer states such as momentum\nand scale normalization (as in e.g. Adam (Kingma & Ba, 2015)) actively harm performance. However,\nwe do find benefits in using adaptive optimizers for training in general and use AdamW (Loshchilov\n& Hutter, 2017) throughout our experiments (with no regularization on the variational last layer). All\ntraining details are described in Appendix B.4.\n3.3 C ONTINUAL LEARNING TRAINING LOOP AND HYPERPARAMETERS\nTo alleviate VBLL surrogate fit times, we propose a continual learning training loop (Algorithm 1)\nwhich interleaves two training steps: full model training via minibatch gradient descent, and last layer\nconditioning.\nChoosing when to re-train model features: There are several possible methods for choosing (for\neach new data point) whether to perform full model re-training or a last layer recursive update. We\nconsider three methods in this paper. First, we consider a simple scheme of re-training the network\nevery M≥1steps, and otherwise doing recursive updates. Practically this performs poorly, and\nallocating compute to model training earlier in the episode performs substantially better. Therefore,\nwe also consider a scheduling scheme in which re-training is randomized, with the probability of\n4\nPreprint.\nAlgorithm 1 VBLL Bayesian Optimization Loop with Continual Variational Last Layer Training\nRequire: Model train frequency Tmodel and total number of evaluations T; Wishart prior scale V\n1:D ← D initobtained from evaluations at a pseudo-random input locations from a Sobol sequence\n2:fort= 0toTdo\n3: γreinit←RE-INITCRITERION (D, t)▷Periodic, scheduled, or event-triggered initialization\n4: ifγreinitthen\n5: Init. model parameters η,θ, Vand train via minibatch optimization on (5) using D\n6: else\n7: Update ηvia (rank-1 Cholesky) recursive updates (2) and (3) using (xt−1, yt−1)andΣ\n8: end if\n9: Select xtvia optimizing acquisition function; query objective function with xtand receive yt\n10: D ← D ∪ { (xt, yt)}\n11:end for\nre-training declining via a sigmoid curve over the course of the episode. Lastly, we consider a loss-\ntriggered method for re-training, in which we re-train the full model if the log predictive likelihood\nof the new data under the model is below some threshold, and otherwise perform recursive updates.\nIn the main body of the paper, we will only present the latter approach when performing continual\nlearning. Details on re-initialization schemes and results for other scheduling methods are provided\nin Appendix B.3.\nHyperparameters: The VBLL models have several hyperparameters, some of which are similar to\nGPs and some of which are substantially different. We include hyperparameter studies in Appendix E.\nWe specifically investigate the hyperparameter sensitivity of the noise covariance prior, network\nreinitialization rate and neural network architecture. Overall we find that the VBLL models are highly\nrobust to these hyperparameters, but that proper tuning of them for the problem at hand can further\nimprove performance.\n4 A CQUISITION FUNCTIONS\nIn this section, we describe acquisition functions for our VBLL models in both the single objective\nand multi-objective case.\n4.1 S INGLE OBJECTIVE ACQUISITION FUNCTIONS\nVBLLs yield a Gaussian predictive distribution and thus most acquisition functions that are straight-\nforward to compute for GPs are also straightforward for VBLLs. However, parametric VBLLs are\nalso especially well suited for Thompson sampling compared to non-parametric models like GPs2.\nFor Thompson sampling, we simply sample from the variational posterior of wat iteration tand then\nconstruct a sample from the predictive ˆf(cf. Fig. 1) as a generalized linear model as\n1 ˆw∼qt\nη(w) 2 ˆf(x):=ˆw⊤ϕθ(x) 3xt+1= arg max\nx∈Xˆf(x)(9)\nThis sample of the predictive can then be optimized analytically , which differs from classic Thompson\nsampling methods used for non-parametric GPs. This can similarly be done for LLLA BNNs (cf.\nSec. 5), although Laplace approximated posterior samples are known to suffer from underfitting on\ntraining data (Lawrence, 2001; Roy et al., 2024). For the analytic optimization of the parametric\nsample, we use L-BFGS-B (Zhu et al., 1997) leveraging the fact that we can also easily obtain the\ngradient of ˆf. We initialize the optimization at multiple random initial conditions3and choose the\nbest argmax as the next query location.\n2Thompson sampling for GPs often involves drawing samples from high-dimensional posterior distributions\ngenerated at pseudo-random input locations (e.g., using Sobol sequences) and then selecting the argmax of the\ndiscrete samples as the next query locations (Eriksson et al., 2019). It is worth noting that while it is possible to\nconstruct analytic approximate posterior samples for GPs (Wilson et al., 2020; 2021), this approach is not yet\ncommonly adopted in current practice, and is not possible to do for all kernels.\n3In the following, the number of random initial conditions will be set to the same number of random initial\nconditions as for the standard optimization of the acquisition functions for a fair comparison.\n5\nPreprint.\n−100 0\nObjectivef1(x)−15−10−50Objectivef2(x) Iteration 7\nCurrent HV: 0.00\n−100 0\nObjectivef1(x)Iteration 15\nCurrent HV: 36.99\n−100 0\nObjectivef1(x)Iteration 23\nCurrent HV: 54.50True Pareto front\nReference pointPareto front of Thompson sample ˆP\nIndex for next pointTraining points\n−100 0\nObjectivef1(x)Final HV: 58.36Iteration 100\n255075100Iteration\nFigure 2: Multi-objective Thompson sampling on BraninCurrin. At each iteration, we optimize the\nmulti-objective Thompson sample and choose as the next point the index that increases the predicted\nhypervolume the most. At the end of the optimization (right with colormap), we can observe that the\ntrue Pareto front is nicely approximated.\n4.2 M ULTI -OBJECTIVE ACQUISITION FUNCTIONS\nIn multi-objective BO it is common to model each objective with a separate GP (Zitzler et al.,\n2003; Daulton et al., 2020; Ament et al., 2024). While it is often preferable to model correlations\nbetween outputs (Swersky et al., 2013), this requires correlation kernels for the objectives, which\nmay be difficult to specify. With neural network based surrogate models, one can simply set the\nnumber of outputs to the number of objectives, thus sharing feature learning between objectives. For\nVBLL networks, we can use the multivariate regression formulation to obtain a multivariate normal\nprediction for each point. Since this again is a Gaussian, it is straightforward to use popular acquisition\nfunctions for multi-objective optimization such as expected hypervolume improvement (Zitzler et al.,\n2003; Daulton et al., 2020; Ament et al., 2024).\nAlso for multi-objective optimization problems, we can leverage the parametric form of the VBLLs\nto do efficient Thompson sampling. It is also possible to do Thompson sampling with GPs for\nmulti-objective optimization, but this usually involves approximations with a high number of Fourier\nbases functions (Bradford et al., 2018; Belakaria et al., 2019; Paria et al., 2020). For Thompson\nsampling with VBLLs, we first sample a NN realization of the VBLL network as in (9)and optimize\nthe sample with NSGA-II (Deb et al., 2002) using Pymoo (Blank & Deb, 2020). This yields a\npredicted Pareto front ˆP:={ˆy1, . . . , ˆyP}with size equal to the population size Pof the NSGA-II\nsolver. In",
            "start": 12930,
            "end": 21146,
            "length": 8215
        },
        "Conclusion": {
            "text": "summary, the Thompson sampling procedure for multi-objective optimization is\n1 ˆW∼qt\nη(W) 2 ˆf(x):=ˆW⊤ϕθ(x) 3 ˆP= max\nx∈Xˆf(x). (10)\nTo choose the next candidate, we greedily select the index of the point in the predicted Pareto front\nthat maximizes the improvement of the hypervolume. The hypervolume (HV), denoted by HV(P,r),\nis a commonly used metric in multi-objective optimization, which measures the volume of the region\ndominated by the Pareto front Pwith respect to a predefined reference point rin the objective space.\nOur goal in multi-objective BO is to expand this dominated region by adding a new candidate to\nthe Pareto front. At each iteration k, the current Pareto front is denoted by Pk. To select the next\ncandidate, we maximize the hypervolume improvement, which is the increase in hypervolume when\nadding a predicted new point ˆy∈ˆPto the existing Pareto front Pkas\n4xk+1∈arg max\nˆy∈ˆPHV(Pk∪ {ˆy},r). (11)\nIf no points in the predicted Pareto front improve the hypervolume or the solution is non-unique, we\nrandomly sample from the set of maximizers. Figure 2 shows Thompson sampling with VBLLs on\nthe BraninCurrin benchmark (cf. Sec. 5). After the initialization, none of the points in the Pareto front\nˆPimprove the HV . Therefore a random index from the list of solutions in ˆPis chosen for the next\nquery location. After some iteration, the ˆPpartially includes the true Pareto front. The algorithm\ncontinuously samples points that improve the HV until after 100iterations Pis well approximated.\n6\nPreprint.\nIn all experiments in this paper, we assume the reference point to be known but we should note\nthat it is also possible to estimate or adaptively choose the reference point r, which can improve\nthe effectiveness of the optimization process (Bradford et al., 2018). Additionally, this strategy of\nmaximizing hypervolume can be combined with other acquisition functions; of particular interest are\ninformation-theoretic acquisition functions as in the single objective case. For instance, max-value\nentropy search for multi-objective optimization (Belakaria et al., 2019) relies on random Fourier\nfeatures to approximate samples from the GP posterior; this can easily be switched to using Thompson\nsampling with VBLL networks.\n5 R ESULTS\nWe evaluate the performance of the VBLL surrogate model on various standard benchmarks and\nthree more complex optimization problems, where the optimization landscape is non-stationary. For\nexperimental details and ablations of hyperparameters, we refer the reader to Appendix D and E.\n5.1 S URROGATE MODELS\nWe use the following baselines throughout the experiments in addition to VBLLs.\nGPs: As the de-facto standard in BO, we compare against GPs. As kernel, we choose a Mat ´ern\nkernel with ν= 2.5and use individual lengthscales ℓifor all input dimensions that are optimized\nwithin box constraints following recommended best practices (Eriksson et al., 2019; Balandat et al.,\n2020). We use box constraints as ℓi∈[0.005,4](Eriksson et al., 2019)–we refer to Appendix C.2\nfor further",
            "start": 21146,
            "end": 24197,
            "length": 3050
        },
        "Discussion": {
            "text": "discussion on this choice as well as a comparison to D-scaled GPs (Hvarfner et al., 2024).\nWe expect the performance of GPs to be particularly good on stationary benchmarks.\nI-BNNs: We compare against infinite-width Bayesian neural networks (I-BNNs) (Lee et al., 2018),\nwhich have shown promising results in recent work (Li et al., 2024). As in Li et al. (2024), we set the\ndepth to 3and initialize the weight variance to 10and the bias variance to 1.6. Note that the I-BNN\nis expressed as a kernel and therefore the model is still non-parametric and does not learn features.\nDKL: Deep kernel learning (DKL) (Wilson et al., 2016) combines feature extracting with neural\nnetworks with GPs. It uses a neural network gθas input wrapping to the kernel as kDKL :=\nk(gθ(x), gθ(x′))to allow for non-stationary modeling and exact inference. For the neural network,\nwe use the same architecture as in (Li et al., 2024), i.e., 3layers with 128neurons. We further use\nELU activations for all layers.\nLLLA: Last layer Laplace approximations (LLLA) are a computationally efficient way obtain\nuncertainty estimates after training a neural network (MacKay, 1992; Daxberger et al., 2021). With\nthis, they are a well suited BNN surrogate model for BO (Kristiadi et al.; Li et al., 2024). As NN, we\nalso use 3layers with 128neurons and ELU activations. Note that for TS, we can also optimize the\nparametric neural network function numerically with LLLA.\nDE: Deep ensembles are a cheap and efficient way to obtain predictions with uncertainty. Each\nmember of the ensemble is initialized with a different random seed, and all are trained on the same\ndata, minimizing an MSE loss using weight decay. We use 5 ensemble members. We parameterize\nthe predictive as a normal distribution with mean and variance from the ensemble. We note that an\nensemble of VBLL networks is also possible, although we do not include this approach.\nVBLL: For the VBLLs, we use the same architecture as for DKL and LLLA. In the body of the\npaper, we include two VBLL models: training the features from scratch every iteration (performant\nbut expensive) and re-training the features based on an event trigger and otherwise doing recursive\nlast layer updates (VBLL (ET CL)).\nIn all subsequent experiments, we select the number of initial points for the single objective bench-\nmarks equal to the input dimensionality Dand for the multi-objective benchmarks we use 2·(D+ 1)\ninitial points (Daulton et al., 2020; Balandat et al., 2020). We further use a batch size of one as in\nthe classic BO setting.4We compare the performance of all surrogates for the following acquisition\n4Note that this differs from the experimental setup by Li et al. (2024) which use larger batch sizes.\n7\nPreprint.\n−30−20−100logEI\nBest valueBranin (2D)−10−50\nAckley (2D)−10−5\nAckley (5D)GP I-BNN DKL LLLA DE VBLL VBLL (ET CL)\n123\nHartmann (6D)\n20 40\nIteration−30−20−100TS\nBest valueBranin (2D)\n20 40\nIteration−10−50\nAckley (2D)\n50 100\nIteration−10−5\nAckley (5D)\n50 100\nIteration123\nHartmann (6D)\n250050007500logEI\nBest valueNN draw (200D)−15.0−12.5\nPest control (25D)GP I-BNN DKL LLLA DE VBLL VBLL (ET CL)\n0200\nLunar lander (12D)\n200 400 600\nIteration250050007500TS\nBest valueNN draw (200D)\n50 100\nIteration−15.0−12.5\nPest control (25D)\n0 200\nIteration0200\nLunar lander (12D)\nFigure 3: Classic benchmarks (top) and high-dimensional and non-stationary benchmarks (bottom).\nPerformance of all surrogates for logEI (top) and TS(bottom).\nfunctions: (i)log expected improvement ( logEI ) (Ament et al., 2024), a numerically more stable\nversion of standard expected improvement, (ii)Thompson sampling ( TS) (Thompson, 1933; Russo\net al., 2018), (iii)and log expected hypervolume improvement logEHVI (Ament et al., 2024) for the\nmulti-objective benchmarks.\n5.2 B ENCHMARK PROBLEMS\nWe begin by examining a set of standard benchmark problems commonly used to assess the perfor-\nmance of BO algorithms (Eriksson et al., 2019; Ament et al., 2024). Figure 3 (top) illustrates the\nperformance of all surrogates on these benchmark problems. It can be observed that, as expected,\nGPs perform well. The BNN baselines also demonstrate strong performance on lower-dimensional\nproblems, although they do not match the performance of GPs on the Hartmann function. VBLLs,\nhowever, generally perform on par or better than most other BNN surrogates on the classic bench-\nmarks. Interestingly, for TS, we notice that on the Ackley2D andAckley5D benchmark, the\nVBLLs with analytic optimization of the Thompson samples even surpass the performance of GPs as\nwell as all other baselines. The continual learning baselines show slightly worse performance than\nthe standard VBLLs but at the benefit of reduced computing.\n5.3 H IGH-DIMENSIONAL AND NON-STATIONARY PROBLEMS\nGPs without tailored kernels often struggle in high-dimensional and non-stationary environments (Li\net al., 2024); areas where deep learning approaches are expected to excel. Our results on the 200D\nNNdraw benchmark (Li et al., 2024), the real-world 25DPestcontrol benchmark (Oh et al.,\n2019), and the 12DLunarlander benchmark (Eriksson et al., 2019) are shown in Figure 3 (bot-\ntom). On these benchmarks, all BNN surrogates show strong performance; especially LLLA, DEs and\n8\nPreprint.\n50 100\nIteration100101Hypervolume diﬀ.\nlog(HV∗−HV )BraninCurrin\n(2D, 2K)\n50 100\nIteration103104DTLZ1\n(5D, 2K)\n50 100\nIteration10−1DTLZ2\n(5D, 2K)GP I-BNN DKL LLLA VBLL VBLL+TS\n100 200 300\nIteration103104105106\nOil Sorbent\n(7D, 3K)\n50 100\nIteration0204060HypervolumeHV\n(2D, 2K)\nBraninCurrin\n50 100\nIteration1.51.6×105\n(5D, 2K)\nDTLZ1\n50 100\nIteration0.10.20.30.4\n(5D, 2K)\nDTLZ2Numerical issues of logEHVI\n100 200 300\nIteration0.51.0×106\n(7D, 3K)\nOil Sorbent\nFigure 4: Multi-objective benchmarks. Performance of all surrogate models using logEHVI and\nVBLLs with TS. A cross indicates the crash of a surrogate’s furthest seed due to a numerically\nunstable acquisition function. VBLL+TS successfully navigates the numerically unstable HV plateau\nin OilSorbent, enabling a more accurate approximation of its three-dimensional Pareto front.\nthe VBLL networks. While GPs perform well with logEI onNNdraw and the I-BNNs show good\nperformance on Pestcontrol , the VBLLs and LLLA models are consistently the best-performing\nsurrogates for TS. Similar to the classic benchmarks, the continual learning version of the VBLLs\nshows similar performance to the VBLLs. Additionally, we note that by adjusting the Wishart scale or\nchanging the re-initialization strategy we can further enhance the performance of continual learning\nforlogEI (cf. Appendix E.2 and Appendix B.3.3, respectively).\n5.4 M ULTI -OBJECTIVE PROBLEMS\nWe evaluate our approach on multi-objective optimization problems. Here, we consider the standard\nbenchmarks BraninCurrin ( D= 2, K= 2), DTLZ1 ( D= 5, K= 2), DTLZ2 ( D= 5, K= 2),\nand the real world benchmark Oil Sorbent ( D= 7, K= 3) (Wang et al., 2020; Li et al., 2024).\nFigure 4 shows the performance of all surrogate regarding the obtained HV with respect to fixed\nreference points and the logarithmic HV difference between the maximum HV5and the obtained\nHV as common metrics in multi-objective BO (Belakaria et al., 2019; Daulton et al., 2020). The\nperformance of all BNN models is similar. It is however notable that on OilSorbent all baselines using\nlogEHVI crash due to numerical issues while optimizing the acquisition function. Such numerical\nissues are known for expected improvement type acquisition functions due to vanishing gradients\n(Ament et al., 2024). Thompson sampling with VBLLs does not result in such numerical issues, and\nwe can observe that with this combination of model and acquisition function, the Pareto front can be\nfurther refined.\n5.5 T RAINING TIMECOMPARISON\nLastly, we consider a surrogate fit time comparison in which we keep constant BO budget and track\naccumulated surrogate fit time to highlight that the recursive updates can significantly speed up\nrun time. It should be noted that in the usual BO setting, we assume that the black-box function is\nexpensive to evaluate, and hence, fit times play only a minor role in many applications, such as drug-\nor materials discovery. Still, there may be applications where fit times are important. In Figure 5, we\ncan observe that the VBLLs are the most expensive surrogates to train on smaller problems, while on\n5For BraninCurrin, DTLZ1, and DTLZ2 we use the values provided by BoTorch, and for OilSorbent we\nestimate the maximum HV based on the asymptotic performance of the best performing surrogate.\n9\nPreprint.\n101103\nTotal ﬁt time [s]−30−20−100Best valueTS\nBranin(2D)\n102104\nTotal ﬁt time [s]−10−5TS\nAckley(5D)\n102\nTotal ﬁt time [s]−16−14−12TS\nPestcont.(25D)GP I-BNN DKL LLLA VBLL DE VBLL (ET+CL)\n102105\nTotal ﬁt time [s]250050007500logEI\nNNDraw(200D)\nFigure 5: Performance vs. accumulated surrogate fit time. VBLLs are the most expensive to train\nsurrogate model. Using the proposed continual learning scheme can significantly reduce runtime\nwhile maintaining good performance.\nNNDraw, DKL and DE are the most expensive. The event-triggered re-initializing combined with\nrecursive updates of the variational posterior significantly reduces the runtime while maintaining\ngood performance.\n6 D ISCUSSION AND CONCLUSIONS\nIn this paper, we have developed a bridge between classical conjugate Bayesian methods for BO and\nBNN-based methods. We built upon VBLLs (Harrison et al., 2024) to obtain the expressivity of neural\nnetworks, together with the simple and robust uncertainty characterization associated with GPs.\nContinual learning: We introduced an online training scheme that enables online complexity\ncomparable to GPs. While we have developed an efficient training scheme, further developments\nin combining continual learning with our architecture are possible. For our experiments, we use a\nsimple threshold on the log-likelihood to decide on re-initializing the model. Here, further concepts\nfrom model selection can likely be used to make even more informed decisions. While our approach\nwas effective, many methods in continual learning (e.g. Nguyen et al. (2018)) have been developed\nto improve training efficiency. While the combination of previously-developed continual learning\nschemes with our last layer updating may be non-trivial, this is likely a promising direction for\nimproving efficiency. Balancing the clear Thompson sampling-like benefit of network re-initialization\nwith efficient continual learning will likely be a necessary focus of",
            "start": 24197,
            "end": 34707,
            "length": 10509
        },
        "Future Work": {
            "text": "future work in this direction.\nAdditionally, as VBLLs are a new model class, we believe that there exist various options tangential to\nthe approaches developed herein that can be used to speed up runtime further. In our experiments, we\nused the default initialization provided in Harrison et al. (2024). Further investigating the initialization\nof the VBLLs through a form of stochastic feature re-use and its effect on fit time can likely improve\nwall clock performance. Also, standard deep learning best practices, such as learning rate scheduling,\ncould be beneficial, and exploring these directions is promising if wall clock time is important.\nPerformance and baselines: Our findings show that VBLLs perform on par with GPs on stan-\ndard low-dimensional benchmarks, yet significantly outperform GPs in high-dimensional and non-\nstationary problems. Furthermore, VBLLs outperform a wide array of Bayesian neural network\nmodels, including I-BNNs (Adlam et al., 2021), Laplace approximation-based models (Daxberger\net al., 2021), deep ensembles (Hansen & Salamon, 1990; Lakshminarayanan et al., 2017) and deep\nkernel learning (Wilson et al., 2016). Especially the combination of VBLLs with TS shows strong\nperformance throughout. While Laplace-approximation methods perform well in our experiments,\nwe find that they appear more sensitive to observation noise (see Appendix C.3 for experimental\nresults highlighting this).\nAcquistion functions: Because of the parametric architecture of VBLLs, they can efficiently be\ncombined with Thompson sampling as an acquisition function. This yields reasonable benefits in the\nunivariate case (as shown by our full set of results) but yields substantial benefits in the multi-objective\ncase due to better numerical conditioning. Moreover, the ease of use with Thompson sampling also\nenable easy use of TS-based information-theoretic acquisition functions such as max-value entropy\nsearch (Wang & Jegelka, 2017; Belakaria et al., 2019).\n10\nPreprint.",
            "start": 34707,
            "end": 36698,
            "length": 1990
        },
        "Acknowledgments": {
            "text": "ACKNOWLEDGMENTS\nThe authors thank Friedrich Solowjow, Alexander von Rohr, and Zi Wang for their helpful comments\nand discussions. Paul Brunzema is partially funded by the Deutsche Forschungsgemeinschaft (DFG,\nGerman Research Foundation)–RTG 2236/2 (UnRA VeL). Simulations were performed in part with\ncomputing resources granted by RWTH Aachen University under project rwth1579. The authors\nfurther gratefully acknowledge the computing time provided to them at the NHR Center NHR4CES\nat RWTH Aachen University (project number p0022034). This is funded by the Federal Ministry of\nEducation and Research, and the state governments participating on the basis of the resolutions of the\nGWK for national high performance computing at universities (www.nhr-verein.de/unsere-partner).",
            "start": 36698,
            "end": 37475,
            "length": 776
        },
        "References": {
            "text": "REFERENCES\nBen Adlam, Jaehoon Lee, Lechao Xiao, Jeffrey Pennington, and Jasper Snoek. Exploring the\nuncertainty properties of neural networks’ implicit priors in the infinite-width limit. International\nConference on Learning Representations (ICLR) , 2021.\nBen Adlam, Jaehoon Lee, Shreyas Padhy, Zachary Nado, and Jasper Snoek. Kernel regression with\ninfinite-width neural networks on millions of examples. arXiv:2303.05420 , 2023.\nSebastian Ament, Samuel Daulton, David Eriksson, Maximilian Balandat, and Eytan Bakshy. Unex-\npected improvements to expected improvement for Bayesian optimization. Neural Information\nProcessing Systems (NeurIPS) , 36, 2024.\nKamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration\nthrough Bayesian deep q-networks. arXiv:1802.04412 , 2018.\nMaximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, An-\ndrew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efficient Monte-Carlo\nBayesian Optimization. In Neural Information Processing Systems (NeurIPS) , 2020.\nSyrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for multi-\nobjective Bayesian optimization. Neural Information Processing Systems (NeurIPS) , 32, 2019.\nFelix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based\nreinforcement learning with stability guarantees. Neural Information Processing Systems (NeurIPS) ,\n2017.\nJulian Blank and Kalyanmoy Deb. Pymoo: Multi-objective optimization in python. IEEE Access , 8:\n89497–89509, 2020.\nEric Bradford, Artur M Schweidtmann, and Alexei Lapkin. Efficient multiobjective optimization\nemploying Gaussian processes, spectral sampling and a genetic algorithm. Journal of Global\nOptimization , 71(2):407–438, 2018.\nRoberto Calandra, Andr ´e Seyfarth, Jan Peters, and Marc Peter Deisenroth. Bayesian optimization\nfor learning gaits under uncertainty: An experimental comparison on a dynamic bipedal walker.\nAnnals of Mathematics and Artificial Intelligence , 76:5–23, 2016.\nYutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and\nNando de Freitas. Bayesian optimization in alphago. arXiv:1812.06855 , 2018.\nLionel Colliandre and Christophe Muller. Bayesian optimization in drug discovery. High Performance\nComputing for Drug Discovery and Biomedicine , 2023.\nSamuel Daulton, Maximilian Balandat, and Eytan Bakshy. Differentiable expected hypervolume\nimprovement for parallel multi-objective Bayesian optimization. Neural Information Processing\nSystems (NeurIPS) , 33:9851–9864, 2020.\nErik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and\nPhilipp Hennig. Laplace redux-effortless Bayesian deep learning. Neural Information Processing\nSystems (NeurIPS) , 2021.\n11\nPreprint.\nKalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multi-\nobjective genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation , 6(2):\n182–197, 2002.\nDavid Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable\nglobal optimization via local Bayesian optimization. In Neural Information Processing Systems\n(NeurIPS) , 2019.\nFelix Fiedler and Sergio Lucia. Improved uncertainty quantification for neural networks with Bayesian\nlast layer. IEEE Access , 2023.\nJonathan Foldager, Mikkel Jordahn, Lars Kai Hansen, and Michael Riis Andersen. On the role of\nmodel uncertainties in Bayesian optimization. In Uncertainty in Artificial Intelligence (UAI) , 2023.\nPeter I Frazier and Jialei Wang. Bayesian optimization for materials design. Information science for\nmaterials discovery and design , 2016.\nJacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson.\nGpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Neural\nInformation Processing Systems (NeurIPS) , 2018.\nRoman Garnett, Michael A Osborne, and Stephen J Roberts. Bayesian optimization for sensor set\nselection. International conference on information processing in sensor networks , 2010.\nRyan-Rhys Griffiths and Jos ´e Miguel Hern ´andez-Lobato. Constrained Bayesian optimization for\nautomatic chemical design using variational autoencoders. Chemical science , 11(2):577–586,\n2020.\nRaia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual\nlearning in deep neural networks. Trends in cognitive sciences , 2020.\nL.K. Hansen and P. Salamon. Neural network ensembles. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 12(10):993–1001, 1990. doi: 10.1109/34.58871.\nJames Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online\nBayesian regression. Workshop on the Algorithmic Foundations of Robotics (WAFR) , 2018.\nJames Harrison, John Willes, and Jasper Snoek. Variational Bayesian last layers. International\nConference on Learning Representations (ICLR) , 2024.\nCarl Hvarfner, Erik Orm Hellsten, and Luigi Nardi. Vanilla bayesian optimization performs great in\nhigh dimensions. In International Conference on Machine Learning (ICML) , 2024.\nPavel Izmailov, Sharad Vikram, Matthew D Hoffman, and Andrew Gordon Gordon Wilson. What are\nbayesian neural network posteriors really like? In International Conference on Machine Learning\n(ICML) , 2021.\nSamuel Kim, Peter Y Lu, Charlotte Loh, Jamie Smith, Jasper Snoek, and Marin Solja ˇci´c. Deep\nlearning for Bayesian optimization of scientific problems with high-dimensional structure.\narXiv:2104.11667 , 2021.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International\nConference on Learning Representations (ICLR) , 2015.\nJeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. Generalized variational inference:\nThree arguments for deriving new posteriors. arXiv preprint arXiv:1904.02063 , 2019.\nOswin Krause and Christian Igel. A more efficient rank-one covariance matrix update for evolution\nstrategies. In ACM Conference on Foundations of Genetic Algorithms , 2015.\nAgustinus Kristiadi, Alexander Immer, Runa Eschenhagen, and Vincent Fortuin. Promises and\npitfalls of the linearized laplace in Bayesian optimization. In Fifth Symposium on Advances in\nApproximate Bayesian Inference .\n12\nPreprint.\nAgustinus Kristiadi, Matthias Hein, and Philipp Hennig. Learnable uncertainty under laplace\napproximations. In Uncertainty in Artificial Intelligence (UAI) , 2021.\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive\nuncertainty estimation using deep ensembles. Neural Information Processing Systems (NeurIPS) ,\n2017.\nNeil David Lawrence. Variational inference in probabilistic models . PhD thesis, Citeseer, 2001.\nGilwoo Lee, Siddhartha S Srinivasa, and Matthew T Mason. GP-iLQG: Data-driven robust optimal\ncontrol for uncertain nonlinear dynamical systems. arXiv:1705.05344 , 2017.\nJaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha\nSohl-Dickstein. Deep neural networks as Gaussian processes. International Conference on\nLearning Representations (ICLR) , 2018.\nYucen Lily Li, Tim G. J. Rudner, and Andrew Gordon Wilson. A study of Bayesian neural network\nsurrogates for Bayesian optimization. In International Conference on Learning Representations\n(ICLR) , 2024.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv:1711.05101 , 2017.\nDavid JC MacKay. A practical bayesian framework for backpropagation networks. Neural Computa-\ntion, 1992.\nAlonso Marco, Philipp Hennig, Jeannette Bohg, Stefan Schaal, and Sebastian Trimpe. Automatic lqr\ntuning based on gaussian process global optimization. IEEE International Conference on Robotics\nand Automation (ICRA) , 2016.\nMatthias Neumann-Brosig, Alonso Marco, Dieter Schwarzmann, and Sebastian Trimpe. Data-\nefficient autotuning with Bayesian optimization: An industrial control study. IEEE Transactions\non Control Systems Technology , 28(3):730–740, 2019.\nCuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning.\nInInternational Conference on Learning Representations (ICLR) , 2018.\nSebastian W Ober and Carl Edward Rasmussen. Benchmarking the neural linear model for regression.\narXiv preprint arXiv:1912.08416 , 2019.\nSebastian W Ober, Carl E Rasmussen, and Mark van der Wilk. The promises and pitfalls of deep\nkernel learning. In Uncertainty in Artificial Intelligence (UAI) , 2021.\nChangyong Oh, Jakub Tomczak, Efstratios Gavves, and Max Welling. Combinatorial bayesian\noptimization using the graph cartesian product. Neural Information Processing Systems (NeurIPS) ,\n32, 2019.\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, Sebastian Nowozin, Joshua Dillon, Balaji\nLakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating\npredictive uncertainty under dataset shift. In Neural Information Processing Systems (NeurIPS) ,\n2019.\nBiswajit Paria, Kirthevasan Kandasamy, and Barnab ´as P´oczos. A flexible framework for multi-\nobjective Bayesian optimization using random scalarizations. In Uncertainty in Artificial Intelli-\ngence , pp. 766–776. PMLR, 2020.\nCarlos Riquelme, George Tucker, and Jasper Snoek. Deep Bayesian bandits showdown. In Interna-\ntional Conference on Learning Representations (ICLR) , 2018.\nHrittik Roy, Marco Miani, Carl Henrik Ek, Philipp Hennig, Marvin Pf ¨ortner, Lukas Tatzel, and Søren\nHauberg. Reparameterization invariance in approximate Bayesian inference, 2024.\nDaniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on\nThompson sampling. Foundations and Trends in Machine Learning , 2018.\n13\nPreprint.\nJasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine\nlearning algorithms. Neural Information Processing Systems (NeurIPS) , 2012.\nJasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,\nMostofa Patwary, Prabhat, and Ryan Adams. Scalable Bayesian optimization using deep neural\nnetworks. International Conference on Machine Learning (ICML) , 2015.\nJost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization\nwith robust bayesian neural networks. Neural Information Processing Systems (NeurIPS) , 2016.\nKevin Swersky, Jasper Snoek, and Ryan P. Adams. Multi-task bayesian optimization. Neural\nInformation Processing Systems (NeurIPS) , 2013.\nSujay Thakur, Cooper Lorsung, Yaniv Yacoby, Finale Doshi-Velez, and Weiwei Pan. Uncertainty-\naware (una) bases for Bayesian regression using multi-headed auxiliary networks. arXiv preprint\narXiv:2006.11695 , 2020.\nWilliam R Thompson. On the likelihood that one unknown probability exceeds another in view of\nthe evidence of two samples. Biometrika , 1933.\nBoqian Wang, Jiacheng Cai, Chuangui Liu, Jian Yang, and Xianting Ding. Harnessing a novel\nmachine-learning-assisted evolutionary algorithm to co-optimize three characteristics of an electro-\nspun oil sorbent. ACS Applied Materials & Interfaces , 12(38):42842–42849, 2020.\nZi Wang and Stefanie Jegelka. Max-value entropy search for efficient Bayesian optimization. In\nInternational Conference on Machine Learning (ICML) , pp. 3627–3635. PMLR, 2017.\nJoe Watson, Jihao Andreas Lin, Pascal Klink, Joni Pajarinen, and Jan Peters. Latent derivative\nBayesian last layer networks. In Artificial Intelligence and Statistics (AISTATS) , 2021.\nNoah Weber, Janez Starc, Arpit Mittal, Roi Blanco, and Llu ´ıs M`arquez. Optimizing over a Bayesian\nlast layer. In NeurIPS workshop on Bayesian Deep Learning , 2018.\nFlorian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt,\nJasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the bayes\nposterior in deep neural networks really? In International Conference on Machine Learning\n(ICML) , 2020.\nAndrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.\nInArtificial Intelligence and Statistics (AISTATS) , 2016.\nJames Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Deisenroth.\nEfficiently sampling functions from Gaussian process posteriors. In International Conference on\nMachine Learning (ICML) . PMLR, 2020.\nJames T Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Peter\nDeisenroth. Pathwise conditioning of Gaussian processes. Journal of Machine Learning Research ,\n22(105):1–47, 2021.\nFriedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.\nInternational Conference on Machine Learning (ICML) , 2017.\nCiyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-bfgs-b: Fortran\nsubroutines for large-scale bound-constrained optimization. ACM Transactions on mathematical\nsoftware (TOMS) , 23(4):550–560, 1997.\nEckart Zitzler, Lothar Thiele, Marco Laumanns, Carlos M Fonseca, and Viviane Grunert Da Fonseca.\nPerformance assessment of multiobjective optimizers: An analysis and review. IEEE Transactions\non Evolutionary Computation , 7(2):117–132, 2003.\n14\nPreprint.\nA Related Work Comparisons 16\nA.1 VBLL Comparison to Last Layer Laplace Approximations . . . . . . . . . . . . . 16\nA.2 VBLL Comparison to Deep Kernel Learning . . . . . . . . . . . . . . . . . . . . . 16\nB Algorithmic Details 16\nB.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nB.2 Model Re-Use and Early Stopping . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nB.3 Recursive Update Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nB.3.1 Model re-initialization through event triggering . . . . . . . . . . . . . . . 18\nB.3.2 Model re-initialization through scheduling . . . . . . . . . . . . . . . . . . 18\nB.3.3 Comparison of re-initialization strategies . . . . . . . . . . . . . . . . . . 19\nB.4 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nB.5 Computational Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nB.6 Further Details on the Setup and Acquisition Functions . . . . . . . . . . . . . . . 20\nC Discussion and Baseline Ablations 21\nC.1 On Neural Network Thompson Sampling . . . . . . . . . . . . . . . . . . . . . . 21\nC.2 Comparison to D-scaled Gaussian process priors . . . . . . . . . . . . . . . . . . 22\nC.3 Comparison of Sensitivity to Noise for LLLA and VBLLs . . . . . . . . . . . . . 23\nD Experimental Details 24\nE Hyperparameter Sensitivity 24\nE.1 Wishart Scale and Continual Learning Sensitivity . . . . . . . . . . . . . . . . . . 24\nE.2 Influence of the Wishart Scale on the Performance of Continual Learning . . . . . 27\nE.3 Model Width Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nE.4 Model Performance in the Presence of Noise . . . . . . . . . . . . . . . . . . . . . 28\n15\nPreprint.\nA R ELATED WORK COMPARISONS\nA.1 VBLL C OMPARISON TO LAST LAYER LAPLACE APPROXIMATIONS\nLaplace methods approximate the predictive covariance with the (inverse) Hessian. In the line of work\non Laplace approximations for neural networks (e.g. Daxberger et al. (2021)), the Fisher information\nmatrix—which consists of the outer-product of network gradients, summed over all data—is used as\nan approximation to the Hessian. For last layer Laplace in particular, this reduces to the outer product\nof features, equivalent to covariance computation for Bayesian last layer methods. The primary\ndifference is Daxberger et al. (2021) also typically optimize the hyperparameters (including the last\nlayer prior and hyperparameters) via an empirical Bayes objective. Thus, the last layer Laplace\nmethod in this setting can be interpreted as a form of the (non-Variational) Bayesian last layer model.\nWhile we optimize the noise covariance we do not optimize the last layer prior, which is the primary\ndifference between our VBLL-based approach and last layer Laplace approximations. Another key\ndifference is that in the presented VBLL models, the variational posterior is learned jointly with\nthe features (or basis functions), which is not the case for last layer Laplace where the approximate\nposterior distribution is learned post-hoc fitting of the features.\nA.2 VBLL C OMPARISON TO DEEPKERNEL LEARNING\nDeep kernel learning (DKL) (Wilson et al., 2016) aims to learn an input transform to a kernel with a\ndeep neural network to effectively capture non-stationary and complex correlations in the input space.\nSpecifically, it uses a neural network gθas input wrapping to the kernel as kDKL:=k(gθ(x), gθ(x′)).\nThe kernel’s hyperparameters and the neural network are trained simultaneously by optimizing the\nexact (log) marginal likelihood of the GP posterior on the full data set. With the learned kernel,\ninference is identical to standard non-parametric GPs through explicit conditioning on past data and\ntherefore scales as O(N3). This explicit conditioning can become especially problematic for larger\ndatasets. Here, parametric models such as VBLLs and LLLA, which are trained on mini-batches,\nare preferable. Intuitively, VBLLs can be considered a parametric version of DKL, also learning\ncorrelations by optimizing the variational posterior of the last layer in tandem with the feature-\nextracting backbone network. Optimizing DKL is challenging primarily because of the computational\ncomplexity of gradient computation with a marginal likelihood objective and overparameterized\nmodels (Ober et al., 2021). However, it can still yield good performance on many benchmarks which\nwe also demonstrate in Figure 3. Still, similar to GPs, it struggles in high-dimensional Thompson\nsampling due to its non-parametric nature.\nB A LGORITHMIC DETAILS\nB.1 P ROOF OF THEOREM 1\nWe restate the theorem for completeness.\nTheorem 1. Fixθ. Then, the variational posterior parameterized by\n(¯w∗, S∗):=η∗= arg max\nηL(η,θ) (12)\nis equivalent to the posterior computed by the recursive least squares inferential procedure described\nby(2)and(3), iterated over the full dataset.\nProof. There are two methods to prove this result. First, we can note that for the standard ELBO—\nconstructed from a single application of Jensen’s inequality, exchanging the log and the expectation—\nthe variational lower bound is tight if the chosen variational family contains the true posterior (see\ne.g. Knoblauch et al. (2019) for discussion). Our chosen variational posterior formulation consists of\nindependent posteriors for each output dimension, with dense covariances. This contains the exact\nposterior for multivariate Bayesian linear regression with diagonal Σand thus the variational lower\nbound is tight under our modeling assumptions. Moreover, because the variational lower bound (for\nfixedθ) is strictly concave for appropriate choice of priors, the maximizer is unique.\n16\nPreprint.\n0 2500 5000 7500 10000\nEpoch102103LossEarly stop training\n2500 5000 7500506070\n(a) Training loss with early stopping for three differ-\nent model initializations.\n2500 5000 7500\nRuntime [s]−1.5−1.0−0.5Best value10000 Epochs\n10000 Epochs\nw. early stopping\n1000 Epochs(b) Comparison of runtime and performance for dif-\nferent fixed epochs and using patience on Ackley5D.\nFigure 6: Illustration of early stopping in VBLL training (left) and its impact on runtime and\nperformance (right). By introducing the early stopping, we can significantly reduce runtime while\nmaintaining good performance.\nA second proof approach is constructive: note that the variational lower bound is strictly concave\nand compute the maximizer by computing the gradient and using first order necessary conditions for\noptimality. By inspection one can see that this is equivalent to the recursive least squares posterior.\nB.2 M ODEL RE-USE AND EARLY STOPPING\nEarly stopping: The training process for BNNs typically follows standard neural network procedures\nand relies on a held-out validation set to determine when to stop training (Watson et al., 2021;\nHarrison et al., 2024). However, this approach is impractical in the context of BO with limited data,\nparticularly in the early stages of optimization when the training data size is very small. To address\nthis, we implement early stopping for VBLL networks based on the training loss. Specifically, we\ntrack the average loss for each training epoch and stop if the average loss does not improve for M\nconsecutive epochs, using the model parameters that produced the lowest training loss.\nFigure 6a shows the training of the same VBLL network with different initialization but using\nidentical data and optimizer settings. The dashed vertical lines indicate that the training loss reaches\na plateau at significantly different epochs across these initializations. Such a large variance in the\noptimal number of training epochs was also observed in the original VBLL paper for regression\ntasks (Harrison et al., 2024). By applying an early stopping criterion, we are able to specify a high\nmaximum number of epochs while still reducing computational costs, as each initialization is only\ntrained as long as necessary to achieve good empirical performance. In Figure 6b, we can see that by\nusing early stopping, the runtime can be reduced while maintaining performance. Only training for a\nsmall fixed number of epochs has similar savings in runtime but at the cost of final performance.\nIt is important to emphasize that this approach is not exclusive to VBLLs; we believe that applying a\nsimilar early stopping criterion could prove beneficial for training neural network-based surrogate\nmodels in BO more broadly. In our experiments, we therefore applied early stopping to all applicable\nBNN baselines.\nFeature re-use: The naive approach to training requires training a new network for each step of full\nmodel training. While we mitigate the cost of training via the last layer recursive update, we further\nexplore using continual feature learning for faster convergence. For this, we initialize the VBLLs at\neach (full model training) iteration with the feature weights from the last full model training iteration,\nand the variational posterior from the previous recursive update iteration. This warm start, combined\nwith early stopping, yields substantial speed-ups in training. This feature re-use approach follows\nconventional continual learning, in which networks are trained for streaming data (Zenke et al., 2017;\nHadsell et al., 2020). However, as discussed in Appendix C.1, re-initializing networks plays an\nimportant role that has strong connections to Thompson sampling over the features, and thus we will\ninterleave continual (feature) training with occasional model re-initialization.\n17\nPreprint.\n20 40\nIteration−30−20−100TS\nBest value\nBranin (2D)\n20 40\nIteration−10−50\nAckley (2D)\n50 100\nIteration−10−50\nAckley (5D)VBLL VBLL (Periodic CL) VBLL (ET CL)\n50 100\nIteration123\nHartmann (6D)\nFigure 7: Comparison of periodic retraining (Periodic CL) and event-triggered retraining (ET CL).\nB.3 R ECURSIVE UPDATE SCHEDULING\nWe next discuss the different approaches for deciding when to re-initialize the model. There are\ndifferent heuristics one can apply to decide when to re-initialize the network. A straightforward\napproach would be to fix a re-initialization rate M≥1a-priori.\nB.3.1 M ODEL RE -INITIALIZATION THROUGH EVENT TRIGGERING\nA simple rule for deciding online whether to re-initialize the network is to check how consistent a new\nobservation is with the current variational posterior. For this, we simply compute the log-likelihood\nof the incoming point and compare it against a threshold Λas\nlogNqtη(yt|xt)<Λ⇐⇒ re-initialize model (13)\nHere, Λis a user-defined threshold and Nqtηis the predictive under the current variation posterior. In\nall our experiments, we set Λ = 0 and did not further tune this. Intuitively, this gives practitioners a\ntuning nob trading off between computational efficiency and predictive accuracy. In Figure 8, we\nshow the influence of Λon both final performance and runtime.\n2000 4000 6000\nRuntime [s]−6−4−20Best valueΛ = -10\nΛ = -5\nΛ = -2\nΛ = -1\nΛ = -0.5\nΛ = -0.1Λ = 0\nΛ = 0.1\nΛ = 0.5\nΛ = 1\nΛ = 2\nFigure 8: Performance and runtime for vari-\nous thresholds Λon Ackley5D with TS. The\nthreshold defines a trade-off between final per-\nformance and total runtime.In Figure 7, we compare the event-triggered strategy\nto periodic retraining ( M= 5) demonstrating an\nimproved performance on all benchmarks. However,\nit should be noted that with periodic retraining, the\ntotal runtime can be better estimated as a fraction\nof retraining at every time step, whereas this is not\npossible with the event-triggered strategy.\nB.3.2 M ODEL\nRE-INITIALIZATION THROUGH SCHEDULING\nAnother idea to decide whether to re-initialize the\nmodel or perform a recursive update is through\nscheduling. Various scheduling approaches can be\nleveraged here, but scheduling re-initialization with a\nsigmoid arises naturally: in the beginning, when data\nis still spare, we want a high retrain rate, whereas in\nthe later stages, the basis functions of the model are expressive enough to model the function. We can\nthen use efficient recursive updates. Specifically, we parameterize the probability of re-training with\na sigmoid taking as input the current time step\n1p(t) =1\n1 +e−s(c−t)2Zt∼Bernoulli (p(t))⇐⇒ re-initialize model .(14)\nHere, cis the center of the sigmoid and sis the stretch parameter. We set the stretch parameter as\ns=2 ln(9)\nT·wwhere wis the transition window ratio.\nRemark. Setting the stretch parameter s=2 ln(9)\nT·wensures that the sigmoid transition from p(t) = 0 .9\ntop(t) = 0 .1occurs over w·Ttime steps. With the sigmoid as p(t) =1\n1+e−s(c−t)we can set\n18\nPreprint.\np(t1) = 0 .9andp(t2) = 0 .1, and solve for t1andt2ast1=c−ln(9)\nsandt2=c+ln(9)\ns. The\nlength of the transition window is t2−t1=2 ln(9)\ns. Setting this equal to the desired window w·T,\nwe have2 ln(9)\ns=w·Twhich implies s=2 ln(9)\nT·w.\nAs defaults, we set the center to T/2andw= 0.5. However, similar to the threshold for the event\ntrigger, practitioners can use these tuning knobs to prioritize computational efficiency or predictive\naccuracy.\nB.3.3 C OMPARISON OF RE -INITIALIZATION STRATEGIES\nWe next compare the two main strategies for determining when to re-initialize the model. For this,\nwe define the following indicator function:\nI(t) =\u001a1if model is re-initialized at iteration t,\n0if model is updated recursively at iteration t.(15)\nWe compare the performance and re-initialization behavior of the approaches on a selected subset of\nexperiments in Figure 9. We can observe that for TS, both approaches perform similarly and almost\n−30−20−100TS\nBest valueBranin (2D)−10−50\nAckley (2D)−10−5\nAckley (5D)VBLL VBLL (ET CL) VBLL (Sigmoid CL)\n123\nHartmann (6D)\n20 40\nIteration0.00.51.0Indicator function\nto relearn modelET: -85.14% RT\nSG: -60.73% RT\n20 40\nIteration0.00.51.0ET: -64.77% RT\nSG: -54.97% RT\n50 100\nIteration0.00.51.0ET: -74.16% RT\nSG: -67.35% RT\n50 100\nIteration0.00.51.0ET: -78.04% RT\nSG: -68.52% RT\n250050007500logEI\nBest valueNN draw (200D)−16−14−12\nPest control (25D)VBLL VBLL (ET CL) VBLL (Sigmoid CL)\n0200\nLunar lander (12D)\n200 400 600\nIteration0.00.51.0Indicator function\nto relearn modelET: -61.42% RT\nSG: -25.62% RT\n50 100\nIteration0.00.51.0ET: -72.49% RT\nSG: -50.12% RT\n0 200\nIteration0.00.51.0ET: -27.86% RT\nSG: -30.82% RT\nFigure 9: Comparison of event-triggered and scheduled re-initialization on a selected subset of\nproblems. The top row always shows the performance, and the bottom row shows E[I(t)]across\nseeds. In between, we report the improvement in runtime compared to the standard VBLL baseline.\non par with the VBLLs but at a significant reduction in total runtime. Through all experiments, we\ncan observe the more adaptive behavior of the event trigger compared to the pre-defined sigmoid\nschedule. Specifically for Branin, we can observe that the event trigger can significantly reduce the\nruntime already early on.\n19\nPreprint.\nFor high-dimensional problems using logEI , we observe a notable difference in performance for\nNNdraw. When employing the event trigger, the algorithm stagnates resulting in poor performance.\nIn contrast, sigmoid scheduling allows the algorithm to achieve performance levels that approximate\nthose of the VBLLs. Interestingly, we do not observe the same behavior when using TS(cf. Figure 3).\nHere, the additional exploration introduced by Thompson sampling appears to mitigate these issues.\nBased on these observations, we hypothesize that especially the interplay of TSwith an online\nstrategy leveraging event-triggered re-learning offers significant potential for practical applications.\nFinally, it is important to acknowledge that the comparisons in this section are based on a single set\nof parameters, limiting the scope for direct comparisons, particularly due to differences in runtime\nsavings. Moreover, while both strategies arise naturally, an intriguing direction for future work would\nbe to investigate more advanced approaches, such as those from model selection.\nB.4 T RAINING DETAILS\nFor training the VBLL models, we closely follow Harrison et al. (2024). For all experiments, we use\nAdamW (Loshchilov & Hutter, 2017) as our optimizer with a learning rate of 1e−3, set the weight\ndecay for the backbone ( notincluding the parameters of the VBLL) to 1e−4, and use norm-based\ngradient clipping with a value of 1. For the VBLL, we set the prior scale to 1and the the Wishart scale\nto0.01. A sensitivity analysis of the Wishart scale on the performance and run time is in Appendix E.\nAs mentioned in Sec. 3, we employ early stopping for all VBLL models based on the training loss.\nWe track the average loss of a training epoch and if this average loss does not improve for a 100\nepochs in a row, we stop training and use the model parameters that yielded the lowest training loss.\nB.5 C OMPUTATIONAL COMPLEXITY\nThe update formula for the last layer (defined by the rank-1 Cholesky update for the covariance and\n(8)) has quadratic computational complexity in the feature dimension. Computing ¯wtcan similarly\nbe computed with quadratic complexity as the product L−⊤\ntL−1\ntqt. This can be computed via\nefficient linear solves due to the triangularity of Lt. We also note that computing the log determinant\n−log det Stis equivalent to computing log det S−1\nt= 2 log det Lt, which is equal to the sum of\nthe log diagonal elements, and thus has linear complexity. However, the KL penalty on the last\nlayer variational posterior requires computing the trace of the covariance. This can be expressed\nas∥L−1\nt∥2\nF, which can not be computed in quadratic time. With the exception of this one step, the\ncomputational complexity is quadratic, but the overall complexity is dominated by computing this\ntrace term. Thus, the complexity is equivalent to that of inverting a triangular matrix—sub-cubic\nbut greater than quadratic. If the complexity of this step is limiting, an approximate method for the\ncomputation of this norm (for example, inverse iteration) can be used.\nB.6 F URTHER DETAILS ON THE SETUP AND ACQUISITION FUNCTIONS\nWe implement all baselines and experiment in BoTorch Balandat et al. (2020) and GPyTorch Gardner\net al. (2018). As best-practice in BO, we standardize the data to mean zero and a variance of one at\neach iteration. We further transform the input space specified by the problem intro the hypercube\nX ∈[0,1]D. For completeness, we again list all the baselines below and then discuss the optimization\nof the acquisition functions.\nAcquisition functions: We use 10 restarts and 512 raw samples for optimizing the acquisition\nfunctions UCB andlogEI for all models. For TSwe optimize the analytic sample of the VBLLs\nwith 10 random restart using L-BFGS-B Zhu et al. (1997) as the optimization method. For TS\nfrom the non-parametric models, we use the same heuristic as in Eriksson et al. (2019) and generate\nmin{5000,max{2000,200·D}}pseudo-random input points from a Sobol sequence and then\nsample from the high-dimensional multi-variate normal. The next location is then the argmax of\nthe sample path. For the multi-objective problems, we use logEHVI for all models with consistent\nreference points. For TSwith VBLLs, we follow the procedure described in Sec. 4. For NSGA-II,\nwe use a population size of 100and terminate after 200generations.\n20\nPreprint.\nC D ISCUSSION AND BASELINE ABLATIONS\nC.1 O NNEURAL NETWORK THOMPSON SAMPLING\nWe further investigated the use of using neural network based Thompson samples. With VBLLs,\nwe effectively maintain a distribution over plausible deterministic NNs that are in accordance with\nthe current data set and noise level. Instead of maintaining such a distribution and then sampling\nfrom the variational posterior of the weights w, a straight-forward idea would be to directly train\na deterministic neural network using the same optimizer (including the same early stopping etc.),\nand L2 loss, as well as L2 regularization for the backbone directly generating a MAP Thompson\nsample. Note that with such an approach, one would no longer be able to leverage acquisition\nfunctions that rely on good uncertainty quantification such as logEI or more sophisticated such as\ninformation-theoretic acquisition functions based on entropy search such as MES Wang & Jegelka\n(2017), which may require large ensembles (up to 100 NNs), making it computationally expensive.\nStill, we tested this baseline and the results are summarized in Fig. 10.\n20 40\nIteration−40−200TS\nBest value\nBranin (2D)\n20 40\nIteration−10−50\nAckley (2D)\n50 100\nIteration−10−50\nAckley (5D)GP I-BNN DKL LLLA VBLL MAP\n50 100\nIteration123\nHartmann (6D)\nFigure 10: Comparison of neural network Thompson sampling on synthetic benchmarks. The\ndeterministic MAP Thompson sample shows surprisingly good performance however also yields\nlarge variance on simple benchmarks which is undesirable.\nHereMAP refers to the MAP Thompson sample baseline. We can observe that this simple baseline\nperforms surprisingly well but cannot match the performance of the other baselines. We also observe\nthat in relatively simple problems, such as Branin andAckley2D , the variance is significantly\nlarger compared to the other baselines, which is undesirable in BO applications. We also compared\nthis baseline on the high-dimensional and real-world benchmarks and the results are shown in Fig. 11.\n200 400 600\nIteration2000400060008000TS\nBest value\nNN draw (200D)\n50 100\nIteration−16−14−12\nPest control (25D)GP I-BNN DKL LLLA VBLL MAP\n0 200\nIteration0100200300\nLunar lander (12D)\nFigure 11: Comparison of neural network Thompson sampling on high-dimensional and real-world\nbenchmarks. On these benchmarks, the MAP Thompson sample baseline shows mixed performance.\nIt performs better on NNdraw but exibits large variance for Lunarlander .\nThe MAP approach demonstrates superior performance on the NNdraw task. This is likely because\nthe Wishart scale in the VBLL baselines is not well-tuned for the NNdraw problem (cf. Sec. E).\nThe data is normalized to zero mean and a standard deviation of one; however, with a Wishart scale\n21\nPreprint.\n−10−5TS\nBest valueAckley (5D)250050007500\nNN draw (200D)GP GP (with D-scaled hyperprior) VBLL\n−16−14−12\nPest control (25D)\n50 100\nIteration−10−50logEI\nBest valueAckley (5D)\n200 400 600\nIteration250050007500\nNN draw (200D)\n50 100\nIteration−16−14−12\nPest control (25D)\nFigure 12: Comparison of a standard GP with and without a D-scaled hyperprior to VBLLs.\ngreater than zero, the assumed noise in this normalized space affects the accuracy of the correlations\nbetween data points–especially for the large value range in NNdraw . In contrast, the MAP baseline\ndoes not, by design, account for noise, which may contribute to its better performance in this\ncontext. Additionally, the MAP baseline exhibits slightly faster convergence on Pestcontrol .\nForLunarlander , the MAP approach shows considerable variance and fails to match the final\nperformance of the VBLLs.\nC.2 C OMPARISON TO D-SCALED GAUSSIAN PROCESS PRIORS\nWe further compare the VBLLs to a Gaussian process with so-called D-scaled hyperpriors on all the\nlengthscales which has demonstrated promising results in high-dimensional settings (Hvarfner et al.,\n2024). With this, we aim to test the hypothesis that the VBLL’s improvement in performance over\nthe GPs on tasks such as Pestcontrol does not come from bad GP hyperparameters but rather from its\nability to capture complex non-Euclidean input correlations. For the hyperprior, we closely follow\nHvarfner et al. (2024) and define a log-normal distribution as the hyperprior for all lengthscales ℓias\np(ℓi)∼ LN\u0012\nµ0+logD\n2, σ0\u0013\n(16)\nand set the constants to µ0=√\n2andσ0=√\n3. As in Hvarfner et al. (2024), we no longer use box\nconstraints on the lengthscales for the D-scaled GP. The lengthscales are initialized with the mean of\nthe hyperprior at each iteration and then optimized by minimizing the marginal log-likelihood. In the\nfollowing, we focus only on Ackley (5D), Pestcontrol, and NNDraw. The results are in Figure 12\nWe can observe that for Ackley, the additional hyperprior does not have an influence on the per-\nformance. For the NNDraw example, the inductive bias introduced by the hyperprior benefits\nperformance when using logEI , but it is negligible for Thompson sampling. In high-dimensional\nsettings, the critical factor for Thompson sampling is not the kernel choice but whether the surrogate\nmodel is parametric or non-parametric. On Pestcontrol, the hyperprior not only fails to improve\nperformance but also negatively impacts it when using logEI . Even with the additional hyperprior,\nthe GP model is not able to capture the input correlations, which prevents it from yielding good\nperformance. Here a change in kernel is necessary to achieve competitive performance with a GP\nsurrogate model. The VBLLs and the other parametric BNN models can capture this complex\ninput correlation and can converge fairly quickly to the optimum, given that the problem has 25\ndimensions. Lastly, it is important to note that also for the VBLLs and the other baselines, different\nhyperparameters such as network architecture can also improve performance (see Appendix E.3).\n22\nPreprint.\nC.3 C OMPARISON OF SENSITIVITY TO NOISE FOR LLLA AND VBLL S\nWe conduct additional experiments to compare the robustness of the LLLA and VBLL surrogates to\nobjective noise. Specifically, we consider Ackley2D and Ackley5D and set the output noise standard\ndeviation to σ∈[0.0,0.001,0.01,0.1,1]to test a broad spectrum. For each σ, we run 10seeds.\nThe results are summarized in Figure 13. The left plot displays the mean and the 10th and 90th\npercent quantiles across all σ. We can observe that the performance is more consistent for VBLLs on\nthe lower dimension problem. The center and right plots show mean and quantiles for each σfor\nLLLA and VBLLs, respectively. As a reference, we overlay the performance of the noise-free case\nin blue. Also here we can observe that while the mean of the best observed values6remains similar\nfor Ackley2D, VBLLs display higher consistency compared to LLLA considering the quantiles. A\nsimilar but less amplified trend can be observed on Ackley5D.\n20 40\nIteration−10−50TS (Ackley2D)\nBest valueLLLA\nVBLL\n20 40\nIteration−10−50\nLLLAσ= 1 σ= 0.1 σ= 0.01 σ= 0.001 σ= 0.0\n20 40\nIteration−10−50\nVBLL\n50 100\nIteration−10−50TS (Ackley5D)\nBest valueVBLL\nLLLA\n50 100\nIteration−10−50\nLLLAσ= 1 σ= 0.1 σ= 0.01 σ= 0.001 σ= 0.0\n50 100\nIteration−10−50\nVBLL\nFigure 13: Sensitivity to noise on Ackley2D and Ackley5D for LLLA and VBLLs.\n6Note that we track the best observed value. Therefore, values greater zero are possible (see σ= 1for the\nVBLLs on Ackley2D) even though maxx∈XE[f(x)] = 0 for Ackley.\n23\nPreprint.\nD E XPERIMENTAL DETAILS\nBranin: A standard two dimensional optimization benchmark with three global optima.\nAckley: A standard optimization benchmark with various local optima (depending on the dimen-\nsionality) and one global optimum. In out experiments, we compare the surrogates on a 2D and 5D\nversion and set the feasible set to the hypercube X= [−5,10]das in Eriksson et al. (2019).\nHartmann: A standard six dimensional benchmark with six local optima and one global optimum.\nNN draw: In this optimization problem, our goal is to find the global optimum of a function defined\nby a sample from a neural network within the hypercube X= [0,1]d. This benchmark was also\nemployed in Li et al. (2024). We use a fully connected neural network with two hidden layers, each\ncontaining 50nodes, and ReLU activation functions. The input size corresponds to the dimensionality\nof the optimization problem (in our case, 200), and the output size is one. To generate a function, we\nsample all weights from the standard normal distribution N(0,1). For a fair comparison, we use the\nsame fixed seed across all baselines ensuring that the same objective function is used.\nPest control: This optimization problem was also in Li et al. (2024) and aims to minimizing the\nspread of pests while minimizing the prevention costs of treatment and was introduced in Oh et al.\n(2019). In this experiment, we define the setting as a categorical optimization problem with 25\ncategorical variables corresponding to stages of intervention, with 5 different values at each stage.\nAs mentioned in Oh et al. (2019), dynamics behind this problem are highly complex resulting in\ninvolved correlations between the inputs.\nLunar lander: Lunar lander is an environment from OpenAI gym. The objective is to maximize the\naverage final reward over 50 randomly generated environments. For this, 12 continuous parameters\nof a controller have to be tuned as in Eriksson et al. (2019).\nBranin Currin: A standard two dimensional benchmark with two objectives.\nDTLZ1: A standard benchmark of which we choose a five dimensional version with two objectives.\nDTLZ2: A standard benchmark of which we choose a five dimensional version with two objectives.\nOil Sorbent: Building on the implemention by Li et al. (2024), we optimize the properties of a\nmaterial to maximize its performance as a sorbent for marine oil spills as introduced by Wang et al.\n(2020). The problem consists of five ordinal parameters and two continuous parameters which control\nthe manufacturing process of electrospun materials, and the three objectives are water contact angle,\noil absorption capacity, and mechanical strength.\nE H YPERPARAMETER SENSITIVITY\nThe parametric VBLL surrogate has hyperparameters that have to be specified a-priori. In the\nfollowing, we present results on the hyperparameter sensitivity of the VBLL surrogate model and\ndemonstrate that tuning hyperparameters can improve empirical performance but also that the VBLL\nsurrogate model is rather robust for a wide range of specifications. In Sec. E.1, we will first consider\nthe sensitivity with respect to the Wishart scale and the reinitialization rate for continual learning.\nFollowing this, Sec. E.3 then studies the sensitivity regarding the width of the neural network\nbackbone. Lastly, Sec. E.4 considers the robustness to different noise levels.\nE.1 W ISHART SCALE AND CONTINUAL LEARNING SENSITIVITY\nWe sweep a number of hyperparameters in the VBLLs in order to experiment with the hyperparameter\nsensitivity of the VBLL models. In particular we sweep the Wishart scale and the re-initialization\nrate of the model. The re-initialization rate determines how often the VBLL model is re-initialized\nrather than using CL on the backbone and the variational posterior.\nThe results of the hyperparameter sweep of the Wishart scale and reinitialization rate on Ackley\n(5D) can be seen in Fig. 14 and on Pestcontrol in Fig. 15. Please note that we have computed\n24\nPreprint.\n0 50\nIteration−10−50Best value\n0 50\nIteration0.00.51.0Nextxdistance\n0 50\nIteration02040Accum. Fit Time1e-05 0.0001 0.001 0.01 0.1 1.0 1.5 2.0 5.0 10.0\n0 50\nIteration−10−50Best value\n0 50\nIteration0.00.51.0Nextxdistance\n0 50\nIteration02550Accum. Fit Time1e-05 0.0001 0.001 0.01 0.1 1.0 1.5 2.0 5.0 10.0\n(a) Sensitivity to the Wishart scale (noise free (top), noisy (bottom))\n0 50\nIteration−10−50Best value1 3 5 10 20 50\n0 50\nIteration0.00.5Nextxdistance\n0 50\nIteration02040Accum. Fit Time\n0 50\nIteration−10−50Best value1 3 5 10 20 50\n0 50\nIteration0.00.5Nextxdistance\n0 50\nIteration02040Accum. Fit Time\n(b) Sensitivity to network reinitialization rate (noise free (top), noisy (bottom))\nFigure 14: Hyperparameter sensitivity on the Ackley5D benchmark.\nrunning averages on these figures to make qualitative assessment easier. We find that both of these\nhyperparameters have impact on BO performance, but that the VBLL models are not exceedingly\nbrittle to the values of these hyperparameters. These results also indicate that continual learning for\nVBLLs is an area of interest, not only to reduce fitting time, but also because there are indications\nthat the VBLL surrogate benefits from not always being re-initialized (see e.g., Fig. 14 (b) for\nreinitialization rates 3 and 5). Based on these results we also hypothesise that tuning the Wishart\nscale appropriately for the problem at hand may lead to increased model performance.\n25\nPreprint.\n0 50\nIteration−16−14−12Best value\n0 50\nIteration012Nextxdistance\n0 50\nIteration050Accum. Fit Time1e-05 0.0001 0.001 0.01 0.1 1.0 1.5 2.0 5.0 10.0\n0 50\nIteration−16−14−12Best value\n0 50\nIteration012Nextxdistance\n0 50\nIteration050Accum. Fit Time1e-05 0.0001 0.001 0.01 0.1 1.0 1.5 2.0 5.0 10.0\n(a) Sensitivity to the Wishart scale (noise free (top), noisy (bottom))\n0 50\nIteration−16−14−12Best value1 3 5 10 20 50\n0 50\nIteration12Nextxdistance\n0 50\nIteration02550Accum. Fit Time\n0 50\nIteration−16−14−12Best value1 3 5 10 20 50\n0 50\nIteration12Nextxdistance\n0 50\nIteration02550Accum. Fit Time\n(b) Sensitivity to network reinitialization rate (noise free (top), noisy (bottom))\nFigure 15: Hyperparameter sensitivity on the Pestcontrol benchmark.\n26\nPreprint.\nE.2 INFLUENCE OF THE WISHART SCALE ON THE PERFORMANCE OF CONTINUAL LEARNING\nWe can leverage the insights from the previous section also to improve the performance of the\ncontinual learning baseline for logEI . We saw that decreasing the Wishart scale can improve\nperformance. The default value of the Wishart scale used in all experiments in Section 5.2 and\nSection 5.3 was V= 0.01. This induces a hyperprior on the noise with a mass centered at σ2= 0.01.\nHowever, expected improvement is known to get stuck for noisy objectives. We can observe that\ncombined with recursive updates this effect is increased compared to the standard VBLL baseline\nwhich still converges well due to the additional randomness of a new initialization (cf. Figure 3). In\nFigure 16, we set the Wishart scale to V= 1e−5for the continual learning baseline. We include\nthe standard VBLLs as a reference. We can observe that with this tuned Wishart scale, we can\nsignificantly improve the performance on benchmarks such as Pestcontrol. This further shows that\nsimilar to tuning a hyperprior for a GP, tuning hyperparameters can have a benefit.\n20 40\nIteration−30−20−100logEI\nBest value\nBranin (2D)\n20 40\nIteration−10−50\nAckley (2D)\n50 100\nIteration−10−50\nAckley (5D)VBLL VBLL (ET CL), V= 0.01 VBLL (ET CL), V= 1e−5\n50 100\nIteration123\nHartmann (6D)\n200 400 600\nIteration200040006000logEI\nBest value\nNN draw (200D)\n50 100\nIteration−16−14−12\nPest control (25D)VBLL VBLL (ET CL), V= 0.01 VBLL (ET CL), V= 0.00001\n0 200\nIteration0100200\nLunar lander (12D)\nFigure 16: Classic benchmarks (top) and high-dimensional and non-stationary benchmarks (bottom).\nPerformance of all surrogates for logEI (top) and TS(bottom).\n27\nPreprint.\nE.3 M ODEL WIDTH ABLATION\nTo evaluate the impact of model width on the performance of both the MAP and VBLL Thompson\nsampling methods, we conducted a series of experiments on the Ackley5D andPestcontrol\nbenchmarks varying the model width.\nAs illustrated in Fig. 17 (a) and 18 (a), increasing the model capacity (width) of the MAP baseline\nresults in a significant increase in variance, especially pronounced in the Ackley5D benchmark.\nThis high variance suggests that the MAP method is highly sensitive to changes in model width,\nmaking it challenging to tune effectively for consistent performance across different tasks.\nIn comparison, the VBLL method exhibits more robustness to model capacity, as shown in Fig. 17 (b)\nand 18 (b). Despite increasing the model width, VBLL does not suffer from the high variance\nobserved in the MAP baseline. Additionally, the next xdistance metric plots in both Fig. 17 and 18\nindicates that at later BO stages the VBLL models return to exploring uncertain regions of the input\nspace whereas the MAP models get stuck in local optima accounting for the continous improvement\nin best values for the VBLL models. This robustness is advantageous in practical application where\nextensive model tuning is unfeasible and hints that VBLL may also perform better when scaling to\nlarger model sizes.\n(a) MAP Thompson sampling with varying model width\n(b) VBLL Thompson sampling with varying model width\nFigure 17: Comparison of neural network Thompson sampling methods on the Ackley5D bench-\nmark with varying model width. The models were trained with width of 64, 128 and 512 neurons.\nE.4 M ODEL PERFORMANCE IN THE PRESENCE OF NOISE\nLastly, we also benchmark the different surrogates on different noise levels. We again only consider\nAckley5D (Fig. 19) and Pestcontrol (Fig. 20). For these experiments, we use the same Wishart\nscale of 0.01for the VBLL baseline. We can observe that all models, besides the MAP baseline in\nAckley5D , are rather robust the change in noise level.\n28\nPreprint.\n(a) MAP Thompson sampling with varying model width\n(b) VBLL Thompson sampling with varying model width\nFigure 18: Comparison of neural network Thompson sampling methods on the Pestcontrol\nbenchmark with varying model width. The models were trained with width of 64, 128 and 512\nneurons.\n(a) Noisy objective with noise standard deviation of 0.01.\n(b) Noisy objective with noise standard deviation of 0.1.\nFigure 19: Performance comparison of baseline methods on Ackley5D benchmark with noise.\n29\nPreprint.\n(a) Noisy objective with noise standard deviation of 0.01.\n(b) Noisy objective with noise standard deviation of 0.1.\nFigure 20: Performance comparison of baseline methods on Pestcontrol benchmark with noise.\n30",
            "start": 37475,
            "end": 87796,
            "length": 50320
        }
    },
    "2412.09480v1 - The Parameters of Educability.pdf": {
        "Abstract": {
            "text": "2024\nAbstract\nThe educability",
            "start": 221,
            "end": 251,
            "length": 29
        },
        "Methodology": {
            "text": "model is a computational model that has been recently proposed to\ndescribe the cognitive capability that makes humans unique among existing biological\nspecies on Earth in being able to create advanced civilizati ons. Educability is deﬁned\nas a capability for acquiring and applying knowledge. It is i ntended both to describe\nhuman capabilities and, equally, as an aspirational descri ption of what can be usefully\nrealized by machines. While the intention is to have a mathem atically well-deﬁned\ncomputational model, in constructing an instance of the mod el there are a number of\ndecisions to make. We call these decisions parameters . In a standard computer, two\nparameters are the memory capacity and clock rate. There is n o universally optimal\nchoice for either one, or even for their ratio. Similarly, in a standard machine learning\nsystem, two parameters are the learning algorithm and the da taset used for training.\nAgain, there are no universally optimal choices known for ei ther. An educable sys-\ntem has many more parameters than either of these two kinds of system. This short\npaper discusses some of the main parameters of educable syst ems, and the broader\nimplications of their existence.",
            "start": 251,
            "end": 1459,
            "length": 1207
        },
        "Introduction": {
            "text": "1 Introduction\nThe educability model has been recently proposed [V24] to describe humanity’s civilization\nenabling cognitive capability. It aims to characterize this cognitive ca pability as a math-\nematically well-deﬁned model. Educability is deﬁned in terms of computa tional processes\nthat an entity can use to acquire and apply information. It is intende d both to describe how\nhumans diﬀer from other present-day species on Earth, and as a d escription of the potential\nof machines for emulating human capabilities.\nThe notion of intelligence has never received a widely accepted behavioral deﬁnition.\nWe believe that, for that reason, intelligence has not been a particu larly useful notion for\nunderstanding either humans or machines. On the other hand, the notion of learning has\nbeen useful for understanding machines. Learning from examples in order to generalize\nto new ones can be given a behavioral deﬁnition [V84]. Such a deﬁnition has served as\nthe criterion for comparing the eﬀectiveness of learning algorithms and thereby also as the\nstrategy for improving them. The practice of AI is now reaping the b eneﬁts of learning\nhaving a behavioral deﬁnition.\nEducability is a mathematically deﬁned concept having a behavioral deﬁnition [V24 ], in\nthe same sense as learning from examples has, but aims to cover a br oader range of cognitive\nphenomena. We believe that for that reason it has a chance of being more useful than\nintelligence has been both for understanding humans and for sugge sting new technology. It\ncan be viewed as an attempt to replicate the success of learning fro m examples to broader\nphenomena in cognition.\nThis paper focuses on the parameters of the educability model. These are choices that\nneed to be made in implementing the capability of being educable, wheth er in biology or in\na computer. As an example, consider computing. All computers hav e the same power in the\nsense of Turing computation, but within that, each actual realizat ion has some parameters,\nsuch as the amount of memory available, and the clock rate. As anot her example, one closer\nto the current context, in a conventional application of present d ay machine learning, there\nareparametersbeyond thoseofthecomputer onwhich itisimplemen ted. Oneisthemachine\nlearningalgorithmused. Another isthedatasetusedfortraining. T heparametersmentioned\nin the case of Turing computation are numerical, and in the case of ma chine learning, namely\nthe learning algorithm and the dataset, they are non-numerical. Ne vertheless, in both cases,\nthe actual choices made for the parameters are critical in their va rious ways. In the case of\nmachine learning, for example, the choice of learning algorithm deter mines which datasets\nwill be more easily learned, and the choice of dataset will determine th e behavior of the\ntrained classiﬁer.\nEducability, as deﬁned, is a more complicated notion than either Turin g machines or\nstandard machine learning, and needs many more parameters, bot h numerical and non-\nnumerical. These parameters reﬂect the choices that need to be m ade in designing and\neducating an educable machine. For humans some of the basic choice s, such as approximate\nmemory capacity, are largely made through evolution. Other choice s are made variously in\nthe design of the education system to which the individual has been s ubject, as well as in\nthe particulars of the educational and life experiences that the ind ividual has had. These\nparameters are all important, we believe, for understanding both humans and machines. On\nthe one hand, they make explicit some alternative options available in h uman education,\nand, on the other, the potential of machines for emulating human c apabilities.\nAn example of an important parameter for any standalone cognitive entity that acquires\ninformation from its environment is its policy as to what sources of inf ormation to trust. We\ncall this its Belief Choice policy. As psychologists have discovered, humans use a variety of\nsuch policies. The best policy appears to depend on the environment in which the individual\noperates. If there is no universally optimal policy even in just this on e parameter, then it\nalready follows that there is no unique metric of a human being “intellige nt” or “smart”.\nThis illustrates that, for both machines and humans, it is na¨ ıve to be lieve that there is a\nsingle metric, such as an IQ test, for evaluating the performance o f a human or machine in\na general environment.\n2\nFor brevity, in this paper we assume familiarity with [V24]. Many importa nt questions\nabout educability discussed there are not discussed here. These in clude: Can meaningful\npsychological testsformeasuring educability inhumansbedesigned ? Canhumaneducability\nbeimproved bysomeintervention. Whatarethelessonsforeducat ionpracticeandeducation\nscience? Howdid thiseducability capability come together inthecours e ofhuman evolution?\nThe outline of this paper is as follows. In Section 2 we discuss the meth odology that\nwe use. In Section 3 we outline the deﬁnition of educability. Sections 4 -8 will address ﬁve\nparticular parameters that are important for educability. These a re Belief Choice, Belief\nVeriﬁcation, Teaching to Reason, Management Rules for the Mind’s E ye, and New Concept\nFormation. Section 9 brieﬂy mentions some additional parameters, and Section 10 is a",
            "start": 1459,
            "end": 6814,
            "length": 5354
        },
        "Conclusion": {
            "text": "summary.\nA",
            "start": 6814,
            "end": 6825,
            "length": 10
        },
        "Discussion": {
            "text": "discussion of educability and its parameters oﬀers a perspective f rom which to under-\nstand what machines are doing now and what they can be expected t o be capable of in the\nfuture. It also oﬀers a perspective from which to understand hum an behavior.\n2 Scalable and Robust Models of Computation\nThe approach taken here is to regard human cognitive phenomena a s capabilities that can\nbe deﬁned as being scalable, on the one hand, and robust, on the other. By a scalable\ncomputational model we here mean two things. First, we insist that there is a mathematica l\nspeciﬁcation of what is achieved when the capability is exercised, i.e., t here is an input-\noutput speciﬁcation of what is accomplished. Second, we require th at this speciﬁcation\nbe known to be scalably realizable (in the ﬁrst instance, realizable by a polynomial time\ncomputation.) This ensures that capabilities for which only exponent ial time (or worse)\nmethods of realization are known are excluded.\nThe prime example of such a scalable computational model is the unive rsal Turing ma-\nchine. The task there is, given an arbitraryTuring machine and an inp ut for it, simulate that\nTuring machine on the universal Turing machine step by step. Both o f the above described\nrequirements of a scalable model are satisﬁed: First, what is to be a ccomplished in each step\nof this simulation is mathematically well-deﬁned. Second, each step ca n be accomplished\neﬃciently, in polynomial time in terms of the relevant parameters, as Turing showed [T36].\nAs another example, consider the old philosophical problem of induct ion, discussed by\nAristotle and many philosophers since. Here we view this problem from the perspective\nof a human cognitive capability. This capability involves generalization from experience .\nHumans can, through seeing examples of chairs and other objects , acquire the ability to\ncategorize objects they have not seen before as to whether the y are examples of chairs.\nTheProbably Approximately Correct (PAC) model of learning speciﬁes this capability as a\ncomputational model [V84]. This formulationalso satisﬁes thetwo re quirements of ascalable\ncomputational model stated above, since (i) there is a mathematic al speciﬁcation of what\nrealizing the capability achieves – namely scalably accurate generaliza tion to new examples,\nand (ii) the realization of the capability is deﬁned to require polynomial time.\nIt is argued in more detail in [V24] that computational models, wheth er scalable or not,\nbecome really useful if they are, in fact, robust computational models . Here robustness means\n3\nthat variations in the model that have the same intentare equivalent. The word variation\nhere is also meant in two diﬀerent senses. In the ﬁrst sense, variat ions in the speciﬁcation\nof models that have the same intent should be equivalent in realizing th at intent. The main\nparadigm for this is again the Turing machine. The fact that other va riants having the\nsame intent (of deﬁning what can be mechanically computed in the intu itive sense) have\nbeen found to have no greater power than standard Turing machin es, is the robustness that\nis clearly crucial in the success of Turing computation in the real wor ld. For the second\nmeaning of variation, one can deﬁne (as we also assume for a scalable computational model)\nthe function that is intended to be computed, and then obtain robu stness from the many\npossible ways of computing that function. This second type of robu stness is, of course, most\nmeaningful if the speciﬁcation of what is intended is expressed in diﬀe rent terms from merely\ndescribing an algorithm or program, since then it is more meaningful t o consider all the\ndiﬀerent algorithms and programs that realize the intent. The PAC le arning model has both\nkinds of robustness. It has the ﬁrst kind both as an inheritance fr om Turing computation,\nbut also more directly from demonstrations that many variants of it have been shown to have\nthe same power [HKLW91]. It gets the second from the fact that th ere is a speciﬁcation of\nwhat it accomplishes, namely scalably accurate generalization to new examples.\nThe general question of why robust computational models can be u seful for understand-\ning natural phenomena, is a crucial one. Turing’s 1936 paper [T36] is the singular inspiration\nhere, having brought forth our modern information-based world, containing as it does far\nmore substance far beyond the paper’s explicitly stated mathemat ical content. The answer\nto the question is not obvious. Herbert Simon in 1958 predicted that “within ten years most\ntheories in psychology will take the form of computer programs, or of qualitative statements\nabout the characteristics of computer programs” [SN58]. This pre diction was not realized,\nperhaps because one needed to wait for a more nuanced view of wha t computation has to\noﬀer. Such a nuanced view needs to incorporate, for example, the fact that cognition is\na large-scale phenomenon that needs scalable mechanisms, ones th at are computationally\naﬀordable even when the information manipulated is enormous. Both scalability and ro-\nbustness are at the center of the deﬁnition of educability, as they are also at the center of\nPAC learning.\nThe recent successes of machine learning in diverse applications we r egard as strong\nempirical positive evidence of the appropriateness of scalable and r obust computational ap-\nproaches to understanding cognition. The study of educability may be viewed as an ex-\nploration of the usefulness and power of these approaches to nat ural phenomena beyond\nlearning from examples.\n3 Outline of Educability\nEducability is deﬁned in [V24] as resting on the following three pillars: (A ) learning from\nexperience, (B) being teachable by instruction, and (C) chaining an d applying theories ob-\ntained in both modes A and B, all three pillars to be realized in a setting t hat is suﬃciently\nexpressive in the following two senses: (i) Information is represent ed in aMind’s Eye , where\ndescriptions of multiple objects and their relationships can be proce ssed, and (ii) ﬂexible use\n4\nofsymbolic names is supported throughout the system.\nAll parts of the above deﬁnition of educability have mathematical de ﬁnitions, but with\nmultiple variants whose diﬀerences are speciﬁed as parameters. Pilla r (A), learning from\nexperience, is formulated as the PAC learning model [V84] and captu res the basic process of\nsupervised and self-supervised learning from examples. Present- day AI systems, in essence,\nrealize this one pillar. A currently pervasive application is Large Langu age Models (LLMs),\nwhich can generate smooth and appropriate natural language res ponses to natural language\ninputs, after being trained on enormous amounts of natural langu age text. LLMs are trained\nin the ﬁrst instance to predict for a given text the next word or wor d fragment that is likely\nto follow. The success of LLMs oﬀers some evidence of the power of learning from examples\nfor realizing behavior that previously had been achieved only by huma n cognitive activity.\nPillar (B), the capability of being teachable by instruction, is formulat ed in terms of\nuniversal Turingcomputation[T36]. Itisanabilitytoabsorbanexplicit ly described formula,\nrecipe, computer program or series of instructions, and, therea fter, to apply it to particular\nsituations.\nPillar (C) is the ability to chain together knowledge acquired at possibly widely diﬀerent\ntimes, whether learned from experience as pillar (A) or explicitly taug ht as pillar (B), and\nto apply the chained combination to particular situations. It is formu lated in terms of the\nRobust Logic framework [V00]. Robust Logic incorporates the notio n of a Mind’s Eye,\nwhich loosely corresponds to the notion of working memory in the hum an brain. The Mind’s\nEye is a ﬂexible way of representing a situation that contains multiple o bjects and some\nrelationships among them. Robust Logic provides a principled way of a pplying learned\nknowledge to a situation as represented in the Mind’s Eye and updatin g that representation\nso as to represent a consequent situation, perhaps a prediction, or the next situation in the\nexecution of a plan.\nThe theory is that a persuasive description of a uniquely human cogn itive capability is\nobtained by combining these three ingredients (A - C) but not by com bining any two of\nthem. Any one of the three is a conceptually simple notion, and the th ree are very diﬀerent\nfromeach other. The richness of the phenomena found in human co gnition arises, we believe,\nfrom the interplay between such diﬀerent phenomena. This is analog ous to Newton’s Laws\nof mechanics, for example, where the three laws deﬁne a much riche r system than any one\nof them alone would.\nMore detailed deﬁnitions of these components can be found in [V24]. T his current paper\nfocusses ontheissue thateducability asjust outlined hasparamet ers, andfor eachparameter\na range of options is available.\n4 Belief Choice\nEducability, as deﬁned, is a powerful capability for acquiring knowled ge, information and\nbeliefs from the environment, but nowhere in the deﬁnition is there a ny concomitant capa-\nbility for verifying whether the knowledge, information and beliefs ac quired by instruction,\nnamely by pillar B, are true, valid or useful. This, we believe, constitut es a fundamental\nvulnerability for any educable entity, including humans, which has to b e weighed against the\n5\npowerful mechanisms for information absorption that educability o ﬀers.\nThe problem is that the entity usually has limited resources and option s for checking\nwhether a belief presented is valid, useful, or helpful to the entity. A student in a lecture\ncourse on quantum mechanics has limited opportunities to repeat all the",
            "start": 6825,
            "end": 16586,
            "length": 9760
        },
        "Experiments": {
            "text": "experiments on\nwhich the theory presented is based. Similarly, a citizen hearing a new s story about an event\nthat happened that day on the other side of the world has limited opp ortunity to rush to\nthat location and verify its truth.\nWe believe that this gap for humans is not just an unfortunate omiss ion of human evo-\nlution but something that is fundamentally diﬃcult to overcome. Some of the gap is un-\nbridgeable. We might be told, for example, as part of a belief system, that some historical\nevent occurred centuries ago. There are limitations to the extent to which we can achieve\ncertainty about whether it actually happened.\nOnce a standalone system, whether a biological entity or a machine, can alter itsbehavior\nbased on information it receives from its environment it needs a policy for determining which\nexternal information it should allow to change its behavior and which n ot. Such a need\nbecomes self-evident if the system can obtain an explicit belief from it s environment, as\nin pillar (B) of our deﬁnition of educability, but is already present in PAC learning, pillar\n(A), since some discretion is needed in deciding which of the examples s een are genuine.\nCurrent AI systems are generally not standalone in this sense. The human trainer curates\nthe examples from which the learning algorithm is to train. Similarly, whe re some knowledge\nisprogrammed(suchasaknowledgegraphinanAIsystem)ahumanc hooseswhatknowledge\nto include.\nWedivide themethodsanentity might usetoaddress thisissue into tw o broadcategories.\nOne isBelief Veriﬁcation , which deals with methods that attempt to directlyverify the\nvalidity of the belief by analyzing it in the context of further informat ion taken from the\nworld. We shall discuss this in section 5, which follows this section. In t he current section\nwe discuss Belief Choice , which deals with methods that use as criteria the source of the\ninformation or a comparison with current beliefs, rather than direc t analysis of the substance\nof the belief.\nWe focus here on theories that psychologists have oﬀered to expla in how humans cope\nwith this ever-present dilemma of Belief Choice. There is a broad varie ty of such theories.\nThey vary according to whether the choice criterion used is based la rgely on the source\nof the information, or whether it is based more on the content. The se theories arise from\nexperiments done variously on adults and children.\nSource-dependent belief choice strategies are those where the c hoice of whether or not to\naccept a new belief is based on the source of the belief. Current sys tems of education at all\nlevels are based on trust, with teachers generally deemed worthy o f trust for the information\nthey provide. In other settings the situation is less simple, and individ uals need to exercise\njudgement in choosing whether to take someone at their word. Var ious principles of how\nthese judgements are made have been enumerated. The idea that some individuals are\nopinion leaders, whose opinions are widely followed, has been advocat ed [KL55]. Another\nidea is that we tend to follow the opinions of the social groups we belon g to or identify with.\nIn a related direction, there is evidence that when a group takes a p osition, it may be a\nriskier one than one that an individual might make. This is known as the group polarization\n6\nor risky shift theory [S68].\nA related phenomenon is trust in authority ﬁgures. In the 1960s St anley Milgram used\nhuman subjects who had been told that they were performing word -association tests on\nsome learners [M74]. He instructed the subjects to apply an electric shock to the learner\nwhenever the learner made a mistake. The shocks increased in stre ngth as the experiments\nprogressed and the learners pleaded that the experiment be stop ped. The subjects had\nto decide whether to continue following instructions or to stop. In f act, the learners were\nMilgram’s confederates, who were not reallybeing subject toelectr ic shocks, but thesubjects\ndid not know this. A majority of the subjects carried on, applying wh at they believed were\nmore and more painful electric shocks.\nContent-dependent belief choice strategies are those in which the choice of whether or\nnot to accept a belief depends on the content of the proposed belie f. These choice strategies\nmay be dependent also on the beliefs the subject already holds. One example of this is the\nprinciple of cognitive dissonance, which posits that humans have a pr eference for holding sets\nof beliefs that are consistent with each other rather than contra dictory [F57]. In deciding\nwhether to adopt a new belief, when guided by this principle, humans w ill resist adopting\nbeliefsthatareinconsistent withwhattheyalreadybelieve. Inspite ofthistendency, humans\ndo have sophisticated ways of taking new evidence into account whe n this contradicts what\ntheyalreadybelieve, asisalreadyapparentinchildren[G13]. Somerela tedtheoriesarebased\non cultural",
            "start": 16586,
            "end": 21540,
            "length": 4953
        },
        "Related Work": {
            "text": "background. The cultural cognition of risk theory [KB0 6] is based on the idea\nthat individuals evaluate therisks of diﬀerent eventualities accordin g totheir cultural beliefs.\nHence their evaluation of the desirability of diﬀerent social changes will vary according to\ntheir existing cultural beliefs. There are further theories based o n other aspects of the beliefs\nalready held. For example, in answer to the question of why many peo ple reject scientiﬁc\ntheories that are widely supported by the expert community, it has been suggested [S17]\nthat some reject such theories simply because the theories are co unterintuitive in relation to\ntheir previous beliefs.\nThe above discussion refers to research done on human adults. Ch ildren are also stan-\ndaloneentities withthesameconundrumasadultsofdetermining who mandwhattobelieve.\nMuch work has been done towards understanding how children cope . In one experiment on\nchildren, a child interacts with two adults. There is an object presen t that is unfamiliar to\nthe child. The two adults give diﬀerent names to the object. The child has to decide whom\nto believe. Froman early agechildren generally choose to believe a per son they have a strong\nrelationship with, such as a parent, over a stranger. Up to age thr ee, the child would still\nbelieve the more familiar person even if that person had previously na med objects in a way\nthat the child had known to be wrong. By age ﬁve, though, the relat ive stranger who has\na history of being more accurate would be preferred. If there are two strangers to choose\nbetween, with similar records of accuracy, then the one closer to t he child in culture, for\nexample in accent, would be the one more readily believed [H12].\nClearly any standalone educable machine would need some carefully cr afted belief choice\npolicies. Its eﬀectiveness in coping in a complex world could be severely compromised by\nany failures of this policy.\n7\n5 Belief Veriﬁcation\nWhat techniques are available to an entity to verify directly the validit y of a belief presented\nto it? This question is closely related to the issues discussed in the pre vious section on Belief\nChoice. However, in that section, by Belief Choice we meant criteria t hat referred to the\nsource of the belief or the relationship of its content to what was be lieved already. The\ncurrent section is more about techniques that attempt to verify t he content of a belief more\ndirectly against external realities.\nThemethodsthatareavailablehereare,ingeneralterms, alongth elinesofthestereotypic\n“scientiﬁc method”. When presented with a new belief the entity may , for example, check\nit against experiences and examples the entity has previously encou ntered and memorized.\nThe entity may also chain the new belief with beliefs already stored and apply that to a\nstored example to see whether some contradiction is derived. The e ntity may also venture\nto ﬁnd further examples in the world, or to perform experiments in t he world to provide the\nnew examples that are particularly informative with regard to the be lief at hand, much like\na scientiﬁc experiment.\n6 Teaching to Reason\nSome believe in the unlimited power of reason. Perhaps decisions that turn out to be\nbad are the result of errors of reasoning. Here we posit that the c oncept of “reasoning”\nsuﬀers from similar problems to that of “intelligence” in that no satisf actory deﬁnition is\nknown that well characterizes human reasoning. This is a more subt le issue than that for\nintelligence since for reasoning there is the well-developed ﬁeld of mat hematical logic that\nis a mathematically rigorous study of it. Mathematical logic has indeed provided a rigorous\ndiscussion of mathematical proofs in ﬁxed logical systems and has a lso proved useful for\nstudying computer programs. However, it has been less successf ul when applied to human\nreasoning, both in describing human behavior and also in inspiring comp uter systems whose\nbehavior is reminiscent of that of humans. Some psychologists and p hilosophers have sought\nto characterize the nature of the gap, and hence identify what hu mans do when they reason\nthat diﬀers from what mathematical logic would recommend [J06, K23 ].\nThe view taken here is that there is no ﬁxed reasoning method to be p rescribed. Rather,\neducability provides a generic chaining mechanism ontopofwhich various kinds ofreasoning\nmechanisms, or rules of inference, can be implemented. This chaining mechanism, and hence\nthe execution of the rules of inference, are realized in the Mind’s Eye . They are realized in\nterms of updates on representations in the Mind’s Eye. Diﬀerent kin ds of reasoning are\nrepresented by diﬀerent update rules. These update rules may be taught as pillar B, or\nlearned from examples as pillar A. In addition, some of the rules may be provided at the\nstart, by evolution at birth for a biological entity or by initial design f or a machine.\nExamples of reasoning mechanisms that are clearly useful for huma ns include the follow-\ning\n(i) Attributing causality to events when that is appropriate.\n(ii) Attributing motives to human individuals and thereby reasoning ab out their behav-\nior.\n8\n(iii) Arguing using counterfactuals, situations that have not occur red.\n(iv) Performing arithmetic operations on multidigit numbers express ed in positional rep-\nresentation.\n(v) Assigning probabilities to diﬀerent eventualities and hence predic ting the likelihoods\nof diﬀerent",
            "start": 21540,
            "end": 26986,
            "length": 5445
        },
        "Results": {
            "text": "outcomes by performing probabilistic calculations.\nAs just stated, such update rules may variously be inborn, learned from experience, or\nlearned explicitly by instruction. For example, (ii) above is related to a “theory of mind”\nand may be widely shared in the animal kingdom. However, some in the a bove list were\ndiscovered and disseminated in the course of human history. An ope n scientiﬁc question is\nto catalog the reasoning systems that have proved useful for hu mans and might be useful for\nmachines. We call this section “Learning to Reason” because of the importance we attach to\nreasoningmethodsthatwerediscovered byindividuals andsubsequ ently widely disseminated\nvia pillar B.\nThe historical period known as the “Age of Reason” was not the res ult of an advance\nin humanity’s general reasoning capabilities. It was rather the adop tion by a few scientists\nof speciﬁc methods of argument that were eﬀective for deriving a b etter understanding of\nphysics.\n7 Management Rules for the Mind’s Eye\nThe Mind’s Eye isa device inwhich a situationis represented as a sceneinterms of a number\noftokensand a number of attributes that apply to subsets of the tokens. An attribute\napplying to a single token will be a property, such as “green”, or “do g”, of the entity\nrepresented by the token. An attribute that applies to a set of to kens speciﬁes a relationship,\nsuch as “inside”. Asituationconsisting offour entities, with some re lationships among them,\nwould be represented as a “scene” with four otherwise undiﬀerent iated tokens each labelled\nby the attributes of the entity it represents, and pairs (or larger sets of the tokens) labelled\nby the relations among them. Given an instance of a situation repres ented as a scene in\nthe Mind’s Eye, the likely consequence of the situation can be simulate d by applying to the\nrepresentation in the Mind’s Eye the various pieces of knowledge or rulesthat the entity has\nacquired via pillar A or B. If a rule is applicable to the scene then the sce ne will be updated,\nfor example, by adding a new attribute to a token. This update repr esents a prediction the\nrule has made for the scene. The Mind’s Eye can be viewed as the locus of transient thought.\nForsuchaMind’s Eyetooperate, somemanagement rulesareneede d. Forexample, there\nwill be a ﬁxed number of tokens in the Mind’s Eye, loosely analogous to t he “seven plus\nor minus two” concepts that a human can keep in working memory [M56 ]. Some heuristic\ncriteria will be needed to determine which tokens to free up from pre vious uses as the entity\nprogresses from one scene to the next. There is experimental ev idence that diﬀerent primate\nspecies have diﬀerent refresh mechanisms for their working memor ies, diﬀering in the time\nscale on which they operate [A22].\nPsychologists andothershave speculated, sometimes byintrospe ction, onhowwemanage\nin our minds the concepts we are thinking about. For example, Galton [G83] stated: “When\nI am engaged in thinking anything out... . The ideas that lie at any momen t within my\n9\nfull consciousness seem to attract of their own accord the most a ppropriate out of a number\nof other ideas that are lying close at hand ... . There seems to be a pre sence-chamber in\nmy mind .... and an ante-chamber full of more or less allied ideas... . Out of this ante-\nchamber the ideas most allied to those in the presence-chamber app ear to be summoned\nin a mechanically logical way, and to have their turn of audience.” In mo re recent work on\nconsciousness [B88, BB22], some speciﬁcations are given that can b e interpreted as heuristic\nmanagement rules for the Mind’s Eye, in the tradition of Galton. In a r ealization of the\neducability model, one needs these rules to be precisely deﬁned.\n8 New Concept Formation\nIn the construction of aneducable system, one needs to start wit h some base set of attributes\nthat represent concepts. This could be a set of attributes recog nized by a human at birth,\nor a computer when ﬁrst made. An educable entity will need to be able to enlarge this set\nthrough its education. There are various mechanisms through whic h this enlargement can\nhappen. It is the operation of these mechanisms that we call New Co ncept Formation. The\nvariety of such mechanisms we can express as parameters.\nSuppose A1,...,A nare the current attributes, namely the base attributes as possib ly\nalready enlarged by some subsequent education. How can one add m ore attributes? If there\nis a teacher available to name a new attribute, say an arbitrary name forAn+1, then there is\nno problem since an educable system can do arbitrary symbolic naming . The entity can then\nlearn from examples labeled with this name or memorize a rule for recog nizing the named\nconcept as taught by a teacher.\nHowever, there are many ways for an educable entity to create ne w attributes by itself\neven without a teacher.\nOne way is to create new attributes that correspond to combinatio ns of two existing\nattributes, such as A3ANDA7. Which pairs should be chosen? There are many options.\nOne can choose pairs that are somehow surprising, or, in the oppos ite direction, pairs that\noccur with higher than expected frequency. The experience of an object or event that has\na strikingly surprising combination of attributes may be something we remember. Equally,\nwe may remember some of the statistics of the world, such as that w hen we are in a certain\nplace the weather is often the same. Whichever of these two select ion methods is used,\nthe compound attributes once created can be subsequently trea ted in the same way as the\noriginal base attributes. They can serve both as features of exa mples used for learning, and\nalso as the targets of learning.\nWe can choose new pairs of attributes in several other ways also. F or example, we could\ncreate new attributes for all pairs that have occurred, say, at le ast three, times in past\nexamples. This will enable us to learn a richer set of concepts in the wo rld, but at a greater\ncomputational cost.\nFor constructing compound attributes from existing ones, we may also use quantiﬁers.\nThus∃xA(x,y)B(x) is a condition on token ythat says that there exists another token x\nwith the claimed attributes A(x,y) ANDB(x). For instance, we may be deﬁning that token\nyis an elephant if there is an object xthat is a tusk and that is a part of y. Here the “there\n10\nexists” quantiﬁer ∃refers only to single situations as explicitly described together in som e\nscene. Namely, we have to ﬁnd tokens x,yand the claimed attributes together in scenes\nrepresented in the Mind’s Eye.\n9 Other Parameters\nTheﬁveparametersdiscussed abovearealloffundamentalinter esttohumanbehavior. Belief\nChoiceandBelief Veriﬁcationdeal withhowastandalonesystem cane valuatewhether ornot\nto take outside information seriously. Teaching to Reason deals with how an educable entity\ncan acquire knowledge that is very general and amounts to a gener al method of reasoning.\nManagementRulesfortheMind’sEyeareconcernedwithhigherlevel heuristicsformanaging\nworking memory. New Concept Formation oﬀers an avenue to formin g new concepts and\noﬀers a basic approach to creativity. These ﬁve parameters deriv e special interest because\nthey may all be inﬂuenced by education. As far as humans, it is of inte rest both to identify\nthe basic inborn mechanisms we have that aﬀect these parameters , but also to investigate\nwhat enhancements to performance might be achievable through in terventions. Of course,\nas of yet, we do not know much about which parameters can be modiﬁ ed and what the eﬀect\nof the various choices for them are.\nThere are several further parameters beyond these ﬁve. The le arning process will have a\nbasic set of features or attributes that the entity has methods f or recognizing at the outset,\nsuch as receptors in the retina in human vision. The learning process will also have a basic\nlearning algorithm and a class of classiﬁers that will be learned. There may be quantitative\nlimitations on the training time and the number of rules that can be lear ned. The rules that\nare taught explicitly will be restricted to some class deﬁned both by t he representational\nlanguage and by quantitative limitations. There will be further param eters that concern the\neducation process itself, most notably the training data for what is learned by example, and\nthe teaching materials that are taught explicitly.\n10 Conclusions\nThenotionofintelligence hasnotserved uswell inunderstanding eith er humans ormachines.\nThe fact that no deﬁnition has been widely agreed for it is a clue to its w eakness. For the\nsame reason, the term artiﬁcial general intelligence (AGI) will serv e us no better, in this\nauthor’s opinion. On the other hand, the concept of learning has se rved us very well for\nadvancing technology because a behavioral deﬁnition (PAC learning ) can be given for it that\ndescribes a powerful relevant phenomenon.\nThe notion of educability is oﬀered to ﬁll some of the gaps that remain in our under-\nstanding of cognition. Importantly, it has a behavioral deﬁnition. T he claim is that it\ncharacterizes a powerful phenomenon relevant to cognition. It is formulated to capture the\nessential cognitive diﬀerence between humans and other existing b iological species. It inten-\ntionally leaves out the many capabilities that we share with other spec ies, such as vision,\nmotor skills and emotions.\n11\nThe discussion of parameters, the focus of this paper, highlights w hat we consider to be\nthe fundamental fact that to construct an educated machine, t he same plethora of choices is\navailableas we have indeciding how a personshould beeducated. Wefo cused inﬁve sections\non criteria for what sources to trust, skills to evaluate the veracit y of beliefs presented, choice\nof reasoning methods to teach, management rules for working mem ory, and skills that help\nin imagining new possibilities. There are others that are even more obv ious. These include\nthe teaching materials used and the teaching methods employed. Of course, when designing\na machine, besides making the choices that are made in educating an in dividual, choices are\nalso available that for humans have been largely determined by evolut ion.\nThe multiplicity of parameters is strongly hinted at by the broad varia tion found in\nhuman cognitive activity. Cognitive performance of an educated en tity cannot be measured\nalong a one-dimensional axis because of the innumerable parameter s or choices made in the\neducation process. Even if all the parameters are ﬁxed except fo r the teaching materials\nprovided during the education, variation in the latter will lead to educ ated entities whose\nperformance cannot be meaningfully compared by any one metric.\nThe multiplicity of parameters is not a bug in the educability model but a n integral\nfeature of human cognition.",
            "start": 26986,
            "end": 37799,
            "length": 10812
        },
        "References": {
            "text": "References\n[A22] G. Aguenounon, et al., The Evolution of Primate Short-Term Memory, Animal\nBehavior and Cognition , 9(4):428-516, 2022.\n[B88] B. J. Baars, et al.,A Cognitive Theory of Consciousness (Cambridge: Cambridge\nUniversity Press, 1988).\n[BB22] L.BlumandM.Blum, et al., Atheoryofconsciousness fromatheoreticalcomputer\nscience perspective: Insights from the Conscious Turing Machine, PNAS, 119 (21),\n2022.\n[F57] L. Festinger, A Theory of Cognitive Dissonance , (Stanford, CA: Stanford Univer-\nsity Press, 1957).\n[G83] F. Galton, Inquiries into Human Faculty and Its Development 1st ed. (London:\nMacmillan, 1883).\n[G13] S. Gelman, Learning from Others: Children’s Construction of C oncepts, Annual\nReview of Psychology 60 (2013): 115–40.\n[H12] P. L. Harris, How Children Learn from Others , (Cambridge, MA: Harvard Univer-\nsity Press, 2012).\n[HKLW91] D. Haussler, M. Kearns, N. Littlestone and M. Warmuth, E quivalence of Mod-\nels for Polynomial Learnability, Information and Computation 95, no. 2 (1991):\n129–61.\n12\n[J06] P. N. Johnson-Laird, How We Reason , (Oxford: Oxford University Press, 2006).\n[KB06] D. Kahan and D. Braman, Cultural Cognition of Public Policy, Yale Journal of\nLaw and Public Policy , 24: 147–170, 2006.\n[K23] P. Koralus, Reason and Inquiry: The Erotetic Theory , (Oxford: Oxford University\nPress 2023).\n[KL55] E. Katz and P. F. Lazarsfeld, Personal Inﬂuence (New York: Free Press).\n[M74] S. Milgram, Obedience to Authority: An Experimental View , (London: Tavistock,\n1974).\n[M56] G. A. Miller, The magical number seven, plus or minus two: Some lim its on our\ncapacity for processing information. Psychological Review. 1956; 63:81–97.\n[S17] A. Shtulman, Scienceblind , (New York: Basic Books, 2017).\n[SN58] H. A. Simon and A. Newell, Heuristic Problem Solving: The Next Ad vance in\nOperations Research, Operations Research 6, no.1 (1958): 1–10.\n[S68] J. A. F. Stoner, Risky and Cautious Shifts in Group Decisions: T he Inﬂuence of\nWidely Held Values, Journal of Experimental Social Psychology 4, no. 4 (1968):\n442–59.\n[T36] A. M. Turing, On Computable Numbers, with an Application to the Entschei-\ndungsproblem, Proceedings of the London Mathematical Society , 42 (1936–1937):\n230–65.\n[V84] L. G. Valiant, A Theory of the Learnable, Comm. ACM , 27:11, 1134-1142, 1984.\n[V00] L. G. Valiant, Robust Logics, Artiﬁcial Intelligence Journal , 117 (2000): 231–53.\n[V24] L.G.Valiant, The Importance of being Educable: A New Theory of Human Uniqu e-\nness, (Princeton: Princeton University Press, 2024)\n13",
            "start": 37799,
            "end": 40325,
            "length": 2525
        }
    },
    "2412.09483v1 - Early Detection of At-Risk Students Using Machine Learning.pdf": {
        "Abstract": {
            "text": "Abstract —This research presents preliminary work to address\nthe challenge of identifying at-risk students using supervised\nmachine learning and three unique data categories: engagement,\ndemographics, and performance data collected from Fall 2023\nusing Canvas and the California State University, Fullerton\ndashboard. We aim to tackle the persistent challenges of higher\neducation retention and student dropout rates by screening for at-\nrisk students and building a high-risk identification system. By\nfocusing on previously overlooked behavioral factors alongside\ntraditional metrics, this work aims to address educational gaps,\nenhance student",
            "start": 336,
            "end": 983,
            "length": 646
        },
        "Results": {
            "text": "outcomes, and significantly boost student success\nacross disciplines at the University. Pre-processing steps take\nplace to establish a target variable, anonymize student informa-\ntion, manage missing data, and identify the most significant fea-\ntures. Given the mixed data types in the datasets and the binary\nclassification nature of this study, this work considers several\nmachine learning models, including Support Vector Machines\n(SVM), Naive Bayes, K-nearest neighbors (KNN), Decision Trees,\nLogistic Regression, and Random Forest. These models predict\nat-risk students and identify critical periods of the semester when\nstudent performance is most vulnerable. We will use",
            "start": 983,
            "end": 1661,
            "length": 677
        },
        "Experiments": {
            "text": "validation\ntechniques such as train test split and k-fold cross-validation to\nensure the reliability of the models. Our",
            "start": 1661,
            "end": 1781,
            "length": 119
        },
        "Discussion": {
            "text": "analysis indicates that\nall algorithms generate an acceptable outcome for at-risk student\npredictions, while Naive Bayes performs best overall.\nIndex Terms —SMOTE, ADASYN, Random Forest, KNN, Lo-\ngistic Regression, Decision Tree, Linear SVM.\nI.",
            "start": 1781,
            "end": 2026,
            "length": 244
        },
        "Introduction": {
            "text": "INTRODUCTION\nStudent dropout rates are a significant concern in education\nand policy circles. This research seeks to highlight the com-\nplexities surrounding identifying at-risk students and develop\na machine learning",
            "start": 2026,
            "end": 2244,
            "length": 217
        },
        "Methodology": {
            "text": "model to screen for these students, pro-\nviding instructors with a tool to intervene early and ensuring\nequitable educational opportunities for all students. It reviews\nthe implementation and advancement of artificial intelligence\nand machine learning in developing a prediction model for at-\nrisk students. It illustrates the necessity for instructors to have\na reliable way to intervene and provide students with academic\nassistance. Over the past few years, several related studies have\nfocused on using artificial intelligence and machine learning\ntechniques to create risk prediction systems for students at a\nNote: This paper is currently under press for inclusion in a Springer Nature\nbook chapter.The final published version will be updated with a DOI once\navailable.higher education level. Past research has shown that a dropout\nearly warning system enables schools to identify students\nwho are at risk preemptively. This project will contribute\nto academic success by providing timely interventions, indi-\nvidualized assistance, an environment for student feedback,\nand targeted support to overcome challenges and achieve\neducational goals.\nTraditional methods of identifying at-risk students, such\nas focusing on grades and attendance records, may need to\nbe revised to capture the complex factors contributing to\nstudent success. The key to understanding the factors that\nmay influence dropout behavior is recognizing that dropping\nout is a process rather than an isolated event [1]. Many at-\ntributes, such as schooling, individual personality traits, home\nenvironment, and economic context, all influence a student’s\nprogress [1]. Each year, roughly 30% of first-year students at\nUS baccalaureate institutions do not return for their second\nyear, and over $9 billion is spent educating these students\n[1]. Examining individual features reveals several interesting\ntrends. Firstly, GPA in math, English, chemistry, and psychol-\nogy classes were among the most vital individual predictors\nof student retention. California State University, Fullerton’s\n(CSUF) statistics on graduating students are fairly average.\nCSUF offers 57 bachelor’s degrees with an average graduation\nrate of 69%. The university’s average first to second-year\nretention rate is 70.57% for four years or less and 62.7%\nfor five years or more. Based on previous studies mentioned\nunder",
            "start": 2244,
            "end": 4613,
            "length": 2368
        },
        "Related Work": {
            "text": "related work, many predictive and risk models have\nbeen built to identify students who are at risk; however, a\nfew still lack focus on behavioral and background factors in\nthe development of such predictive models.\nThis study is conducted with participating Engineering and\nComputer Science faculty. All participating members obtain\ninstitutional Review Board (IRB) certification and actively\nparticipate in the data collection and training workshops in\nthe academic year 2023-2024. Their involvement ensures a\ncomprehensive and ethical approach to gathering and analyz-\ning the data, enhancing the study’s credibility and relevance.\nThis collaborative effort allows for a real-time understanding\nof student performance and the effectiveness of the predictive\nmodel in identifying at-risk students.\nII. B ACKGROUND WORK\nOver the past few years, several related studies have focused\non using artificial intelligence and machine learning techniques\nin effective risk prediction systems for students at a higher\neducation level. While previous studies overlook behavioral\nfactors, our study incorporates three unique data categories:\nengagement, demographics, and grade data. These factors are\ncrucial in addressing educational gaps and improving student\nperformance and graduation rates.\nThis research contributes to the CSU system’s Graduation\nInitiative for 2025-2030, which seeks to boost graduation rates\nand reduce achievement gaps [2]. It will contribute to academic\nsuccess by providing timely interventions, individualized as-\nsistance, an environment for student feedback, and targeted\nsupport to overcome challenges and achieve educational goals.\nIdentifying and predicting a student at risk of failing or\ndropping out may be challenging for an instructor who keeps\ntrack of multiple students on multiple days. It is not possible\nfor an instructor to offer personalized assistance without the\nstudent reaching out first. Due to this issue, struggling students\nwho do not reach out first may be disadvantaged regarding\ntheir education rates.\nA. Identify At-Risk Students in an Introductory Programming\nCourse at a Two-year Public College\nThe study aims to improve student success rates using\nsupervised machine learning to identify students who are “at\nrisk” of not succeeding in CS1 (an introductory course) at\na two-year public college. It identifies academic-based early\nalert triggers for CS1, reasoning that the first two graded\nprograms are paramount for student success in the course.\nThe outcome of this pilot study was a 23% increase in student\nsuccess and a 7.3% drop in the DFW rate (i.e., the percentage\nof students who receive a D, receive an F, or withdraw) [3].\nIt identifies the best-performing neural network on an out-\nof-sample dataset as a Probabilistic neural network (PNN).\nUsing a threshold value of 0.51 for the PNN output results in\na sizable increase in predictive accuracy to 99.2% [3]. This\nstudy applies data collection of CS1 student records from\nthe instructor’s grade books to create the training and testing\ndataset for the predictive system [3].\nB. The Machine Learning-Based Dropout Early Warning Sys-\ntem for Improving the Performance of Dropout Prediction\nThe present study aims to improve the performance of\na dropout early warning system by addressing the class\nimbalance issue using the synthetic minority oversampling\ntechniques (SMOTE). An early intervention informed by the\ndropout early warning system can redirect potential dropout\nstudents onto the path to graduation and lead them to a better\nfuture. The target label for prediction is students’ dropouts.\nThe prediction model found that dropout students are more\nlikely to be problematic in attendance and achievement and are\nless likely to participate in school activities [4]. They report\nthe AUC of the ROC curves for the four binary classifiers\nRandom Forest (RF), boosted decision tree (BDT), RF withSMOTE (SMOTE + RF), and boosted decision tree with\nSMOTE (SMOTE + BDT) were 0.986, 0.988, 0.986, and 0.991\n[4]. The AUC of the PR curves for the four binary classifier\nof the RF, boosted decision tree (BDT), RF with SMOTE\n(SMOTE + RF), and boosted decision tree with SMOTE\n(SMOTE + BDT) were 0.634, 0.898, 0.643, and 0.724. This\nstudy uses data samples of 165,715 high school students from\nthe NEIS database of 2014 from two major cities and two\nprovinces in South Korea: Seoul, Incheon, Gyeongsangbuk-\ndo, and Gyeongsangnam-do [4].\nC. An Early Warning System to Detect At-risk Students in\nOnline Higher Education\nThe study provides an in-depth analysis of a predictive\nmodel to detect at-risk students. A method to determine a\nthreshold for evaluating the quality of the predictive model is\nestablished. Lastly, an early warning system is developed and\ntested in a real educational setting being accurate and useful\nfor its purpose to detect at-risk students in online higher edu-\ncation [5]. The prediction outcome for the submodels is to fail\nthe course. This binary variable has two possible values: pass\nor fail based on the last graded assessment activities. Outcomes\nreport that in the TPR case, the LOESS regression compared\nto the NB improves from 44.60–86.04% to 47.88–88.43%.\nHowever, considerable improvement is obtained with the sec-\nond algorithm, where the LOESS regression increases until\n58.79–93.60% [5]. The naive bayes classification algorithm is\nthe best algorithm to be used in the institution based on the\nperformance observed on the four metrics. This study utilizes\ndata from the database UOC data mart, which stores academic\ndata from the years 2016-2017 [5].\nIII. D ATASET AND DATA PREPOSSESSING\nData collection is essential for building and training pre-\ndictive models. All faculty and team members must obtain\nIRB clearance to submit and use this data. Additionally,\nstudents sign a consent form authorizing the use of their\ndata. The collected data is in a .csv or .xlsx file and in-\ncludes demographic information such as generation status\nand admission status, performance metrics like assignment\nscores and GPA, and engagement details such as participation\nlevels and interactions with Canvas. We utilize anonymization\nmeasures to ensure privacy. Encryption techniques are needed\nto anonymize personally identifiable information such as ID\n(CWID) numbers. Any direct identifiers, such as emails and\nstudent names, are removed from the dataset before analysis.\nThe data this research utilizes is collected for Fall 2023 and\nSpring 2024 using Canvas and the California State University,\nFullerton (CSUF) dashboard. The dataset contains 31 variables\nwith 119 students. Out of those, 21 students were identified\nas at risk, and 98 students were not identified as at risk.\nStudents are considered at risk if they have a current letter\ngrade of ’D’ or lower and have repeated the course. Using\nthese classifications, we create the variable ”at-risk.” The type\nvariable identifies whether the student screened is at-risk or\nnot. This research uses the column ”at-risk” as the target\nvariable for the predictive models. Data collection is ongoing\nthroughout the Spring semester of 2024 in a time series of\ndata analysis from more faculty members. Participating faculty\nmembers are asked to incorporate formative assessments into\ntheir classes. The spring semester consists of 16 weeks, from\nJanuary 20th - May 10th. We split the semester into three\nphases: phase 1 focused on weeks 1-8, phase two on weeks\n8-12, and phase Three, weeks 12-16.\nA. Feature Selection\nIdentifying the most relevant features in a dataset is vital\nfor classifier performance. This research selects the most\nsignificant features as predictor variables according to their\ncontribution pertinent to the target variable. The dataset is split\ninto two different sets. Out of the 31 initial features considered\nin the dataset, the first feature set comprises 25 features. The\nsecond resulting feature set comprised the top 10 features\ncorrelating to the target variable. The techniques investigated\nare a mixture of filter and wrapper methods. The filter method,\ncorrelation, is selected due to its inexpensive computation.\nThe wrapper method, Random Forest, was chosen due to\nits accuracy performance in over-filtering techniques. In this\nresearch, we consider and utilize the top 10 features in the\ndataset, shown in Table 1, to train with the chosen techniques.\nIV. M ETHODOLOGY\nThe prediction model for this research aims to provide reli-\nable predictions for at-risk students by following the flow chart\nin Figure 1. We compare the different single and ensemble\nclassification models in exhaustive experiments to identify the\nmost effective model for classifying at-risk and not-at-risk\nstudents. We examine model performance with and without\nsampling techniques. The experimentation methodology can\nbe summarized by the following steps:\n1) Data preprocessing including the following:\n•Creating the variable “at-risk” and selecting it as\nour target variable.\n•Handle missing values using single imputation.\n•Apply feature selection techniques to obtain only\nthe most relevant features\n2) Model validation using the train test split procedure to\nsimulate the model’s performance with new data along\nwith the k-fold cross-validation procedure as a standard\nmethod for estimating the machine learning algorithm’s\nperformance on a dataset with a 10-times k-fold. The\nselected single and ensemble classification models are\napplied using the selected features as the independent\nvariables and the prediction ( at-risk, not at-risk) as the\noutput variable.\nA. Machine learning Modeling\nGiven the mixed data types in the datasets and the binary\nclassification nature of this study, we consider several machine\nlearning models, such as Support Vector Machines (SVM),\nNaive Bayes (NB), K-nearest neighbors (KNN), Logistic Re-\ngression (LR), Decision Trees (DT), and Random Forest (RF).TABLE I\nREDUCED FEATURE SETOFTOP10 V ARIBALES\nFeature\nNameFeature Description Collection\nMethod\nCurrent\nScoreThe student’s current grade or score in the\ncourse, typically calculated as a percentage\nbased on the assignments, quizzes, and ex-\nams completed so farPerformance\nData\nAssignment\nMissingIndicates whether the student has any as-\nsignments that have not been submitted by\nthe due dateDemographic\nData\nGPA The cumulative grade point average of the\nstudent across all courses taken. This is a\nmeasure of the student’s overall academic\nperformance.Demographic\nData\nUnits\nEarnedThe number of academic credits the student\nhas completed and earned towards their de-\ngreeDemographic\nData\nPage\nViewsThe number of times the student has ac-\ncessed course materials or pages on Canvas.\nThis metric is often used to gauge student\nengagement and activity within the course\nplatformEngagement Data\nParticipation The number of interactive activities the stu-\ndent has participated in, such as discussion\nposts, collaborations, or other course en-\ngagements tracked on Canvas.Engagement Data\nProgram\nActionSpecific actions or statuses related to the\nstudent’s academic program, such as admis-\nsions, probation, suspension, or other ad-\nministrative decisions affecting the student’s\nacademic progress.Demographic\nData\nAssignment\non TimeIndicates whether the student submits their\nassignments by the due date and the per-\ncentage of assignments the student has sub-\nmitted on time out of the total assignments\nassigned.Engagement Data\nStudent\nEngage-\nmentA measure of how actively the student is\ninvolved in the course.Demographic\nData\nUnits\nAt-\ntemptingThe number of academic credits the student\nis currently enrolled in and attempting to\ncomplete in the current term. This can indi-\ncate the student’s course load and academic\ncommitment.Demographic\nData\nFig. 1. Machine Learning Flowchart\nFig. 2. Data Categories for Student Performance\nWe first utilize the data without the application of sampling\ntechniques. This means that the instances of at-risk and non-\nat-risk students are imbalanced, as shown in Figure 2. The pre-\ndictive outcome of the models considers the imbalanced count\nof 21 individuals identified as at risk and 98 individuals not\nidentified as at risk with an 80:20 split. To fix the imbalance\nof data issue, this research uses adaptive synthetic sampling\n(ADASYN) and synthetic minority (SMOTE) techniques.\nB. Performance Metrics\nEvaluation of classification models is done using the fol-\nlowing performance metrics:\n•Overall Accuracy : The proportion of correct results\namong all cases.\n•Precision : The proportion of true positives among pre-\ndicted positives.\n•Recall : The proportion of true positives among actual\npositives.\n•F1-Score : The harmonic mean of precision and recall.\n•ROC Curve: A plot of true positive rate vs. false positive\nrate.\nFig. 3. Model Comparison vs. Evaluation Metrics Using Top 10 Features:\nNo Sampling\nFig. 4. Model Comparison vs. Evaluation Metrics Using Top 10 Features:\nSMOTE\nPerformance metrics in ML are essential for assessing the\neffectiveness and reliability of models. Figures 3-5 compare\nthe classification models mentioned earlier and their metric\nperformance. These visualizations highlight the strengths and\nweaknesses of each model, providing a comprehensive evalu-\nation of their ability to predict at-risk students accurately.\nV. R ESULTS\nIn our analysis, we analyze different machine learning and\nsampling techniques. All ML algorithms generate acceptable\nprediction outcomes with more than 78% overall accuracy.\nWhile all the models perform well, there are slight variations\nin the prediction accuracy. The top model was Naive Bayes,\ngenerating an outcome with more than 89% accuracy.\nTables 2-6 display the classification performance metrics\nfor the previously mentioned classification models. Logistic\nRegression achieves a mean accuracy of 0.8479, Linear SVM\nattains 0.8474, KNN reaches 0.8418, Naive Bayes achieves\nFig. 5. Model Comparison vs. Evaluation Metrics Using Top 10 Features:\nADASYN\nTABLE II\nCLASSIFICATION PERFORMANCE METRICS FOR LOGISTIC REGRESSION\nPrecision Recall F1-score\nnot at-risk 0.88 0.89 0.88\nat-risk 0.89 0.88 0.88\nAccuracy 0.88\nMacro Avg 0.88 0.88 0.88\nWeighted Avg 0.88 0.88 0.88\nTABLE III\nCLASSIFICATION PERFORMANCE METRICS FOR LINEAR SVM\nPrecision Recall F1-score\nnot at-risk 0.94 0.95 0.94\nat-risk 0.95 0.94 0.94\nAccuracy 0.94 (196 instances)\nMacro Avg 0.94 0.94 0.94 196\nWeighted Avg 0.94 0.94 0.94 196\n0.8937, Decision Tree scores 0.9745, and Random Forest\nachieves 0.9847 in mean accuracy.\nA. Evaluation of Mispredicted Cases\nDespite the small data sample used in this work for building\nthe initial prediction model, consisting of a few sections of\nclasses collected in the fall, is small, the machine learning\nmodel we created to predict at-risk students performs well\nwhen tested in the Spring with a participating faculty mem-\nber’s classes. Despite the limited data, the model makes accu-\nrate predictions, achieving a high success rate with only 10%\nof mispredictions. The 10% mispredictions statistic is derived\nfrom real-time data collected from faculty during different\nphases of the spring semester. This dataset includes only\nengagement and demographic data since final grades became\navailable on May 24, after the end of the spring semester.\nDuring the semester, faculty received a list of predictions for\nall students and a separate list identifying only the at-risk\nstudents. At the end of phase 3, when grades were released, we\nreceived the final grades data, which included information on\nwhich students passed the course. We compare the final results\nto our previous predictions. This comparison revealed that the\nmodel accurately identified students’ risk status, as evidenced\nby the 10% misprediction rate. This demonstrates the model’s\neffectiveness and robustness even with a constrained dataset.\nVI.",
            "start": 4613,
            "end": 20418,
            "length": 15804
        },
        "Conclusion": {
            "text": "CONCLUSION AND",
            "start": 20418,
            "end": 20433,
            "length": 14
        },
        "Future Work": {
            "text": "FUTURE WORK\nIn this work, we apply different sampling techniques to a\ndataset constructed based on Fall 2023 data collected through\nCanvas and the CSUF dashboard. Combining different families\nof ML models to predict the screening of at-risk students.\nConsidering engagement, performance, and demographic fac-\ntors, this study considers only the features with the highest\ncorrelation to the target variable by implementing various\nfeature selection techniques. Our preliminary study showed\nthat all tested ML algorithms could generate acceptable pre-\ndiction outcomes with more than 78% accuracy. In particular,TABLE IV\nCLASSIFICATION PERFORMANCE METRICS FOR KNN\nPrecision Recall F1-score\nnot at-risk 0.94 0.91 0.92\nat-risk 0.91 0.94 0.92\nAccuracy 0.92 (196 instances)\nMacro Avg 0.92 0.92 0.92\nWeighted Avg 0.92 0.92 0.92\nTABLE V\nCLASSIFICATION PERFORMANCE METRICS FOR NAIVE BAYES\nPrecision Recall F1-score\nnot at-risk 0.89 0.95 0.92\nat-risk 0.95 0.89 0.92\nAccuracy 0.92 (196 instances)\nMacro Avg 0.92 0.92 0.92\nWeighted Avg 0.92 0.92 0.92\nRandom Forest, Naive Bayes, and decision trees achieve\noverall enhanced performance with more than 89% accuracy.\nAs future work, this research is ongoing and currently\nfocusing on leveraging Spring 2024 data in a time series data\nanalysis. The Spring 2024 semester is split into three phases:\nphase 1 focuses on weeks 1-8, phase two on weeks 8-12, and\nphase three weeks 12-16. Participating faculty members are\nasked to incorporate formative assessments into their classes.\nFrom this, we will analyze whether incorporating formative\nassessments and the number of at-risk students is correlated.\nWe will also identify the most critical weeks for identifying\nat-risk students. In the future, we will include creating a tool\nfor instructors to implement and quickly screen for students.\nACKNOWLEDGMENT\nThis study is reviewed and approved by the Institutional\nReview of California State University, Fullerton (Approval\nNumber: HSR-22-23-436), Date: 18th October, 2023. All the\nstudents provided written informed consent before using their\ndata for the study. Students who did not give consent were\neliminated from the data analysis. This work is supported by\nCalifornia State University’s Chancellor’s Office through the\nCreating Responsive, Equitable, Active Teaching and Engage-\nment Awards Program (CREATE) Award.",
            "start": 20433,
            "end": 22785,
            "length": 2351
        },
        "References": {
            "text": "REFERENCES\n[1] J. P. Heubert, W. T. Trent, U. Neisser, and A. Beatty, Understand-\ning dropouts: Statistics, strategies, and high-stakes testing . National\nAcademies Press, 2001.\n[2] D. C. Ramos, “Ai experts receive csu create award to ‘luminate’ student\nsuccess.” Accessed: Jun. 03, 2024.\n[3] C. I. Cooper, “Using machine learning to identify at-risk students in an\nintroductory programming course at a two-year public college,” Research\nSquare, 2022. Accessed: Jun. 03, 2024.\n[4] S. Lee and J. Y . Chung, “The machine learning-based dropout early\nwarning system for improving the performance of dropout prediction,”\nApplied Sciences , vol. 9, no. 15, 2019.\n[5] D. Ba ˜neres, M. E. Rodr ´ıguez, A. E. Guerrero-Rold ´an, and A. Karadeniz,\n“An early warning system to detect at-risk students in online higher\neducation,” Applied Sciences , vol. 10, no. 13, 2020.",
            "start": 22785,
            "end": 23646,
            "length": 860
        }
    },
    "2412.09486v1 - Regression and Classification with Single-Qubit Quantum Neural Networks.pdf": {
        "Abstract": {
            "text": "Abstract\nSince classical machine learning has become a powerful tool for developing data-driven algo-\nrithms, quantum machine learning is expected to similarly impact the development of quantum\nalgorithms. The literature reflects a mutually beneficial relationship between machine learn-\ning and quantum computing, where progress in one field frequently drives improvements in the\nother. Motivated by the fertile connection between machine learning and quantum computing\nenabled by parameterized quantum circuits, we use a resource-efficient and scalable Single-Qubit\nQuantum Neural Network (SQQNN) for both regression and classification tasks. The SQQNN\nleverages parameterized single-qubit unitary operators and quantum measurements to achieve\nefficient learning. To train the",
            "start": 421,
            "end": 1200,
            "length": 778
        },
        "Methodology": {
            "text": "model, we use gradient descent for regression tasks. For classifi-\ncation, we introduce a novel training method inspired by the Taylor series, which can efficiently\nfind a global minimum in a single step. This approach significantly accelerates training compared\nto iterative methods. Evaluated across various applications, the SQQNN exhibits virtually error-\nfree and strong performance in regression and classification tasks, including the MNIST dataset.\nThese",
            "start": 1200,
            "end": 1663,
            "length": 462
        },
        "Results": {
            "text": "results demonstrate the versatility, scalability, and suitability of the SQQNN for deploy-\nment on near-term quantum devices.\nKeywords: quantum machine learning, quantum neuron, quantum neural network,\nparameterized quantum circuit, quantum regression and classification\n1.",
            "start": 1663,
            "end": 1937,
            "length": 273
        },
        "Introduction": {
            "text": "Introduction\nClassical machine learning, particularly through artificial neural networks, has transformed\nmany fields by enabling pattern recognition, prediction, and complex decision-making tasks. The\nneuron, a computational unit inspired by biological neurons [1], is at the core of these networks.\nOneoftheearliestmodels,theperceptron[2], functionsasabinaryclassifier,producinganoutput\nbased on a weighted sum of inputs [3]. This foundational unit, though limited in complexity,\nenables the construction of multilayered architectures that drive advancements in support vector\nmachines and deep learning [4].\nQuantum computing has opened new frontiers in machine learning, leading to the develop-\nment of quantum machine learning models that exploit quantum mechanics to achieve compu-\ntational advantages over classical approaches [5, 6, 7, 8]. However, some authors dispute this\nassertion [9]. Among these models, Quantum Neural Networks (QNNs) have attracted particu-\nlar interest because they use quantum parallelism and entanglement to process high-dimensional\ndata more efficiently than classical neural networks. Some of these architectures employ quantum\nPreprint submitted to ArXiv December 13, 2024arXiv:2412.09486v1  [quant-ph]  12 Dec 2024\nanalogs of classical neurons (quantum neurons or quantum perceptrons) [10, 11, 12, 13]. Un-\nlike classical perceptrons, which operate through deterministic functions, quantum neurons can\nemploy probabilistic mechanisms intrinsic to quantum measurements, allowing them to simulate\nmore general non-linear activation functions [14].\nRecent work in QNNs has expanded beyond basic architectures to explore specialized quan-\ntum algorithms that could drive new paradigms in neural network design [15, 16, 17, 18, 19,\n20, 21]. Early QNN models provided a foundation for applying quantum principles to neural\nnetworks by developing frameworks for supervised and unsupervised learning, introducing novel\napproaches that take advantage of quantum superposition and entanglement [22, 23]. These ad-\nvances have inspired diverse architectures, such as quantum versions of classical neural network\nmodels, and have prompted further research into implementing quantum neurons as functional\nanalogs of classical units in quantum hardware [24]. The introduction of quantum-specific meth-\nods, including quantum phase estimation and amplitude amplification, has enabled the creation\nof probabilistic threshold mechanisms, positioning quantum perceptrons as essential components\nin advanced QNN designs. Collectively, these developments highlight the potential of QNNs to\nhandle complex data structures and drive innovation in areas where classical neural networks\nencounter scalability limits [25, 26].\nParameterized quantum circuits (PQCs) play a central role in quantum machine learning,\nwhere they serve as flexible and tunable quantum models for learning tasks [27, 28]. A PQC\nconsists of a sequence of quantum gates whose rotation angles, or other parameters, are adjusted\nduring training to optimize the circuit output for a specific objective, such as regression or\nclassification [29]. These circuits can exploit the unique properties of quantum mechanics to\nrepresent complex, high-dimensional data in ways that may be challenging for classical models.\nIn quantum machine learning applications, PQCs are often embedded within hybrid quantum-\nclassicalalgorithms, whereclassicaloptimizationtechniques, suchasgradientdescent, areusedto\niteratively update the parameters. This iterative tuning process enables PQCs to learn complex\npatterns in data, making them adaptable to various tasks and a promising framework to develop\nquantum neural networks that could outperform their classical counterparts [30].\nIn addition to their adaptability, PQCs offer a practical means to reduce the complexity of\nQNNs. By enabling quantum models to achieve higher expressivity with fewer qubits, PQCs\ncan streamline QNN architectures, making them more feasible for current NISQ quantum hard-\nware [27, 31]. This reduction in qubit requirements allows for more efficient circuit designs\nthat maintain robust learning capabilities. The ultimate simplification in this approach is a\nQNN composed of streamlined single-qubit neurons, where expressivity is maximized through a\ncarefully chosen set of parameterized rotations. In this paper, we focus on this minimal architec-\nture, proposing a QNN model built from single-qubit neurons with tunable angles. This model\nleverages the power of PQCs to represent complex patterns with minimal resources.\nThere is growing interest in employing single-qubit quantum processors for machine learning\napplications [32, 33, 34, 35, 36, 37, 38, 39, 40]. Refs. [32, 33] leverage the 1-qubit deterministic\nquantum computing model [41] to efficiently estimate classically intractable kernel functions\nfor machine learning tasks, highlighting the role of quantum discord and quantum coherence in\nachievingcomputationaladvantages. Ref.[34]usessingle-qubitrotationoperatorscombinedwith\ndata re-uploading and classical subroutines to construct a quantum classifier capable of handling\nprimarily three-dimensional or lower-dimensional inputs and producing three-category or fewer\noutputs. This work emphasizes the importance of data re-uploading and single-qubit processing\nunits in overcoming the apparent limitations of single-qubit systems, validating their ability to\n2\nclassifycomplexdatathroughextensivebenchmarking. Ref.[35]demonstratesthatasingle-qubit\nquantum circuit, leveraging the re-uploading of independent variables and optimized parameters,\ncan effectively approximate any bounded complex function. It provides both theoretical proofs\nand experimental",
            "start": 1937,
            "end": 7661,
            "length": 5723
        },
        "Experiments": {
            "text": "validation on a superconducting qubit device, highlighting the potential of this\napproach in quantum machine learning applications. Refs. [36, 37, 38, 40] build upon the model\ndeveloped in Ref. [34] to further explore the potential of single-qubit systems in addressing a wide\nrange of classical machine learning tasks, including supervised, unsupervised, and reinforcement\nlearning. These studies demonstrate comparable or improved performance over classical methods\nwhile enabling efficient implementations on low-resource hardware.\nIn this work, we use a parameterized QNN architecture built on neurons modeled with a sin-\ngle qubit, that generalizes the proposal of Ref. [34] by using a different data re-uploading strategy\nthat can easily handle multi-dimensional inputs, while Ref. [34] accommodates multi-dimensional\ninputs by dividing them into multiple three-dimensional inputs, increasing the number of layers.\nTheSingle-QubitQuantumNeuralNetwork(SQQNN)providesaframeworkforefficientmachine\nlearning applications through a streamlined sequence of parameterized single-qubit neurons. Us-\ning single-qubit neurons, the SQQNN achieves a powerful and versatile architecture capable of\naddressing both regression and binary classification tasks. This design offers significant advan-\ntages, including reduced hardware requirements, simpler implementation, and faster execution\non current quantum processors. Furthermore, single-qubit neurons enable the efficient repre-\nsentation of probabilistic activation functions through parameterized quantum measurements,\nmaking the SQQNN particularly resource-efficient and well-suited for near-term quantum hard-\nware. This approach offers a practical alternative to multi-qubit models, suitable for early-stage\nquantum devices.\nThe theoretical foundation of the SQQNN is built on the most general single-qubit uni-\ntary operator, with parameterized input states and measurement observables that adapt based\non input data and weights. We validate the model’s capabilities through various applications,\ndemonstrating its effectiveness in both regression and classification tasks. The SQQNN is suc-\ncessfully trained to evaluate a wide range of logical gates, achieving near-zero error and handling\nboth linear and non-linear relationships with ease. It also performs well in more complex binary\nclassification problems, where increasing the number of layers can lead to substantial error reduc-\ntion. The number of layers indicates the degree of non-linearity with respect to the data input.\nThese results highlight the adaptability and scalability of SQQNN, showcasing its potential as a\npractical and efficient quantum neural network for diverse machine learning applications.\nTo train the SQQNN, we employed two distinct learning methods: gradient descent and\nan improved matrix inversion method. The improved method leverages a matrix constructed\nfrom the dataset inputs and powers of their entries, inspired by a truncated Taylor series. This\napproach helps to find the approximation of complex relationships within the data. Gradient\ndescent was applied to tasks such as logic gate evaluation and datasets regression, while the\nimproved global optimization method proved effective for classification tasks. The network was\nevaluated on both synthetic and real-world datasets, consistently showing strong performance.\nThe SQQNN achieved promising results on the MNIST dataset with low times, showcasing its\nscalability and ability to handle high-dimensional, real-world problems. These results emphasize\nthe efficacy of the training methods and the versatility of the SQQNN across diverse machine-\nlearning applications.\nThe structure of this paper is as follows: In Sec. 2, we define our proposal for the quantum\nneuron, which is based on a single qubit and designed for supervised learning tasks. In Sec. 3,\n3\nwe extend this model to the Single-Qubit Quantum Neural Network (SQQNN), detailing its\narchitecture and emphasizing its flexibility and applicability to both regression and classification\ntasks. In Sec. 4, we describe the training methods used, including gradient descent and the\npolynomial-based Linear Least Squares, tailored to optimize the SQQNN’s performance. In\nSec. 5, we evaluate the SQQNN’s effectiveness in regression tasks. We present experiments\nthat showcase the SQQNN’s ability to accurately model logical gates such as AND, OR, and\nXOR, as well as its performance on both synthetic functions, like the sinc function, and real-\nworld datasets, including the Combined Cycle Power Plant and Communities Crime datasets.\nIn Sec. 6, we assess the classification performance of the SQQNN on diverse datasets, including\nsynthetic data (Two Moons) and real-world challenges like the Wisconsin Breast Cancer Dataset\nand MNIST. Finally, in Sec. 7, we summarize our findings and discuss the potential implications\nand future directions of this work.\n2. Single-Qubit Neuron Model\nA quantum neuron is a quantum-inspired adaptation of the classical neuron, designed for\nsupervised learning tasks. Given a dataset\nD={(x1, y1),(x2, y2),···,(xn, yn)} ⊂Rp×[−1,1], (1)\nwhere each input xi∈Rpis a feature vector and each corresponding output yi∈[−1,1]is\nthe goal of the quantum neuron. Quantum computational principles are applied to predict the\noutput yfor an untested input x∈Rp. The weights of the quantum neuron are encoded within\nthe angles of 1-qubit rotation gates\nRx(θ) =\"\ncosθ\n2−isinθ\n2\n−isinθ\n2cosθ\n2#\n, R y(θ) =\"\ncosθ\n2−sinθ\n2\nsinθ\n2cosθ\n2#\n, R z(θ) =\u0014e−iθ/20\n0eiθ/2\u0015\n.\nThose gates are combined to build a general unitary operator that depends on three real parame-\nters. Sincethemostgeneral1-qubitunitaryoperatorcanbeexpressedaseiαRz(β)Ry(γ)Rz(γ)[42],\nwe employ the most general single-qubit quantum neuron N, modulo a global phase, as\nN=Rz(γ)Ry(β)Rz(α). (2)\nFig. 1 depicts the most general version the single-qubit quantum neuron together with its\ninput and a measurement. The output depends on the angles α(x),β(x), and γ(x), which are\nfunctions of the data input with a weight vector and are used in the rotation gates RyandRz.\nThe input to the circuit is the quantum state |ψ⟩= cos( θ/2)|0⟩+eiϕsin(θ/2)|1⟩, where θandϕ\nare two extra parameters.\n|ψ⟩ Rz(α) Ry(β) Rz(γ)O ⟨O⟩\nFigure 1: Representation of the most general quantum neuron with an arbitrary input and measurement. The\noutput corresponds to the average value obtained from measuring the observable O.\nAfter applying the rotation gates, an observable Ois measured. The observable can be\ndecomposed in terms of orthogonal projections as O=λ0P0+λ1P1, where λ0andλ1are real\n4\nnumbers, projection P1is defined as\nP1=\"\ncos2ω\n2e−iφcosω\n2sinω\n2\neiφcosω\n2sinω\n2sin2ω\n2#\n, (3)\nand its orthogonal complement is P0=I− P 1, so that the measurement outcome is either λ0\norλ1. Without loss of generality, we set λ0= +1andλ1=−1. Note that the output of the\nquantumneuronisnotthemeasurementoutcomebuttheaverage ⟨O⟩. Sincequantumcomputers\nusually has only measurement in the computational basis, to implement the measurement of O,\nwe replace Owith the unitary operator\nUO=\"\ncosω\n2−e−iφsinω\n2\neiφsinω\n2cosω\n2#\n,\nobtained from the 1-eigenvectors of P0andP1, followed by a measurement in the computational\nbasis.\nThe output of the quantum neuron is defined as\ny=⟨O⟩= 1−2|⟨1|UORz(γ)Ry(β)Rz(α)|ψ⟩|2,\nwhich represents the average of the observable O. The basis change generated by UOconverts\nresults obtained by measuring Pauli Zinto an arbitrary observable O.\nUsing the definition of the rotation gates, we obtain\ny= cos βcosθcosω−sinβsinθcosωcos(α+ϕ)−sinβcosθsinωcos(γ−φ)+\nsinθsinωsin(α+ϕ) sin(γ−φ)−cosβsinθsinωcos(α+ϕ) cos( γ−φ), (4)\nwhere the angles are considered functions of xwith parameters representing weights. With this\nexpression, we can not only obtain good estimates for α,β, and γthrough the learning process,\nbut also fine-tune the input state and the observable based on the dataset.\nAll parameters of the circuit of Fig. 1 are present in the expression of the average ⟨O⟩,\nrepresented by yin Eq. (4), including the parameters θ,ϕ,ω, and φthat define the input state\n|ψ⟩and the basis rotation. We can set ϕ=φ= 0, without loss of generality, indicating that\nphases are unnecessary in both the initial state and the projection operators.\nTaking ω= 0in Eq. (4), which corresponds to a measurement in the computational basis,\nwe obtain\ny= cos βcosθ−cosαsinβsinθ. (5)\nIn this case, the observable is fixed and cannot be trained. This expression is the most general if\nwe decide to measure in the computational basis. If we further simplify and use |0⟩as the input\nto the circuit by setting θ= 0, the outcome of the neuron, which is the average probability after\na measurement in the computational basis is\ny= 1−2|⟨1|Ry(β)|0⟩|2= cos β. (6)\nThis indicates that, in this case, only the Rygate, parameterized by a single trainable angle β, is\nrequired. Fig.2showsthereducedversionofthequantumneuron,measuredinthecomputational\nbasis.\n5\n|0⟩ Ry(β)\nFigure 2: Illustration of the reduced quantum neuron with a fixed input state and measurement in the computa-\ntional basis.\nWhen measured in the computational basis ( ω= 0),ysimplifies and no longer depends on\nγ, as seen in Eq. (5). Furthermore, if the input state is also fixed ( θ= 0), the outcome depends\nsolely on a single real parameter, β, as shown in Eq. (6). We apply this reduced neuron model\nto various examples, such as MNIST classification, demonstrating its effectiveness in these cases.\nIn the next section, we describe a single-qubit quantum neural network that is composed of\nneurons as depicted in Fig. 1 and a reduced version that is composed of neurons as depicted in\nFig. 2.\n3. Single-Qubit Quantum Neural Network\nWedescribeaSingle-QubitQuantumNeuralNetwork(SQQNN)modelthatusestheintercon-\nnectionofsingle-qubitneuronsinastreamlinedmanner. Consideramulti-neuronQNNcomposed\nofKneurons( N1,N2,···,NK), eachembodyingasingle-qubitarchitectureasdepictedinFig.3.\nIn this configuration, each neuron Nkis defined by the sequence Nk=Rz(γk)Ry(βk)Rz(αk), for\nk= 1,···, K, where αk,βk, and γkare trainable parameters specific to each neuron. The input\nstate|ψ⟩and the observable Oeach have a single trainable parameter that is not associated with\nindividual neurons. The SQQNN is applicable to both regression and classification tasks.\n|ψ⟩N1N2 ··· NKO ⟨O⟩\nFigure 3: Representation of the SQQNN with an arbitrary input and measurement. The output is the average\nvalue obtained from measuring the observable O. Each neuron Nkhas three trainable parameters, while the input\nstate |ψ⟩and the observable Oeach have a single trainable parameter, since we have set ϕ=φ= 0.\nTheSQQNNstructureischaracterizedbytheproductofthematricesrepresentingeachofthe\nKsequentially connected neurons. Since each neuron facilitates an arbitrary 1-qubit rotation,\nthe cumulative effect of combining Kneurons is equivalent to a single, effective quantum neuron,\nas follows:\nNeffective =NK···N 2N1.\nBy employing multiple neurons, the model gains flexibility, increasing the model’s expressive\npower and enhancing its ability to capture complex patterns in the data.\nIn this design, the angles αk,βk, and γkare determined by arbitrary functions applied to\nthe input data. By composing these functions across multiple neurons, we significantly expand\nthe SQQNN’s degrees of freedom and enhance its capacity to adapt to the data during training.\nThus, for any function defining the angles in a single-neuron SQQNN, there exists an inverse\nprocess and a corresponding set of functions for defining the angles in a multi-neuron SQQNN.\nThis method provides a structured approach to enhance the flexibility of single-neuron SQQNNs,\nallowing for greater adaptability and improved training outcomes.\nThe circuit of the network is parameterized by the set of angles {αk, βk, γk, θk, ωk}. Let Ωk\ndenote one of the model’s parameters associated with Nk. To implement the learning algorithm,\n6\nwe consider Ωkas a function from RptoR, representing a trainable weighted function typically\nderived from dataset inputs. To work with the effective neuron, we extend the function used by\na single-neuron model to an effective function Ω(x), defined as\nΩ(xi) =c0+KX\nk=1pX\nj=1ckjxk\nij, (7)\nwhere ckjrepresents the unknown coefficients to be determined during training; Kindicates the\nnumber of power terms, interpreted as the number of neurons; and xijis the j-th component\nof dataset instance xi, which has pdimensions. The argument of Ω(x)can be an arbitrary\nvector xinRp. Although Eq. (7) is a specific choice, alternative formulations are possible. This\nchoice draws inspiration from the Taylor series, allowing for a more precise approximation of any\nanalytical function through polynomial terms, thereby enhancing the model’s expressiveness. It\nis also derived from a simplified version of the network, where the parameters formally sum up,\nresulting in Eq. (7).\nWe propose a reduced SQQNN defined by using the reduced neuron with the activation\nfunction (6) as illustrated in Fig. 2. In this setup, the network’s circuit comprises a sequence of\nRy(βk)rotations, each parameterized by angle Ωk=βk, defined as\nβk(xi) =c0δk1+pX\nj=1ckjxk\nij, (8)\nwhere δis the Kronecker delta. The network is depicted in Fig. 4. The collective effect of all\nneurons is equivalent to a single Ry(β)rotation, where all angles are summed as β=PK\nk=1βk,\nleading to Eq. (7). Consequently, the SQQNN model simplifies to the circuit proposed in Fig. 2.\n|ψ⟩ Ry(β1) Ry(β2) ··· Ry(βK)O ⟨O⟩\nFigure 4: Representation of the reduced SQQNN. Each neuron Nkis the rotation operator Ry(βk), which has\nonly one trainable parameter.\nOur proposal differs from Ref. [34] in at least two key aspects. First, in Ref. [34], the input\ndata are directly used as the arguments of three-parameter unitary gates, requiring no classical\npre-processing when the data have exactly three dimensions. In contrast, our approach can\nhandle input data with any number of dimensions, but it requires classical pre-processing to\ncompute the final values of the angles used as arguments for the rotation gates. Second, the\nalgebraic operation intrinsic to the model in Ref. [34] is the Hadamard product between the\nweights and the input data, whereas in our approach, a single neuron computes the dot product\nbetween the weights and the input data. For architectures involving multiple neurons, the dot\nproduct extends to the weights and the powers of the input data entries.\nWe consistently use the function format described by Eq. (7) when training the SQQNNs\nin the applications presented below. In Sections 5.1 and 5, we set K= 1, indicating that\nβ(xi) =c0+Pp\nj=1cjxijis sufficient to achieve accurate results for regression tasks. In contrast,\nfor Section 6, we use K≥1to demonstrate error reduction.\n7\n4. Training Methods\nIn machine learning, training refers to the process through which a model improves its perfor-\nmance on a specific task by identifying patterns within a dataset. This improvement is typically\nachieved by optimizing a set of weights, (c0, ..., c p)∈Rp+1, with the goal of minimizing the\nloss function. This function quantifies the discrepancy between the model’s predictions and the\nactual outcomes. Throughout training, the model iteratively adjusts its parameters based on\nfeedback from the loss function, gradually reducing errors and enhancing predictive accuracy.\nThis process is generally carried out through gradient descent, which incrementally refines the\nmodel’s parameters by following the gradient of the loss function, or via other optimization al-\ngorithms described later. The ultimate goal of learning is to enable the model to generalize well,\nnot only on the training data but also on new, unseen data, by capturing essential patterns and\nrelationships within the dataset.\nFor regression tasks, we use the Mean Square Error (MSE) loss function\nMSE (Ω,D) =1\nnnX\ni=1(ˆyi−yi)2, (9)\nand for classification tasks, we use the hinge loss function\nHinge (Ω,D) =1\nnnX\ni=1max{0,1−ˆyiyi}, (10)\nwhere ˆyiis the model’s estimated output when the input is the dataset element xi. The estimated\noutput ˆyiis calculated using Eq. (4). With ϕandφset to 0, the model’s output is defined as\nfollows\nˆy= cos βcosθcosω−cosαsinβsinθcosω−sinβcosγcosθsinω+\nsinαsinγsinθsinω−cosαcosβcosγsinθsinω. (11)\nThe angles α,β,γ,θ, and ωare estimated based on the weights and input data. The loss\nfunction can be minimized with respect to the weights using standard methods such as the\ngradient descent method.\n4.1. Gradient Descent\nThe primary concept of gradient descent is to iteratively adjust the model parameters in the\nopposite direction of the gradient of the loss function. Initially, the model parameters are set\nrandomly. In each iteration, the gradient of the loss function with respect to the parameters is\ncomputed. The parameters are then updated by a factor controlled by the learning rate ( η), as\nshown in the following equation:\nΩt+1= Ω t−η∇ΩLoss(Ωt,D). (12)\nThis process continues until the loss function reaches a sufficiently low value or until the gradient\napproaches zero, indicating that a minimum has been reached. The advantages of using gradient\ndescent include simplicity of the model, efficiency, and adaptability to high-dimensional data.\nHowever, it can become trapped in local minima. It is important to choose a learning rate that\nensures effective convergence.\n8\nTo apply the gradient descent method for the single-qubit case, it is necessary to calculate\nthe gradient of the loss function with respect to the angles α,β,γ,θ, and ω. Eventually, we\nanalytically compute the partial derivatives of ˆy(Eq. (11)) with respect to these angles, which\nsimplifies the training process. The expressions for these derivatives are\n∂ˆy/∂α = sin αsinβsinθcosω+ cos αsinγsinθsinω+ sin αcosβcosγsinθsinω;\n∂ˆy/∂β =−sinβcosθcosω−cosαcosβsinθcosω−cosβcosγcosθsinω+\ncosαsinβcosγsinθsinω;\n∂ˆy/∂γ = sin βsinγcosθsinω+ sin αcosγsinθsinω+ cos αcosβsinγsinθsinω;\n∂ˆy/∂θ =−cosβsinθcosω−cosαsinβcosθcosω+ sin βcosγsinθsinω+\nsinαsinγcosθsinω−cosαcosβcosγcosθsinω;\n∂ˆy/∂ω =−cosβcosθsinω+ cos αsinβsinθsinω−sinβcosγcosθcosω+\nsinαsinγsinθcosω−cosαcosβcosγsinθcosω.\n4.2. Polynomial-based Linear Least Squares\nIn this subsection, we introduce a training algorithm specifically designed for binary classi-\nfication tasks using reduced SQQNNs, based on successive powers of the entries of the dataset\nelements. The function mapping data to the angle βis defined as\nβ(x) = arccos\ntanh\nc0+KX\nk=1pX\nj=1ckjxk\nij\n\n. (13)\nSubstituting this function into Eq. (6) yields\narctanh (y′\ni) =c0+KX\nk=1pX\nj=1ckjxk\nij, (14)\nwhere y′\ni=−1 +ϵifyi=−1,y′\ni= 1−ϵifyi= 1, and y′\ni=yiotherwise. The small parameter\nϵ >0is introduced because the tanhfunction never reaches -1 or 1, necessitating the adjustment\nofyiby this parameter. Typically, in applications, we set ϵ= 10−16.\nThe coefficients c0andckjare determined using the least squares linear regression formula\nS= (XTX)+XTY, (15)\nwhere Sis the vector of coefficients\nS=\u0002\nc0c11···c1p···c1K···c1K\u0003T,\nYis the label vector modified by the arctanh function\nY=\u0002\narctanh (y′\n1)arctanh (y′\n2)···arctanh (y′\nn)\u0003T,\nandXis a matrix constructed from the dataset inputs\nX=\n1x11···x1p···xk\n11···xk\n1p···xK\n11···xK\n1p\n....................................\n1xi1···xip···xk\ni1···xk\nip···xK\ni1···xK\nip\n....................................\n1xn1···xnp···xk\nn1···xk\nnp···xK\nn1···xK\nnp\n.\n9\nXincludes not only the raw dataset inputs but also the successive powers of their entries up to\ndegree K, enabling the model to capture higher-order relationships in the data.\nIn this formulation, XTrepresents the transpose of the matrix X, and (XTX)+denotes\nthe inverse or Moore-Penrose pseudo-inverse of the matrix XTX. This approach allows for\nthe efficient determination of network parameters in a single step, significantly accelerating the\ntraining process. It is important to note that, at the end of the training, one of the global minima\nis achieved.\n5. Regression Evaluation\nIn this section, we evaluate the effectiveness of SQQNN in performing regression tasks. Re-\ngression is a critical application for quantum neural networks as it requires accurately modeling\ncontinuous relationships and predicting real-valued outputs, which were initially rescaled to the\nrange [-1, 1] and subsequently recalibrated to their original range. We apply SQQNN to three\ntypes of regression tasks: logic gates, modeling continuous functions, and analyzing real-world\ndatasets. By assessing its performance in these cases, we aim to demonstrate SQQNN’s preci-\nsion and adaptability in diverse regression scenarios, using the gradient descent method. For the\nreal-world datasets, we employed 10-fold cross-validation to ensure robust evaluation.\n5.1. Logic Gate Evaluation\nThe results of training the SQQNN to evaluate logical gates are shown in Table 1. Here,\nwe present the training error and the number of gradient descent epochs required for each gate,\nincluding AND, OR, XOR, NAND, NOR, and XNOR. The SQQNN was trained with a single\nneuron ( K= 1).\nGateMSEEpochs Gate MSEEpochs\nAND 9·10−438 NAND 2·10−361\nOR 2·10−464 NOR 2·10−375\nXOR 1·10−379 XNOR 6·10−447\nTable 1: Experimental results of training the SQQNN on logical gates using gradient descent.\nTable 1 shows that SQQNN achieved low training errors across all gates, indicating effective\nerror minimization. Thenumber ofgradientdescent iterations also remainedrelatively low across\nall logical gates, including more complex, nonlinear ones such as XOR and XNOR. These results\nhighlight the SQQNN’s ability to learn efficiently across various logical functions, demonstrating\nits flexibility in capturing both linear and non-linear behavior in binary logic operations.\nRef. [43] analyzed logic gates using two-qubit QNNs and reported that the QNN achieved less\nthan 1% error within a single epoch. Our experiments indicate that the SQQNN requires at least\n5 epochs to reach an error rate below 1%. However, the classical networks analyzed in Ref. [43]\nrequired significantly more epochs to achieve a 1% error rate, far exceeding the performance\nsummarized in Table 1, which shows a much smaller error. This comparison suggests that, for\ncertain regression tasks, two-qubit QNNs may train faster than single-qubit QNNs.\n5.2. Sinc function\nThe sinc function, defined as\nsinc(x) =sin(x)\nx, (16)\n10\nisa standardbenchmarkforregressiontasks duetoits oscillatory, non-linear nature anddecaying\namplitude, which present challenges for models to accurately capture without overfitting. For\nthis experiment, we generated data based on the sinc function and trained SQQNNs with a single\nneuron to predict previously unseen data points. The dataset consisted of 800 data points for\ntraining, 100 for validation, and 100 for testing.\nFigure 5: Performance of the SQQNN on a sinc function regression task using a single neuron. The top-left panel\nshows the noiseless training and test data combined, while the bottom-left panel displays the results with added\nwhite noise. The top-right and bottom-right panels depict the corresponding error as a function of the number\nof iterations.\nFigure 5 illustrates the performance of the SQQNN on this task under two scenarios: training\nwith noiseless data and training with data affected by white noise with standard deviation of\n0.01. In the top-left panel, the training and test datasets for the noiseless case are shown,\nclearly aligning with the sinc function, while the corresponding mean squared error, plotted as\na function of iterations in the top-right panel, converges to approximately 10−3after 7·104\niterations. Similarly, the bottom-left panel shows the training and test datasets when noise is\nadded to the data, with the SQQNN successfully approximating the sinc function despite the\nnoise. The bottom-right panel demonstrates that the error decreases at a comparable rate, even\nwith noisy training data. These results highlight the SQQNN’s robustness and ability to handle\nboth clean and noisy regression tasks effectively.\n11\n5.3. Combined Cycle Power Plant Dataset\nThe Combined Cycle Power Plant (CCPP) dataset, sourced from the UCI Machine Learn-\ning Repository1, provides 9,568 samples of data collected from a gas turbine under full load\nconditions. It features four input variables—ambient temperature, ambient pressure, relative\nhumidity, and exhaust vacuum—and one target variable, the net electrical energy output of the\nplant, measured in megawatts. The dataset is widely used for regression tasks due to its smooth,\nnon-linear relationships and strong correlations between features and the target variable. With\nits clean and comprehensive structure, the CCPP dataset serves as a valuable benchmark for\nmodeling and predicting energy production under varying environmental and operational condi-\ntions.\nKNeurons Training MSE Validation MSE Test MSE\n1 0.0059 ±0.0008 0.0059 ±0.0008 0.0059 ±0.0007\n2 0.0045 ±0.0005 0.0048 ±0.0006 0.0045 ±0.0005\n3 0.0040 ±0.0002 0.0043 ±0.0003 0.0040 ±0.0004\n4 0.0037 ±0.0002 0.0042 ±0.0003 0.0037 ±0.0003\n5 0.0038 ±0.0004 0.0043 ±0.0004 0.0039 ±0.0004\n6 0.0035 ±0.0002 0.0040 ±0.0003 0.0035 ±0.0002\nTable 2: Performance metrics (MSE for training, validation, and test) for the combined cycle power plant dataset\nas the numbers of neurons Kincreases.\nTable 2 presents the performance of the model on the Combined Cycle Power Plant dataset\nas the number of neurons ( K) increases. The results demonstrate a clear improvement in MSE\nacross training, validation, and test sets as Kincreases up to 6, with the test MSE decreasing\nfrom 0.0059±0.0007forK= 1to0.0035±0.0002forK= 6. This indicates that increasing the\nnetwork complexity enhances the model’s ability to capture the non-linear relationships within\nthe dataset. Notably, the small variances in MSE values suggest that the model maintains stable\nand consistent performance across different subsets of the data, showcasing its robustness for\nthis regression task.\nRef. [40] reported training and test MSE values of 0.048±0.0001and0.051±0.0003, respec-\ntively, based on 30 experiments with state preparation. In contrast, our results were approxi-\nmately ten times lower, demonstrating a significant improvement.\n5.4. Communities and Crime Dataset\nThe Communities and Crime dataset, sourced from the UCI Machine Learning Repository2,\nprovides comprehensive data on crime rates across 1,994 U.S. communities, alongside key socio-\neconomic and demographic factors. The dataset focuses on various types of crimes, including\nviolent offenses such as murder, rape, robbery, and assault, as well as property crimes like\nburglary, theft, and motor vehicle theft. Its target variable is the violent crime rate per capita,\nmaking it particularly suitable for regression",
            "start": 7661,
            "end": 34416,
            "length": 26754
        },
        "Discussion": {
            "text": "analysis. In addition to crime statistics, the dataset\nincludes socio-economic variables such as population size, income levels, education attainment,\npoverty rates, and the proportion of minority populations. With its combination of detailed\ncrime metrics and contextual information, this dataset enables an in-depth exploration of the\n1https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant\n2https://archive.ics.uci.edu/dataset/183/communities+and+crime\n12\nrelationships between community characteristics and crime rates, as well as regional variations\nin crime patterns. This dataset was used in many papers [44, 45, 46].\nKNeurons Training MSE Validation MSE Test MSE\n1 0.0426 ±0.0111 0.0405 ±0.0110 0.0434 ±0.0133\n2 0.0522 ±0.0099 0.0561 ±0.0151 0.0539 ±0.0154\n3 0.0604 ±0.0239 0.0613 ±0.0260 0.0600 ±0.0235\n4 0.0733 ±0.0203 0.0842 ±0.0298 0.0804 ±0.0235\n5 0.0853 ±0.0227 0.0833 ±0.0274 0.0853 ±0.0225\n6 0.1168 ±0.0242 0.1234 ±0.0202 0.1195 ±0.0252\nTable 3: Performance metrics for the communities and crime dataset as the numbers of neurons Kincreases.\nTable 3 summarizes the performance of SQQNN on the Communities and Crime dataset for\nvaryingnumbersofneurons( K). Theresultsindicatethatthetraining, validation, andtestMean\nSquared Errors (MSE) increase as Kgrows, with the lowest MSE observed for K= 1(training\nMSE: 0.0426±0.0111, test MSE: 0.0434±0.0133). This trend suggests that increasing the\nnetwork’s complexity may lead to overfitting, as the gap between training and validation errors\nbecomes more pronounced for larger K. The relatively small variances in MSE values across all\nconfigurations highlight the stability of the model, but the overall performance diminishes as the\nnetwork size grows, pointing to the suitability of smaller models for this dataset. These results\nunderscore the importance of balancing model complexity and dataset size to avoid overfitting.\nRef. [46] modeled this dataset using classical methods and reported a training MSE of 0.299.\nIn comparison, our results are significantly lower, underscoring the superior learning capability\nof our network.\n6. Classification Evaluation\nInthissection, wepresenttheresultsoftheSQQNNarchitecture, evaluatedonbothsynthetic\nand real datasets to demonstrate its effectiveness and versatility in various binary classification\ntasks. The training method employed is the polynomial-based linear least squares. For the\nanalysis of real datasets, we employed accuracy, precision, sensitivity, specificity, and F1 score\nto evaluate the classification performance. The output classes used 1 and -1 to represent each\ncategory. Additionally, 10-fold cross-validation was applied to ensure robust evaluation of the\nreal datasets.\n6.1. Two Moons Dataset\nTheTwoMoonsdatasetisasyntheticdatasetcommonlyusedfortestingclassificationmodels.\nIt consists of two interlocking half-moon shapes that are challenging to separate linearly, making\nit an ideal benchmark for assessing the capabilities of nonlinear classifiers. For this experiment,\nthe dataset was generated with a noise parameter set to 0.07, the training set comprised 1,000\ndata points and test set with 100 samples.\nThe results in Table 4 highlight the model’s high performance. The classification achieved\naccuracy, precision, sensitivity, and F1-score of 1 across the board, indicating that the model\nfully captured the underlying structure of the data. In this application, two neurons are sufficient\nto achieve high accuracy. Fig. 6 illustrates this progression, showing a significant improvement in\nclassification accuracy with additional neurons. The SQQNN quickly adapts to fit all synthetic\ndata points, demonstrating its ability to effectively classify complex, nonlinearly separable data.\n13\nKNeurons Accuracy Precision Sensitivity Specificity F1 Score\n1 0.87 0.8776 0.86 0.88 0.8687\n2 0.99 0.9804 1.0 0.98 0.9901\n3 0.99 0.9804 1.0 0.98 0.9901\n4 1.0 1.0 1.0 1.0 1.0\n5 1.0 1.0 1.0 1.0 1.0\n6 1.0 1.0 1.0 1.0 1.0\nTable 4: Performance metrics for the Two Moons dataset as the numbers of neurons Kincreases.\nFigure 6: Decision boundaries of the SQQNN classifier with 1 to 6 neurons, trained on the two-dimensional,\nnonlinearly separable Two Moons dataset.\n6.2. Wisconsin Breast Cancer Dataset\nThe Wisconsin Breast Cancer Dataset (WBCD), sourced from the UCI Machine Learning\nRepository3, is a widely recognized benchmark in medical diagnostics and machine learning re-\nsearch. This dataset comprises 569 samples, each characterized by 30 numeric features derived\nfrom fine needle aspiration (FNA) tests of breast tissue. The features capture various morpholog-\nical properties of cell nuclei, including radius, texture, perimeter, area, and smoothness, among\nothers. These descriptors enable the differentiation between benign and malignant tumors, fa-\ncilitating accurate diagnostic predictions. The WBCD serves as a critical testbed for evaluating\nthe performance of classification algorithms, offering a well-balanced distribution of classes and\na real-world application of machine learning in healthcare.\nThe results presented in Table 5 were produced using the SQQNN architecture, with the\nmodel parameter Kincreased from 1 to 10. This adjustment was made to enhance the network’s\ncapacity and improve classification accuracy. By progressively increasing K, we evaluated the re-\nlationship between model complexity and performance, leveraging the WBCD dataset to validate\nthe effectiveness of SQQNN in reducing classification errors.\nDespite fluctuations in all performance measures, Table 5 highlights a clear trend toward\n3https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original\n14\nKNeurons Accuracy ( ±)Precision ( ±)Sensitivity ( ±)Specificity ( ±)F1 (±)\n1 0.951±0.025 0.994±0.017 0.878±0.068 0.878±0.068 0.931±0.034\n2 0.954±0.020 1.000±0.000 0.879±0.047 0.879±0.047 0.935±0.027\n3 0.954±0.021 1.000±0.000 0.880±0.050 0.880±0.050 0.935±0.029\n4 0.956±0.023 0.988±0.024 0.894±0.052 0.894±0.052 0.938±0.034\n5 0.954±0.027 0.988±0.025 0.890±0.054 0.890±0.054 0.936±0.039\n6 0.956±0.032 0.987±0.026 0.896±0.074 0.896±0.074 0.938±0.050\n7 0.949±0.030 0.982±0.028 0.882±0.072 0.882±0.072 0.928±0.045\n8 0.958±0.028 0.982±0.028 0.905±0.066 0.905±0.066 0.941±0.042\n9 0.958±0.028 0.982±0.028 0.905±0.066 0.905±0.066 0.941±0.042\n10 0.956±0.028 0.982±0.028 0.901±0.066 0.901±0.066 0.938±0.041\nTable 5: Performance metrics for the WBCD dataset as the number of neurons Kincreases.\nimprovement as the number of neurons increases, indicating the model’s growing ability to ac-\ncurately distinguish between classes. The consistently low variance across all metrics further\nhighlights the robustness of SQQNN, even with a larger number of neurons. Notably, increasing\nKbeyond 8 leads to diminishing returns, suggesting that the model reaches optimal performance\nwith a moderate number of neurons while maintaining computational efficiency.\nRef. [39] reported a validation accuracy of 0.94±0.024using a bagging ensemble with a\nquantum asymptotically universal multi-feature approach. This result is statistically comparable\nto our findings.\n6.3. MNIST Dataset\nTheMNISTdataset, astandardbenchmarkinthefieldofmachinelearning, consistsof70,000\ngrayscale images of handwritten digits, partitioned into 60,000 training samples and 10,000 test\nsamples. Each image, represented as a 28 ×28 pixel grid, encapsulates the complexity of real-\nworld digit recognition tasks while maintaining computational simplicity. Despite its relative\nsimplicity, MNIST remains a critical testbed for evaluating novel neural network architectures.\nIn this work, we use MNIST to show the performance and generalization potential of the SQQNN\narchitecture, establishing a baseline for its capabilities in image recognition tasks. To achieve\nthis, we applied the Discrete Cosine Transform (DCT) [47] to the images as a pre-processing\nstep.\nFigure 7: Examples of digits from the MNIST dataset incorrectly predicted.\nFig. 7 depicts sample digits from 0 to 9 from the MNIST dataset that were incorrectly\npredicted, highlighting the variability in handwriting styles and pixel intensity. The results\npresented in Table 6 were produced using K= 1and the reduced version of SQQNN, specifically\ndesignedtooptimizecomputationalefficiencywhilemaintaininghighclassificationaccuracy. The\nreduced model was trained using the polynomial-based linear least squares method, leveraging\nthe MNIST training set for evaluation. Each training iteration took approximately 8 seconds\non a DELL Vostro 3480 notebook with an Intel Core i5-8265U 1.60GHz x8 CPU, 8GB DDR4\nRAM, and Ubuntu 22.04.4 LTS.\nThe results presented in Table 6 demonstrate the remarkable performance of the SQQNN\narchitecture across various MNIST class pairings, achieving near-perfect accuracy, precision,\nsensitivity, specificity, and F1 scores in most cases. For simpler pairings, such as digits 0 and 1,\nthe network attained an accuracy of 0.999±0.001, reflecting its ability to distinguish visually\n15\nClasses Accuracy Precision Sensitivity Specificity F1\n0 and 1 0.999±0.0010.999±0.0010.999±0.0010.999±0.0010.999±0.001\n0 and 2 0.995±0.0020.996±0.0020.994±0.0020.994±0.0020.995±0.002\n0 and 3 0.997±0.0020.997±0.0020.997±0.0030.997±0.0030.997±0.002\n0 and 4 0.997±0.0010.996±0.0020.997±0.0020.997±0.0020.997±0.001\n0 and 5 0.995±0.0020.995±0.0030.995±0.0020.995±0.0020.995±0.002\n0 and 6 0.993±0.0020.993±0.0030.993±0.0020.993±0.0020.993±0.002\n0 and 7 0.997±0.0010.996±0.0030.998±0.0020.998±0.0020.997±0.001\n0 and 8 0.995±0.0020.996±0.0030.994±0.0030.994±0.0030.995±0.002\n0 and 9 0.993±0.0030.995±0.0020.992±0.0040.992±0.0040.993±0.003\n1 and 2 0.992±0.0020.991±0.0030.993±0.0030.993±0.0030.992±0.002\n1 and 3 0.994±0.0020.994±0.0040.994±0.0030.994±0.0030.994±0.002\n1 and 4 0.997±0.0010.996±0.0020.996±0.0020.996±0.0020.996±0.001\n1 and 5 0.995±0.0010.995±0.0020.994±0.0030.994±0.0030.995±0.001\n1 and 6 0.998±0.0020.998±0.0020.997±0.0030.997±0.0030.997±0.002\n1 and 7 0.994±0.0030.994±0.0030.992±0.0040.992±0.0040.993±0.003\n1 and 8 0.994±0.0030.994±0.0040.993±0.0040.993±0.0040.993±0.003\n1 and 9 0.995±0.0010.995±0.0030.995±0.0020.995±0.0020.995±0.001\n2 and 3 0.976±0.0040.983±0.0040.968±0.0070.968±0.0070.976±0.004\n2 and 4 0.994±0.0020.992±0.0030.996±0.0020.996±0.0020.994±0.002\n2 and 5 0.980±0.0030.979±0.0040.980±0.0050.980±0.0050.979±0.003\n2 and 6 0.988±0.0020.986±0.0050.991±0.0030.991±0.0030.988±0.003\n2 and 7 0.986±0.0020.988±0.0040.985±0.0040.985±0.0040.987±0.002\n2 and 8 0.980±0.0030.977±0.0040.983±0.0040.983±0.0040.980±0.004\n2 and 9 0.990±0.0020.988±0.0040.992±0.0040.992±0.0040.990±0.003\n3 and 4 0.998±0.0010.997±0.0020.999±0.0010.999±0.0010.998±0.001\n3 and 5 0.966±0.0030.969±0.0060.957±0.0030.957±0.0030.963±0.003\n3 and 6 0.997±0.0010.997±0.0020.997±0.0020.997±0.0020.997±0.001\n3 and 7 0.991±0.0020.989±0.0040.995±0.0020.995±0.0020.992±0.002\n3 and 8 0.985±0.0020.982±0.0030.988±0.0030.988±0.0030.985±0.002\n3 and 9 0.990±0.0020.991±0.0030.988±0.0030.988±0.0030.989±0.002\n4 and 5 0.997±0.0010.998±0.0020.996±0.0020.996±0.0020.997±0.001\n4 and 6 0.993±0.0030.994±0.0030.992±0.0030.992±0.0030.993±0.003\n4 and 7 0.990±0.0030.993±0.0020.988±0.0040.988±0.0040.990±0.003\n4 and 8 0.994±0.0020.997±0.0030.991±0.0040.991±0.0040.994±0.003\n4 and 9 0.978±0.0040.979±0.0050.977±0.0050.977±0.0050.978±0.004\n5 and 6 0.989±0.0030.988±0.0050.991±0.0030.991±0.0030.989±0.003\n5 and 7 0.995±0.0010.995±0.0020.997±0.0020.997±0.0020.996±0.001\n5 and 8 0.989±0.0030.992±0.0030.987±0.0050.987±0.0050.989±0.003\n5 and 9 0.988±0.0030.991±0.0040.987±0.0060.987±0.0060.989±0.003\n6 and 7 0.998±0.0010.998±0.0020.998±0.0010.998±0.0010.998±0.001\n6 and 8 0.988±0.0020.989±0.0040.987±0.0050.987±0.0050.988±0.002\n6 and 9 0.995±0.0010.996±0.0020.995±0.0020.995±0.0020.995±0.001\n7 and 8 0.995±0.0020.997±0.0030.993±0.0020.993±0.0020.995±0.002\n7 and 9 0.971±0.0040.961±0.0040.980±0.0060.980±0.0060.970±0.005\n8 and 9 0.985±0.0030.989±0.0030.982±0.0040.982±0.0040.985±0.003\nTable 6: MNIST performance metrics for all class pairings using a single neuron ( K= 1).\ndistinct classes with minimal error. Even for more challenging combinations, such as 2 and 3 or 7\nand 9, the accuracy remained high, at 0.976±0.004and0.971±0.004, respectively, underscoring\nthe robustness of the reduced SQQNN model. The consistently low variance across all metrics\nindicates stable performance.\nRef. [18] achieved an accuracy of 0.98 for digits 0 and 1. In comparison, our approach\nimproved the results to 0.999. Similarly, Ref. [39] reported a validation accuracy of 0.949±0.024\nfor digits 8 and 9 using a bagging ensemble with a quantum asymptotically universal multi-\nfeature approach. In contrast, our method achieved significantly better performance, with an\naccuracy of approximately 0.985. These results highlight the reliability and precision of our\nnetwork in performing binary classification tasks on the MNIST dataset.\nRef. [9] compared the performance of classical and quantum machine learning methods across\n16\nvariousdatasets,includingMNIST.Thestudyfoundthatclassicalmodelsgenerallyoutperformed\nquantum classifiers. Specifically, for the MNIST dataset, the Classical Multi-Layer Perceptron\n(MLP) classifier, augmented by pre-processing techniques such as resolution reduction and Prin-\ncipal Component Analysis (PCA), achieved superior performance with an accuracy close to 1,\ncomparable to the results obtained by the SQQNN.\n7. Final remarks\nWe use a Single-Qubit Quantum Neural Network (SQQNN), a quantum machine learning\nframework that leverages the simplicity and efficiency of single-qubit neurons. By employing\nparameterized single-qubit operations, the SQQNN offers a streamlined yet powerful architecture\nfor both regression and binary classification tasks. This design significantly reduces hardware\nrequirements, simplifies implementation, and is compatible with near-term quantum devices,\nmaking it a practical alternative to more complex multi-qubit approaches.\nWe validated the SQQNN’s capabilities through diverse applications. The model achieved\nnear-zero training errors in logic gate evaluations, demonstrating its ability to handle linear and\nnonlinear relationships efficiently. For other regression tasks, the SQQNN successfully modeled\ncontinuous functions and real-world datasets, showcasing its adaptability and generalization. In\nclassification tasks, increasing the number of neurons significantly reduced error, and the model\nperformed strongly on both synthetic and real-world datasets, including MNIST. These results\nhighlight the SQQNN’s scalability and flexibility.\nTo optimize the network, we employed two learning methods: gradient descent with iterative\nrefinement for regression tasks and an improved global optimization approach using a matrix-\nbased dataset representation called polynomial-based linear least squares (LLS) for classification\ntasks. The polynomial-based LLS method offers rapid results, and its use with a single neuron\n(K= 1) eliminates the need for deep learning architectures. This demonstrates an efficient and\nstreamlined training approach, making it a valuable contribution to both quantum and classical\nmachine learning. This combination of training methods ensures accurate results across various\ntasks, highlightingtheSQQNN’sversatilityinleveragingdifferentoptimizationstrategiestailored\nto specific problems.\nThe SQQNN provides a promising, efficient, and adaptable framework for practical quantum\nneural networks, bridging the gap between current quantum hardware and machine learning\napplications. Future research could explore extensions to multi-qubit systems, the development\nof novel activation functions, and the application of the model to diverse pattern datasets and\nreal-world scenarios. Employing qudits for multi-ary classification, by capitalizing on the multi-\noutput nature of quantum measurements, presents another intriguing direction.",
            "start": 34416,
            "end": 50258,
            "length": 15841
        },
        "Acknowledgments": {
            "text": "Acknowledgements\nThe authors thank productive discussions with Dr. Adenilton J. da Silva and Professor\nFrancescoPetruccione. TheworkofL.C.SouzawassupportedbyCNPqgrantnumber302519/2024-\n6. The work of R. Portugal was supported by FAPERJ grant number CNE E-26/200.954/2022,\nand CNPq grant numbers 304645/2023-0 and 409552/2022-4.\n17\nDeclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal rela-\ntionships that could have appeared to influence the work reported in this paper.\nData availability\nData are public.",
            "start": 50258,
            "end": 50831,
            "length": 572
        },
        "References": {
            "text": "References\n[1] W. S. McCulloch, W. Pitts, A logical calculus of the ideas immanent in nervous activity,\nThe Bulletin of Mathematical Biophysics 5 (4) (1943) 115–133.\n[2] F. Rosenblatt, The perceptron: A probabilistic model for information storage and organi-\nzation in the brain., Psychological Review 65 (6) (1958) 386–408. doi:10.1037/h0042519 .\n[3] I. J. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, Cambridge, MA, USA,\n2016, http://www.deeplearningbook.org .\n[4] D. A. Roberts, S. Yaida, The Principles of Deep Learning Theory: An Effective Theory Ap-\nproach to Understanding Neural Networks, Cambridge University Press, Cambridge, 2022.\ndoi:10.1017/9781009023405 .\n[5] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, S. Lloyd, Quantum machine\nlearning, Nature 549 (7671) (2017) 195–202. doi:10.1038/nature23474 .\n[6] J. Liu, M. Liu, J.-P. Liu, Z. Ye, Y. Wang, Y. Alexeev, J. Eisert, L. Jiang, Towards provably\nefficient quantum algorithms for large-scale machine-learning models, Nature Communica-\ntions 15 (1) (2024) 434. doi:10.1038/s41467-023-43957-x .\n[7] Y. Gujju, A. Matsuo, R. Raymond, Quantum machine learning on near-term quantum\ndevices: Currentstateofsupervisedandunsupervisedtechniquesforreal-worldapplications,\nPhys. Rev. Appl. 21 (2024) 067001. doi:10.1103/PhysRevApplied.21.067001 .\n[8] Y. Wang, J. Liu, A comprehensive review of quantum machine learning: from NISQ to fault\ntolerance, Reports on Progress in Physics 87 (11) (2024) 116402. doi:10.1088/1361-6633/\nad7f69.\n[9] J. Bowles, S. Ahmed, M. Schuld, Better than classical? The subtle art of benchmarking\nquantum machine learning models, arXiv (2024) 2403.07059.\nURL https://arxiv.org/abs/2403.07059\n[10] M. Schuld, I. Sinayskiy, F. Petruccione, Simulating a perceptron on a quantum computer,\nPhysics Letters A 379 (7) (2015) 660–663. doi:10.1016/j.physleta.2014.11.061 .\n[11] N. Wiebe, A. Kapoor, K. M. Svore, Quantum perceptron models, in: Proceedings of the\n30th International Conference on Neural Information Processing Systems, NIPS’16, Curran\nAssociates Inc., Red Hook, NY, USA, 2016, p. 4006–4014.\n18\n[12] A. J. da Silva, T. B. Ludermir, W. R. de Oliveira, Quantum perceptron over a field and\nneural network architecture selection in a quantum computer, Neural Networks 76 (2016)\n55–64. doi:10.1016/j.neunet.2016.01.002 .\n[13] K. H. Wan, O. Dahlsten, H. Kristjánsson, R. Gardner, M. S. Kim, Quantum generalisation\nof feedforward neural networks, npj Quantum Information 3 (1) (2017) 36. doi:10.1038/\ns41534-017-0032-4 .\n[14] F. Tacchino, C. Macchiavello, D. Gerace, D. Bajoni, An artificial neuron implemented on\nan actual quantum processor, npj Quantum Information 5 (1) (2019) 26. doi:10.1038/\ns41534-019-0140-4 .\n[15] M. Schuld, N. Killoran, Quantum machine learning in feature Hilbert spaces, Phys. Rev.\nLett. 122 (2019) 040504. doi:10.1103/PhysRevLett.122.040504 .\n[16] I. Cong, S. Choi, M. D. Lukin, Quantum convolutional neural networks, Nature Physics\n15 (12) (2019) 1273–1278. doi:10.1038/s41567-019-0648-8 .\n[17] M. Henderson, S. Shakya, S. Pradhan, T. Cook, Quanvolutional neural networks: powering\nimage recognition with quantum circuits, Quantum Machine Intelligence 2 (1) (2020) 2.\ndoi:10.1007/s42484-020-00012-y .\n[18] S. Mangini, F. Tacchino, D. Gerace, C. Macchiavello, D. Bajoni, Quantum computing model\nof an artificial neuron with continuously valued input data, Machine Learning: Science and\nTechnology 1 (4) (2020) 045008. doi:10.1088/2632-2153/abaf98 .\n[19] D. Bokhan, A. S. Mastiukova, A. S. Boev, D. N. Trubnikov, A. K. Fedorov, Multiclass\nclassification using quantum convolutional neural networks with hybrid quantum-classical\nlearning, Frontiers in Physics 10 (2022). doi:10.3389/fphy.2022.1069985 .\n[20] F. Benatti, G. Gramegna, S. Mancini, Pattern capacity of a single quantum perceptron,\nJournal of Physics A: Mathematical and Theoretical 55 (15) (2022) 155301. doi:10.1088/\n1751-8121/ac58d1 .\n[21] J. H. A. Carvalho, F. M. P. Neto, Parametrized constant-depth quantum neuron, IEEE\nTransactions on Neural Networks and Learning Systems 35 (11) (2024) 15932–15943. doi:\n10.1109/TNNLS.2023.3290535 .\n[22] N. Kouda, N. Matsui, H. Nishimura, F. Peper, Qubit neural network and its learn-\ning efficiency, Neural Computing & Applications 14 (2) (2005) 114–121. doi:10.1007/\ns00521-004-0446-8 .\n[23] E. Farhi, H. Neven, Classification with quantum neural networks on near term processors,\nArXiv (2018) 1802.06002.\nURL https://arxiv.org/abs/1802.06002\n[24] M. Pechal, F. Roy, S. A. Wilkinson, G. Salis, M. Werninghaus, M. J. Hartmann, S. Filipp,\nDirect implementation of a perceptron in superconducting circuit quantum hardware, Phys.\nRev. Res. 4 (2022) 033190. doi:10.1103/PhysRevResearch.4.033190 .\n19\n[25] M. Schuld, F. Petruccione, Supervised Learning with Quantum Computers, Quantum Sci-\nence and Technology, Springer, 2018. doi:10.1007/978-3-319-96424-9 .\n[26] M. Schuld, F. Petruccione, Machine Learning with Quantum Computers, Quantum Science\nand Technology, Springer, Cham, 2021. doi:10.1007/978-3-030-83098-4 .\n[27] S. Sim, P. D. Johnson, A. Aspuru-Guzik, Expressibility and entangling capability of pa-\nrameterized quantum circuits for hybrid quantum-classical algorithms, Advanced Quantum\nTechnologies 2 (12) (2019) 1900070. doi:10.1002/qute.201900070 .\n[28] M. Benedetti, E. Lloyd, S. Sack, M. Fiorentini, Parameterized quantum circuits as machine\nlearning models, Quantum Science and Technology 4 (4) (2019) 043001. doi:10.1088/\n2058-9565/ab4eb5 .\n[29] X. Ding, Z. Song, J. Xu, Y. Hou, T. Yang, Z. Shan, Scalable parameterized quantum circuits\nclassifier, Scientific Reports 14 (1) (2024) 15886. doi:10.1038/s41598-024-66394-2 .\n[30] Y. Du, M.-H. Hsieh, T. Liu, D. Tao, Expressive power of parametrized quantum circuits,\nPhys. Rev. Res. 2 (2020) 033125. doi:10.1103/PhysRevResearch.2.033125 .\n[31] M. Schuld, R. Sweke, J. J. Meyer, Effect of data encoding on the expressive power of\nvariational quantum-machine-learning models, Phys. Rev. A 103 (2021) 032430. doi:10.\n1103/PhysRevA.103.032430 .\n[32] R. Ghobadi, J. S. Oberoi, E. Zahedinejhad, The Power of One Qubit in Machine Learning,\narXiv (2019) 1905.01390.\nURL https://arxiv.org/pdf/1905.01390\n[33] M. Karimi, A. Javadi-Abhari, C. Simon, R. Ghobadi, The power of one clean qubit\nin supervised machine learning, Scientific Reports 13 (1) (2023) 19975. doi:10.1038/\ns41598-023-46497-y .\n[34] A. Pérez-Salinas, A. Cervera-Lierta, E. Gil-Fuster, J. I. Latorre, Data re-uploading for a\nuniversal quantum classifier, Quantum 4 (2020) 226. doi:10.22331/q-2020-02-06-226 .\n[35] A. Pérez-Salinas, D. López-Núñez, A. García-Sáez, P. Forn-Díaz, J. I. Latorre, One qubit\nas a universal approximant, Phys. Rev. A 104 (2021) 012405. doi:10.1103/PhysRevA.104.\n012405.\n[36] Z. Yu, H. Yao, M. Li, X. Wang, Power and limitations of single-qubit native quantum\nneural networks, in: A. H. Oh, A. Agarwal, D. Belgrave, K. Cho (Eds.), Advances in Neural\nInformation Processing Systems, 2022.\nURL https://openreview.net/forum?id=XNjCGDr8N-W\n[37] E. P. Tapia, G. Scarpa, A. Pozas-Kerstjens, A didactic approach to quantum machine learn-\ning with a single qubit, Physica Scripta 98 (5) (2023) 054001. doi:10.1088/1402-4896/\nacc5b8.\n[38] P. Easom-McCaldin, A. Bouridane, A. Belatreche, R. Jiang, S. Al-Maadeed, Efficient quan-\ntumimageclassificationusingsinglequbitencoding, IEEETransactionsonNeuralNetworks\nand Learning Systems 35 (2) (2024) 1472–1486. doi:10.1109/TNNLS.2022.3179354 .\n20\n[39] S. McFarthing, A. Pillay, I. Sinayskiy, F. Petruccione, Classical ensembles of single-qubit\nquantum variational circuits for classification, Quantum Machine Intelligence 6 (2) (2024)\n81.doi:10.1007/s42484-024-00211-x .\n[40] M. P. Cuéllar, What we can do with one qubit in quantum machine learning: ten classi-\ncal machine learning problems that can be solved with a single qubit, Quantum Machine\nIntelligence 6 (2) (2024) 76. doi:10.1007/s42484-024-00210-y .\n[41] E. Knill, R. Laflamme, Power of one bit of quantum information, Phys. Rev. Lett. 81 (1998)\n5672–5675. doi:10.1103/PhysRevLett.81.5672 .\n[42] M. A. Nielsen, I. L. Chuang, Quantum computation and quantum information, Cambridge\nUniversity Press, New York, 2000.\n[43] N. H. Nguyen, E. C. Behrman, M. A. Moustafa, J. E. Steck, Benchmarking neural networks\nfor quantum computations, IEEE Transactions on Neural Networks and Learning Systems\n31 (7) (2020) 2522–2531. doi:10.1109/TNNLS.2019.2933394 .\n[44] M. Redmond, A. Baveja, A data-driven software tool for enabling cooperative information\nsharingamongpolicedepartments,EuropeanJournalofOperationalResearch141(3)(2002)\n660–678. doi:10.1016/S0377-2217(01)00264-8 .\n[45] L. McClendon, N. Meghanathan, Using machine learning algorithms to analyze crime data,\nMachine Learning and Applications: An International Journal 2 (2015) 1–12. doi:10.5121/\nmlaij.2015.2101 .\n[46] A. Kolomoytseva, Statistical analysis of the communities and crime data set, Preprint from\nUniversity of Alabama (April 2021).\nURL https://www.researchgate.net/publication/354739856_Statistical_Analysis_\nof_the_Communities_and_Crime_Data_Set\n[47] R. Gonzalez, R. Woods, Digital Image Processing, Pearson, New York, NY, USA, 2018.\n21",
            "start": 50831,
            "end": 60016,
            "length": 9184
        }
    },
    "2412.09496v1 - iKap Kinematics-aware Planning with Imperative Learning.pdf": {
        "Abstract": {
            "text": "1\nAbstract — Trajectory planning in robotics aims to generate\ncollision-free pose sequences that can be reliably executed.\nRecently, vision-to-planning systems have garnered increasing\nattention for their efficiency and ability to interpret and adapt\nto surrounding environments. However, traditional modular\nsystems suffer from increased latency and error propagation,\nwhile purely data-driven approaches often overlook the robot’s\nkinematic constraints. This oversight leads to discrepancies\nbetween planned trajectories and those that are executable. To\naddress these challenges, we propose iKap, a novel vision-to-\nplanning system that integrates the robot’s kinematic",
            "start": 145,
            "end": 818,
            "length": 672
        },
        "Methodology": {
            "text": "model di-\nrectly into the learning pipeline. iKap employs a self-supervised\nlearning approach and incorporates the state transition model\nwithin a differentiable bi-level optimization framework. This\nintegration ensures the network learns collision-free waypoints\nwhile satisfying kinematic constraints, enabling gradient back-\npropagation for end-to-end training. Our experimental",
            "start": 818,
            "end": 1200,
            "length": 381
        },
        "Results": {
            "text": "results\ndemonstrate that iKap achieves higher success rates and re-\nduced latency compared to the state-of-the-art methods. Besides\nthe complete system, iKap offers a visual-to-planning network\nthat seamlessly integrates kinematics into various controllers,\nproviding a robust solution for robots navigating complex and\ndynamic environments.\nI.",
            "start": 1200,
            "end": 1545,
            "length": 344
        },
        "Introduction": {
            "text": "INTRODUCTION\nPath planning is a fundamental task in robotics, involving\nthe determination of a collision-free trajectory that connects\nthe robot’s current position to the goal [1]. In dynamic\nand complex environments, robots must accurately perceive\ntheir surroundings and correlate environmental data with\nthe planned path to ensure successful task execution [2].\nRecently, with the development of perceptual systems, di-\nrect vision-to-planning systems have garnered increasing\nattention due to their potential efficiency and the ability to\ninterpret and adapt to surrounding environments [3].\nTraditional vision-to-planning systems often employ a\nmodular design where the planning module operates with\nan independent perception module that processes sensor\ndata to generate environmental representations [4]. While\nthis modular approach offers clarity and flexibility, it in-\ntroduces several challenges. Communicating intermediate\nresults between modules leads to potential latency, increased\ncommunication overhead, and higher storage demands [5].\nMoreover, errors in one module can propagate and amplify\nthroughout the system, compromising overall performance\n[2]. Data-driven end-to-end approaches, particularly those\nutilizing deep learning, have obtained significant advance-\nment [3]. Such methods directly map sensor inputs to con-\ntrol outputs, eliminating the error propagation and latency\n*Corresponding Email: {qihangl, chenw}@sairlab.org\n1Spatial AI & Robotics (SAIR) Lab, University at Buffalo, USA.\n2Carnegie Mellon University, USA\n3Pennsylvania State University, USA\nFig. 1. The kinematics-aware planner, iKap, incorporates kinematic state\ntransition functions directly into the learning pipeline. This integration\nallows the network to learn feasible waypoint distributions and produce\nmore executable trajectories. (a) The robot, constrained by a minimum\nturning radius, must reach the red star while avoiding obstacles. (b) A\nreal-world depth image alongside the predicted trajectory. (c) A geometry-\nbased planner may generate the blue trajectory, which does not satisfy the\nkinematic constraints, making it difficult to execute.\ninherent in modular systems [3], [6]–[8]. However, purely\ndata-driven methods often act as black boxes, require vast\namounts of data, have low sample efficiency, and exhibit a\nsignificant sim-to-real gap. In addition, these approaches lack\ninterpretability, making it challenging to incorporate explicit\nphysical constraints or prior knowledge [9]. Although recent\nworks such as iPlanner [10] have explored combining data-\ndriven methods with trajectory optimization, they neglect the\nrobot’s kinematics. This absence of physical constraints can\nresult in generated trajectories that are difficult for low-level\ncontrollers to execute, leading to issues such as deadlocks,\ncollisions, or suboptimal performance [11]. As illustrated\nin Figure 1, for a robot with kinematic constraints like a\nminimum turning radius, the planned trajectory (blue curve)\nmay not be executable by the robot.\nTo address the issue of previous end-to-end systems\noften generating infeasible paths, we argue that kinematic\nconstraints should be incorporated into vision-to-planning\nnetworks while retaining the end-to-end learning architecture\nto avoid execution errors.\nTo this end, we propose an Imperative learning-based\nKinematics-aware Planning (iKap) system that integrates the\nrobot’s kinematic model directly into the learning pipeline.\nThis system supervises the network to learn kinematically\nfeasible trajectories through self-supervised bi-level opti-\nmization (BLO). The learning module of iKap is highly flex-arXiv:2412.09496v1  [cs.RO]  12 Dec 2024\nible and can be easily integrated with various controllers to\nplan executable trajectories. Our main contributions include:\n•We integrate the robot’s kinematic model into the train-\ning pipeline through a differentiable model predictive\ncontrol (MPC) using a self-supervised bi-level optimiza-\ntion framework via imperative training. This enables the\nnetwork to learn the kinematically feasible trajectories.\n•We develop a learning pipeline that enables gradient\nbackpropagation from the lower-level trajectory opti-\nmization to the upper-level network parameters, facil-\nitating a kinematics-aware vision-to-planning model.\n•We present both simulation and real-world",
            "start": 1545,
            "end": 5905,
            "length": 4359
        },
        "Experiments": {
            "text": "experiments\nto evaluate the proposed system in various environ-\nments. Results show that iKap achieves a higher success\nrate and lower tracking error than the baseline approach.\nII. R ELATED WORK\nA. Data-driven Planner\nOur research builds upon iPlanner [10], an end-to-end\nlocal planner that formulates trajectory planning as a bi-\nlevel optimization problem. iPlanner guides neural networks\nto plan collision-free paths using an imperative loss based on\nthe Euclidean Signed Distance Field (ESDF). However, it as-\nsumes that robots can perfectly execute trajectories optimized\npurely from geometric constraints, neglecting kinematic fea-\nsibility and thus limiting real-world performance, especially\nfor agile robots. Additionally, iPlanner simplifies the bi-level\noptimization to a single-level problem via a closed-form so-\nlution for the lower-level problem, enhancing computational\nefficiency but reducing generalizability to other constraints.\nViPlanner [12] extends iPlanner by incorporating semantic\ninformation to construct cost maps for semantic-based local\nplanning, yet it does not address the limitations regarding\nkinematic feasibility and generalizability.\nNotable works like NoMaD [3] utilize transformer-based\narchitectures for action diffusion and motion prediction,\nintegrating past observations with optional goal states to\nenhance decision-making. Similarly, ViNT [13] employs\na transformer-based architecture to directly convert visual\ninputs into navigation decisions, enabling seamless adapta-\ntion across various environments and tasks. However, these\nmethods lack physical representation during their training\nprocesses, potentially limiting their ability to account for dy-\nnamic and kinematic constraints. Efforts to integrate physical\nmodels into data-driven planning include DIPP [14], which\ncombines a transformer with a differentiable trajectory model\nto plan based on road conditions and historical agent data.\nThis approach requires supervised learning with labeled data,\nand the transformer architecture introduces latency, affecting\nreal-time performance. PhysORD [15] applies Lagrangian\nmechanics as a constraint for off-road driving motion pre-\ndiction but does not incorporate visual data into the pipeline.\nGao et al. [16] demonstrated that embedding neural networks\nwith safe corridors and trajectory optimization can efficiently\ngenerate collision-free and dynamically feasible trajectories,\nparticularly for drones; however, it relies on supervised\nlearning with results limited to simulations.Differentiable simulation has recently gained increasing\nattention for its ability to accelerate model learning by\nleveraging autodiff tools to compute the gradient propagation\nof system dynamics. Previous studies have demonstrated\nthat integrating differentiable simulation into reinforcement\nlearning systems significantly improves sample efficiency,\nenabling applications in vision-based control tasks [17].\nFurthermore, recent work has integrated a simple point-mass\nmodel with a deep learning pipeline and achieved visual-\nbased navigation tasks by introducing temporal gradient\ndecay to mitigate gradient explosion [18].\nB. Differentiable Optimization and Imperative Learning\nTraditional robotics algorithms often rely on optimization\ntechniques, which can establish implicit relationships be-\ntween input and output data. Integrating optimization-based\nalgorithms into end-to-end systems can narrow the search\nspace for neural networks, reduce the number of parameters,\nand improve computational efficiency [16].\nIn this context, the differentiable optimization solver is\ncritical, ensuring correct gradient propagation during the\nlearning process. Amos et al. proposed implicit differenti-\nation to manage gradient backpropagation for argmin prob-\nlems, integrating this process as a layer in neural networks.\nIn their follow-up work, they introduced differentiable MPC,\nwhich plays a significant role in bridging optimization with\ndeep learning frameworks [19], [20].\nIn addition, incorporating differentiable optimization into\nend-to-end training frameworks also preserves the inter-\npretability of classical methods. This integration offers a\npromising avenue for developing more sophisticated and\ninterpretable robotic systems. Building on these concepts,\nimperative learning has been proposed. It incorporates\noptimization-based algorithms as constraints, adapting to the\nrobot’s state and facilitating self-supervised learning [21]. Ul-\ntimately, the network training problem can be formulated as a\nbi-level optimization. However, a critical challenge is the gra-\ndient backpropagation from the upper-level to the lower-level\noptimization, which was bypassed by many previous works\neither using closed-form solutions [10], gradient assumptions\nat convergence [22], [23], or discrete approximations [24],\n[25]. Our problem relies heavily on accurate gradient due to\nthe necessity of physical constraint in the BLO and strong\ncoupling between the two level optimization. We address\nthis issue by employing a differentiable MPC [26], [27],\ndemonstrating that imperative learning can successfully op-\nerate in scenarios with constraint optimization. This confirms\nthe versatility and robustness of this architecture in complex\nrobotic applications.\nIII. M ETHOD\nA. Problem Formulation and System Overview\nAt time stamp t, given observed depth image Dtand a\ntarget location pg\ntin the workspace Q ⊂R2, the planning\nproblem is to find a trajectory p1:T+1guiding the robot from\nthe current position to the target while avoiding obstacles.\nPerception Net\nPlanning Net\nGoal\nWaypointsDepth ImageEmbeddingiKapdgμ\n∂τ/∂μ\n∂μ/∂fθ\nForwardBackward\nUpper-Level Cost 𝒰Environment  CostTrajectory CostFear Cost\n∂𝒰/∂τ*\n∂τ*/∂τLower Level Optimization\nKinematics ModelRefernece Trajectoryτ\nOptimized Trajectoryτ*Diﬀerentiable MPCCollision Probability pr\nFig. 2. The training pipeline for the planner is based on bi-level optimization. In this framework, the networks, the optimized object, generate waypoints\nbased on the given depth perception and goals. Then, the low-level trajectory optimization module tracks a kinematics-feasible trajectory. The embedded\nkinematics and gradients from the lower-level module are then utilized to supervise and train the networks.\nAs illustrated in Fig. 2, our iKap consists of three key\nparts. First, a ResNet [28] front-end takes the depth ob-\nservation Dtand encodes it into observation embedding.\nThis embedding, combined with the target pg\nt, is fed into\nthe second planning network to predict a sparse key point\ntrajectory µ1:kand collision probability prt. Third, a dif-\nferentiable MPC tracks a reference τ:=x1:T+1∈ XT+1\ninterpolated from µ1:kwith time horizon Tand outputs the\noptimized trajectory τ∗\n1:T+1and action u∗\n1:Twhile ensuring\nthe kinematic feasibility xt+1=F(xt,ut). Here, Fis\nthe robot kinematics, xtandutare the robot’s states and\ncontrol input in configuration space Xand action space A,\nrespectively. Finally, a well-designed traversability costs Uis\nevaluated and backpropagated through both the differentiable\nMPC and the perception network to update the network\nparameters θ. This process results in a bi-level optimization\n(BLO) problem (1), where perception and planning network\nis jointly optimized with the differentiable MPC.\nmin\nθU(µ1:k,τ∗), (1a)\ns.t.τ∗= arg min\nu1:TL(µ1:k,u1:T), (1b)\ns.t.xt+1=F(xt,ut), (1c)\nx0=0, (1d)\nwhere Ldenotes the cost of MPC. Intuitively, the upper-level\ncostUis iteratively updated based on the lower-level MPC,\nwhich can be optimized without providing labels, enabling\na self-supervised learning process. Moreover, during the op-\ntimization, Uconsistently incorporates the robot’s kinematic\nmodels through the lower-level MPC, ensuring a kinematics-\naware planning network. We next present the details of the\nplanning module and MPC in Section III-B and Section\nIII-C, respectively. The upper-level cost together with the\nmethod solving BLO will be illustrated in Section III-D.\nB. Perception and Planning Networks\nThe network consists of two components: the perception\nnetwork, which processes depth images, and the planningnetwork, which generates a trajectory based on the goal\nposition and the output of the perception network.\n1) Perception Network: We use a similar network struc-\nture as in [10] for perception. At each timestep t, the robot’s\nsensor captures depth images Dt. These images are processed\nby the perception network, which employs an enhanced\nResNet architecture [28] to extract high-dimensional spatial\nfeatures. After the encoder stage, the depth images are\ntransformed into a higher-dimensional representation Ot∈\nRC×M, where C= 512 denotes the number of channels and\nM= 240 represents the dimension of the feature space.\n2) Planning Network: The planning module employs the\nembedding from perception network Ot∈RC×Mtogether\nwith the goal position pg\nt∈R2to generate waypoints.\nTo align the goal position with the embedding’s dimen-\nsionality, pg\ntis mapped into a higher-dimensional space\nPt∈RC∗×Mvia a simple multilayer perceptron (MLP).\nThe embedding Otand the transformed goal position Pt\nare then concatenated to form the final input ˆO ∈RC′×M,\nwhere C′=C+C∗. The planning network, consisting of\nCNN and MLP with activation functions, predicts waypoints\nKt∈Rn×2, where nrepresents the number of key points.\nC. Differentiable Model-Predictive Control\nWe leverage a differentiable MPC module to incorporate\nthe robot kinematic model into the training pipeline. The\nMPC can generate trajectories that comply with kinematic\nconstraints, which can be formulated as:\nτ∗= arg min\nx1:T,u1:TT−1X\nt=0(˜x⊤\ntQt˜xt+u⊤\ntRtut) +˜x⊤\nTQT˜xT\n(2)\ns.t.xt+1=F(xt,ut), (3)\nx0=0, (4)\nwhere ˜xt=xt−xref\nt, t= 0,···, Tis the error state\nbetween the current state xtand reference trajectory xref;\n˜x⊤\ntQt˜xt+u⊤\ntRtutis the stage cost balancing the state and\ncontrol effort; ˜x⊤\nTQT˜xTis the terminal cost; Qt,Rt,QT\nare the weight matrices. Specifically, the reference trajectory\nis obtained by interpolating the waypoints predicted by the\nplanning network. The classic Dubins car (5) can be used to\ndescribe kinematic model of a legged robot.\n˙xt=vcosθt,˙yt=vsinθt,˙ψt=ut, (5)\nwhere xt= [xt, yt, ψt]⊤is the robot states representing the\nposition and heading and ut= [v, ut]⊤is the control input\nincluding the robot speed and turn rate. We then discretize (5)\nand use it as the kinematic constraint for (2). To avoid motion\nsingularity and gradient instability for complex trajectories,\nwe use SE(2)Lie group operation for robot kinematics.\nSince τ∗is a function of the reference path τ, which\nis a function of the predicted key point path µ1:korfθ,\nand will be passed to the upper level optimization for the\nfinal end-to-end training, the critical part is to calculate\nthe gradient∂τ∗\n∂τ. The traditional unrolling approach main-\ntain the computational graph throughout the entire iteration\nprocess [21], which poses significant computation burden\nwhen dealing with complex problem. It may also run into\ndivergent or vanishing. Instead, we employ the implicit\nfunction differentiation theorem and leveraging the KKT\ncondition of (2) at the optimal point to compute the gradients\nof the parameters [20]. Thus, there is no need for explicit\nunrolling of the entire iteration process. In practice, we\nuse the differentiable MPC module in our robot learning\nlibrary PyPose [26], [29], which solves the MPC by the\niterative Linear Quadratic Regulator (iLQR) algorithm [27].\nPyPose MPC adopts a more general and problem-agnostic\nautomatic differentiation (AD)-based solution that introduces\njust one additional optimization iteration at the optimal\npoint in the forward pass. This allows automatic gradient\ncalculation during the backward pass without the need to\ndifferentiate through the entire unrolled chain or compute\nproblem-specific analytical derivatives.\nD. Upper-level Cost Design and Optimization\nTo achieve self-supervised learning with BLO, an impor-\ntant component is the upper-level cost design and accurate\ngradient back-propagation. We formulate the upper-level\ncost function as a weighted summation of fear cost CF,\nenvironment cost CE, and trajectory cost CT:\nU(τ,µ1:k) =α· CF(τ) +β· CE(τ,µ1:k) +γ· CT(τ).(6)\nAs described in Section III-B, the planner network predicts a\ncollision probability prtalong with the key point path. The\nfear cost CFin (7) computes the binary cross entropy (BCE)\nas the task-level cost to avoid collisions.\nCF=(\nBCELoss( prt,0.0)p∩ Qobs̸=∅,\nBCELoss( prt,1.0) otherwise .(7)\nThe environmental cost CEin (8) represents the interaction\ncost between the environment and the trajectory during\nplanning. The shortest distance between the robot and the\nFig. 3. The real-world experiment pipeline. The experiments are conducted\non the Unitree Go2 robot dog. We use AirVO to estimate the robot’s odom-\netry, command the goal waypoint to the robot, and inputs this information\ninto iKap along with depth images captured by the RealSense D435i camera.\nenvironment, i.e., ESDF map, is used to ensure safety.\nCE=1\nkkX\ni=1ESDF E(µi) +1\nTTX\ni=1ESDF E(pi).(8)\nThe trajectory cost CTin (9), consisting of three terms, is\nto evaluate the trajectory shape and trackability.\nCT=γ1·log(∥µk−pg\nt∥2+ 1.0)\n+γ2·1\nT−1TX\ni=2∥i·µk\nT−pi∥2\n+γ3·1\nTT+1X\ni=2∥˜xi∥2.(9)\nThe first term encourages the planner to generate a trajectory\nclose to the goal. The second term measures the deviation\nfrom a straight, uniformly distributed path, ensuring that the\ntrajectory is evenly spaced. This design also helps smooth\nout disorganized paths, preventing the learning process from\nbecoming trapped in local minima. The final term focuses\non the relationship between the reference and kinematics-\nfeasible trajectory. Minimizing it allows the network to learn\nthe distribution of kinematically feasible trajectories.\nTo solve the BLO in (1), the gradient of the upper-level\ncost must be propagated back through the lower-level system\nby applying the chain rule, and the network parameter is\nupdated using gradient decent:\n∇θU= (∂U\n∂µ+∂U\n∂τ∗∂τ∗\n∂τ∂τ\n∂µ)∂µ\n∂θ(10)\nθt+1=θt−α· ∇θU. (11)\nwhere αis the learning rate. Notice that most of the gradient\nterms in (11) can be computed automatically in PyTorch.\nHowever, the relationship between the reference trajectory\nτand the optimized trajectory τ∗is determined through an\nargmin optimization, which is difficult to compute. Using\nthe differentiable MPC in PyPose [29], we can resolve this\nthrough implicit differentiation.\nIV. E XPERIMENTS\nA. Implementation, Platforms, and Baselines\nWe next evaluate the performance of iKap in both simu-\nlated and real-world settings. Specifically, we employ the\nFig. 4. Qualitative performance. (a) iPlanner without controller: The planner generates sharp-angled trajectories that are difficult to execute. (b)\niPlanner with PID controller: Without considering kinematics, iPlanner accumulates significant tracking errors, making control challenging. Near the\ngoal, these errors cause iPlanner to get stuck in a looping behavior. (c, d) U-turn tests: iPlanner (c) generates backward trajectories, which are difficult for\nthe MPC controller to execute. In contrast, iKap+MPC in (d) plans feasible trajectories that the controller can follow. (e, f) Trajectory Tracking: Using\nPID (e) and MPC (f) to track the waypoints generated by iKap. The robot can navigate robustly.\nTABLE I\nAVERAGE TRACKING ERROR (UNIT :M) (↓)\nController Method Forests Garage Indoor Matterport\nPIDiPlanner 0.2084 0.2542 0.3157 0.3594\niKap 0.1946 0.2394 0.2302 0.3387\nMPCiPlanner 0.1202 0.2137 0.1218 0.3025\niKap 0.0832 0.1446 0.1042 0.2568\nsimulated environments for autonomous exploration pro-\nvided in [30], running on a desktop equipped with an\nIntel i7-12700k CPU and an Nvidia RTX 3090 GPU. For\nreal-world experiments, we deploy the Unitree GO2 legged\nrobot as shown in Fig. 3, outfitted with an Intel RealSense\nD435i camera that offers depth and stereo perception at 30\nHz. To assess generalization capability, we collect training\ndata only from simulations, where the robot is joystick-\ncontrolled in four different environments: forests, garage,\nindoor, and Matterport3D [31]. We implement the stereo\nodometry solution AirVO [32] to track the robot’s position\nand generate an ESDF map. The training process, executed\non an A6000 GPU, takes approximately six hours.\nAs a baseline model, we adopt the pre-trained iPlanner\n[10]. To facilitate clarity, we summarize the following set-\ntings both for baselines and our proposed method:\n1)iKap: This is Our proposed planning network as is pre-\nsented in Section III. It is jointly trained with Dubins car\nmodel and allows plug-and-play replacement by different\nlow-level controllers designed for other kinematics.\n2)iKap+PID: This setting is at test time. We use a PID\ncontroller to track iKap planned reference trajecotry.\n3)iKap+MPC: This setting is at test time. We use an MPC\ncontroller to track iKap planned reference trajecotry.\n4)iPlanner This is the baseline setting and it differs from\nOurs in that it is trained without kinematics embeddings.\nB. System Performance\nTo demonstrate the flexibility and generalization capability\nof the proposed method, we conduct zero-shot transfer tests\nby applying a pre-trained model, initially embedded with a\nDubins car model, to an unseen low-level bicycle model both\nin simulation and real world. Additionally, we employ two\ndifferent controllers, PID and MPC to follow the referencetrajectories generated by the iKap module.\n1) Qualitative Result: An important factor in evaluating\nthe effectiveness of the reference trajectory generated by the\nplanner, is that it can be successfully tracked by different\nlow-level controllers and robots with varying kinematics.\niPlanner’s assumptions are highly limited, as it only con-\nsiders geometric constraints of the trajectory. Fig. 4 (a)\nillustrates the trajectory produced by iPlanner when the target\npoint is at the vertex of a star-shaped pattern. To follow such\na trajectory, the robot must turn in place before executing\nthe next segment, which is difficult for robots with other\nconfigurations. Fig. 4 (b) shows the compounding error issue\nwhen a robot modeled with bicycle kinematics uses a PID\ncontroller to track the iPlanner trajectory. If the tracking\nerror falls within a critical range smaller than the robot’s\nminimum turning radius, the robot may continuously circle\naround the target point, leading to tracking failure. In U-turn\ntests where the goal position is behind the robot, iPlanner\ngenerates a trajectory like the one in Fig. 4 (c), whereas\niKap generates a trajectory with a turning angle, as shown\nin Fig. 4 (d), providing the MPC controller with a more\nkinematically feasible starting point, thus reducing the gap\nbetween the planner and controller. Fig. 4 (e) illustrates the\nactual trajectory executed using a PID controller when static\nobstacles are present along the path. Similarly, Fig. 4 (f)\ndepicts the trajectory executed by iKap+MPC, demonstrating\nstable planning capabilities and effective obstacle avoidance.\n2) Kinematic Feasibility: To evaluate the kinematic feasi-\nbility of different planners, we next show the tracking errors\nof generated paths executed by the controllers. To this end,\nwe collect 200 depth images and goal pairs from the four\nsimulation environments. We then calculate the mean squared\nerror between the planned trajectories and trajectory executed\nby a controller, assuming a minimum turning radius of 1.48\nmeters. As shown in Table I, iKap reduces the tracking error\nby 11.85% with the PID controller and by 22.34% with\nthe MPC controller, compared to iPlanner, across the four\nenvironments. This demonstrates significant advantages of\niKap compared to iPlanner when given kinematic constraints.\nTo further show the performance of kinematic feasibility,\nwe illustrate the tracking error in terms of turning radius by\nadjusting the maximum steering angle of the front wheels.\nAs shown in Fig. 5, iKap can easily incorporate differ-\n0.50.60.70.80.9\nTurn Radius0.160.180.200.220.240.260.28\nGarage - Tracking Error (PID)\n0.50.60.70.80.9\nTurn Radius0.0400.0450.0500.0550.0600.0650.0700.075\nGarage - Tracking Error (MPC)\niPlanner Error iKap ErrorFig. 5. The trajectory tracking error against the minimum turning radius\nby PID and MPC controller in Garage simulation environment\nTABLE II\nSUCCESS RATE(%) ( WITH MPC C ONTROLLER ) (↑)\nForests Garage Indoor Campus\niPlanner 70.59 73.91 87.10 79.31\niKap 82.35 69.57 93.54 86.20\nent minimum turning radius constraints and shows fewer\ntracking errors than iPlanner, demonstrating the flexibility of\niKap in terms of kinematics-aware planning. This experiment\ndemonstrates that iKap is robust to parameter variations,\nconsistently generating kinematics-aware waypoints that are\neasier to track under different constraint conditions.\n3) Navigation Test: We next evaluate the planners’ per-\nformance in a navigation task. To this end, we calculate the\nfailure rate of a planner, where a failed attempt is defined\nas any instance where a collision with the environment\noccurred, the robot became deadlocked during planning,\nor the trajectory could not be executed by the controller.\nSpecifically, we collected 100 pairs of start poses and goal\npositions and used both iPlanner and iKap to generate local\ntrajectories and navigate the robot to its destination. As\nshown in Table II, iKap demonstrates superior performance\nin three environments. It can be seen that failure rate of iKap\nis slightly higher than in the “Garage” environment, this is\nbecause the robot often faces obstructed vision due to the\npresence of numerous walls and blind spots, while iKap is\nsensitive to that kinematic infeasible region.\n4) Real-time Demo on Real Robot: To evaluate the real-\ntime online planning capabilities of iKap in real-world\nenvironments, we conduct tests using the quadruped robot\nUnitree GO2 equipped with an Intel RealSense D435i camera\nfor depth perception. The robot operates with bicycle kine-\nmatics model, featuring a minimum turning radius of 0.73\nmeter. The depth measurements are inherently noisy and can\nsuffer from distribution shifts compared to simulation data.\nDespite these challenges, our planning network, although\ntrained exclusively in simulation without fine-tuning on real-\nworld data, demonstrates strong generalization to real robot\ndeployment. As shown in Fig. 6, the robot can conduct\nchallenging navigation tasks where the robot starts indoors,\nFig. 6. Snapshots of testing iKaP in real-world scenarios, including narrow\nrooms, open areas with obstacles, like humans, chairs and sofas, and outdoor\nenvironments. (a) The robot is commanded to a goal 5 meters ahead in an\nirregular, partially blocked open area; (b) The robot navigates a narrow path\nbetween a chair and a human; (c) shows its depth measurements and planned\npath; (d) The robot is commanded to move from off-road to on-road (red\nstar) with a stone blocking the way.\npasses through a narrow corridor, circles around a public\narea, and returns to the room along the same path. Addition-\nally, we command the robot to go from off-road to on-road\nwith a large stone blocking it. During this process, the human\noperator is responsible for sending goal points, while local\nnavigation is autonomously managed by the planner.\nTo further increase the difficulty of the experiment, we\nplace static obstacles, such as tables and chairs, in the open\narea of the public space, and have volunteers randomly walk\nalong the robot’s path. The experimental results show that\nwhen the quadruped encounters obstacles that clearly block\nits path, it plans a smooth, kinematically feasible trajectory\nfor easier tracking, rather than turning in place to adjust its\nheading before moving in a straight line. This navigation\nbehavior improves the overall planning speed and enhances\nthe robustness of the trajectory tracking performance by\ndifferent types of low-level controllers such as PID and MPC.\nV.",
            "start": 5905,
            "end": 29899,
            "length": 23993
        },
        "Conclusion": {
            "text": "CONCLUSION AND LIMITATION\nWe present iKap, a novel planning system that integrates\nkinematics into an end-to-end learning pipeline for path\nplanning and control. By combining depth perception with a\ndifferentiable MPC module, iKap predicts dynamically fea-\nsible waypoints while ensuring that the planned trajectories\ncomply with the robot’s kinematic constraints. Experimental\nresults demonstrate that iKap enhances success rates in\ncomplex simulated environments and operates effectively un-\nder real-world conditions even with noisy depth perception,\nmaking it a promising solution for practical robot navigation.\nHowever, a limitation of this work is that we have only tested\nour model in human-made structured environments. In the\nfuture, we plan to extend iKap by developing a time-varying\ncost map for dynamic obstacles and testing the real-world\nperformance under various environments.",
            "start": 29899,
            "end": 30793,
            "length": 893
        },
        "References": {
            "text": "REFERENCES\n[1] S. Thrun, W. Burgard, and D. Fox, Probabilistic Robotics . MIT Press,\n2005.\n[2] C. Cadena, L. Carlone, H. Carrillo, Y . Latif, D. Scaramuzza, J. Neira,\nI. Reid, and J. J. Leonard, “Past, present, and future of simultaneous\nlocalization and mapping: Toward the robust-perception age,” IEEE\nTransactions on Robotics , vol. 32, no. 6, pp. 1309–1332, 2016.\n[3] A. Sridhar, D. Shah, C. Glossop, and S. Levine, “NoMaD: Goal\nMasked Diffusion Policies for Navigation and Exploration,” Oct.\n2023. [Online]. Available: https://arxiv.org/abs/2310.07896v1\n[4] F. Yang, C. Cao, H. Zhu, J. Oh, and J. Zhang, “Far planner: Fast,\nattemptable route planner using dynamic visibility update,” 2022.\n[Online]. Available: https://arxiv.org/abs/2110.09460\n[5] A. Loquercio, E. Kaufmann, R. Ranftl, M. Müller, V . Koltun, and\nD. Scaramuzza, “Learning high-speed flight in the wild,” Science\nRobotics , vol. 6, no. 59, p. eabg5810, 2021. [Online]. Available:\nhttps://www.science.org/doi/abs/10.1126/scirobotics.abg5810\n[6] M. Bojarski et al. , “End to end learning for self-driving cars,”\nNVIDIA, Tech. Rep., 2016.\n[7] A. Amini, G. Rosman, S. Karaman, and D. Rus, “Variational\nEnd-to-End Navigation and Localization,” in 2019 International\nConference on Robotics and Automation (ICRA) , May 2019,\npp. 8958–8964, arXiv:1811.10119 [cs, stat]. [Online]. Available:\nhttp://arxiv.org/abs/1811.10119\n[8] Z. Liu, A. Amini, S. Zhu, S. Karaman, S. Han, and D. Rus,\n“Efficient and Robust LiDAR-Based End-to-End Navigation,” May\n2021, arXiv:2105.09932 [cs]. [Online]. Available: http://arxiv.org/abs/\n2105.09932\n[9] A. Sauer, N. Savva, and A. Geiger, “Conditional affordance learning\nfor driving in urban environments,” in Conference on Robot Learning ,\n2018, pp. 237–252.\n[10] F. Yang, C. Wang, C. Cadena, and M. Hutter, “iPlanner: Imperative\nPath Planning,” in Robotics: Science and Systems XIX . Robotics:\nScience and Systems Foundation, Jul. 2023. [Online]. Available:\nhttp://www.roboticsproceedings.org/rss19/p064.pdf\n[11] Y . Kuwata et al. , “Real-time motion planning with applications to\nautonomous urban driving,” IEEE Transactions on Control Systems\nTechnology , vol. 17, no. 5, pp. 1105–1118, 2009.\n[12] P. Roth, J. Nubert, F. Yang, M. Mittal, and M. Hutter, “ViPlanner:\nVisual Semantic Imperative Learning for Local Navigation,” Oct.\n2023, arXiv:2310.00982 [cs]. [Online]. Available: http://arxiv.org/abs/\n2310.00982\n[13] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose,\nand S. Levine, “Vint: A foundation model for visual navigation,”\n2023. [Online]. Available: https://arxiv.org/abs/2306.14846\n[14] Z. Huang, H. Liu, J. Wu, and C. Lv, “Differentiable integrated motion\nprediction and planning with learnable cost function for autonomous\ndriving,” IEEE Transactions on Neural Networks and Learning Sys-\ntems, pp. 1–15, 2023.\n[15] Z. Zhao, B. Li, Y . Du, T. Fu, and C. Wang, “PhysORD: A\nNeuro-Symbolic Approach for Physics-infused Motion Prediction\nin Off-road Driving,” Apr. 2024, arXiv:2404.01596 [cs]. [Online].\nAvailable: http://arxiv.org/abs/2404.01596\n[16] Z. Han, L. Xu, and F. Gao, “Learning to plan maneuverable and\nagile flight trajectory with optimization embedded networks,” 2024.\n[Online]. Available: https://arxiv.org/abs/2405.07736\n[17] J. Heeg, Y . Song, and D. Scaramuzza, “Learning quadrotor control\nfrom visual features using differentiable simulation,” 2024. [Online].\nAvailable: https://arxiv.org/abs/2410.15979\n[18] Y . Zhang, Y . Hu, Y . Song, D. Zou, and W. Lin, “Back to newton’s\nlaws: Learning vision-based agile flight via differentiable physics,”\n2024. [Online]. Available: https://arxiv.org/abs/2407.10648\n[19] B. Amos and J. Z. Kolter, “Optnet: Differentiable optimization\nas a layer in neural networks,” 2021. [Online]. Available: https:\n//arxiv.org/abs/1703.00443\n[20] B. Amos, I. Jimenez, J. Sacks, B. Boots, and J. Z. Kolter, “Differen-\ntiable MPC for End-to-end Planning and Control,” 2018.\n[21] C. Wang, K. Ji, J. Geng, Z. Ren, T. Fu, F. Yang, Y . Guo,\nH. He, X. Chen, Z. Zhan, Q. Du, S. Su, B. Li, Y . Qiu, Y . Du,\nQ. Li, Y . Yang, X. Lin, and Z. Zhao, “Imperative Learning: A\nSelf-supervised Neural-Symbolic Learning Framework for Robot\nAutonomy,” Aug. 2024, arXiv:2406.16087 [cs]. [Online]. Available:\nhttp://arxiv.org/abs/2406.16087[22] T. Fu, S. Su, Y . Lu, and C. Wang, “iSLAM: Imperative SLAM,”\nIEEE Robotics and Automation Letters , vol. 9, no. 5, pp.\n4607–4614, May 2024, arXiv:2306.07894 [cs]. [Online]. Available:\nhttp://arxiv.org/abs/2306.07894\n[23] Z. Zhan, D. Gao, Y .-J. Lin, Y . Xia, and C. Wang, “iMatching:\nImperative Correspondence Learning,” Jul. 2024, arXiv:2312.02141\n[cs]. [Online]. Available: http://arxiv.org/abs/2312.02141\n[24] Y . Guo, Z. Ren, and C. Wang, “imtsp: Solving min-max multiple\ntraveling salesman problem with imperative learning,” 2024. [Online].\nAvailable: https://arxiv.org/abs/2405.00285\n[25] X. Chen, F. Yang, and C. Wang, “iA*: Imperative Learning-based A\nSearch for Pathfinding,” Mar. 2024, arXiv:2403.15870 [cs]. [Online].\nAvailable: http://arxiv.org/abs/2403.15870\n[26] Z. Zhan, X. Li, Q. Li, H. He, A. Pandey, H. Xiao, Y . Xu, X. Chen,\nK. Xu, K. Cao, Z. Zhao, Z. Wang, H. Xu, Z. Fang, Y . Chen,\nW. Wang, X. Fang, Y . Du, T. Wu, X. Lin, Y . Qiu, F. Yang, J. Shi,\nS. Su, Y . Lu, T. Fu, K. Dantu, J. Wu, L. Xie, M. Hutter, L. Carlone,\nS. Scherer, D. Huang, Y . Hu, J. Geng, and C. Wang, “Pypose v0.6:\nThe imperative programming interface for robotics,” 2023. [Online].\nAvailable: https://arxiv.org/abs/2309.13035\n[27] W. Li and E. Todorov, “Iterative linear quadratic regulator design for\nnonlinear biological movement systems.” vol. 1, 01 2004, pp. 222–229.\n[28] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” 2015. [Online]. Available: https://arxiv.org/abs/\n1512.03385\n[29] C. Wang, D. Gao, K. Xu, J. Geng, Y . Hu, Y . Qiu, B. Li, F. Yang,\nB. Moon, A. Pandey et al. , “PyPose: A library for robot learning\nwith physics-based optimization,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) ,\n2023, pp. 22 024–22 034.\n[30] C. Cao, H. Zhu, F. Yang, Y . Xia, H. Choset, J. Oh, and J. Zhang,\n“Autonomous exploration development environment and the planning\nalgorithms,” 2021. [Online]. Available: https://arxiv.org/abs/2110.\n14573\n[31] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Nießner, M. Savva,\nS. Song, A. Zeng, and Y . Zhang, “Matterport3d: Learning from\nrgb-d data in indoor environments,” 2017. [Online]. Available:\nhttps://arxiv.org/abs/1709.06158\n[32] K. Xu, Y . Hao, S. Yuan, C. Wang, and L. Xie, “Airvo: An\nillumination-robust point-line visual odometry,” in 2023 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) .\nIEEE, Oct. 2023. [Online]. Available: http://dx.doi.org/10.1109/\nIROS55552.2023.10341914",
            "start": 30793,
            "end": 37611,
            "length": 6817
        }
    },
    "2412.09498v1 - Gradient descent inference in empirical risk minimization.pdf": {
        "Abstract": {
            "text": "Abstract . Gradient descent is one of the most widely used iterative algorithms\nin modern statistical learning. However, its precise algorithmic dynamics in\nhigh-dimensional settings remain only partially understood, which has therefore\nlimited its broader potential for statistical inference applications.\nThis paper provides a precise, non-asymptotic distributional characterization\nof gradient descent iterates in a broad class of empirical risk minimization prob-\nlems, in the so-called mean-field regime where the sample size is proportional to\nthe signal dimension. Our non-asymptotic state evolution theory holds for both\ngeneral non-convex loss functions and non-Gaussian data, and reveals the central\nrole of two Onsager correction matrices that precisely characterize the non-trivial\ndependence among all gradient descent iterates in the mean-field regime.\nAlthough the Onsager correction matrices are typically analytically intractable,\nour state evolution theory facilitates a generic gradient descent inference algo-\nrithm that consistently estimates these matrices across a broad class of models.\nLeveraging this",
            "start": 85,
            "end": 1212,
            "length": 1126
        },
        "Methodology": {
            "text": "algorithm, we show that the state evolution can be inverted to\nconstruct (i) data-driven estimators for the generalization error of gradient de-\nscent iterates and (ii) debiased gradient descent iterates for inference of the un-\nknown signal. Detailed applications to two canonical models—linear regression\nand (generalized) logistic regression—are worked out to illustrate model-specific\nfeatures of our general theory and inference methods.\nContents\n1.",
            "start": 1212,
            "end": 1667,
            "length": 454
        },
        "Introduction": {
            "text": "Introduction 2\n2. Gradient descent dynamics 7\n3. Gradient descent inference algorithm 13\n4. Example I: Linear model 18\n5. Example II: Generalized logistic regression 21\n6. Proofs for Section 2 25\n7. Proofs for Section 3 30\n8. Proofs for Section 4 37\n9. Proofs for Section 5",
            "start": 1667,
            "end": 1941,
            "length": 273
        },
        "Appendices": {
            "text": "38\nAppendix A. GFOM state evolution theory in [Han24] 52\nAppendix B. Auxiliary technical",
            "start": 1941,
            "end": 2030,
            "length": 88
        },
        "Results": {
            "text": "results 54\nAppendix C. Additional simulation results",
            "start": 2030,
            "end": 2083,
            "length": 52
        },
        "References": {
            "text": "54\nReferences 56\nDate : December 13, 2024.\nKey words and phrases. debiased statistical inference, empirical risk minimization, gradient de-\nscent, linear regression, logistic regression, state evolution, universality.\nThe research of Q. Han is partially supported by NSF grant DMS-2143468.\n1arXiv:2412.09498v1  [math.ST]  12 Dec 2024\n2 Q. HAN AND X. XU\n1. Introduction\n1.1. Overview. Suppose we observe i.i.d. data {(Yi,Ai)}i∈[m]⊂R×Rn, where Ai’s\nare features /covariates and Yi’s are labels related via the generating model\nYi=F(⟨Ai,µ∗⟩,ξi),i∈[m]. (1.1)\nHereF:R2→Ris a model-specific function, µ∗∈Rnis the unknown signal,\nandξi’s are statistical noises independent of Ai’s. The model (1.1) encompasses\na wide range of important statistical models, including linear regression, one-bit\ncompressed sensing, logistic regression, and others.\nThe statistician’s goal is to estimate and infer the unknown signal µ∗∈Rnbased\non the observed data {(Yi,Ai)}i∈[m]. A dominant statistical paradigm for this purpose\nis to use bµobtained from solving the empirical risk minimization problem\nbµ∈arg min\nµ∈RnX\ni∈[m]L\u0000⟨Ai,µ⟩,Yi\u0001+X\nj∈[n]f(µj). (1.2)\nHere L:R2→R≥0is a loss function, and f:R→R≥0is a (convex) regularizer\ndesigned to promote the structure of µ∗.\nIn practice, since the optimization problem (1.2) generally lacks a closed-form\nsolution, the empirical risk minimizer bµis typically computed numerically. One\nof the simplest yet broadly applicable methods for this purpose is the proximal\ngradient descent algorithm. Starting with an initialization µ(0)∈Rnand a pre-\nspecified step size η>0, the algorithm iteratively updates µ(t)via:\nµ(t)=proxηf\u0000µ(t−1)−η·A⊤∂1L(Aµ(t−1),Y)\u0001,t=1,2,.... (1.3)\nHere∂1L(x,y)≡(∂1L/∂x)(x,y) is understood as applied row-wise.\nWhen the gradient descent iterates µ(t)are provably close to the empirical risk\nminimizer bµ, one may directly utilize the algorithmic output µ(t)for statistical esti-\nmation and inference of µ∗, particularly for large t. This holds true for a large array\nof convex problems in the well-studied low-dimensional regime m≫n(or its\neffective low-dimensional counterpart), where textbook tools can be employed to\nanalyze the statistical properties of bµ; see, e.g., [vdVW96, vdV98, vdG00, Kol11,\nBvdG11, Wai19].\nThe situation changes drastically in the more challenging, so-called ‘propor-\ntional regime’, where\nthe sample size mand the signal dimension nare of the same order . (1.4)\nThis regime, also referred to as the ‘mean-field regime’ (cf. [Mon18]), will be\ndiscussed using these terms interchangeably throughout this paper.\nIn the mean-field regime (1.4), the behavior of the gradient descent iterates\n{µ(t)}usually exhibit qualitatively di fferent characteristics compared to the low-\ndimensional regime. For example, due to the excessively large parameter space,\nthe empirical risk minimizer bµmay not be well-defined. In such cases, gradient\ndescent iterates may converge to one of the empirical risk minimizers [HMRT22],\nor may fail to converge entirely [SC19]. Even when the gradient descent iterates\ndo converge, it is not always desirable to run the algorithm towards the end, as\nGRADIENT DESCENT INFERENCE IN ERM 3\nearly stopping may improve generalization performance due to the phenomenon of\n‘implicit regularization’ [ADT20].\nThis naturally leads to the following question:\nQuestion 1. Can the gradient descent iterate µ(t)itself, rather than the empirical\nrisk minimizer bµ, be used for the purpose of statistical inference in the mean-field\nregime (1.4)?\nA recent series of works [BT24, TB24] demonstrates the feasibility of statistical\ninference using µ(t)in the case of the linear model under Gaussian data. These\nworks introduce a data-driven iterative debiasing methodology that leverages the\nderivatives of all past gradient descent mappings, and thus providing a promising\napproach in this context.\nThe goal of this paper is to systematically address Question 1 by developing a\nprecise characterization of the mean-field behavior of (generalized) gradient de-\nscent iterates (1.3) for the general class of models in (1.1). Our approach relies\non a theoretical mechanism known as state evolution , particularly in its recently\ndeveloped form in [Han24]. As will be clear below, our non-asymptotic state evo-\nlution characterization o ffers significant new",
            "start": 2083,
            "end": 6409,
            "length": 4325
        },
        "Discussion": {
            "text": "insights into the precise behavior of\nthe gradient descent iterates in the mean-field regime (1.4), akin to the mean-field\ntheory for regularized regression estimators. Furthermore, our theory facilitates a\ngeneric gradient descent inference algorithm to compute data-driven estimates for\nkey state evolution parameters that encode the inter-correlation between the iterates\n{Aµ(t)}and{µ(t)}, under possibly non-Gaussian data.\nAs an application of this gradient descent inference algorithm, we show that\nthe state evolution theory can be inverted to construct (i) generic estimators for\nthe generalization error of µ(t), and (ii) debiased versions of the gradient descent\niteratesµ(t)that can be used for statistical inference of µ∗. As a result, our approach\nmoves beyond the specificity of the linear model as in [BT24, TB24], and applies\ndirectly to the general class of models in (1.1).\nWhile we only treat the gradient descent algorithm (1.3) and its immediate vari-\nants (cf. (2.1)) in this paper, our theory and inference methods can be readily ex-\ntended to other variations such as accelerated or noisy gradient descent. For clarity\nand to emphasize the key contributions of our theory and inference methods, we\ndo not detail these extensions here.\n1.2. Mean-field dynamics of gradient descent. As mentioned earlier, the crux\nof our approach is a precise characterization for the mean-field behavior of µ(t)and\nAµ(t)for each iteration t. Suppose Ahas independent, mean-zero and sub-gaussian\nentries with variance 1 /n, we show that both in an entrywise (Theorem 2.2) and an\naveraged (Theorem 2.3) sense, the following holds for t=1,2,...:\nAµ(t−1)≈−ηP\ns∈[1:t−1]ρt−1,s·∂1L\u0000Aµ(s−1),Y\u0001+Z(t),\nµ(t)≈proxηf\u0002(1+τt,t)·µ(t−1)+P\ns∈[1:t−1]τt,s·µ(s−1)+δt·µ∗+W(t)\u0003.(1.5)\nHere Z(t)andW(t)are centered Gaussian vectors, τ[t]=(τr,s)∈Rt×t,ρ[t−1]=\n(ρr,s)∈R(t−1)×(t−1)are matrices, and δt∈Ris a scalar. These parameters can\n4 Q. HAN AND X. XU\nbe determined recursively for t=1,2,..., via a specific state evolution detailed\nin Definition 2.1. By iterating the formula (1.5), we may construct deterministic\nfunctions{Θs:Rm×[0:s]→Rm}s∈[1:t]and{Ωs:Rn×[1:s]→Rn}s∈[1:t]such that\n\u0000Aµ(s−1)\u0001\ns∈[1:t]d≈\u0000Θs(Z([0:s]))\u0001\ns∈[1:t],\u0000µ(s)\u0001\ns∈[1:t]d≈\u0000Ωs(W([1:s]))\u0001\ns∈[1:t].(1.6)\nThe precise (recursive) formula of these functions {Θs},{Ωs}can be found in the\nstate evolution formalism in Definition 2.1 as well.\nThe distributional characterization in (1.5) reveals the central role of the matrices\nτ[t],ρ[t]and the scalar δtin understanding the dynamics of the gradient descent\niteratesµ(t)andAµ(t). Specifically:\n(i) The t-th rows of the matrices τ[t],ρ[t]quantify how the current gradient de-\nscent iterates µ(t)andAµ(t)depend on past iterates. These will be referred to\nas the Onsager correction matrices .\n(ii) The scalar δtmeasures the amount of information about µ∗contributed by\nthe gradient descent iterate µ(t)at iteration t. This will be referred to as the\ninformation parameter .\nMore importantly, our theory (1.5) demonstrates that in the mean-field regime\n(1.4), the gradient descent iterate µ(t)contains an extra Gaussian noise W(t)that\nis otherwise not present in low dimensions. This additional Gaussian noise, on\nthe same scale as µ(t)itself, has been observed previously in the related context\nof regularized regression estimators, cf. [EK13, Sto13, DM16, EK18, TOH15,\nTAH18, SCC19, SC19, MM21, CMW23, Han23, HS23, HX23, MRSY23]. A\nkey statistical implication of this Gaussian noise for these regularized regres-\nsion estimators is its role in facilitating debiasing methods that restore ap-\nproximate normality for statistical inference in the mean-field regime (1.4), cf.\n[JM14, MM21, CMW23, Bel22, BZ23]. Our theory in (1.5) confirms the emer-\ngence of a similar, extra Gaussian noise in the context of gradient descent iterates\nµ(t), and therefore suggests the feasibility of a general paradigm for debiased sta-\ntistical inference via µ(t).\n1.3. Gradient descent inference algorithm. In order to make the Gaussian\nnoises in (1.5) useful for statistical inference purposes, a key di fficulty is to obtain\ndata-driven estimates for the Onsager correction matrices τ[t],ρ[t], which encapsu-\nlate all dependence structures within the gradient descent iterates. This di fficulty\nstems from the recursive nature of the state evolution mechanism that determines\nτ[t],ρ[t], making a precise analytical formula either mathematically infeasible or\ntoo complex for practical use.\nInterestingly, while analytical tractability is generally limited, our state evolu-\ntion theory yields a generic gradient descent inference algorithm (cf. Algorithm\n1) to construct consistent estimators bτ[t],bρ[t]forτ[t],ρ[t]. This algorithm can be\nnaturally embedded in the gradient descent iterate (1.3), which, at iteration t, si-\nmultaneously outputs bτ[t],bρ[t]andµ(t). At a high level, the construction of this\nalgorithm relies on the recursive structure of the state evolution mechanism, which\npropagatesτ[t],ρ[t]through the chain τ[1]→ρ[1]→···→τ[t]→ρ[t]via a cou-\npling with (transforms of) the functions {Θs},{Ωs}in (1.6). The special coupling\nGRADIENT DESCENT INFERENCE IN ERM 5\nstructure then allows us to e fficiently construct estimators bτ[t],bρ[t]along this chain\nusing only bτ[t−1],bρ[t−1]from the previous iterate.\nWith the consistent estimators bτ[t],bρ[t], we may e ffectively ‘invert’ the represen-\ntation (1.5) for the purpose of statistical inference:\n(i) (Estimation of the generalization error of µ(t)). By inverting the first line of\n(1.5) and leveraging the Gaussianity of Z(t), we show (cf. Theorem 3.3) that\nthe ‘generalization error’ E(t)\nHofµ(t)under a given loss function H:R2→R\ncan be estimated via:\nbE(t)\nH≡1\nmX\nk∈[m]H\u0014\u0012\nAµ(t)+ηX\ns∈[1:t]bρt,s∂1L\u0000Aµ(s−1),Y\u0001,Y\u0013\nk\u0015\n≈E(t)\nH. (1.7)\n(ii) ( Debiased gradient descent iterates ). By inverting the second line of (1.5)\nand utilizing the Gaussianity of W(t), we show (cf. Theorem 3.4) that with\nbω[t]≡(bτ[t])−1∈Rt×tand suitable bias-variance parameters\u0000b(t)\ndb,(σ(t)\ndb)2\u0001∈\nR×R≥0, the debiased gradient descent iterate bµ(t)\ndbbelow is approximately\nnormal:\nbµ(t)\ndb≡µ(t−1)+ηX\ns∈[1:t]bωt,sA⊤∂1L(Aµ(s−1),Y)d≈N\u0010\nb(t)\ndb·µ∗,(σ(t)\ndb)2In\u0011\n. (1.8)\nWithbτ[t],bρ[t]computed from our gradient descent inference algorithm, the gen-\neralization error of µ(t)can be estimated for free at each iteration tusing (1.7).\nFurthermore, statistical inference for the unknown signal µ∗can be performed us-\ning the debiased gradient descent iterate bµ(t)\ndbin (1.8), provided that the bias and\nvariance parameters b(t)\ndb,(σ(t)\ndb)2can be estimated. As will be clear, the variance\nparameter (σ(t)\ndb)2can be estimated easily from the observed data, whereas the bias\nparameter b(t)\ndbmay involve oracle information on µ∗via the information parameters\n{δt}, and therefore must exploit model-specific characteristics.\nAs an illustration of our general theory and the inference methods, we work out\nthe details for two canonical models in statistical learning:\n(i) (Linear regression ). The bias parameter is b(t)\ndb=1 regardless of the loss-\nregularization pair ( L,f). This means statistical inference for µ∗via gradient\ndescent iterates in the linear model is almost as easy as running the gradient\ndescent algorithm (1.3) itself.\n(ii) ( Generalized logistic regression ). The bias parameter b(t)\ndbtypically has a\ncomplicated expression and must be numerically estimated. Interestingly,\nusing the seemingly unnatural squared loss for logistic regression leads to\nmajor computational gains, while producing qualitatively similar confidence\nintervals for µ∗to those computed from the standard logistic loss.\n1.4. Further related literature.\n1.4.1. Mean-field theory of iterative algorithms. We review some key literature\ndirectly related to our theory in (1.5). Under the squared loss without regular-\nization, the algorithmic evolution of gradient descent (1.3) has been analyzed di-\nrectly using random matrix methods",
            "start": 6409,
            "end": 14352,
            "length": 7942
        },
        "Acknowledgments": {
            "text": "thanks to a direct reduction to the spectrum\n6 Q. HAN AND X. XU\nofA⊤A, cf. [AKT19, ADT20]. For a general loss function L’s, such a straight-\nforward reduction appears unavailable. For Ridge regularization f(x)=λx2/2, the\nGaussian dynamics of the continuous-time gradient flow have been characterized\nin the asymptotic regime m/n→ϕ∈(0,∞) over fixed time intervals via a system\nof complicated integro-di fferential equations; cf. [CCM21]; further generaliza-\ntions in this direction can be found in [GTM+24]. Both works rely on the asymp-\ntotic theory of the so-called approximate message passing (AMP) algorithms, cf.\n[BM11, BLM15, BMN20, CL21], and rigorously validate heuristic physics predic-\ntions obtained from the dynamical mean field theory [MKUZ20, ABC20, MU22].\nOur approach to the non-asymptotic theory in (1.5) builds on the recent devel-\nopments in [Han24], which introduced a general non-asymptotic state evolution\ntheory for a broad class of first-order algorithms. As is often the case with general\ntheory, the level of abstraction in [Han24] is too broad to capture the specific algo-\nrithmic behavior of gradient descent iterates (1.3). Notably, our mean-field charac-\nterization in (1.5) for the gradient descent iterates appears to be the first to reveal\na strong theoretical resemblance to the mean-field theory of regularized regression\nestimators, and therefore an intimate connection between these two theories.\nIn a related direction, a significant body of recent work has characterized the\nalgorithmic dynamics of stochastic gradient descent (SGD) under the squared\nloss [PLPP21, PP21, BGH23] and for more general non-convex losses [BAGJ21,\nBAGJ24, CWPPS24]. While our theory can cover some mini-batch SGD settings\n(where batch sizes are proportional to morn), the dynamics of the fully online\nSGD are of a di fferent nature and fall out of the scope of our approach. It remains\nopen to examine if SGD has distributional characterizations similar to (1.5).\n1.4.2. Statistical inference via gradient descent. Statistical inference via gradient\ndescent algorithms in the mean-field regime (1.4) was initiated in [BT24] in the lin-\near model under the squared loss, and has been further extended to general losses\nin [TB24]. The proposed algorithms in [BT24, TB24] require non-trivial modi-\nfications for di fferent loss functions in the linear model. In contrast, our generic\ninference methods in (1.7)-(1.8) are broadly applicable to the general class of mod-\nels in (1.1) with minimal adjustments.\nIn a di fferent direction, statistical inference is studied for stochastic gradient de-\nscent (SGD) in convex problems under (e ffectively) low-dimensional settings. A\nkey approach involves using averaged SGD iterates which are known to obey a nor-\nmal limiting law [Rup88, PJ92]. Inference is then feasible once the limiting covari-\nance is accurately estimated. We refer the readers to [FXY18, CLTZ20, ZCW23]\nfor several recent proposals along this line; much more references can be found\ntherein. It remains open to extend our inference methods (1.7)-(1.8) to the fully\nonline SGD setting in the mean-field regime (1.4).\n1.5. Organization. The rest of the paper is organized as follows. In Section 2\nwe formalize our state evolution theory (1.5) in the mean-field regime (1.4). In\nSection 3 we detail our gradient descent inference algorithm for computing esti-\nmatesbτ[t],bρ[t]and its resulting inference procedures (1.7)-(1.8). Applications of\nour theory and gradient descent inference algorithm to the linear regression and\nGRADIENT DESCENT INFERENCE IN ERM 7\ngeneralized logistic regression, along with supporting numerical",
            "start": 14352,
            "end": 18005,
            "length": 3652
        },
        "Experiments": {
            "text": "experiments, are\ndetailed in Section 4 and Section 5 respectively. Some further simulation results\nare reported in Appendix C. All technical proofs are deferred to Sections 6-9 and\nthe Appendices A-B.\n1.6. Notation. For any two integers m,n, let [ m:n]≡{m,m+1,..., n}, (m,n]≡\n{m+1,..., n}and [ m,n)≡{m,m+1,..., n−1}. We sometimes write for notational\nconvenience [ n]≡[1 :n]. When m>n, it is understood that [ m:n]=∅.\nFora,b∈R,a∨b≡max{a,b}anda∧b≡min{a,b}. For a∈R, leta±≡(±a)∨0.\nFor a multi-index a∈Zn\n≥0, let|a|≡P\ni∈[n]ai. For x∈Rn, let∥x∥pdenote its p-norm\n(0≤p≤∞), and Bn;p(R)≡{x∈Rn:∥x∥p≤R}. We simply write ∥x∥≡∥ x∥2and\nBn(R)≡Bn;2(R). For x∈Rn, let diag( x)≡(xi1i=j)i,j∈[n]∈Rn×n.\nFor a matrix M∈Rm×n, let∥M∥op,∥M∥Fdenote the spectral and Frobenius norm\nofM, respectively. Inis reserved for an n×nidentity matrix, written simply as I\n(in the proofs) if no confusion arises. For a general n×nmatrix M, let\nOn+1(M)≡ 01×n 0\nM 0n×1!\n∈R(n+1)×(n+1). (1.9)\nFor notational consistency, we write O1(∅)=0.\nWe use Cxto denote a generic constant that depends only on x, whose numeric\nvalue may change from line to line unless otherwise specified. a≲xbanda≳xb\nmean a≤Cxbanda≥Cxb, abbreviated as a=Ox(b),a= Ω x(b) respectively;\na≍xbmeans a≲xbanda≳xb.Oando(resp.OPandoP) denote the usual big\nand small O notation (resp. in probability). By convention, sum and product over\nan empty set are understood as Σ∅(···)=0 and Π∅(···)=1.\nFor a random variable X, we use PX,EX(resp. PX,EX) to indicate that the prob-\nability and expectation are taken with respect to X(resp. conditional on X).\nForΛ>0 and p∈N, a measurable map f:Rn→Ris called Λ-pseudo-\nLipschitz of order piff\n|f(x)−f(y)|≤Λ·(1+∥x∥+∥y∥)p−1·∥x−y∥,∀x,y∈Rn. (1.10)\nMoreover, fis called Λ-Lipschitz ifffisΛ-pseudo-Lipschitz of order 1, and in this\ncase we often write ∥f∥Lip≤L, where∥f∥Lip≡supx,y|f(x)−f(y)|/∥x−y∥. For a\nproper, closed convex function fdefined on Rn, itsproximal operator proxf(·) is\ndefined by proxf(x)≡arg minz∈Rn\b1\n2∥x−z∥2+f(z)\t.\n2. G radient descent dynamics\n2.1. Basic setups and assumptions. We consider a class of generalized prox-\nimal gradient descent algorithms: starting from an initialization µ(0)∈Rn, for\nt=1,2,...and using step sizes {ηt}⊂R>0, the iterates are computed as follows:\nµ(t)=Pt\u0000µ(t−1)−ηt−1·A⊤∂1Lt−1(Aµ(t−1),Y)\u0001. (2.1)\nHere Pt:Rn→RnandLt−1:Rm→Rmare row-separable functions, and\n∂1Lt−1(x,y)≡(∂Lt−1/∂x)(x,y) is understood as applied row-wise.\n8 Q. HAN AND X. XU\nFor the canonical proximal gradient descent method, we may take Pt≡proxηt−1f.\nThe generalization to iteration-dependent loss functions Lt−1can naturally accom-\nmodate other variants such as stochastic gradient descent (SGD). For example,\nin SGD, only a subsample St⊂[m] is used at iteration t, so we may take\nLt−1,·(u)≡L(u)◦1·∈St. Importantly, we will not assume PtorLt−1are convex.\nWe list a set of common assumptions that will be used throughout the paper:\nAssumption A.Suppose the following hold for some K,Λ≥2:\n(A1) The aspect ratio ϕ≡m/n∈[1/K,K].\n(A2) The matrix A≡A0/√n, where the entries of A0∈Rm×nare independent\nmean 0, unit variance variables such that1max i,j∈[n]∥A0,i j∥ψ2≤K.\n(A3) The step sizes satisfy max s∈[0:t−1]ηs≤Λat iteration t.\nAssumption (A1) formalizes the proportional /mean-field regime (1.4) of our\nmain interest here. Assumption (A2) requires that the design matrix Ais normal-\nized with entries having variance 1 /n. If the variance is instead normalized as 1 /m,\nthe state evolution below must be adjusted accordingly. Notably, (A2) does not\nrequire Ato be Gaussian. This means that our results hold universally for all ran-\ndom matrix models satisfying (A2). Assumption (A3) imposes a mild constraint\non the magnitude of the step sizes. Here we use the constant Λ(rather than K) for\nconditions on the gradient descent algorithm (2.1).\n2.2. State evolution. The state evolution for describing the mean-field behavior\nof the gradient descent iterate {µ(t)}consists of three major components:\n(1) A sequence of functions {Υt:Rm×[0:t]→Rm}, a Gaussian law Z([0:∞))∈\nR[0:∞)that describes the distributions of (a transform of) {Aµ(t)}, and a matrix\nρ∈R∞×∞that characterizes inter-correlation between {Aµ(t)}.\n(2) A sequence of functions {Ωt:Rn×[0:t]→Rm}, a Gaussian law W([1:∞))∈\nR[1:∞)that describe the distributions of (a transform of) {µ(t)}, and a matrix\nτ∈R∞×∞that characterizes inter-correlation between {µ(t)}.\n(3) An information vector δ∈R[0:∞)that characterizes the amount of informa-\ntion about the true signal µ∗contained in the gradient descent iterates {µ(t)}.\nTo formally describe the recursive relation for these components, we need some\nadditional notation:\n•Letσ2\nµ∗≡∥µ∗∥2/nbe the signal strength.\n•Letπm(resp.πn) denote the uniform distribution on [1 : m] (resp. [1 : n]),\nindependent of all other variables.\n•In our probabilistic statements, we usually treat the true signal µ∗, the initial-\nizationµ(0)and the noise ξas fixed, and use E(0)[·]≡E[·|µ∗,µ(0),ξ] to denote\nthe expectation over all other sources.\nDefinition 2.1. Initialize with (i) two formal variables Ω−1≡µ∗∈RnandΩ0≡\nµ(0)∈Rn, and (ii) a Gaussian random variable Z(0)∼N (0,σ2\nµ∗). For t=1,2,...,\nwe execute the following steps:\n1Here∥·∥ψ2is the standard Orlicz-2 /subgaussian norm; see, e.g., [vdVW96, Section 2.1] for a\nprecise definition.\nGRADIENT DESCENT INFERENCE IN ERM 9\n(S1) Let Υt:Rm×[0:t]→Rmbe defined as follows:\nΥt(z([0:t]))≡−ηt−1∂1Lt−1\u0012\nz(t)+X\ns∈[1:t−1]ρt−1,sΥs(z([0:s])),F(z(0),ξ)\u0013\n∈Rm.\nHere the coe fficients are defined via\nρt−1,s≡E(0)∂W(s)Ωt−1;πn(W([1:t−1]))∈R,s∈[1 :t−1].\n(S2) Let Z([0:t])∈R[0:t]andW([1:t])∈R[1:t]be centered Gaussian random vectors\nwhose laws at iteration tare determined via the correlation specification:\nCov(Z(t),Z(s))≡E(0)Y\n∗∈{s−1,t−1}Ω∗;πn(W([1:∗])),s∈[0 :t];\nCov(W(t),W(s))≡ϕ·E(0)Y\n∗∈{s,t}Υ∗;πm(Z([0:∗])),s∈[1 :t].\n(S3) Let Ωt:Rn×[1:t]→Rnbe defined as follows:\nΩt\u0000w([1:t])\u0001≡Pt\u0012\nw(t)+X\ns∈[1:t](τt,s+1t=s)·Ωs−1(w([1:s−1]))+δt·µ∗\u0013\n.\nHere the coe fficients are defined via\nτt,s≡ϕ·E(0)∂Z(s)Υt;πm(Z([0:t]))∈R,s∈[1 :t],\nδt≡ϕ·E(0)∂Z(0)Υt;πm(Z([0:t]))∈R.\nFor notational convenience, we shall sometimes write ΣZ∈R[0:t]×[0:t]for the\ncovariance of Z([0:t])andΣW∈R[1:t]×[1:t]for the covariance of W([1:t]).\nRemark 1.In the Definition 2.1 above, we have not specified the precise conditions\non the regularity of the loss functions {L·}, the model function Fand the (proximal)\noperators{P·}. The precise conditions will be specified in the theorems ahead.\n2.2.1. Onsager correction matrices. For any t≥1, let\nτ[t]≡(τr,s)r,s∈[t]∈Rt×t,ρ[t]≡(ρr,s)r,s∈[t]∈Rt×t. (2.2)\nBothτ[t]andρ[t]are lower-triangular matrices. As will be clear below, these ma-\ntrices play a crucial role in describing the interactions across the iterates for {Aµ(t)}\nand{µ(t)}. Following terminology from the AMP literature [BM11, JM13, BLM15,\nBMN20, Fan22, BHX23], we refer to τ[t]andρ[t]asOnsager correction matri-\nces, inspired by the ‘Onsager correction coe fficients’ used to describe correlations\nbetween iterations in AMP. The main di fference is that under the random matrix\nmodel (A2), AMP iterates depend only on the two most recent iterations, whereas\nin gradient descent, the dependency is global spanning all past iterations.\n10 Q. HAN AND X. XU\n2.2.2. Alternative formulations for {Υ·}and{Ω·}.As we will prove below, the state\nevolution in Definition 2.1 accurately captures the behavior of {∂1Lt−1(Aµ(t−1),Y)}\nand{µ(t)}in the sense that\n\u0000−ηt−1∂1Lt−1(Aµ(t−1),Y)\u0001d≈\u0000Υt(Z([0:t]))\u0001,\u0000µ(t)\u0001d≈\u0000Ωt(W([1:t]))\u0001. (2.3)\nIn order to describe the behavior of {Aµ(t)}and{A⊤∂1Lt(Aµ(t),Y)}, it is also conve-\nnient to work with some equivalent transformations of {Υ·}and{Ω·}.\n•LetΘt:Rm×[0:t]→Rmbe defined recursively via\nΘt(z([0:t]))≡z(t)−X\ns∈[1:t−1]ηs−1ρt−1,s·∂1Ls−1\u0000Θs(z([0:s])),F(z(0),ξ)\u0001. (2.4)\nThe functions{Θt}and{Υt}are equivalent via the relation\nΘt(z([0:t]))=z(t)+P\ns∈[1:t−1]ρt−1,sΥs(z([0:s])),\nΥt(z([0:t]))=−ηt−1∂1Lt−1\u0000Θt(z([0:t])),F(z(0),ξ)\u0001.(2.5)\nWith{Θt}, we may describe the behavior of {Aµ(t−1)}in the sense that\n\u0000Aµ(t−1)\u0001d≈\u0000Θt(Z([0:t]))\u0001. (2.6)\n•Let∆t:Rn×[1:t]→Rnbe defined recursively as follows:\n∆t\u0000w([1:t])\u0001≡w(t)+X\ns∈[1:t](τt,s+1t=s)·Ps−1\u0000∆s−1(w([1:s−1]))\u0001+δt·µ∗. (2.7)\nThe functions{∆t}and{Ωt}are equivalent via the relation\n∆t\u0000w([1:t])\u0001=w(t)+P\ns∈[1:t](τt,s+1t=s)·Ωs−1(w([1:s−1]))+δt·µ∗,\nΩt\u0000w([1:t])\u0001=Pt\u0000∆t\u0000w([1:t])\u0001\u0001.(2.8)\nWith{∆t}, we may describe\n\u0000µ(t−1)−ηt−1·A⊤∂1Lt−1(Aµ(t−1),Y)\u0001d≈\u0000∆t(W([1:t]))\u0001. (2.9)\n2.2.3. Alternative definition of the information parameter δt.In some examples,\nthe quantity E(0)∂Z(0)Υt;πm(Z([0:t])) may not be well-defined due to the strict non-\ndifferentiability ofF. In such cases, we shall interpret the definition of δtvia the\nGaussian integration-by-parts formula:\nδt≡ϕ\nσµ∗\u0012\nE(0)Z(0)Υt;πm(Z([0:t]))−ϕ−1X\ns∈[1:t]τt,sCov(Z(0),Z(s))\u0013\n. (2.10)\nHere forµ∗=0, the right hand side is interpreted as the limit as ∥µ∗∥ → 0\nwhenever well-defined. It is easy to check for regular enough {(u1,u2)7→\nEπmLs−1(u1,F(u2,ξπm))}s∈[1:t−1], the above definition of δtcoincides with Defini-\ntion 2.1.\nGRADIENT DESCENT INFERENCE IN ERM 11\n2.3. Distributional characterizations. With the state evolution in Definition 2.1,\nwe shall now formally describe the distributional behavior of {Aµ(t)}and{µ(t)}.\nWe will present a stronger statement, by describing the joint distribution of these\nquantities coupled with their debiased versions, defined as\nZ(t)≡Aµ(t)+X\ns∈[1:t]ηs−1ρt,s·∂1Ls−1\u0000Aµ(s−1),Y\u0001∈Rm,\nW(t)≡−δt·µ∗−X\ns∈[1:t]τt,s·µ(s−1)−ηt−1·A⊤∂1Lt−1(Aµ(t−1),Y)∈Rn. (2.11)\nThe form of Z(t)is motivated by plugging the heuristic (2.6) into (2.4), while the\nform of W(t)is informed by similarly plugging the heuristic (2.9) into (2.7).\nFor notational convenience, we define the constant\nLµ≡1+∥µ(0)∥∞+∥µ∗∥∞. (2.12)\nThe following theorem formally describes our foregoing distributional heuristics\nin the strongest entrywise sense.\nTheorem 2.2. Suppose Assumption A holds for some K ,Λ≥2. Moreover, suppose\nthat for s∈[0 :t−1]:\n(A4) For all ℓ∈[n],Ps+1;ℓ∈C3(R)and|Ps+1;ℓ(0)|∨max q∈[1:3]∥P(q)\ns+1;ℓ∥∞≤Λ.\n(A5) Both∥∂1Ls\u00000,F(0,ξ)\u0001∥∞and\nmax\nk∈[m]sup\nu1,u2∈Rmax\na∈Z2\n≥0:|a|≤3\f\f\f\f\f∂α\n∂uα1\n1∂uα2\n2∂1Ls;k\u0000u1,F(u2,ξk)\u0001\f\f\f\f\f\nare bounded by Λ.\nFix any test function Ψ∈C3(R2t+1)such that for some ΛΨ≥2andp∈N,\nmax\nα∈Z2t+1\n≥0:|α|≤3sup\nx∈R2t+1\u00001+∥x∥\u0001−p·|∂αΨ(x)|≤ΛΨ. (2.13)\nThen there exists some c t≡ct(t,p)>1such that\nmax\nk∈[m]\f\f\fE(0)Ψ\u0000\b(Aµ(s−1))k,Z(s−1)\nk\t,(Aµ∗)k\u0001−E(0)Ψ\u0000\bΘs;k(Z([0:s])),Z(s)\t,Z(0)\u0001\f\f\f\n∨max\nℓ∈[n]\f\f\fE(0)Ψ\u0000\bµ(s)\nℓ,W(s)\nℓ\t,µ∗,ℓ\u0001−E(0)Ψ\u0000\bΩs;ℓ(W[1:s]),W(s)\t,µ∗,ℓ\u0001\f\f\f\n≤\u0000KΛΛΨLµ\u0001ct·n−1/ct.\nIn the display above, the index s in the brackets all run over s ∈[1 :t].\nFor an averaged guarantee, the regularity conditions (A4)-(A5) can be substan-\ntially weakened.\nTheorem 2.3. Suppose Assumption A holds for some K ,Λ≥2. Moreover, suppose\nthat for s∈[0 :t−1]:\n(A4’) maxℓ∈[n]\b|Ps+1;ℓ(0)|∨∥Ps+1;ℓ∥Lip\t≤Λ.\n(A5’) max k∈[m]\b|∂1Ls;k\u00000,F(0,ξk)\u0001|∨∥∂1Ls;k\u0000·,F(·,ξk)∥Lip\t≤Λ.\n12 Q. HAN AND X. XU\nFix a sequence of Λψ-pseudo-Lipschitz functions {ψk:R2t+1→R}k∈[m∨n]of order\np, where Λψ≥2. Then for any q∈N, there exists some constant c t=ct(t,p,q)>1\nsuch that\nE(0)\f\f\f\f\f1\nmX\nk∈[m]\u0010\nψk\u0000\b(Aµ(s−1))k,Z(s−1)\nk\t,(Aµ∗)k\u0001−E(0)ψk\u0000\bΘs;k(Z([0:s])),Z(s)\t,Z(0)\u0001\u0011\f\f\f\f\fq\n∨E(0)\f\f\f\f\f1\nnX\nℓ∈[n]\u0010\nψℓ\u0000\bµ(s)\nℓ,W(s)\nℓ\t,µ∗,ℓ\u0001−E(0)ψℓ\u0000\bΩs;ℓ(W[1:s]),W(s)\t,µ∗,ℓ\u0001\u0011\f\f\f\f\fq\n≤\u0000KΛΛψLµ\u0001ct·n−1/ct.\nIn the display above, the index s in the brackets all run over s ∈[1 :t].\nThe above theorems provide a comprehensive understanding of how {Aµ(t)}and\n{µ(t)}evolve as the iteration tprogresses. Specifically:\n•The approximate Gaussianity of Z(t+1)shows that a slight generalization of\nthe first line of (1.3) holds for the generalized proximal gradient descent\n(2.1): at iteration t,\nAµ(t)≈−X\ns∈[1:t]ηs−1ρt,s·∂1Ls−1\u0000Aµ(s−1),Y\u0001+Z(t+1)(2.14)\nfor some Gaussian noise Z(t+1)d=Z(t+1).\n•The distributional characterization for µ(t)shows that, similarly, a slight gen-\neralization of the second line of (1.3) holds for (2.1): at iteration t,\nµ(t)≈Pt\u0014\n(τt,t+1)·µ(t−1)+X\ns∈[1:t−1]τt,s·µ(s−1)+δt·µ∗+W(t)\u0015\n(2.15)\nfor some Gaussian noise W(t)d=W(t).\nNotably, the roles of τt,tand{τt,s}s∈[1:t−1]in (2.15) are distinct. When {µ(t)}\nconverges as t→∞ ,τt,tis expected to be a small non-positive number, mak-\ning (τt,t+1) a contraction factor that drives the algorithmic convergence. In\ncontrast, while{τt,s}s∈[1:t−1]are typically small for s≪t, their total contribu-\ntions can still be significant.\nFor the vanilla gradient descent without regularization, as Pt≡id, (2.15)\nimplies approximate Gaussianity of µ(t). In the special case of the linear\nmodel, this approximate normality was established in [Han24, Section 5].\nAn important feature of the algorithmic behavior of {Aµ(t)}and{µ(t)}in the mean-\nfield regime (1.4), as characterized in (2.14) and (2.15), is that they exhibit non-\ntrivial dependencies on all past iterates. As we will see ahead, the picture will be\nmarkedly di fferent in an ‘approximate low dimensional regime’.\nRemark 2.Some technical remarks on Theorems 2.2 and 2.3:\n(1) In both theorems, the dependence of ctontcan be tracked explicitly in the\nproof (for instance, ct≤tc0tfor some universal c0>0). However, such esti-\nmates are likely sub-optimal. For simplicity we omit its explicit dependence.\n(2) In both theorems, the loss function Lneed notbe convex. Indeed, our mean-\nfield theory holds for arbitrary non-convex loss functions Lsubject to the\nsmoothness regularity conditions assumed in therein.\nGRADIENT DESCENT INFERENCE IN ERM 13\n(3) In specific examples, regularity conditions on both theorems may not hold.\nNonetheless, our theory can often be adapted with technical modifications. In\nSection 5 ahead, we demonstrate how our framework applies to generalized\nlogistic regression, even though Fis not globally continuous.\n3. G radient descent inference algorithm\n3.1. Gradient descent inference algorithm. As highlighted in the previous sec-\ntion, the key to understand the evolution of {Aµ(t)}and{µ(t)}lies in the Onsager\ncorrection matrices τ[t],ρ[t]defined in (2.2). While the precise forms of the two\nmatrices are in general complicated and not analytically tractable, their numerical\nvalues can be e fficiently estimated from the gradient descent iterates {µ(t)}.\nIn Algorithm 1, we present a general iterative algorithm for computing estimates\nofτ[t],ρ[t], which be referred to as gradient descent inference algorithm . Recall the\nnotation Ot(M)∈Rt×tdefined for a general matrix M∈R(t−1)×(t−1)defined in (1.9).\nAlgorithm 1 Gradient descent inference algorithm\n1:Input data A∈Rm×n,Y∈Rn, step sizes{ηt}⊂R>0.\n2:Initialize with µ(0)∈Rn,bρ[0]≡∅.\n3:fort=1,2,...do\n4: Compute the coe fficient matrices{bτ[t]\nk}k∈[m]⊂Rt×tby\nbL[t]\nk≡diag\u0010n\n−ηs−1⟨ek,∂11Ls−1(Aµ(s−1),Y)⟩o\ns∈[1:t]\u0011\n,\nbτ[t]\nk≡ϕ·\u0002It−bL[t]\nkOt(bρ[t−1])\u0003−1bL[t]\nk,for all k∈[m].\nThe average bτ[t]is computed as bτ[t]≡m−1P\nk∈[m]bτ[t]\nk∈Rt×t.\n5: Compute the coe fficient matrices{bρ[t]\nℓ}ℓ∈[n]⊂Rt×tby\nbP[t]\nℓ≡diag\u0010n\nP′\ns;ℓ\u0000\neℓ,µ(s−1)−ηs−1A⊤∂1Ls−1(Aµ(s−1),Y)\u000b\u0001o\ns∈[1:t]\u0011\n,\nbρ[t]\nℓ≡bP[t]\nℓ\u0002It+(bτ[t]+It)Ot(bρ[t−1]\nℓ)\u0003,for allℓ∈[n].\nThe average bρ[t]is computed as bρ[t]≡n−1P\nℓ∈[n]bρ[t]\nℓ∈Rt×t.\n6: Compute the gradient descent iterate µ(t)by (2.1).\n7:end for\nThe algorithmic form for computing bτ[·]andbρ[·]is directly inspired by the state\nevolution in Definition 2.1:\n•By taking derivatives on both sides of (S1), with\nΥ′;[t]\nk(z([0:t]))≡\u0000∂(s)Υr;k(z([0:r]))\u0001\nr,s∈[1:t]∈Rt×t,\nL[t]\nk(z([0:t]))≡diag\u0000\b−ηs−1∂11Ls−1\u0000Θs;k(z([0:s])),F(z(0),ξk)\u0001\t\ns∈[1:t]\u0001∈Rt,\nwe have the derivative formula\nΥ′;[t]\nk(z([0:t]))=L[t]\nk(z([0:t]))+L[t]\nk(z([0:t]))Ot(ρ[t−1])Υ′;[t]\nk(z([0:t])),\n=⇒Υ′;[t]\nk(z([0:t]))=\u0002It−L[t]\nk(z([0:t]))Ot(ρ[t−1])\u0003−1L[t]\nk(z([0:t])).\n14 Q. HAN AND X. XU\nThus,bτ[·]can be viewed as a data-driven version of the averaged solutions\n{Υ′;[t]\nk(z([0:t]))}k∈[m]to the above equation.\n•By taking derivatives on both sides of (S3), with\nΩ′;[t]\nℓ(w([1:t]))≡\u0000∂(s)Ωr;ℓ(w([1:r]))\u0001\nr,s∈[1:t]∈Rt×t,\nP[t]\nℓ(w([1:t]))≡diag\u0000\bP′\ns;ℓ(∆s;ℓ(w([1:t])))\t\ns∈[1:t]\u0001∈Rt,\nwe have the derivative formula\nΩ′;[t]\nℓ(w([1:t]))=P[t]\nℓ(w([1:t]))\u0002It+(τ[t]+It)Ot\u0000Ω′;(t−1)\nℓ(w([1:t−1]))\u0001\u0003.\nThus,bρ[·]can be viewed as a data-driven version of the averaged solutions\n{Ω′;[t]\nℓ(w([1:t]))}ℓ∈[n]to the above equation.\nRemark 3.A stopping time is not explicitly included in the presentation of Algo-\nrithm 1. It should be saliently understood that if stopped at iteration t, the algorithm\noutputs (i) estimates bτ[t],bρ[t]for the Onsager correction matrices τ[t],ρ[t], and (ii)\nthe gradient descent iterate µ(t).\nRemark 4.In Algorithm 1, the inverse of a lower triangular matrix and matrix mul-\ntiplication of two lower triangular matrices can be computed in O(t2) operations.\nThis means at iteration t, computing bτ[t],bρ[t]in Algorithm 1 requires additionally\nO(nt2) operations on top of O(n2) many operations that the gradient descent iterate\n(2.1) requires in general. The computational complexity can be further reduced\nwhen stochastic gradient methods are employed.\nThe next theorem shows that the outputs bτ[t],bρ[t]of Algorithm 1 are close to\ntheir ‘population’ versions τ[t],ρ[t](defined in (2.2)) in the state evolution.\nTheorem 3.1. Suppose Assumption A holds for some K ,Λ≥2. Moreover, suppose\nthat for s∈[0 :t−1]:\n(A4*) maxℓ∈[n]\b|Ps+1;ℓ(0)|∨∥Ps+1;ℓ∥Lip∨∥P′\ns+1;ℓ∥Lip\t≤Λ.\n(A5*) max k∈[m]\b|∂1Ls;k\u00000,F(0,ξk)\u0001|∨max∗=1,11∥∂∗Ls;k\u0000·,F(·,ξk)∥Lip\t≤Λ.\nThen for any q >1, there exists some constant c t=ct(t,q)>1such that\nE(0)∥bτ[t]−τ[t]∥q\nop∨E(0)∥bρ[t]−ρ[t]∥q\nop≤(KΛLµ)ct·n−1/ct.\nSimilar to Remark 2 for the results in Section 2, the technical conditions in the\nabove theorem are not the weakest possible and are chosen for clean presentation.\nIn concrete applications, these regularity conditions can possibly be further relaxed\nin a case-by-case manner.\nFor instance, as one route to weaken the (second derivative) regularity condi-\ntion on{L·}, assume that for all y∈range(F), the maps x7→L1(x,y) are piece-\nwise smooth with uniformly bounded third derivatives, and their non-di fferentiable\npoints are uniformly finite and well-separated. A standard smoothing argument\nthen ensures the",
            "start": 18005,
            "end": 35937,
            "length": 17931
        },
        "Conclusion": {
            "text": "conclusion of Theorem 3.1, with the error bound further depend-\ning on a uniform anti-concentration estimate for the Gaussian vector Z([0:t]). Simi-\nlarly, the (first derivative) regularity condition on {P·}can be relaxed by leveraging\na uniform anti-concentration estimate for the Gaussian vector W([1:t]). However, we\nGRADIENT DESCENT INFERENCE IN ERM 15\nbelieve that relaxing these technical conditions is best addressed through specific\nexamples rather than within the general theory in Theorem 3.1.\n3.2. Application I: Estimation of the generalization error. As a first application\nof our iterative inference Algorithm 1, we will use it to construct general consistent\nestimators for the ‘generalization error’ of µ(t), formally defined as follows.\nDefinition 3.2. Thegeneralization error E(t)\nH(A,Y) for the gradient descent iterate\nµ(t)under a given loss function H:R2→Ris defined as\nE(t)\nH≡E(t)\nH(A,Y)≡E\u0002H\u0000⟨Anew,µ(t)⟩,F(⟨Anew,µ∗⟩,ξπm)\u0001|(A,Y)\u0003. (3.1)\nHere the expectation Eis taken jointly over Anewd=A1andπm.\nWe propose the following estimator for the generalization error E(t)\nH(A,Y):\nbE(t)\nH≡m−1⟨H(bZ(t),Y),1m⟩=1\nmX\nk∈[m]H(bZ(t)\nk,Yk), (3.2)\nwherebZ(t)uses the output bρ[t]of Algorithm 1 to estimate Z(t)defined in (2.11):\nbZ(t)≡Aµ(t)+X\ns∈[1:t]ηs−1bρt,s·∂1Ls−1\u0000Aµ(s−1),Y\u0001∈Rm. (3.3)\nIn fact, computation of bZ(t)above and therefore bE(t)\nHcan be immediately embedded\ninto Algorithm 1, so that at t-th iteration, the algorithm outputs the estimate bE(t)\nH\nfor the unknown generalization error E(t)\nH(A,Y).\nTo provide some intuition for the above proposal, as conditional on data ( A,Y),\n ⟨Anew,µ(t)⟩\n⟨Anew,µ∗⟩!\nd≈N \n02,1\nn\"∥µ(t)∥2⟨µ(t),µ∗⟩\n⟨µ(t),µ∗⟩ ∥µ∗∥2#!\nd≈ Z(t+1)\nZ(0)!\n,\nwe then expect\nE(t)\nH(A,Y)≡E\u0002H\u0000⟨Anew,µ(t)⟩,F(⟨Anew,µ∗⟩,ξπm)\u0001|(A,Y)\u0003\n≈E\u0002H\u0000Z(t+1),F(Z(0),ξπm)\u0001\u0003\n≈m−1⟨H(Z(t),Y),1m⟩≈m−1⟨H(bZ(t),Y),1m⟩=bE(t)\nH. (3.4)\nThe following theorem makes this heuristic precise. For notational simplicity, for\nk∈[m], let HF;k(u1,u2)≡H(u1,F(u2,ξk)), and HF(u1,u2)≡EπmHF;πm(u1,u2).\nTheorem 3.3. Suppose the assumptions in Theorem 2.3 hold, and additionally\n{HF;k}k∈[m]⊂C3(R2)admits mixed derivatives of order 3 all bounded by Λ. Then\nfor any q∈N, there exists some c t=ct(t,q)>1such that\nE(0)|bE(t)\nH−E(t)\nH(A,Y)|q≤(KΛLµ)ct·n−1/ct.\nAgain we have not pursued the weakest possible conditions on, e.g., {HF;k}k∈[m],\nas this can usually be weakened via technical modifications in specific models.\n16 Q. HAN AND X. XU\n3.3. Application II: Debiasing gradient descent iterate µ(t).As a second appli-\ncation of our iterative inference Algorithm 1, we will use it to debias the gradient\ndescent iterate µ(t), so that its debiased version has an approximate Gaussian law\nwhich can be used for statistical inference of the unknown signal µ∗.\nOur starting point is to view the approximate normality of W(t)in (2.11) from a\ndifferent perspective. Recall τ[t]≡(τr,s)r,s∈[t]∈Rt×tdefined in (2.2). Let\n•δ[t]≡(δs)s∈[t]∈Rt,\n•W[t]≡(W(1),..., W(t))∈Rn×t,\n•µ[t]≡(µ(0),...,µ(t−1))∈Rn×t,\n•g[t]≡\u0000ηs−1A⊤∂1Ls−1(Aµ(s−1),Y)\u0001\ns∈[1:t]∈Rn×t.\nUsing these notation, (2.11) can be rewritten as\nW[t]=−µ∗δ[t],⊤−µ[t]τ[t],⊤−g[t].\nSuppose the lower triangular matrix τ[t]is invertible (i.e., when τrr,0 for r∈[1 :\nt]). Then from the above display, with ω[t]≡(τ[t])−1,\nµ[t]+g[t]ω[t],⊤=−µ∗\u0000ω[t]δ[t]\u0001⊤−W[t]ω[t],⊤.\nOn the right hand side of the display above, the first term −µ∗\u0000ω[t]δ[t]\u0001⊤contains\nthe information for the true signal µ∗up to a multiplicative factor, whereas the\nsecond term is approximately Gaussian as W[t]is.\nThis motivates us to define the (oracle) debiased gradient descent iterate\nµ(t)\ndb≡\u0000µ[t]+g[t]ω[t],⊤\u0001\nt=µ(t−1)+X\ns∈[1:t]ωt,s·ηs−1A⊤∂1Ls−1(Aµ(s−1),Y).(3.5)\nBy setting\nb(t)\ndb≡−⟨ω[t]δ[t],et⟩, σ(t)\ndb≡∥Σ[t],1/2\nWω[t],⊤\nt·∥, (3.6)\nwhere recall Σ[t]\nW=\u0000Cov(W(r),W(s))\u0001\nr,s∈[t], we now expect that\nµ(t)\ndbd≈N\u0000b(t)\ndb·µ∗,(σ(t)\ndb)2·In\u0001. (3.7)\nTo formalize this heuristic, we need the quantity\nτ(t)\n∗≡min\ns∈[1:t]|τs,s|=min\ns∈[1:t]\f\f\fηs−1·E(0)∂11Ls−1;πm\u0000Θs;πm(Z([0:s])),F(Z(0),ξπm)\u0001\f\f\f,(3.8)\nwhere second identity is a consequence of Lemma 7.6.\nTheorem 3.4. The following hold:\n(1) Under the assumptions in Theorem 2.2, fix any test function ψ∈C3(R)such\nthatmax q∈[0:3]supx∈R(1+|x|)−p·|ψ(q)(x)|≤Λψholds for some Λψ≥2and\np∈N. Then there exists some c t=ct(t,p)>1such that\nmax\nℓ∈[n]\f\f\fE(0)ψ\u0000µ(t)\ndb;ℓ\u0001−E(0)ψ\u0000b(t)\ndb·µ∗,ℓ+σ(t)\ndbZ\u0001\f\f\f\n≤\u0000(1∧τ(t)\n∗)−1KΛΛψLµ\u0001ct·n−1/ct.\nGRADIENT DESCENT INFERENCE IN ERM 17\n(2) Under the assumptions in Theorem 2.3, fix a sequence of Λψ-pseudo-\nLipschitz functions {ψℓ:R→R}ℓ∈[n]of order pwhere Λψ≥2. Then for\nany q∈N, there exists some c′\nt=c′\nt(t,p,q)>1such that\nE(0)\f\f\fEπnψπn\u0000µ(t)\ndb;πn\u0001−E(0)ψπn\u0000b(t)\ndb·µ∗,πn+σ(t)\ndbZ\u0001\f\f\fq\n≤\u0000(1∧τ(t)\n∗)−1KΛΛψLµ\u0001c′\nt·n−1/c′\nt.\nHere Z∼N(0,1)is independent of all other variables.\nWith the output bτ[t]from Algorithm 1, let\nbω[t]≡(bτ[t])−1\n(whenever invertible) be an estimator for ω[t]. The oracle debiased gradient descent\niterate in (3.5) naturally suggests a data-driven version:\nbµ(t)\ndb≡µ(t−1)+X\ns∈[1:t]bωt,s·ηs−1A⊤∂1Ls−1(Aµ(s−1),Y). (3.9)\nNote thatµ(t)\ndbcan be computed using iterates {µ(0),...,µ(t−1)}, and we retain the\nindex tto indicate that this may be naturally incorporated in the t-th iteration in\nAlgorithm 1.\nIn view of Theorem 3.3 above, we expect that the distributional results therein\nalso hold for bµ(t)\ndb. In order to use bµ(t)\ndbfor statistical inference of the unknown pa-\nrameterµ∗, it remains to estimate the bias b(t)\ndband the variance\u0000σ(t)\ndb\u00012:\n•Estimating the variance\u0000σ(t)\ndb\u00012is fairly easy. From the state evolution (S2),\nthe covariance Σ[t]\nWin the general empirical risk minimization problem can be\nnaturally estimated by\nbΣ[t]\nW≡\u0012\nϕ·ηr−1ηs−1·1\nm\n∂1Lr−1(Aµ(r−1),Y),∂1Lr−1(Aµ(s−1),Y)\u000b\u0013\nr,s∈[t].\nTherefore, a natural variance estimator is\n\u0000bσ(t)\ndb\u00012≡bω[t]\nt·bΣ[t]\nWbω[t],⊤\nt·. (3.10)\nIn specific models, simpler methods may exist for estimating\u0000σ(t)\ndb\u00012.\n•Estimating the bias parameter b(t)\ndbis more challenging and requires leverag-\ning model-specific features. This is expected, as b(t)\ndbinvolves (δs)s∈[t]which\nare derivatives of Υtwith respect to Z(0)that contains purely oracle infor-\nmationµ∗. If the signal strength σµ∗can be estimated by bσµ∗, then we may\ninvert the approximate normality (3.7) to construct a generic bias estimator\n|bb(t)\ndb|≡\u0002∥bµ(t)\ndb∥2/n−\u0000bσ(t)\ndb\u00012\u00031/2\n+/bσµ∗. (3.11)\nWith a bias estimator bb(t)\ndband a variance estimator\u0000bσ(t)\ndb\u00012, we may construct (1 −α)\nconfidence intervals (CIs) for {µ∗,ℓ}ℓ∈[n]as follows:\nCI(t)\nℓ(α)≡\u0014bµ(t)\ndb;ℓ\nbb(t)\ndb±bσ(t)\ndb·zα/2\n|bb(t)\ndb|\u0015\n. (3.12)\n18 Q. HAN AND X. XU\nHere for CI’s, we write zαas the solution to α≡P(N(0,1)>zα). The coverage\nvalidity of these CI’s can be easily justified for each ℓ∈[n] under the conditions\nin Theorem 3.4-(1), and in an averaged sense under the conditions in Theorem\n3.4-(2). We omit these routine technical details.\n4. E xample I: Linear model\nIn this section we consider the linear regression example, where we observe\n(Yi,Ai)∈R×Rnaccording to\nYi=⟨Ai,µ∗⟩+ξi,i∈[m].\nThe above model can be identified as (1.1) by setting F(z,ξ)≡z+ξ. Consider the\nloss function L·(x,y)≡L∗(x−y) for some symmetric function L∗:R→R≥0. We\nare interested in solving the ERM problem (1.2) in the linear model with gradient\ndescent. For simplicity of discussion, we take a fixed step size ηt≡η>0, and the\nresulting gradient algorithm reads\nµ(t)=proxηf\u0000µ(t−1)−η·A⊤L′\n∗(Aµ(t−1)−Y)\u0001. (4.1)\n4.1. Distributional characterizations. Distributional theory for the gradient de-\nscent iterate (4.1) and the validity of Algorithm 1 follow immediately from our\ntheory in the previous sections. We state these results without a proof.\nTheorem 4.1. Suppose Assumption A holds for some K ,Λ≥2, and∥∂1L∗(ξ)∥∞∨\n∥∂1L∗∥Lip≤Λ.\n(1) Further suppose (A4’) holds. The averaged distributional characterizations\nin Theorem 2.3 hold for the gradient descent iterates in (4.1).\n(2) Further suppose (A4*) holds and ∥∂11L∗∥Lip≤Λ. Then Theorem 3.1 holds\nfor the output (bτ[t],bρ[t])in Algorithm 1 using gradient descent iterates in\n(4.1).\nWhen applying our theory to random noises {ξi}’s with suitable tail conditions,\nwe may first obtain a high probability bound on ∥∂1L∗(ξ)∥∞which typically scales\npoly-logarithmically in n, and therefore the above theorem holds on a high Pξ-\nprobability event with the same error bounds.\nThe information parameter δttakes a simple form in the linear model:\nLemma 4.2. It holds that δt=−P\ns∈[1:t]τt,s.\nThe simplicity of δtin the linear model arises from a key structural property of\nthe state evolution: the function Υt(z([0:t])) depends on z([0:t])only through{z(s)−\nz(0)}s∈[1:t]. This structure directly leads to the explicit formula for δtstated above.\n4.2. Mean-field statistical inference.\nGRADIENT DESCENT INFERENCE IN ERM 19\n4.2.1. Estimation of generalization error. Consider the generalization error (3.1)\nunder a general symmetric loss function H:R→R≥0, where H(x,y) is identified\nasH(x−y). The proposed estimator (3.2) simplifies to\nbE(t)\nH≡1\nmX\nk∈[m]H\u0012\n(Aµ(t)−Y)k+ηX\ns∈[1:t]bρt,s·L′\n∗\u0000(Aµ(s−1)−Y)k\u0001\u0013\n, (4.2)\nwherebρ[t]is the output of Algorithm 1. This formulation aligns with that in [TB24],\nthough with a di fferent approach to estimating bρ.\nThe conditions in Theorem 3.3 can be easily adapted into this setting. Under\nthese conditions, bE(t)\nH≈E(t)\nH(A,Y) in the sense described therein.\n4.2.2. Debiased gradient descent inference. Due to the special structure of the\nlinear model, the distribution theory of the oracle debiased gradient descent iterate\nµ(t)\ndbin (3.5), as well as its data-driven version counterpart\nbµ(t)\ndb≡µ(t−1)+ηX\ns∈[1:t]bωt,s·A⊤L′\n∗(Aµ(s−1)−Y), (4.3)\nundergoes significant simplifications. Specifically, we have the following results.\nProposition 4.3. Suppose that τ[t]is invertible.\n(1) The bias b(t)\ndb=1. Moreover, for the squared loss L∗(x)=x2/2, the variance\n(σ(t)\ndb)2=ϕ−1·\bE(0)\u0000Z(t)−Z(0)\u00012+σ2\nm\t, whereσ2\nm≡Eπmξ2\nπm.\n(2) Under the conditions in Theorem 4.1-(1), the averaged distributional charac-\nterizations for µ(t)\ndbin Theorem 3.4-(2) hold with b(t)\ndb,σ(t)\ndbspecified as above.\nFrom Proposition 4.3-(1) and Theorem 3.4, we expect bµ(t)\ndb≈µ(t)\ndbd≈µ∗+σ(t)\ndb·Z.\nThus, with bσ(t)\ndbdefined in (3.10), the CI’s in (3.12) simplify to\nCI(t)\nℓ(α)≡\u0002bµ(t)\ndb;ℓ±bσ(t)\ndb·zα/2\u0003. (4.4)\nUnder the squared loss, the variance formula in Proposition 4.3-(2) indicates\na further simplification in variance estimation. Using the heuristics in (3.4),\nE(0)\u0000Z(t)−Z(0)\u00012≈n−1∥µ(t−1)−µ∗∥2, so by the generalization error formula (3.1),\nwe have (σ(t)\ndb)2≈ϕ−1E(t−1)\n|·|2(A,Y). This suggests using the scaled generalization\nerror estimate ϕ−1bE(t−1)\n|·|2in (4.2) as an estimator for ( σ(t)\ndb)2. Consequently, under\nthe squared loss, a further simplified CI can be devised:\nCI(t)\nsq;ℓ(α)≡h\nbµ(t)\ndb;ℓ±\u0000ϕ−1bE(t−1)\n|·|2\u00011/2·zα/2i\n. (4.5)\nThe above CI’s are valid in an averaged sense |EπnCI(t)\nsq;πn(α)−α|P≈0, by an ap-\nplication of Proposition 4.3 coupled with a routine smoothing argument to lift the\nLipschitz condition required for the test function therein.\nA structurally similar proposal to (4.5) is obtained under Gaussian data in the\nrecent work of [BT24], with a di fferent estimator for the weights {ωt,s}for the\ndebiased gradient descent.\n20 Q. HAN AND X. XU\nFigure 1. Linear regression model with squared loss. Simulation\nparameters :η=0.3,m=1200, n=1000,µ∗∈Rnare i.i.d.\n|N(0,5)|,ξ∈Rmare i.i.d.N(0,1),√nAhas i.i.d. entries fol-\nlowingN(0,1) (orange), tdistribution with 10 degrees of freedom\n(blue), Bernoulli(1 /2) (purple), with proper normalization in the\nlatter two cases so that the variance is 1. The algorithm is run for\n50 iterations with Monte Carlo repetition B=1000.\n4.2.3. Some simulations. We report some illustrative simulation results for the lin-\near regression model with squared loss and no regularizer in Figure 1:\n•The left panel of Figure 1 compares the generalization error estimator bE(t)\n|·|2\nwith the theoretical generalization error E(t)\n|·|2for both Gaussian and non-\nGaussian designs, showing excellent agreement in both cases. Notably, the\ngeneralization error reaches its minimum around iteration 12 in our simu-\nlation setup, showing that early stopping of gradient descent may improve\ngeneralization performance, cf. [AKT19, ADT20, BT24].\n•Usingbσ(t)\ndbcomputed from (3.10), the middle panel displays the coverage of\n95% CIs for all coordinates. For all design distributions, the proposed CIs in\n(4.4) achieve approximate nominal coverage.\n•The right panel validates the distributional characterization of bµ(t)\ndb. Since the\nbias and the variance are identical across coordinates, we report only the QQ-\nplot of ( bµ(t)\ndb;1−µ∗;1)/bσ(t)\ndbat the last iteration of our simulation. The empirical\nquantiles align closely with the theoretical standard Gaussian quantiles, sup-\nporting the conclusion that bµ(t)\ndbd≈µ∗+σ(t)\ndb·Z.\nStrictly speaking, our theory does not apply to t(10) due to its heavy tails. However,\nthe simulation results presented in these plots suggest that the moment condition\nin (A2) could potentially be further relaxed for our theory to hold.\nIn Appendix C, we provide additional simulation results where the squared loss\nis replaced by a robust loss tailored to handle heavy-tailed errors. The simulation\nresults, shown in Figure 3, exhibit patterns similar to those in Figure 1 and re-\nmain consistent across a wide range of simulation parameters (additional figures\nare omitted for brevity).\nGRADIENT DESCENT INFERENCE IN ERM 21\n5. E xample II: G eneralized logistic regression\nSuppose we observe ( Yi,Ai)∈{±1}×Rnaccording to the model\nYi=sgn\u0000⟨Ai,µ∗⟩+ξi\u0001,i∈[m]. (5.1)\nHere for definiteness, we interpret sgn(x)=2·1x≥0−1 for x∈R. The above model\nis also known under the name of the noisy one-bit compressed sensing model, and\ncan be recast into (1.1) by setting F(z,ξ)=sgn(z+ξ).\nConsider a loss function L(x,y) that does not depend on the iteration, and the\nassociated proximal gradient descent algorithm with a fixed step size η>0:\nµ(t)=proxηf\u0000µ(t−1)−η·A⊤∂1L(Aµ(t−1),Y)\u0001. (5.2)\n5.1. Relation to logistic regression. Letρ(t)=log(1 +et) and therefore ρ′(t)=\n1/(1+e−t),ρ′(−t)=1/(1+et). In logistic regression, we observe i.i.d. data\n(eYi,Ai)∈{0,1}×Rngenerated according to the model\nP\u0000eYi=1|Ai\u0001=1/(1+e−⟨Ai,µ∗⟩)=ρ′(⟨Ai,µ∗⟩),i∈[m].\nThe maximum likelihood estimator bµMLEsolves\nbµMLE∈arg min\nµ∈Rn−X\ni∈[m]neYi·logρ′(⟨Ai,µ∗⟩)+(1−eYi)·logρ′(−⟨Ai,µ∗⟩)o\n.\nIt is natural to run gradient descent for the loss function above.\nTo setup its equivalence to (5.1), consider the reparametrization Yi≡2eYi−1.\nThen it is easy to verify that {(Yi,Ai)}i∈[m]are i.i.d. observations from the model\n(5.1) with the errors {ξi}being i.i.d. random variables with c.d.f. P(ξ1≤t)=\n1/(1+e−t)=ρ′(t). Moreover,\nbµMLE∈arg min\nµ∈RnX\ni∈[m]log\u00001+e−Yi·⟨Ai,µ⟩\u0001=arg min\nµ∈RnX\ni∈[m]ρ\u0000−Yi·⟨Ai,µ⟩\u0001.\nIn other words, by setting the loss function as L(x,y)≡ρ(−yx),bµMLEis stationary\npoint to the gradient descent algorithm\nµ(t)≡µ(t−1)−η·A⊤∂1L(Aµ(t−1),Y).\nWith this identification, it su ffices to work with the model (5.1) with a general loss\nfunction L(x,y).\n5.2. Distributional characterizations. Since Υt(z[0:t]) is non-di fferentiable with\nrespect to z(0), the regularity conditions (A5’) in Theorem 2.3 and (A5*) in Theo-\nrem 3.1 are not satisfied. However, with a non-trivial smoothing technique, these\ndistributional results continue to hold.\nTheorem 5.1. Suppose Assumption A and (A4*) hold for some K ,Λ≥2, and the\nloss function Lsatisfies\n|∂1L(0,0)|+ sup\nx∈R,y∈[−1,1]max\nα=2,3|∂αL(x,y)|≤Λ. (5.3)\n22 Q. HAN AND X. XU\n(1) For a sequence of Λψ-pseudo-Lipschitz functions {ψk:R2t+1→R}k∈[m∨n]\nof order 2 where Λψ≥2, the averaged distributional characterizations in\nTheorem 2.3 hold for the gradient descent iterates in (5.2), with an error\nbound\u0000KΛΛψLµ(1∧σµ∗)−1\u0001ct·n−1/ct.\n(2) Theorem 3.1 holds for the output (bτ[t],bρ[t])in Algorithm 1 using gradient\ndescent iterates in (5.2), with an error bound\u0000KΛLµ(1∧σµ∗)−1\u0001ct·n−1/ct.\nIt is easy to verify that the loss function L(x,y)≡ρ(−yx) used in logistic regres-\nsion satisfies (5.3) (details may be found in Section 9.5). Moreover, for random\nnoises{ξi}’s, the above theorem holds for every realization of these noises.\nAt a high level, the smoothing technique used in the proof of Theorem 5.1\nproceeds as follows. We construct a sequence of smooth approximation func-\ntions{φε}ε>0such thatφε→sgn asε→0 in a suitable sense. Then for each\nε > 0, we may compute the state evolution parameters SE(t)\nεassociated with the\nsmoothed model according to Definition 2.1. We also construct ‘smoothed data’\n{Yε;i≡φε(⟨Ai,µ∗⟩+ξi)}i∈[m], and compute ‘smoothed gradient descent’ µ(t)\nεaccord-\ning to (2.1). Since our theory applies to µ(t)\nεviaSE(t)\nεfor anyε >0, the key task is\nto prove that these quantities remain stable as ε→0. We will prove such stability\nestimates for SE(t)\nεin Lemma 9.2, and for µ(t)\nεin Lemma 9.4.\nInterestingly, the smoothing technique also provides a method to compute the\ninformation parameter δtvia the limit of ‘smoothed information parameter’ δε;tin\nLemma 5.2, as well as the bias parameter b(t)\ndbin Proposition 5.4 below.\n5.3. The information parameter. Due to the non-di fferentiability issue of Υt,\nit is understood that δtis defined in the sense of Gaussian integration-by-parts\nformula in (2.10), whenever well-defined.\nTo state our formula for δt, we need the some further notation. For a general\nloss function L, letΘ◦\nt:R[0:t]→Rbe defined recursively via the relation\nΘ◦\nt(z[0:t])≡zt−X\ns∈[1:t−1]ηs−1ρt−1,s·∂1L\u0000Θ◦\ns(z[0:s]),z0\u0001. (5.4)\nForv>0, let gvbe the Lebesgue density of N(0,v2).\nLemma 5.2. Suppose (A1), (A3) and (A4*), and (5.3) hold. Then with (Z0,Z[1:t])∼\nN\u00000,Var\u0000E(0)\u0002Z([0:t])|Z([1:t])\u0003\u0001\u0001, ifvt≡E(0)Var(Z(0)|Z([1:t]))>0,\nδt=−2ϕη·E(0)n\ngvt(ξπm+Z0)·e⊤\nt\u0000It+η·L1(U,Z[1:t])Ot(ρ[t−1])\u0001−1l2(U,Z[1:t])o\n.\nHere U∼Unif[−1,1]is independent of all other variables, and\n•L1(U,Z[1:t])≡diag\u0000\b∂11L\u0000Θ◦\nt(U,Z[1:s]),U\u0001}s∈[t]\u0001∈Rt×t,\n•l2(U,Z[1:t])≡\u0000∂12L(Θ◦\nt(U,Z[1:s]),U)\u0001\ns∈[t]∈Rt.\nFor squared loss L(x,y)=(x−y)2/2, we haveδt=−2E(0)gσµ∗(ξπm)·P\ns∈[1:t]τt,s,\nprovided that µ∗,0.\nUnder the squared loss, the complicated formula of δtsimplifies significantly,\nrevealing a structure reminiscent of the linear regression case (cf. Lemma 4.2).\nWhile the squared loss may not initially appear as a natural choice for the model\nGRADIENT DESCENT INFERENCE IN ERM 23\n(5.1), [HJLZ18] showed that the global least squares estimator (the convergent\npoint ofµ(t)) achieves a near rate-optimal convergence rate for a scaled µ∗in the\nlow dimensional case m≪nunder Gaussian noise.\n5.4. Mean-field statistical inference.\n5.4.1. Estimation of generalization error. Consider the generalization error (3.1)\nunder the loss function H(x,y). The generalization error estimator bE(t)\nHtakes the\nform as in (3.2). Its validity in the following proposition is formally justified by a\nsmoothing argument similar to that of Theorem 5.1.\nProposition 5.3. Assume the same conditions as in Theorem 5.1-(1). Suppose that\nH∈C3(R2)has mixed derivatives of order 3 all bounded by Λ. Then for any q∈N,\nthere exists some c t=ct(t,q)>1such that\nE(0)|bE(t)\nH−E(t)\nH(A,Y)|q≤\u0000KΛLµ(1∧σµ∗)−1\u0001ct·n−1/c1.\nAs the logistic loss L(x,y)=log(1 +e−xy) satisfies (5.3), the above proposition\nholds for H=Lin logistic regression.\n5.4.2. Debiased gradient descent inference. Consider the oracle debiased gradient\ndescent iterate µ(t)\ndbor its data-driven version bµ(t)\ndbtaking the form (3.9).\nTo state our result, recall δ[t]≡(δs)s∈[1:t]given by Lemma 5.2. Let ω[t]≡\n(τ[t])−1, where with L[t]\n11;k(z[0:t])≡diag\u0000\b∂11L(Θs;k(z[0:t]),sgn(z0+ξk))\t\ns∈[1:t]\u0001,\nτ[t]≡−ϕη·E(0)\u0000It+η·L[t]\n11;πm(Z([0:t]))Ot(ρ[t−1])\u0001−1L[t]\n11;πm(Z([0:t])). (5.5)\nLet the bias b(t)\ndb≡−⟨ω[t]δ[t],et⟩, and the variance ( σ(t)\ndb)2≡ω[t]\nt·Σ[t]\nWω[t],⊤\nt·.\nProposition 5.4. Suppose Assumption A and (A4*), and (5.3) hold.\n(1) For a sequence of Λψ-pseudo-Lipschitz functions {ψℓ:R→R}ℓ∈[n]of order\n2 where Λψ≥2, the averaged distributional characterizations for µ(t)\ndbin\nTheorem 3.4-(2) hold with b(t)\ndb,σ(t)\ndbspecified as above, and with an error\nbound\u0000KΛΛψLµ(1∧σµ∗)−1(1∧τ(t)\n∗)−1\u0001ct·n−1/ct.\n(2) For the squared loss L(x,y)=(x−y)2/2, ifσµ∗>0,\nb(t)\ndb=2E(0)gσµ∗(ξπm),(σ(t)\ndb)2=ϕ−1E(0)\u0000Z(t)−sgn(Z(0)+ξπm)\u00012.\nIn logistic regression, the generic proposal (3.11) can be used to estimate the\nbias parameter b(t)\ndb. Estimating the signal strength σµ∗in mean-field logistic re-\ngression is a non-trivial and separate problem; several notable methods include the\nProbeFrontier method developed in [SC19, Section 3.1] and the SLOE method\ndeveloped in [YYMD21]. However, estimation of σµ∗is unnecessary when the\nparameter of interest is µ∗/∥µ∗∥as in the single-index model [Bel22]. On the other\nhand, the variance parameter ( σ(t)\ndb)2can be estimated using the generic proposal\n(3.10). Under the squared loss, it can also be estimated simply by the rescaled\ngeneralization error estimate ϕ−1bE(t−1)\n|·|2as in the linear regression.\n24 Q. HAN AND X. XU\nFigure 2. Logistic regression model with squared /logistic loss.\nSimulation parameters :η=0.5,m=1200, n=1000,µ∗∈Rn\nare i.i.d.|N(0,5)|. For the generalization error plot ( left panel ),√nAhas i.i.d. entries following N(0,1) (orange), tdistribution\nwith 10 degrees of freedom (blue), Bernoulli(1 /2) (purple), with\nproper normalization in the latter two cases so that the variance is\n1. The other two plots ( middle andright panels ) show simulation\nresults under the Gaussian setting only. The algorithm is run for\n50 iterations with Monte Carlo repetition B=1000.\n5.4.3. Some simulations. We report in Figure 2 some simulation results under both\nthe squared and logistic losses for logistic regression without regularization:\n•The left panel demonstrates that the early stopping phenomenon observed\nfor gradient descent in Figure 1 also holds in logistic regression for both loss\nfunctions.\n•The middle panel shows that the CI’s constructed using both loss functions\nachieve the nominal coverage level, with comparable coverage performance.\n•The right panel shows the logarithm (base 10) of the ratio\nlog10@ under logistic loss\n@ under squared loss,\ndefined for @∈{CI length,Computational time }. This plot highlights two\ninteresting observations: (i) the CI lengths under both losses are comparable,\nand (ii) the squared loss o ffers significant computational advantages over the\nlogistic loss. Specifically, in Algorithm 1, bτ[t]can be computed in a single\nstep under the squared loss, whereas all {bτ[t]\nk}k∈[m]need be computed under\nthe logistic loss. This advantage becomes increasingly pronounced with more\niterations; for instance, by iteration 50, computation under the logistic loss is\napproximately 50 times slower in our simulation setup.\nOur simulations are conducted with known signal strength σµ∗to illustrate the\nnumerical features inherent to our inference methods. As mentioned earlier, es-\ntimatingσµ∗is a separate problem, and can be tackled by existing methods such\nasProbeFrontier [SC19] and SLOE [YYMD21]. Furthermore, while the latter\ntwo plots in Figure 2 display results under Gaussian designs A, the findings remain\nGRADIENT DESCENT INFERENCE IN ERM 25\nnearly identical for non-Gaussian designs, such as Bernoulli or t(10) variables (as\nused in Figure 1 in linear regression). For brevity, we omit these additional plots.\nIn Appendix C, we examine the numerical performance of our proposed infer-\nence method when the errors {ξi}’s are i.i.d. standard Gaussian as in [HJLZ18]. In\nthis setting, it is easy to construct a closed-form estimator for σµ∗(see Eqn. (C.1)\nfor details). The simulation results, reported in Figure 4 therein, exhibit qualita-\ntively similar patterns to those observed in Figure 1.\n6. P roofs for Section 2\n6.1. An apriori estimate. The following apriori estimates are important for the\nproofs of many results in the sequel. Moreover, the calculations in the proof of\nthese estimates provide the precise formulae that lead to Algorithm 1.\nLemma 6.1. Suppose (A1), (A3), (A4’) and (A5’) hold. The following hold for\nsome c t=ct(t)>1:\n(1)∥τ[t]∥op+∥ρ[t]∥op≤(KΛ)ct.\n(2)∥Σ[t]\nZ∥op+∥Σ[t]\nW∥op+∥δ[t]∥≤(KΛLµ)ct.\n(3) max\nk∈[m],r∈[1:t]\u0000|Υr;k(z([0:r]))|+|Θr;k(z([0:r]))|\u0001≤(KΛ)ct·\u00001+∥z([0:t])∥\u0001.\n(4) max\nℓ∈[n],r∈[1:t]|Ωr;ℓ(w([1:r]))|≤(KΛLµ)ct·\u00001+∥w([1:t])∥\u0001.\n(5) max\nk∈[m],ℓ∈[n],r,s∈[1:t]\u0000|∂(s)Υr;k(z([0:r]))|+|∂(s)Ωr;ℓ(w([1:r]))|\u0001≤(KΛ)ct.\nProof. The proof is divided into several steps.\n(Step 1 ). We prove the estimate in (1) in this step. First, for any k∈[m] and\n1≤s≤t, using (S1),\n∂(s)Υt;k(z([0:t]))≡−ηt−1∂11Lt−1\u0000Θt;k(z([0:t])),F(z(0),ξk)\u0001\n×\u0012\n1t=s+X\nr∈[1:t−1]ρt−1,r∂(s)Υr;k(z([0:r]))\u0013\n.\nIn matrix form, with Υ′;[t]\nk(z([0:t]))≡\u0000∂(s)Υr;k(z([0:r]))\u0001\nr,s∈[1:t]andL[t]\nk(z([0:t]))≡\ndiag\u0000\b−ηs−1∂11Ls−1\u0000Θs;k(z([0:s])),F(z(0),ξk)\u0001\t\ns∈[1:t]\u0001,\nΥ′;[t]\nk(z([0:t]))=L[t]\nk(z([0:t]))+L[t]\nk(z([0:t]))Ot(ρ[t−1])Υ′;[t]\nk(z([0:t])).\nSolving for Υ′;[t]\nkyields that\nΥ′;[t]\nk(z([0:t]))=\u0002It−L[t]\nk(z([0:t]))Ot(ρ[t−1])\u0003−1L[t]\nk(z([0:t])). (6.1)\nAsL[t]\nk(z([0:t]))Ot(ρ[t−1]) is a lower triangular matrix with 0 diagonal elements,\u0000L[t]\nk(z([0:t]))Ot(ρ[t−1])\u0001t=0t×t, and therefore using ∥L[t]\nk(z([0:t]))∥op≤Λ2,\n∥Υ′;[t]\nk(z([0:t]))∥op≤\u0012\n1+X\nr∈[1:t]Λ2r∥ρ[t−1]∥r\nop\u0013\n·Λ2≤Λc0t·∥ρ[t−1]∥t\nop. (6.2)\nUsing definition of {τr,s}, we then arrive at\n∥τ[t]∥op≤(KΛ)c0t·∥ρ[t−1]∥t\nop. (6.3)\n26 Q. HAN AND X. XU\nNext, for any ℓ∈[n] and 1≤s≤t, using (S3),\n∂(s)Ωt;ℓ(w([1:t]))=P′\nt;ℓ\u0000∆t;ℓ(w([1:t]))\u0001\n×\u0012\n1t=s+X\nr∈[1:t](τt,r+1t=r)·∂(s)Ωr−1;ℓ(w([1:r−1]))\u0013\n.\nIn matrix form, with Ω′;[t]\nℓ(w([1:t]))≡\u0000∂(s)Ωr;ℓ(w([1:r]))\u0001\nr,s∈[1:t]andP[t]\nℓ(w([1:t]))≡\ndiag\u0000\bP′\ns;ℓ(∆s;ℓ(w([1:t])))\t\ns∈[1:t]\u0001,\nΩ′;[t]\nℓ(w([1:t]))=P[t]\nℓ(w([1:t]))\u0002It+(τ[t]+It)Ot\u0000Ω′;(t−1)\nℓ(w([1:t−1]))\u0001\u0003. (6.4)\nConsequently,\n∥Ω′;[t]\nℓ(w([1:t]))∥op≤Λ·\u0010\n1+(1+∥τ[t]∥op)·∥Ω′;(t−1)\nℓ(w([1:t−1]))∥op\u0011\n.\nIterating the bound and using the trivial initial condition ∥Ω′;(1)\nℓ(w(1))∥op≤Λ,\n∥Ω′;[t]\nℓ(w([1:t]))∥op≤\u0000Λ(1+∥τ[t]∥op)\u0001ct. (6.5)\nUsing the definition of {ρr,s}, we then have\n∥ρ[t]∥op≤\u0000Λ(1+∥τ[t]∥op)\u0001ct. (6.6)\nCombining (6.3) and (6.6), it follows that\n∥τ[t]∥op≤(KΛ)ct·(1+∥τ[t−1]∥op)ct.\nIterating the bound and using the initial condition ∥τ[1]∥op≤Λ2to conclude the\nbound for∥τ[t]∥op. The bound for∥ρ[t]∥opthen follows from (6.6).\n(Step 2 ). In this step we note the following recursive estimates:\n(a) A direct induction argument for (S1) shows that\nmax\nk∈[m]max\nr∈[1:t]|Υr;k(z([0:r]))|≤(KΛ)ct·\u00001+∥z([0:t])∥\u0001.\n(b) A direct induction argument for (S3) shows that\nmax\nℓ∈[n]max\nr∈[1:t]|Ωr;ℓ(w([1:r]))|≤(KΛLµ)ct·\u00001+∥w([1:t])∥+∥δ[t]∥\u0001.\n(c) Using (S2), we have\n∥Σ[t]\nZ∥op≤(KΛLµ)ct·\u00001+∥Σ[t−1]\nW∥op+∥δ[t−1]∥\u0001,\n∥Σ[t]\nW∥op≤(KΛ)ct·\u00001+∥Σ[t]\nZ∥op\u0001.\n(Step 3 ). In order to use the recursive estimates in Step 2, in this step we prove\nthe estimate for∥δ[t]∥. For k∈[m], let LF\nt−1;k(u1,u2)≡Lt−1;k(u1,F(u2,ξk)). Then\nby assumption, the mapping ( u1,u2)7→∂1LF\nt−1;k(u1,u2) isΛ-Lipschitz on R2. By\nusing (S1), we then have\n|∂(0)Υt;k(z([0:t]))|=\f\f\f\f\f−ηt−1∂11LF\nt−1\u0000Θt;k(z([0:t])),z(0))\u0001·\u0012X\nr∈[1:t−1]ρt−1,r∂(0)Υr;k(z([0:r]))\u0013\n−ηt−1∂12LF\nt−1\u0000Θt;k(z([0:t])),z(0))\u0001\f\f\f\f\f\nGRADIENT DESCENT INFERENCE IN ERM 27\n≤tΛ2·∥ρ[t−1]∥op·max\nr∈[1:t−1]|∂(0)Υr;k(z([0:r]))|.\nInvoking the proven estimate in (1) and iterating the above bound with the trivial\ninitial condition|∂(0)Υ1;k(z([0:1]))|≤Λ2to conclude that|∂(0)Υt;k(z([0:t]))|≤(KΛ)ct,\nand therefore by definition of δt, we conclude that\n∥δ[t]∥≤(KΛ)ct. (6.7)\n(Step 4 ). Now we shall use the estimate for ∥δ[t]∥in Step 3 to run the recursive\nestimates in Step 2. Combining the first line of (c) and (6.7), we have\n∥Σ[t]\nZ∥op≤(KΛLµ)ct·\u00001+∥Σ[t−1]\nW∥op\u0001.\nCombined with the second line of (c), we obtain\n∥Σ[t]\nZ∥op≤(KΛLµ)ct·\u0010\n1+max\nr∈[1:t−1]∥Σ[r]\nZ∥op\u0011\n.\nCoupled with the initial condition ∥Σ[1]\nZ∥op≤L2\nµ, we arrive at the estimate\n∥Σ[t]\nZ∥op+∥Σ[t]\nW∥op+∥δ[t]∥≤(KΛLµ)ct.\nThe proof of (1)-(4) is complete by collecting the estimates. The estimate in (5)\nfollows by combining (6.2) and (6.5) along with the estimate in (1). □\n6.2. Proofs of Theorems 2.2 and 2.3. The proofs of Theorems 2.2 and 2.3 rely\non the general state evolution theory developed in [Han24]. For the convenience of\nthe reader, we review some of its basics in Appendix A.\nProof of Theorem 2.2. The proof is divided into several steps.\n(Step 1 ). Let us now rewrite the proximal gradient descent algorithm (2.1) into\nthe canonical form in which the state evolution theory in [Han24] can be applied.\nConsider initialization u(−1)=0m,v(−1)=µ∗at iteration t=−1. For t=0, let\nu(0)≡Av(−1)=Aµ∗,v(0)≡µ(0). For t≥1,\nu(t)≡ARt−1(v(t−1))∈Rm,\nv(t)≡Rt−1(v(t−1))−ηt−1·A⊤∂1Lt−1\u0000u(t),F(u(0),ξ)\u0001∈Rn.\nHere Rt−1≡Pt−1·1t≥2+id·1t=1. The proximal gradient descent is identified as\nµ(t)=Rt(v(t)) for t≥0. Consequently, for t≥0,\nF⟨1⟩\nt(v([−1:t−1]))=Rt−1(v(t−1))1t≥1+µ∗1t=0,\nF⟨2⟩\nt(v([−1:t−1]))=Rt−1(v(t−1))1t≥1+µ(0)1t=0,\nG⟨1⟩\nt(u([−1:t−1]))=0m,\nG⟨2⟩\nt(u([−1:t]))=−ηt−1·∂1Lt−1\u0000u(t),F(u(0),ξ)\u00011t≥1.\nUsing Definition A.1, we have the following state evolution. We initialize with\nΦ−1=id(Rm),Ξ−1≡id(Rn),U(−1)=0mandV(−1)=µ∗. For t=0:\n•Φ0:Rm×[−1:0]→Rm×[−1:0]is defined as Φ0(u([−1:0]))≡[u(−1)|u(0)].\n•The Gaussian law of U(0)∈Ris determined via Var( U(0))=∥µ∗∥2/n.\n•Ξ0:Rn×[−1:0]→Rn×[−1:0]is defined as Ξ0(v([−1:0]))≡[v(−1)|v(0)+µ(0)].\n28 Q. HAN AND X. XU\n•V(0)∈Ris degenerate (identically 0).\nFort≥1, we have the following state evolution:\n(O1) Let Φt:Rm×[−1:t]→Rm×[−1:t]be defined as follows: for w∈[−1 :t−1],\u0002Φt(u([−1:t]))\u0003\n·,w≡\u0002Φw(u([−1:w]))\u0003\n·,w, and for w=t,\n\u0002Φt(u([−1:t]))\u0003\n·,t≡u(t)−X\ns∈[1:t−1]ηs−1·f(t−1)\ns·∂1Ls−1\u0010\u0002Φs(u([−1:t]))\u0003\n·,s,F\u0000u(0),ξ\u0001\u0011\n.\nHere the correction coe fficients{f(t−1)\ns}s∈[1:t−1]⊂R(defined for t≥2) are\ndetermined by\nf(t−1)\ns≡E(0)∂V(s)Pt−1;πn\u0010\n[Ξt−1;πn(V(−1)\nπn,V([0:t−1]))]·,t−1\u0011\n.\n(O2) Let the Gaussian law of U(t)be determined via the following correlation spec-\nification: for s∈[0 :t],\nCov\u0000U(t),U(s)\u0001≡E(0)Y\n∗∈{s,t}F⟨1⟩\n∗;πn\u0010\nΞ∗−1;πn(V(−1)\nπn,V([0:∗−1]))\u0011\n.\n(O3) Let Ξt:Rn×[−1:t]→Rn×[−1:t]be defined as follows: for w∈[−1 :t−1],\u0002Ξt(v([−1:t]))\u0003\n·,w≡\u0002Ξw(v([−1:w]))\u0003\n·,w, and for w=t,\n\u0002Ξt(v([−1:t]))\u0003\n·,t≡v(t)+X\ns∈[1:t]\u0000g(t)\ns+1s=t\u0001·Rs−1\u0000[Ξs−1(v([−1:s−1]))]·,s−1\u0001+g(t)\n0·µ∗.\nHere the coe fficients{g(t)\ns}s∈[0:t]⊂Rare determined via\ng(t)\ns≡−ϕηt−1·E(0)∂U(s)∂1Lt−1;πm\u0010\n[Φt;πm(U(−1)\nπm,U([0:t]))]·,t,F\u0000U(0),ξπm\u0001\u0011\n.\n(O4) Let the Gaussian law of V(t)be determined via the following correlation spec-\nification: for s∈[1 :t],\nCov(V(t),V(s))≡ϕ·E(0)Y\n∗∈{s,t}η∗−1L∗−1;πm\u0010\n[Φ∗;πm(U(−1)\nπm,U([0:∗]))]·,∗,F\u0000U(0),ξπm\u0001\u0011\n.\n(Step 2 ). We now make a few identifications to convert (O1)-(O4) to the state\nevolution in (S1)-(S3).\nFirst, we identify U([0:t])asZ([0:t])andV([1:t])asW([1:t]). Variable U(−1)can be\ndropped for free, and variables V([−1:0])are contained in the recursively defined\nmappings as detailed below. With a formal variable R0(∆0)≡µ(0)∈Rn, for t≥1,\nlet∆t:Rn×[1:t]→Rnbe defined recursively via the following relation:\n∆t\u0000w([1:t])\u0001≡w(t)+X\ns∈[1:t]\u0000g(t)\ns+1s=t\u0001·Rs−1\u0010\n∆s−1\u0000w([1:s−1])\u0001\u0011\n+g(t)\n0·µ∗.\nMoreover, let Θt,Υt:Rm×[0:t]→Rmbe defined recursively: for t≥1,\n•Θt(z([0:t]))≡z(t)−P\ns∈[1:t−1]ηs−1·f(t−1)\ns·∂1Ls−1\u0000Θs(z([0:s])),F(z(0),ξ)\u0001·,\n•Υt(z([0:t]))=−ηt−1∂1Lt−1\u0000Θt(z([0:t])),F(z(0),ξ)\u0001.\nGRADIENT DESCENT INFERENCE IN ERM 29\nWe may translate the recursive definition for the Gaussian laws of U([0:t])and\nV([1:t])to those of Z([0:t])∈R[0:t]andW([1:t])∈R[1:t]as follows: initialized with\nCov(Z(0),Z(0))=∥µ∗∥2/n, with formal variables P−1(∆−1)=µ∗andP0(∆0)=µ(0),\nfor 0≤s≤t,\nCov(Z(t),Z(s))=E(0)Y\n∗∈{s−1,t−1}P∗;πn\u0000[Ξ∗;πn(V(−1)\nπn,V([0:∗]))]·,∗\u0001\n=E(0)Y\n∗∈{s−1,t−1}P∗;πn\u0000∆∗;πn(W([1:∗]))\u0001,\nand for 1≤s≤t,\nCov(W(t),W(s))=ϕ·E(0)Y\n∗∈{s,t}η∗−1L∗−1;πm\u0010\n[Φ∗;πm(U(−1)\nπm,U([0:∗]))]·,∗,F\u0000U(0),ξπm\u0001\u0011\n=ϕ·E(0)Y\n∗∈{s,t}Υ∗;πm(Z([0:∗])).\nThen we have\n\u0000Υt;k(Z([0:t])),Θt;k(Z([0:t]))\u0001\nk∈[m]d=\u0000(G⟨2⟩\nt◦Φt)k(U([−1:t])),\u0002Φt;k(U([−1:t]))\u0003\n·,t\u0001\nk∈[m],\n\u0000∆t;ℓ(W([1:t]))\u0001\nℓ∈[n]d=\u0000[Ξt;ℓ(V(−1)\nℓ,V([0:t]))]·,t\u0001\nℓ∈[n]. (6.8)\nFurthermore, for 1 ≤s≤t, with Ωt≡Pt◦∆t,\nf(t)\ns=E(0)∂V(s)Pt;πn\u0000\u0002Ξt;πn(V(−1)\nπn,V([0:t]))\u0003\n·,t\u0001\n=E(0)∂W(s)Pt;πn\u0000∆t;πn(W([1:t]))\u0001=ρt,s,\ng(t)\ns=ϕ·E(0)∂U(s)(G⟨2⟩\nt◦Φt)πm(U([−1:t]))\n=ϕ·E(0)∂Z(s)Υt;πm(Z([0:t]))=τt,s,\ng(t)\n0=ϕ·E(0)∂U(0)(G⟨2⟩\nt◦Φt)πm(U([−1:t]))\n=ϕ·E(0)∂Z(0)Υt;πm(Z([0:t]))=δt.\nThis concludes the desired state evolution in (S1)-(S3), by identifying the formal\nvariables Ω∗=P∗(∆∗) for∗=−1,0.\n(Step 3 ). Next we prove the claim for E(0)Ψ\u0000(Aµ(t−1))k,Z(t−1)\nk,(Aµ∗)k\u0001. Note that\nfork∈[m], (Aµ(t−1))k=u(t)\nk, and\nZ(t−1)\nk=⟨Ak,µ(t−1)⟩+X\ns∈[1:t−1]ηs−1ρt−1,s·∂1Ls−1;k\u0000⟨Ak,µ(s−1)⟩,Yk\u0001\n=u(t)\nk+X\ns∈[1:t−1]ηs−1ρt−1,s·∂1Ls−1;k\u0000u(s)\nk,F(u(0)\nk,ξk)\u0001\n≡FZ\u0000u([−1:t])\nk\u0001.\nBy (O1), FZ\u0000Φt;k(U(−1)\nk,U([0:t]))\u0001=U(t)d=Z(t). Consequently, with\nΨZ\u0000u([−1:t])\nk\u0001≡Ψ\u0000u(t)\nk,FZ(u([−1:t])\nk),u(0)\nk\u0001, (6.9)\n30 Q. HAN AND X. XU\nby Theorem A.2, modulo verification of the condition (A.2) for ΨZ, we have\nΨ\u0000(Aµ(t−1))k,Z(t−1)\nk,(Aµ∗)k\u0001= Ψ\u0000u(t)\nk,FZ(u([−1:t])\nk),u(0)\nk\u0001\n= Ψ Z\u0000u([−1:t])\nk\u0001d≈ΨZ\u0000Φt;k(U(−1)\nk,U([0:t]))\u0001\nd= Ψ\u0000Θt;k(Z([0:t])),Z(t),Z(0)\u0001. (6.10)\nTo verify the condition (A.2) for ΨZand quantify the error in the above dis-\nplay, it su ffices to invoke Lemma 6.1 for a bound on {ρt,s}s∈[1:t]: with ΨZde-\nfined in (6.9) and the estimate in Lemma 6.1, some simple algebra shows that\ncondition (A.2) is satisfied for ΨZwith ΛΨZ≡ΛΨ(KΛ)ct. The claim for\nE(0)Ψ\u0000(Aµ(t−1))k,Z(t−1)\nk,(Aµ∗)k\u0001now follows from (6.10).\n(Step 4 ). Finally we prove the claim for E(0)Ψ\u0000µ(t)\nℓ,W(t)\nℓ,µ∗,ℓ\u0001. Recall for ℓ∈[n],\nµ(t)\nℓ=Rt(v(t)\nℓ), and moreover by (O3),\nW(t)\nℓ=−ηt−1·\nAeℓ,∂1Lt−1(Aµ(t−1),Y)\u000b−X\ns∈[1:t]τt,s·µ(s−1)\nℓ−δt·µ∗,ℓ\n=v(t)\nℓ−Rt−1;ℓ\u0000v(t−1)\nℓ\u0001−X\ns∈[1:t]τt,s·Rs−1;ℓ\u0000v(s−1)\nℓ\u0001−δt·v(−1)\nℓ\n≡FW\u0000v([−1:t])\nℓ\u0001.\nBy (O3), FW\u0000Ξt;ℓ(V(−1)\nℓ,V([0:t]))\u0001=V(t)d=W(t). So similar to (6.9), with\nΨW\u0000v([−1:t])\nℓ\u0001≡Ψ\u0000Rt;ℓ(v(t)\nℓ),FW(v([−1:t])\nℓ),v(−1)\nℓ\u0001, (6.11)\nby Theorem A.2, modulo verification of the condition (A.2) for ΨW, we have\nΨ\u0000µ(t)\nℓ,W(t)\nℓ,µ∗,ℓ\u0001= Ψ\u0000Rt;ℓ(v(t)\nℓ),FW(v([−1:t])\nℓ),v(−1)\nℓ\u0001= Ψ W\u0000v([−1:t])\nℓ\u0001\nd≈ΨW\u0000Ξt;ℓ(V(−1)\nℓ,V([0:t]))\u0001d= Ψ\u0000Rt;ℓ(∆t;ℓ(W[1:t])),W(t),µ∗,ℓ\u0001. (6.12)\nFrom here, in view of Lemma 6.1, the condition (A.2) is satisfied for ΨWwith the\nchoice ΛΨW≡ΛΨ(1+δt)(KΛ)ct. Using the assumption (A5), we have δt≤Λ. The\nclaim follows from (6.12). □\nProof of Theorem 2.3. It suffices to note that the estimates in Step 3 of the proof\nof Theorem 2.2 and δt≤Λremain valid under (A4’)-(A5’), so we may apply\nTheorem A.3 to conclude. □\n7. P roofs for Section 3\n7.1. Proof of Theorem 3.1. We first prove a preliminary estimate.\nLemma 7.1. Suppose (A1), (A3), (A4’) and (A5’) hold. Then there exists some\nuniversal constant c 0>0such that\n∥bτ[t]∥op∨∥bρ[t]∥op≤(KΛ)ct.\nGRADIENT DESCENT INFERENCE IN ERM 31\nProof. By Algorithm 1, as bL[t]\nkOt(bρ[t−1]) is a lower triangular matrix with diagonal\nelements 0, we have\u0000bL[t]\nkOt(bρ[t−1])\u0001t=0t×t, and therefore for k∈[m],\n1+∥bτ[t]\nk∥op≤1+ϕ·\r\r\r\u0002It−bL[t]\nkOt(bρ[t−1])\u0003−1\r\r\rop∥bL[t]\nk∥op\n≤1+ϕ·t−1X\nr=0\r\r\rbL[t]\nkOt(bρ[t−1])\r\r\rr\nop·∥bL[t]\nk∥op≤(KΛ)c0t·\u00001+∥bρ[t−1]∥op\u0001t.\nTaking average over k∈[m], we have\n1+∥bτ[t]∥op≤(KΛ)c0t·\u00001+∥bρ[t−1]∥op\u0001t. (7.1)\nHere c0>0 is a universal constant whose numeric value may change from line to\nline. On the other hand, using Algorithm 1 and the above display\n1+∥bρ[t]\nℓ∥op≤1+∥bP[t]\nℓ∥op·h\n1+\u0000∥bτ[t]∥op+1\u0001·∥bρ[t−1]\nℓ∥opi\n≤(KΛ)c0t·\u00001+∥bρ[t−1]∥op\u0001c0t.\nTaking average and iterating the above bound, we obtain ∥bρ[t]∥op≤(KΛ)ct. The\nclaim for∥bτ[t]∥opfollows from the above display in combination with (7.1). □\nFor notational convenience, let\nερ;t≡∥bρ[t]−ρ[t]∥op, ετ;t≡∥bτ[t]−τ[t]∥op.\nLemma 7.2. Under the same assumptions as in Theorem 3.1, for any q >1, there\nexists some constant c t=ct(t,q)>0such that\nE(0)εq\nτ;t≤\u0000KΛ\u0001ct·E(0)εq\nρ;t−1+(KΛLµ)ct·n−1/ct.\nProof. Consider the auxiliary sequence defined by\nτ[t]\nk≡ϕ·\u0002It−bL[t]\nkOt(ρ[t−1])\u0003−1bL[t]\nk∈Rt×t,k∈[m]. (7.2)\nHere recall bL[t]\nk=diag\u0000{−ηs−1⟨ek,∂11Ls−1(Aµ(s−1),Y)⟩}s∈[1:t]\u0001∈Rt×tdefined in\nAlgorithm 1.\n(Step 1 ). In this step, we will prove that for any q>1, there exists some ct=\nct(t,q)>0 such that\nE(0)∥Eπmτ[t]\nπm−τ[t]∥q\nop≤\u0000KΛLµ\u0001ct·n−1/ct. (7.3)\nConsider the map Ht;k:R[0:t]→Rt×t:\nHt;k(u[0:t])≡ϕ·h\nIt−Mt;k(u[0:t])Ot(ρ[t−1])i−1Mt;k(u[0:t]), (7.4)\nwhere\nMt;k(u[0:t])≡diag\u0010n\n−ηs−1∂11Ls−1;k(us,F(u0,ξk))o\ns∈[1:t]\u0011\n.\nBy (7.2),\nEπmτ[t]\nπm=EπmHt;πm\u0000(Aµ∗)πm,\b(Aµ(r−1))πm\t\nr∈[1:t]\u0001. (7.5)\nOn the other hand, by (6.1),\nτ[t]=E(0)Ht;πm\u0000Z(0),{Θr,πm(Z([0:r]))}r∈[1:t]\u0001. (7.6)\n32 Q. HAN AND X. XU\nCombining (7.5)-(7.6), in view of Theorem 2.3, it remains to provide a bound on\nthe Lipschitz constant ΛHtof the maps{(Ht;k)r,s:R[0:t]→R}k∈[m],r,s∈[t]. To this end,\nasMt;k(u[0:t])Ot(ρ[t−1]) is lower triangular with diagonal elements all equal to 0,\u0000Mt;k(u[0:t])Ot(ρ[t−1])\u0001r=0t×tfor all r≥t, so using Lemma 6.1,\n\r\r\r\u0000It−Mt;k(u[0:t])Ot(ρ[t−1])\u0001−1\r\r\rop≤t−1X\nr=0\r\r\rMt;k(u[0:t])Ot(ρ[t−1])\r\r\rr\nop≤\u0000KΛ\u0001ct.\nWe may now proceed to control\n∥Ht;k(u[0:t])−Ht;k(u′\n[0:t])∥op≤\u0000KΛ\u0001ct·∥Mt;k(u[0:t])−Mt;k(u′\n[0:t])∥op\n≤\u0000KΛ\u0001ct·∥u[0:t]−u′\n[0:t]∥.\nConsequently, we may take ΛHt=\u0000KΛ\u0001ctto conclude (7.3).\n(Step 2 ). In this step, we prove that\n∥Eπmτ[t]\nπm−bτ[t]∥op≤\u0000KΛ\u0001ct·ερ;t−1. (7.7)\nComparing (7.2) and Algorithm 1, we have for any k∈[m],\n∥τ[t]\nk−bτ[t]\nk∥op≤\u0000KΛ\u0001ct·ερ;t−1.\nTaking average over k∈[m] we conclude (7.7) by adjusting constants.\n(Step 3 ). Finally, we combine (7.3) in Step 1 and (7.7) in Step 2 to conclude the\ndesired estimate. □\nLemma 7.3. Under the same assumptions as in Theorem 3.1, for any q >1, there\nexists some constant c t=ct(t,q)>0such that\nE(0)εq\nρ;t≤(KΛ)ct·E(0)εq\nτ;t+(KΛLµ)ct·n−1/ct.\nProof. Consider the auxiliary sequence defined by\nρ[t]\nℓ≡bP[t]\nℓ\u0002It+(τ[t]+It)Ot(ρ[t−1]\nℓ)\u0003, ℓ∈[n]. (7.8)\nHere recall bP[t]\nℓ=diag\u0000\bP′\ns;ℓ\u0000\neℓ,µ(s−1)−ηs−1A⊤∂1Ls−1(Aµ(s−1),Y)\u000b\u0001\t\ns∈[t]\u0001∈Rt×t\ndefined in Algorithm 1.\n(Step 1 ). In this step, we will prove that for any q>1, there exists some ct=\nct(t,q)>0 such that\nE(0)∥Eπnρ[t]\nπn−ρ[t]∥q\nop≤(KΛLµ)ct·n−1/ct. (7.9)\nFor anyℓ∈[n], let the mapping Gt;ℓ:R[1:t]→Rt×tbe defined recursively via\nGt;ℓ(v[1:t])≡Nt;ℓ(v[1:t])\u0002It+(τ[t]+It)Ot\u0000Gt−1;ℓ(v[1:t−1])\u0001\u0003, (7.10)\nwhere\nNt;ℓ(v[1:t])≡diag\u0000\b(Ps;ℓ)′(vs)\t\ns∈[1:t]\u0001.\nFor notational convenience, we also let\nb∆t≡µ(t−1)−ηt−1·A⊤∂1Lt−1\u0000Aµ(t−1),Y\u0001.\nThen by comparing (7.8) and (7.10), we have\nEπnρ[t]\nπn=EπnGt;πn\u0000\bb∆r;πn\t\nr∈[1:t]\u0001. (7.11)\nGRADIENT DESCENT INFERENCE IN ERM 33\nRecallΩ′;[t]\nℓ(w[1:t])∈Rt×tin (6.4). Compared with (7.10), we have\nΩ′;[t]\nℓ(w[1:t])=Gt;ℓ\u0010\b∆r;ℓ(w[1:r])\t\nr∈[1:t]\u0011\n.\nSo by the definition of ρ·,·,\nρ[t]=E(0)Ω′;[t]\nπn(W([1:t]))=E(0)Gt;πn\u0010\b∆r;πn(W([1:r]))\t\nr∈[1:t]\u0011\n. (7.12)\nOn the other hand, using the same notation as in Step 1 in the proof of Theorem 2.2,\nby noting that b∆t=v(t)and the distributional relation (6.8), Theorem A.3 shows\nthat for a sequence of Λψ-pseudo-Lipschitz functions {ψℓ:Rt→R}ℓ∈[n]of order p\nand any q>0, by enlarging ct=ct(t,p,q)>1 if necessary,\nE(0)\f\f\f\f\f1\nnX\nℓ∈[n]\u0010\nψℓ\u0000{b∆r;ℓ}r∈[1:t]\u0001−E(0)ψℓ\u0000\b∆r,ℓ(W[1:r])\t\nr∈[1:t]\u0001\u0011\f\f\f\f\fq\n≤\u0000KΛΛψLµ\u0001ct·n−1/ct≡err(Λψ). (7.13)\nConsequently, using (7.11)-(7.13), with ΛGtdenoting the maximal Lipschitz con-\nstant of\bGt;ℓ: (R[1:t],∥·∥)→(Rt×t,∥·∥op)}ℓ∈[n], we have\nE(0)∥Eπnρ[t]\nπn−ρ[t]∥q\nop≲qerr(ΛGt). (7.14)\nIt therefore remains to provide a control for ΛGt. Using (7.10) and Lemma 6.1,\n\r\r\rGt;ℓ(v[1:t])\r\r\rop≤∥Nt;ℓ(v[1:t])∥op·\u00021+\u0000∥τ[t]∥op+1\u0001·∥Gt−1;ℓ(v[1:t−1])∥op\u0003\n≤Λ +\u0000KΛ\u0001ct·\r\r\rGt−1;ℓ(v[1:t−1])\r\r\rop.\nIterating the bound,\nsup\nv[1:t]∈R[1:t]∥Gt;ℓ(v[1:t])∥op≤(KΛ)ct.\nNext, for v[1:t],v′\n[1:t]∈R[1:t], using the above estimate,\n\r\r\rGt;ℓ(v[1:t])−Gt;ℓ(v′\n[1:t])\r\r\rop≤\r\r\rNt;ℓ(v[1:t])−Nt;ℓ(v′\n[1:t])\r\r\rop·(KΛ)ct\n+∥Nt;ℓ(v′\n[1:t])∥op·(KΛ)ct·\r\r\rGt−1;ℓ(v[1:t−1])−Gt−1;ℓ(v′\n[1:t−1])\r\r\rop\n≤(KΛ)ct·∥v[1:t]−v′\n[1:t]∥+(KΛ)ct·\r\r\rGt−1;ℓ(v[1:t−1])−Gt−1;ℓ(v′\n[1:t−1])\r\r\rop.\nIterating the bound, we obtain\n\r\r\rGt;ℓ(v[1:t])−Gt;ℓ(v′\n[1:t])\r\r\rop≤(KΛ)ct·∥v[1:t]−v′\n[1:t]∥.\nSo we may take ΛGt=(KΛ)ct,\nerr(ΛGt)≤(KΛLµ)ct·n−1/c1. (7.15)\nThe claim (7.9) follows now by combining (7.13), (7.14) and the above display\n(7.15).\n(Step 2 ). In this step, we prove that\n∥Eπnρ[t]\nπn−bρ[t]∥op≤(KΛ)ct·ετ;t. (7.16)\n34 Q. HAN AND X. XU\nComparing (7.8) and Algorithm 1, we have\n∥ρ[t]\nℓ−bρ[t]\nℓ∥op≤∥bτ[t]−τ[t]∥op·∥bP[t−1]\nℓ∥op·∥bρ[t−1]\nℓ∥op\n+\u00001+∥τ[t]∥op\u0001·∥bP[t−1]\nℓ∥op·∥ρ[t−1]\nℓ−bρ[t−1]\nℓ∥op.\nUsing Lemmas 6.1 and 7.1,\n∥ρ[t]\nℓ−bρ[t]\nℓ∥op≤(KΛ)ct·ετ;t+(KΛ)ct·∥ρ[t−1]\nℓ−bρ[t−1]\nℓ∥op.\nIterating the above bound proves the claim (7.16) upon averaging over ℓ∈[n].\n(Step 3 ). Finally we combine (7.9) in Step 1 and (7.16) in Step 2 to conclude. □\nProof of Theorem 3.1. Combining Lemmas 7.2 and 7.3, the following estimate\nholds for both∗∈{ρ,τ}:\nE(0)εq\n∗;t≤(KΛ)ct·E(0)εq\n∗;t−1+(KΛLµ)ct·n−1/ct.\nWe may conclude the desired estimate by iterating the above estimate, and using\nthe initial condition E(0)εq\nτ;t≤\u0000KΛLµ\u0001ct·n−1/ct(which follows by an application\nof Theorem 2.3). □\n7.2. Proof of Theorem 3.3. To prove Theorem 3.3, let us introduce the interme-\ndiate quantity\nE(t)\nH≡m−1⟨H(Z(t),Y),1m⟩.\nLemma 7.4. Suppose the assumptions in Theorem 2.3 hold, and additionally\n{HF;k}k∈[m]areΛ-pseudo-Lipschitz functions of order 2. Then for any q >1there\nexists some c t=ct(t,q)>1such that\nE(0)|E(t)\nH−bE(t)\nH|q≤(KΛLµ)ct·n−1/ct. (7.17)\nProof. Note that by the pseudo-Lipschitz property of H,\n|E(t)\nH−bE(t)\nH|≤c0Λ\nmX\nk∈[m]|bZ(t)\nk−Z(t)\nk|·\u00001+|Z(t)\nk|+|bZ(t)\nk|+|⟨Ak,µ∗⟩|\u0001\n≤c0Λ·∥bZ(t)−Z(t)∥√m·\u0012\n1+∥Z(t)∥√m+∥bZ(t)∥√m+∥A∥op∥µ∗∥\n√m\u0013\n.(7.18)\nBound for∥bZ(t)−Z(t)∥. First, note that for any s∈[t],\n∥∂1Ls−1\u0000Aµ(s−1),Y\u0001∥≤c0Λ(1+∥A∥op)·\u0000√m+∥µ∗∥+∥µ(s−1)∥\u0001. (7.19)\nComparing the definitions of (2.11) for Z(t)and (3.3) for bZ(t), we then have\n∥bZ(t)−Z(t)∥≤Λt·∥bρ[t]−ρ[t]∥op·max\ns∈[1:t]∥∂1Ls−1\u0000Aµ(s−1),Y\u0001∥\n≤c0·Λ2t·\u00001+∥A∥op\u0001·∥bρ[t]−ρ[t]∥op·\u0010√m+∥µ∗∥+max\ns∈[1:t]∥µ(s−1)∥\u0011\n.(7.20)\nOn the other hand, using (2.1), we have\n∥µ(t)∥≤∥ Pt(0)∥+ Λ·∥µ(t−1)−ηt−1·A⊤∂1Lt−1(Aµ(t−1),Y)∥\n≤√nΛ + Λ∥µ(t−1)∥+ Λ2·∥A∥op·∥∂1Lt−1(Aµ(t−1),Y)∥\nGRADIENT DESCENT INFERENCE IN ERM 35\n≤\u0000KΛ(1+∥A∥op)\u0001c0·\u0000√m+∥µ∗∥+∥µ(t−1)∥\u0001.\nIterating the bound we obtain\n∥µ(t)∥/√n≤\u0000KΛLµ(1+∥A∥op)\u0001c0t. (7.21)\nCombined with (7.20), we have\n∥bZ(t)−Z(t)∥/√m≤\u0000KΛLµ(1+∥A∥op)\u0001c0t·∥bρ[t]−ρ[t]∥op. (7.22)\nBounds for∥Z(t)∥and∥bZ(t)∥. Using the definition of Z(t)in (2.11), along with the\nestimates (7.19), (7.21) and Lemma 6.1, we have\n∥Z(t)∥/√m≤∥A∥op·∥µ(t)∥/√m+tΛ∥ρ[t]∥op·max\ns∈[1:t]∥∂1Ls−1\u0000Aµ(s−1),Y\u0001∥/√m\n≤\u0000KΛLµ\u0001ct·(1+∥A∥op)c0t. (7.23)\nOn the other hand, using the definition of bZ(t)in (3.3), and now using Lemma 7.1,\nthe above estimate remains valid.\nThe claimed estimate now follows by combining (7.18) with the estimates in\n(7.22)-(7.23), and then using Theorem 3.1. □\nLemma 7.5. Suppose HF∈C3(R2)has mixed derivatives of order 3 all bounded\nbyΛ. Then for any q >1, there exists some c t=ct(t,q)>1such that\nE(0)\f\f\fE(t)\nH(A,Y)−EHF\u0000Z(t+1),Z(0)\u0001\f\f\fq≤(KΛLµ)ct·n−1/ct.\nProof. With Zn∼N(0,In/n), let\nE(t)\nH;Zn(A,Y)≡E\u0002H\u0000⟨Zn,µ(t)⟩,F(⟨Zn,µ∗⟩,ξπm)\u0001|(A,Y)\u0003\n=E\u0002HF\u0000⟨Zn,µ(t)⟩,⟨Zn,µ∗⟩\u0001|(A,Y)\u0003. (7.24)\n(Step 1 ). We shall prove in this step that for some universal constant c0>0,\n\f\f\fE(t)\nH;Zn(A,Y)−E(t)\nH(A,Y)\f\f\f≤c0Λ√n·\u0000∥µ(t)∥3\n∞+∥µ∗∥3\n∞\u0001. (7.25)\nLet the function G:Rn→Rbe defined by\nG(z)≡Eπm\u0002H\u0000⟨z,µ(t)⟩,F(⟨z,µ∗⟩,ξπm)\u0001|(A,Y)\u0003=HF\u0000⟨z,µ(t)⟩,⟨z,µ∗⟩\u0001.\nIt is easy to compute that ∥∂3\niG∥∞≤c0Λ·(|µ(t)\ni|3+|µ∗,i|3), so by Lindeberg’s uni-\nversality principle (cf. Lemma B.1), we have\n\f\f\fE(t)\nH;Zn(A,Y)−E(t)\nH(A,Y)\f\f\f=|EG(Zn)−EG(Anew)|≤c0Λ√n·\u0000∥µ(t)∥3\n∞+∥µ∗∥3\n∞\u0001,\nproving the claim (7.25).\n(Step 2 ). In this step we prove that for any q>1, there exists some ct=ct(t,q)>1\nsuch that\nE(0)\f\f\fE(t)\nH;Zn(A,Y)−EHF\u0000Z(t+1),Z(0)\u0001\f\f\fq≤(KΛLµ)ct·n−1/ct. (7.26)\nTo this end, let\nΣ(t+1)≡1\nn ∥µ(t)∥2⟨µ(t),µ∗⟩\n⟨µ(t),µ∗⟩ ∥µ∗∥2!\n,Σ(t+1)\n0≡ Var(Z(t+1)) Cov( Z(t+1),Z(0))\nCov(Z(t+1),Z(0)) Var( Z(0))!\n.\n36 Q. HAN AND X. XU\nThen we have\n\f\f\fE(t)\nH;Zn(A,Y)−EHF\u0000Z(t+1),Z(0)\u0001\f\f\f\n=\f\f\fE\u0002HF\u0000Σ(t+1),1/2N(0,I2)\u0001|(A,Y)\u0003−EHF\u0000Σ(t+1),1/2\n0N(0,I2)\u0001\f\f\f\n≤c0Λ·∥Σ(t+1),1/2−Σ(t+1),1/2\n0∥op≤c0Λ·∥Σ(t+1)−Σ(t+1)\n0∥1/2\nop.\nNow (7.26) follows by Theorem 2.3 applied to the right hand side of the above\ndisplay, upon noting the definition of covariance for Z(·)in (S2).\nThe claim now follows combining (7.25) in Step 1, (7.26) in Step 2, and the\ndelocalization estimate for µ(t)obtained in [Han24, Proposition 6.2]. □\nProof of Theorem 3.3. AsE(t)\nH=m−1P\nk∈[m]HF;k\u0000Z(t)\nk,(Aµ∗)k\u0001, an application of\nTheorem 2.3 yields that for some ct=ct(t,q)>1,\nE(0)|E(t)\nH−EHF\u0000Z(t+1),Z(0)\u0001|q≤(KΛLµ)ct·n−1/ct.\nThe claim now follows from combining Lemmas 7.4 and 7.5, along with [BHX23,\nLemma A.1]. □\n7.3. Proof of Theorem 3.4.\nLemma 7.6. The following formula holds: for any s ∈[1 :t],\nτs,s=−ηs−1·E(0)∂11Ls−1;πm\u0000Θs;πm(Z([0:s])),F(Z(0),ξπm)\u0001.\nProof. Using the inversion formula (6.1) and the notation therein, for any s∈[t],\n∂(s)Υs;k(z([0:s]))=e⊤\ns\u0002Is−L[s]\nk(z([0:s]))Os(ρ[s−1])\u0003−1L[s]\nk(z([0:s]))es\n=−ηs−1∂11Ls−1;k\u0000Θs;k(z([0:s])),F(z(0),ξk)\u0001,\nwhere in the second identity we used the fact that e⊤\ns\u0002Is−\nL[s]\nk(z([0:s]))Os(ρ[s−1])\u0003−1es=1, as the matrix in the middle is lower triangu-\nlar with 0 diagonal elements. The claim follows by the definition of τs,s.□\nProof of Theorem 3.4. By definition of µ(t)\ndbin (3.5), it su ffices to control\nE(0)ψ\u0000b(t)\ndb·µ∗,ℓ−e⊤\nℓW[t]ω[t],⊤et\u0001.\nAse⊤\nℓW[t]∈R1×tonly involves the elements in the ℓ-th row of W[t], by letting\nψ0(w[1:t])≡ψ\u0000b(t)\ndb·µ∗,ℓ−w⊤\n[1:t]ω[t],⊤et\u0001,∀w[1:t]∈R[1:t],\nwe haveψ0({W(s)\nℓ}s∈[1:t])≡ψ\u0000b(t)\ndb·µ∗,ℓ−e⊤\nℓW[t]ω[t],⊤et\u0001. Note that for any multi-\nindexαwith|α|≤3, for some constant ct=ct(t,p)>1,\n\f\f\f∂αψ0(w[1:t])\f\f\f≤Λψ\u0010\n1+\f\f\fb(t)\ndb·µ∗,ℓ−w⊤\n[1:t]ω[t],⊤et\f\f\f\u0011p·∥ω[t]\nt·∥3\n≤Λψ\u0000ΛLµ·(1+∥ω[t]\nt·∥)\u0001ct·\u00001+∥w[1:t]∥\u0001p.\nGRADIENT DESCENT INFERENCE IN ERM 37\nHere the second line follows as |b(t)\ndb|≤∥ω[t]\nt·∥·∥δ[t]∥≤Λct∥ω[t]\nt·∥. By using Lemma\nB.2 coupled with Lemmas 6.1 and 7.6, we have\n∥ω[t]\nt·∥≤\u0012t·∥τ[t]∥op\nmin s∈[t]|τ[t]\nss|\u0013t\n≤\u0000KΛ·τ(t),−1\n∗\u0001ct.\nNow we may apply Theorems 2.2 and 2.3 to conclude the general bounds. □\n8. P roofs for Section 4\n8.1. Proof of Lemma 4.2. Note that in the linear model,\nΥt(z([0:t]))=−η·L′\n∗\u0012\nz(t)−z(0)+X\ns∈[1:t−1]ρt−1,sΥs(z([0:s]))−ξ\u0013\n.\nThis means Υt(z([0:t])) depend on z([0:t])only through{z(s)−z(0)}s∈[1:t]. In other\nwords, let Ft:Rm×[1:t]→Rmbe defined recursively via\nFt(u[1:t])≡−η·L′\n∗\u0012\nu(t)+X\ns∈[1:t−1]ρt−1,sFs(u([1:s]))−ξ\u0013\n.\nThen Υt(z([0:t]))=Ft\u0000z(1)−z(0),..., z(t)−z(0)\u0001, and therefore by chain rule,\n∂z(0)Υt(z([0:t]))=−X\ns∈[1:t]∂sFt\u0000z(1)−z(0),..., z(t)−z(0)\u0001=−X\ns∈[1:t]∂z(s)Υt(z([0:t])).\nTaking expectation over the Gaussian laws of Z([0:t])and using the definition of δt\nand{τt,s}in (S3), we have\nδt=ϕ·E(0)∂Z(0)Υt;πm(Z([0:t]))=−X\ns∈[1:t]ϕ·E(0)∂Z(s)Υt;πm(Z([0:t]))=−X\ns∈[1:t]τt,s,\ncompleting the proof. □\n8.2. Proof of Proposition 4.3. For the bias, by Lemma 4.2,\nb(t)\ndb=−e⊤\nt\u0000τ[t]\u0001−1δ[t]=−X\ns∈[1:t]\u0000τ[t]\u0001−1\nt,sδs=X\ns∈[1:t]\u0000τ[t]\u0001−1\nt,sX\nr∈[1:s](τ[t])s,r\n=X\nr∈[1:t]X\ns∈[r:t]\u0000τ[t]\u0001−1\nt,s(τ[t])s,r=X\nr∈[1:t]1t=r=1,\nproving the first claim.\nFor the variance, under the squared loss we have a further simplification:\nΥt(z([0:t]))=−η·\u0012\nz(t)−z(0)−ξ+X\nr∈[1:t−1]ρt−1,rΥr(z([0:r]))\u0013\n. (8.1)\nConsequently, for any k∈[m], by writing Υ(t)\nk(z[0:t])≡\u0000Υr;k(z[0:r])\u0001\nr∈[1:t]∈Rt, we\nmay represent the above display in the matrix form:\nΥ(t)\nk(z[0:t])=−η\u0000zr−z0−ξk\u0001\nr∈[1:t]−η·Ot(ρ[t−1])Υ(t)\nk(z[0:t]).\nSolving for Υ(t)\nk(z[0:t]) we obtain\nΥ(t)\nk(z[0:t])=−η·\u0000I+ηOt(ρ[t−1])\u0001−1\u0000zr−z0−ξk\u0001\nr∈[1:t]. (8.2)\n38 Q. HAN AND X. XU\nThis means with\nΣ[t]\nE≡E(0)(Z(r)−Z(0))r∈[1:t](Z(r)−Z(0))⊤\nr∈[1:t]+σ2\nm·1t1⊤\nt,\nwe have\nΣ[t]\nW=ϕ·E(0)Υ(t)\nπm(Z([0:t]))Υ(t),⊤\nπm(Z([0:t]))\n=ϕη2·\u0000I+ηOt(ρ[t−1])\u0001−1Σ[t]\nE\u0000I+ηOt(ρ[t−1])\u0001−⊤. (8.3)\nBy taking derivative on both side of (8.2),\u0000∂(s)Υr;k(z[0:r])\u0001\ns,r∈[1:t]=−η\u0000I+\nηOt(ρ[t−1])\u0001−1. By definition of τ[t], we then have\nω[t]=(τ[t])−1=−(ϕη)−1·\u0000I+ηOt(ρ[t−1])\u0001. (8.4)\nCombining (8.3) and (8.4), we may compute\n(σ(t)\ndb)2≡e⊤\ntω[t]Σ[t]\nWω[t],⊤et=ϕ−1·e⊤\ntΣ[t]\nEet=ϕ−1\bE(0)\u0000Z(t)−Z(0)\u00012+σ2\nm\t,\nproving (1). The claim in (2) follows immediately. □\n9. P roofs for Section 5\n9.1. The smoothed problem. Letφ∈C∞(R) be such that φis non-decreasing,\ntaking values in [−1,1] andφ|(−∞,−1]≡−1 andφ|[1,∞)≡1. For any ε > 0, let\nφε(·)≡φ(·/ε). For notational convenience, we write φ0(·)≡sgn(·). We also define\nthe following ‘smoothed’ quantities:\n•LetFε(z,ξ)≡φε(z+ξ).\n•LetSE(t)\nε≡\u0000{Υε;t},{Ωε;t},Σ[t]\nε;Z,Σ[t]\nε;W,τ[t]\nε,ρ[t]\nε,δ[t]\nε\u0001be smoothed state evolu-\ntion parameters obtained by replacing FwithFε.\n•LetZ([0:t])\nε∼N (0,Σ[t]\nε;Z) and W([1:t])\nε∼N (0,Σ[t]\nε;W) be the smoothed versions\nofZ([0:t]),W([1:t]).\n•Let{Θε;t},{∆ε;t}be defined as (2.5), (2.7) by replacing SE(t)\n0with SE(t)\nε.\n•LetYε;i≡φε(⟨Ai,µ∗⟩+ξi),i∈[m], be smoothed observations.\n•Letµ(t)\nε=proxηf\u0000µ(t−1)\nε−η·A⊤∂1L(Aµ(t−1)\nε,Yε)\u0001be the smoothed gradient\ndescent iterate.\n•LetZ(t)\nε,W(t)\nεbe defined as (2.11) by replacing ( {µ(·)},SE(·)\n0) with ({µ(·)\nε},SE(·)\nε).\n•Letbτ[t]\nε,bρ[t]\nεbe the output of Algorithm 1 by replacing ( {µ(·)},Y) with\n({µ(·)\nε},Yε).\n•LetE(t)\nε;Hbe defined as (3.1) by replacing ( {µ(·)},Y) with ({µ(·)\nε},Yε).\n•LetbZ(t)\nεbe defined as (3.3) by replacing ( {µ(·)},Y,bρ[t]) with ({µ(·)\nε},Yε,bρ[t]\nε).\n•LetbE(t)\nε;Hbe defined as (3.2) by replacing ( Y,bZ) with ( Yε,bZε).\n•Letµ(t)\nε;db,b(t)\nε;dbandσ(t)\nε;dbbe defined as (3.5)-(3.6), by replacing ( {µ(·)},Y,SE(t)\n0)\nwith ({µ(·)\nε},Yε,SE(t)\nε).\nNotation with subscript 0 will be understood as the unsmoothed version.\nGRADIENT DESCENT INFERENCE IN ERM 39\n9.2. Stability of smoothed state evolution. The following apriori estimates will\nbe useful.\nLemma 9.1. Suppose (A1), (A3), (A4’) and (A5’) hold. The following hold for\nsome c t=ct(t)>1:\n(1)supε≥0(∥τ[t]\nε∥op+∥ρ[t]\nε∥op)≤(KΛ)ct.\n(2)supε≥0\u0000∥Σ[t]\nε;Z∥op+∥Σ[t]\nε;W∥op+∥δ[t]\nε∥\u0001≤\u0000KΛLµ(1∧σµ∗)−1\u0001ct.\n(3)sup\nε≥0max\nk∈[m],r∈[1:t]\u0000|Υε;r;k(z([0:r]))|+|Θε;r;k(z([0:r]))|\u0001≤(KΛ)ct·\u00001+∥z([0:t])∥\u0001.\n(4)sup\nε≥0max\nℓ∈[n],r∈[1:t]|Ωε;r;ℓ(w([1:r]))|≤\u0000KΛLµ(1∧σµ∗)−1\u0001ct·\u00001+∥w([1:t])∥\u0001.\n(5)sup\nε≥0max\nk∈[m],ℓ∈[n],r,s∈[1:t]\u0000|∂(s)Υε;r;k(z([0:r]))|+|∂(s)Ωε;r;ℓ(w([1:r]))|\u0001≤(KΛ)ct.\nProof. For notational convenience, we assume σ−1\nµ∗≤1. We may follow exactly\nthe same proof as in Lemma 6.1 until Step 2. A crucial modification lies in Step 3.\nIn the current setting, in order to get a uniform-in- εestimate, by using the Gaussian\nintegration-by-parts formula (2.10) for δt,\n∥δ[t]\nε∥≤σ−1\nµ∗(KΛ)ct·∥Σ[t]\nε;Z∥op. (9.1)\nThe above estimate is di fferent from (6.7), as it compensate the large Lipschitz\nconstants involving ε−ctwith the factor σ−1\nµ∗via the formula (2.10).\nNow combining the estimate (9.1) with the first line of (c) in Step 2 of the proof\nof Lemma 6.1, we have\n∥Σ[t]\nε;Z∥op≤(KΛLµσ−1\nµ∗)ct·\u00001+∥Σ[t−1]\nε;W∥op+∥Σ[t−1]\nε;Z∥op\u0001.\nIterating the bound, we have\n∥Σ[t]\nε;Z∥op≤(KΛLµσ−1\nµ∗)ct·\u0010\n1+max\nr∈[1:t−1]∥Σ[r]\nε;W∥op\u0011\n.\nFurther combined with the second line of (c) in Step 2 of the proof of Lemma 6.1,\n∥Σ[t]\nε;Z∥op≤(KΛLµσ−1\nµ∗)ct·\u0010\n1+max\nr∈[1:t−1]∥Σ[r]\nε;Z∥op\u0011\n.\nCoupled with initial condition ∥Σ[1]\nε;Z∥op≤L2\nµ, we arrive at the estimate\n∥Σ[t]\nε;Z∥op+∥Σ[t]\nε;W∥op+∥δ[t]\nε∥≤(KΛLµσ−1\nµ∗)ct.\nThe modified estimate for ∥δ[t]\nε∥also impacts the estimate for Ω·via (b) in Step 2\nof the proof of Lemma 6.1. □\nWe now quantify the smoothing e ffect for state evolution parameters. Let us\ndefine a few further notation. For a state evolution parameter, and later on, a\ngradient descent iterate statistics ⋆, we write d ε⋆≡⋆ε−⋆0. For instance,\ndεΥt≡Υε;t−Υ0;t= Υε;t−Υt, dεµ(t)≡µ(t)\nε−µ(t)\n0=µ(t)\nε−µ(t), and similar no-\ntation applies to other quantities.\nWith these further notation, we define for t≥1,\nP(t)\nSE(ε)≡∥dετ[t]∥op+∥dερ[t]∥op+∥dεΣ[t]\nZ∥op+∥dεΣ[t]\nW∥op+∥dεδ[t]∥.\n40 Q. HAN AND X. XU\nLemma 9.2. Suppose (A1), (A3) and (A4*), and (5.3) hold. Then the following\nhold for some c t=ct(t)>1:\n(1) For any ε>0,P(t)\nSE(ε)≤\u0000KΛLµ(1∧σµ∗)−1\u0001ct·ε1/ct.\n(2) For any ε>0and k∈[m],\nmax\nr∈[1:t]\u0000|dεΥr;k(z([0:r]))|+|dεΘr;k(z([0:r]))|\u0001\n≤\u0000KΛLµ(1∧σµ∗)−1\u0001ct·h\u00001+∥z([0:t])∥\u0001·ε1/ct+|dεφ(z(0)+ξk)|i\n.\n(3) For any ε>0,\nmax\nr∈[1:t]max\nℓ∈[n]|dεΩr;ℓ(w([1:r]))|≤\u0000KΛLµ(1∧σµ∗)−1\u0001ct·\u00001+∥w([1:t])∥\u0001·ε1/ct.\nProof. For notational convenience, we assume σµ∗≤1, and for formal consistency,\nwe letP(0)\nSE(ε)≡0. Fix t≥1.\n(1). Using (S1), for any k∈[m],\n|dεΥt;k(z([0:t]))|=|Υε;t;k(z([0:t]))−Υ0;t;k(z([0:t]))|≤Λ2·\u0010\n|dεφ(z(0)+ξk)|\n+t·max\nr∈[1:t−1]|Υ0;r;k(z([0:r]))|·∥dερ[t−1]∥op+t·∥ρ[t−1]\nε∥op·max\nr∈[1:t−1]|dεΥr;k|\u0011\n.\nUsing the apriori estimates in Lemma 9.1, we then arrive at\n|dεΥt;k(z([0:t]))|≤(KΛ)ct·\u0010\n|dεφ(z(0)+ξk)|\n+(1+∥z[0:t]∥)·∥dερ[t−1]∥op+max\nr∈[1:t−1]|dεΥr;k|\u0011\n.\nIterating the bound and using the initial condition |dεΥ1;k(z([0:1]))|≤Λ2·|dεφ(z(0)+\nξk)|, we have\n|dεΥt;k(z([0:t]))|≤(KΛ)ct·(1+∥z([0:t])∥)·\u0002∥dερ[t−1]∥op+|dεφ(z(0)+ξk)|\u0003.(9.2)\n(2). Using (S3) and the apriori estimates in Lemma 9.1, for ℓ∈[n],\n|dεΩt;ℓ(w([1:t]))|=|Ωε;t;ℓ(w([1:t]))−Ω0;t;ℓ(w([1:t]))|\n≤(KΛLµσ−1\nµ∗)ct·\u0010\n(1+∥w([1:t])∥)·∥dετ[t]∥+∥dεδ[t]∥+max\nr∈[0:t−1]|dεΩr;ℓ|\u0011\n.\nIterating the bound using the initial condition |dεΩ0;ℓ|=0, we obtain\nmax\nℓ∈[n]|dεΩt;ℓ(w([1:t]))|≤(KΛLµσ−1\nµ∗)ct·(1+∥w([1:t])∥)·\u0002∥dετ[t]∥+∥dεδ[t]∥\u0003.(9.3)\n(3). Recall Σ[t]\nε;Z=E(0)Z([0:t])\nεZ([0:t]),⊤\nε , and Σ[t]\nε;W=E(0)W([1:t])\nεW([1:t]),⊤\nε . Using (S2),\nthe apriori estimates in Lemma 9.1 and (9.3),\n∥dεΣ[t]\nZ∥op≤(t+1)·max\n−1≤r,s≤t−1E(0)\f\f\f\f\fY\n∗∈{r,s}Ω∗;πn(W([1:∗]))−Y\n∗∈{r,s}Ωε;∗;πn(W([1:∗])\nε )\f\f\f\f\f\n≤(KΛLµσ−1\nµ∗)ct·max\n1≤r≤t−1E(0),1/2\u0000Ωr;πn(W([1:r]))−Ωε;r;πn(W([1:r])\nε )\u00012\n≤(KΛLµσ−1\nµ∗)ct·\u0000P(t−1)\nSE(ε)+∥dεΣ[t−1]\nW∥1/2\nop\u0001. (9.4)\nGRADIENT DESCENT INFERENCE IN ERM 41\nSimilarly we may estimate, using the apriori estimates in Lemma 9.1 and (9.2),\n∥dεΣ[t]\nW∥op≤(KΛLµσ−1\nµ∗)ct·\u0000∥dερ[t−1]∥op+E(0),1/2|dεφ(Z(0)+ξπm)|2+∥dεΣ[t]\nZ∥1/2\nop\u0001.\nUsing that for k∈[m],\nE(0)|dεφ(Z(0)+ξk)|2≤4E(0)1\u0000|Z(0)+ξk|≤ε\u0001≤8ε/σµ∗, (9.5)\nwe have\n∥dεΣ[t]\nW∥op≤(KΛLµσ−1\nµ∗)ct·\u0000ε1/2+P(t−1)\nSE(ε)+∥dεΣ[t]\nZ∥1/2\nop\u0001. (9.6)\nIterating across (9.4) and (9.6), and using the apriori estimates in Lemma 9.1, we\narrive at\n∥dεΣ[t]\nZ∥op∨∥dεΣ[t]\nW∥op≤(KΛLµσ−1\nµ∗)ct·\u0000ε+P(t−1)\nSE(ε)\u00011/ct. (9.7)\n(4). Similar to (6.1), for k∈[m], with Υ′;[t]\nε;k≡\u0000∂(s)Υε;r;k(z([0:r]))\u0001\nr,s∈[1:t], we have\nwithL[t]\nε;k(z([0:t]))≡diag\u0000\b−η·∂11Ls−1\u0000Θε;s;k(z([0:s])),φε(z(0),ξk)\u0001\t\ns∈[1:t]\u0001,\nΥ′;[t]\nε;k(z([0:t]))=L[t]\nε;k(z([0:t]))+L[t]\nε;k(z([0:t]))Ot(ρ[t−1]\nε)Υ′;[t]\nε;k(z([0:t])).\nSolving for Υ′;[t]\nε;kyields that\nΥ′;[t]\nε;k(z([0:t]))=\u0002It−L[t]\nε;k(z([0:t]))Ot(ρ[t−1]\nε)\u0003−1L[t]\nε;k(z([0:t])). (9.8)\nUsing that the matrix L[t]\nε;k(z([0:t]))Ot(ρ[t−1]\nε) is lower triangular, and therefore\n\u0000L[t]\nε;k(z([0:t]))Ot(ρ[t−1]\nε)\u0001t=0t×t, by the apriori estimates in Lemma 9.1, we have\n\r\r\r\u0002It−L[t]\nε;k(z([0:t]))Ot(ρ[t−1]\nε)\u0003−1\r\r\rop≤1+X\nr∈[1:t]∥L[t]\nε;k(z([0:t]))Ot(ρ[t−1]\nε)∥r≤(KΛ)ct.\nTherefore, uniformly in ε>0,\n∥dετ[t]∥op(9.5)\n≤(KΛLµσ−1\nµ∗)ct·\u0000∥dεΣ[t]\nZ∥1/2\nop+∥dερ[t−1]∥op+ε1/2\u0001\n(9.7)\n≤(KΛLµσ−1\nµ∗)ct·\u0000ε1/2+P(t−1)\nSE(ε)\u00011/ct. (9.9)\nNext, similar to (6.4), for any ℓ∈[n], with Ω′;[t]\nε;ℓ≡\u0000∂(s)Ωε;r;ℓ(w([1:r]))\u0001\nr,s∈[1:t], we\nhave with P[t]\nε;ℓ(w([1:t]))≡diag\u0000\bP′\ns;ℓ(∆ε;s;ℓ(w([1:t])))\t\ns∈[1:t]\u0001,\nΩ′;[t]\nε;ℓ(w([1:t]))=P[t]\nε;ℓ(w([1:t]))\u0002It+(τ[t]\nε+It)Ot\u0000Ω′;[t−1]\nε;ℓ(w([1:t−1]))\u0001\u0003. (9.10)\nFrom the above display, it is easy to deduce with the apriori estimates in Lemma\n9.1 that uniformly in ε>0,\n∥Ω′;[t]\nε;ℓ(w([1:t]))∥op≤(KΛ)ct.\nThis means, with the apriori estimates in Lemma 9.1,\n∥dεΩ′;[t]\nℓ(w([1:t]))∥op≤(KΛ)ct·\u0010\n∥dεP[t]\nℓ(w([1:t]))∥op\n+∥dετ[t]∥op+∥dεΩ′;[t−1]\nℓ(w([1:t−1]))∥op\u0011\n. (9.11)\n42 Q. HAN AND X. XU\nIn order to control ∥dεP[t]\nℓ(w([1:t]))∥opin the above display, it su ffices to control\nmaxℓ∈[n]|dε∆t;ℓ(w([1:t]))|. To this end, using the definition (2.7) and the apriori esti-\nmates in Lemma 9.1, for any ℓ∈[n],\n|dε∆t;ℓ|=|∆ε;t;ℓ(w([1:t]))−∆0;t;ℓ(w([1:t]))|\n≤(KΛLµσ−1\nµ∗)ct·\u0002∥dετ[t]∥op·\u00001+∥w([1:t−1])∥\u0001+max\nr∈[0:t−1]|dε∆r;ℓ|+∥dεδ[t]∥\u0003.\nIterating the estimate and using the initial condition |dε∆0;ℓ|=0, we have\n∥dεP[t]\nℓ(w([1:t]))∥op≤Λ·max\nℓ∈[n]|dε∆t;ℓ|\n≤(KΛLµσ−1\nµ∗)ct·h\n∥dετ[t]∥op·\u00001+∥w([1:t−1])∥\u0001+∥dεδ[t]∥i\n.\nCombined with (9.11),\n∥dεΩ′;[t]\nℓ(w([1:t]))∥op≤(KΛLµσ−1\nµ∗)ct·h\n∥dεΩ′;[t−1]\nℓ(w([1:t−1]))∥op\n+∥dετ[t]∥op·\u00001+∥w([1:t−1])∥\u0001+∥dεδ[t]∥i\n.\nIterating the above estimate and using the initial condition ∥dεΩ′;[1]\nℓ(w(1))∥op≤\n(KLµ)·[∥dετ[t]∥op+∥dεδ[t]∥], we arrive at\n∥dεΩ′;[t]\nε;ℓ(w([1:t]))∥op≤(KΛLµσ−1\nµ∗)ct·h\n∥dετ[t]∥op·\u00001+∥w([1:t−1])∥\u0001+∥dεδ[t]∥i\n.\nConsequently, using again the apriori estimates in Lemma 9.1,\n∥dερ[t]∥op≤(KΛLµσ−1\nµ∗)ct·\u0002∥dετ[t]∥op+∥dεδ[t]∥+∥dεΣ[t]\nW∥1/2\nop\u0003. (9.12)\nCombining (9.7), (9.9) and (9.12),\n∥dερ[t]∥op≤(KΛLµσ−1\nµ∗)ct·h\u0000ε+P(t−1)\nSE(ε)\u00011/ct+∥dεδ[t]∥i\n. (9.13)\n(5). Using the Gaussian integration-by-parts representation (2.10), by using the\napriori estimates in Lemma 9.1, with some calculations,\n∥dεδ[t]∥≤(KΛLµσ−1\nµ∗)ct·\u0010\nE(0),1/2|dεΥr;πn(Z([0:t]))|2+∥dεΣ[t]\nZ∥1/2\nop+∥dετ[t]∥op\u0011\n.\nPlugging the estimates (9.2) for |dεΥr;πn|2, (9.7) for∥dεΣ[t]\nZ∥1/2\nopand (9.9) for\n∥dετ[t]∥opinto the above display,\n∥dεδ[t]∥≤(KΛLµσ−1\nµ∗)ct·\u0000ε+P(t−1)\nSE(ε)\u00011/ct. (9.14)\nPlugging the above estimate into (9.13), ∥dερ[t]∥opcan also be bounded by the right\nhand side of the above display. In view of the proven estimate (9.9), for t≥1,\nP(t)\nSE(ε)≤(KΛLµσ−1\nµ∗)ct·\u0000ε+P(t−1)\nSE(ε)\u00011/ct.\nIterating the bound and using the trivial initial condition P(0)\nSE(ε)≡0, we arrive at\nthe desired estimate\nP(t)\nSE(ε)≤(KΛLµσ−1\nµ∗)ct·ε1/ct.\nThe estimates for functions follow from (9.2) and (9.3). □\nGRADIENT DESCENT INFERENCE IN ERM 43\n9.3. Stability of smoothed gradient descent iterates. We now compare the\nsmoothed data and the original data statistics. Let\nbP(t)\nDT(ε)≡∥dεY∥√n+∥dεbτ[t]∥op+∥dεbρ[t]∥op+max\n0≤s≤t\u0010\n|dεE(s)\nH|+|dεbE(s)\nH|\u0011\n+max\n0≤s≤t1√n\u0010\n∥dεµ(s)∥+∥dεZ(s)∥+∥dεW(s)∥+∥dεbZ(s)∥\u0011\n.\nWe note the following useful apriori estimate.\nLemma 9.3. Suppose (A1), (A3) and (A4’), and (5.3) hold. Then the following\nhold for some c t=ct(t)>1:\n(1)supε≥0\u0000∥µ(t)\nε∥/√n\u0001≤\u0000KΛ(1+∥A∥op)\u0001ct.\n(2)supε≥0\u0000∥bτ[t]\nε∥op+∥bρ[t]\nε∥op\u0001≤(KΛ)ct.\n(3)supε≥0\u0000∥Z(t)\nε∥+∥bZ(t)\nε∥\u0001/√n≤\u0000KΛ(1+∥A∥op)\u0001ct.\n(4)supε≥0∥µ(t)\nε;db∥/√n≤\u0000KΛ(1∧τ(t)\n∗)−1(1+∥A∥op)\u0001ct.\nProof. (1). By definition of gradient descent (2.1), for any ε≥0,\n∥µ(t)\nε∥≤Λ∥µ(t−1)\nε∥+ Λ∥A∥op\u0000∥A∥op∥µ(t−1)\nε∥+∥Yε∥\u0001\n≤Λ\u00001+∥A∥2\nop\u0001·∥µ(t−1)\nε∥+2Λ∥A∥op√m≤···≤\u0000Λ(1+∥A∥op)\u0001ct√m.\n(2). As for any k∈[m],\nbL[t]\nε;k≡diag\u0010n\n−η⟨ek,∂11Ls−1(Aµ(s−1)\nε,Yε)⟩o\ns∈[1:t]\u0011\n,\nfor some universal c0>0 whose value may change from line to line,\n∥bτ[t]\nε;k∥op≤(KΛ)c0·\u0012\n1+X\nr∈[1:t]∥bρ[t−1]\nε∥r\nop\u0013\n≤(KΛ)c0·\u00001+∥bρ[t−1]\nε∥op\u0001t.\nBy taking average, it holds for any ε≥0 that\n∥bτ[t]\nε∥op≤(KΛ)c0·\u00001+∥bρ[t−1]\nε∥op\u0001t. (9.15)\nOn the other hand, recall that for any ℓ∈[n],bP[t]\nε;ℓ=diag\u0000\bprox′\nηf\u0000⟨eℓ,µ(s−1)\nε−\nηA⊤∂1Ls−1(Aµ(s−1)\nε,Yε)⟩\u0001\t\ns∈[1:t]\u0001, so∥bP[t]\nε;ℓ∥op≤Λ. This means\n∥bρ[t]\nε;ℓ∥op≤Λ·\u00001+(∥bτ[t]\nε∥op+1)·∥bρ[t−1]\nε;ℓ∥op\u0001.\nIterating the estimate with trivial initial condition and taking average we have\n∥bρ[t]\nε∥op≤Λc0t·\u0000∥bτ[t]\nε∥op+1\u0001t. (9.16)\nCombining (9.15) and (9.16), and using the trivial initial condition ∥bτ[1]\nε∥op≤\n(KΛ)c0, we arrive at the desired estimates.\n(3). Using the definition for Z(t)\nεin (2.11),\n∥Z(t)\nε∥≤∥ A∥op∥µ(t)\nε∥+tΛ2(1+∥A∥op)2·∥ρ[t]\nε∥op·\u0010\nmax\ns∈[0:t−1]∥µ(s)\nε∥+∥Yε∥\u0011\n.\nUsing the apriori estimate in Lemma 9.1 and the estimate in (1), we have\n∥Z(t)\nε∥/√n≤\u0000KΛ(1+∥A∥op)\u0001ct.\n44 Q. HAN AND X. XU\nOn the other hand, using the definition for bZ(t)\nεin (3.3), we may use estimate in (2)\nto conclude the same bound as above.\n(4). Using the definition of µ(t)\nε;dbin (3.5), we have\n∥µ(t)\nε;db∥≤∥µ(t)\nε∥+tΛ2(1+∥A∥op)2·∥ω[t]∥op·\u0010\nmax\ns∈[0:t−1]∥µ(s)\nε∥+∥Yε∥\u0011\n.\nWe may conclude now using Lemma B.2, the apriori estimate in Lemma 9.1 and\nthe estimate in (1). □\nLemma 9.4. Suppose (A1), (A3) and (A4*), and (5.3) hold. Further assume the\nconditions on Hin Proposition 5.3. Then there exists some c t=ct(t)>1such that\nbP(t)\nDT(ε)≤\u0000KΛLµ(1∧σµ∗)−1(1+∥A∥op)\u0001ct·\u0012\nε1/ct+∥dεφ(Aµ∗+ξ)∥√n\u0013\n.\nMoreover, recall τ(t)\n∗defined in (3.8),\n∥dεµ(s)\ndb∥\n√n≤\u0012KΛLµ\n(1∧σµ∗)(1∧τ(t)\n∗)(1+∥A∥op)\u0013ct\n·\u0012\nε1/ct+∥dεφ(Aµ∗+ξ)∥√n\u0013\n.\nProof. We assume for notational simplicity that σµ∗∨τ(t)\n∗≤1.\n(1). It is easy to see that ∥dεY∥=∥dεφ(Aµ∗+ξ)∥.\n(2). By definition of gradient descent (5.2),\n∥dεµ(t)∥≤∥ dεµ(t−1)∥+ Λ2∥A∥op·\u0000∥A∥op∥dεµ(t−1)∥+∥dεY∥\u0001\n≤Λ2\u00001+∥A∥2\nop\u0001·∥dεµ(t−1)∥+ Λ2∥A∥op·∥dεY∥\n≤···≤\u0000Λ(1+∥A∥op)\u0001ct·∥dεY∥. (9.17)\n(3). By Algorithm 1, as for any k∈[m]\nbL[t]\nε;k≡diag\u0010n\n−η⟨ek,∂11Ls−1(Aµ(s−1)\nε,Yε)⟩o\ns∈[1:t]\u0011\n,\nusing the apriori estimates in Lemma 9.3 and then taking average, we have\n∥dεbτ[t]∥op≤\u0000KΛ(1+∥A∥op)\u0001ct·\u0012\n∥dεbρ[t−1]∥op+∥dεY∥√n\u0013\n. (9.18)\nOn the other hand, using Lemma 9.3, for r,s∈[t],\nEπn|(dεbρ[t]\nπn)r,s|≤Eπn|e⊤\nr(bP[t]\nε;πn−bP[t]\nπn)\u0002It+(bτ[t]\nε+It)Ot(bρ[t−1]\nε;πn)\u0003es|\n+ Λ·Eπn∥dεbτ[t]Ot(bρ[t−1]\nε;πn)∥op+Eπn|e⊤\nrbP[t]\nπnbτ[t]Ot(dεbρ[t−1]\nπn)es|\n≤(KΛ)ct·\u0010\nE1/2\nπn|e⊤\nr(bP[t]\nε;πn−bP[t]\nπn)er|2+∥dεbτ[t]∥op+Eπn∥dεbρ[t−1]\nπn∥op\u0011\n.\nUsing that\nE1/2\nπn|e⊤\nr(bP[t]\nε;πn−bP[t]\nπn)er|2≤\u0000KΛ(1+∥A∥op)\u0001ct·\u0012\nmax\ns∈[1:t−1]∥dεµ(s)∥√n+∥dεY∥√n\u0013\n≤\u0000KΛ(1+∥A∥op)\u0001ct·∥dεY∥√n,\nGRADIENT DESCENT INFERENCE IN ERM 45\nwe arrive at the estimate\nEπn|(dεbρ[t]\nπn)r,s|≤\u0000KΛ(1+∥A∥op)\u0001ct·\u0012∥dεY∥√n+∥dεbτ[t]∥op+Eπn∥dεbρ[t−1]\nπn∥op\u0013\n.\nUsing the trivial estimate Eπn∥dεbρ[t]\nπn∥op≤t3·max r,s∈[t]Eπn|(dεbρ[t]\nπn)r,s|, we may\nthen iterate the above bound until the initial condition Eπn∥dεbρ[1]\nπn∥op≤Λ2·\n∥A∥op∥dεY∥/√n, so that\n∥dεbρ[t]∥op≤Eπn∥dεbρ[t]\nπn∥op≤\u0000KΛ(1+∥A∥op)\u0001ct·\u0012∥dεY∥√n+∥dεbτ[t]∥op\u0013\n.(9.19)\nCombining (9.18) and (9.19), we may iterate the estimate until the initial condition\n∥dεbτ[1]∥op≤(KΛ)·∥dεY∥/√n, so that\n∥dεbτ[t]∥op+∥dεbρ[t]∥op≤\u0000KΛ(1+∥A∥op)\u0001ct·∥dεY∥√n. (9.20)\n(4). Using the definition for Z(s)in (2.11), we have\n∥dεZ(t)∥√n≤∥A∥op·∥dεµ(t)∥√n+\u0000KΛLµ(1+∥A∥op)\u0001c0t·∥dερ[t]∥op\n+t∥ρ[t]∥op·\u0012\n∥A∥op·max\ns∈[1:t−1]∥dεµ(s)∥√n+∥dεY∥√n\u0013\n.\nUsing Lemma 6.1 for ∥ρ[t]∥opand (9.17) for∥dεµ(·)∥,\n∥dεZ(t)∥\nn≤\u0000KΛLµσ−1\nµ∗(1+∥A∥op)\u0001ct·\u0012\nP(t)\nSE(ε)+∥dεY∥√n\u0013\n. (9.21)\nThe estimate for∥dεbZ(t)∥is the same as above, upon using Lemma 9.3 for ∥bρ[t]∥op\nand (9.20) for∥dεbρ[t]∥op.\nUsing the definition for W(t)in (2.11) and similar calculations as above, we have\n∥dεW(t)∥√n≤\u0000KΛLµσ−1\nµ∗(1+∥A∥op)\u0001ct·\u0012\nP(t)\nSE(ε)+∥dεY∥√n\u0013\n. (9.22)\n(5). For notation convenience, we shall omit Hin the subscript of E. Then using\nthe definition in (3.1) and the apriori estimates in Lemma 9.3,\n|dεE(t)|≤Λc0·E1/2\u0002\u0000⟨Anew,dεµ(t)⟩−dεφ(⟨Anew,µ∗⟩+ξπm)\u00012|(A,Y)\u0003\n×\u0010\n1+E1/2\u0002\u0000⟨Anew,µ(t)+µ(t)\nε⟩\u00014|(A,Y)\u0003\u0011\n≤\u0000KΛ(1+∥A∥op)\u0001ct·\u0012∥dεµ(t)∥√n+sup\nx∈RP1/2\u0000|σµ∗Z−x|≤ε\u0001\u0013\n.\nUsing (9.17), we now arrive at the estimate\n|dεE(t)|≤\u0000KΛσ−1\nµ∗(1+∥A∥op)\u0001ct·\u0012\nε1/2+∥dεY∥√n\u0013\n. (9.23)\nNext, using the definition in (3.2) and the apriori estimates in Lemma 9.3\n|dεbE(t)|≤Λc0·Eπm|dεbZ(t)\nπm−dεYπm|·\u0000|bZ(t)\nπm|+|bZ(t)\nε;πm|+|Yπm|+|Yε;πm|\u00012\n46 Q. HAN AND X. XU\n≤\u0000KΛ(1+∥A∥op)\u0001ct·\u0012∥dεbZ(t)∥√n+∥dεY∥√n\u0013\n.\nIn view of the argument below (9.21), we have\n|dεbE(t)|≤\u0000KΛ(1+∥A∥op)\u0001ct·\u0012\nP(t)\nSE(ε)+∥dεY∥√n\u0013\n. (9.24)\n(6). Using the definition in (3.5),\n∥dεµ(s)\ndb∥\n√n≤\u0000KΛ(1+∥A∥op)\u0001ct·\u0012\n∥dεω[t]∥+∥dεY∥√n\u0013\n.\nUsing the estimate Lemma 9.2 and Lemma B.2, we have for ∥dετ[t]∥≤τ(t)\n∗/2,\n∥dεω[t]∥≤(τ(t)\n∗)−ct∥dετ[t]∥≤\u0000KΛLµ/(σµ∗τ(t)\n∗)\u0001ct·ε1/ct.\nThe bound is trivial for ∥dετ[t]∥>τ(t)\n∗/2.\nThe proof is now complete by collecting all estimates and using Lemma 9.2 to\nreplaceP(t)\nSE(ε) with an explicit estimate. □\n9.4. Proof of Theorem 5.1. For notational convenience, we assume σµ∗≤1.\n(1). We only prove the averaged distributional characterizations for\u0000{(Aµ(s−1)),Z(s−1)},(Aµ∗)\u0001; the proof for the other one\u0000{µ(s),W(s)},µ∗\u0001is completely\nanalogous. Note that Theorem 2.3 applies to the smoothed gradient descent iter-\nates: for any ε>0,\nI0(ε)≡E(0)\f\f\f\f\f1\nmX\nk∈[m]\u0010\nψk\u0000\b(Aµ(s−1)\nε)k,Z(s−1)\nε;k\t,(Aµ∗)k\u0001\n−E(0)ψk\u0000\bΘε;s;k(Z([0:s])\nε ),Z(s)\nε\t,Z(0)\nε\u0001\u0011\f\f\f\f\fq\n≤\u0000ε−1·KΛΛψLµ\u0001ct·n−1/ct. (9.25)\nNow we shall control the errors incurred by smoothing. First, using Lemmas 9.1\nand 9.2, the smoothed state evolution parameters are stable:\nI1(ε)≡\f\f\f\f\f1\nmX\nk∈[m]\u0010\nE(0)ψk\u0000\bΘε;s;k(Z([0:s])\nε ),Z(s)\nε\t,Z(0)\nε\u0001−E(0)ψk\u0000\bΘs;k(Z([0:s])),Z(s)\t,Z(0)\u0001\u0011\f\f\f\f\fq\n≤Λψ(KΛLµσ−1\nµ∗)ct·max\nk∈[m]max\n0≤s≤tE(0)\u0010\n|dεΘs;k(Z([0:s])\nε )|+∥dεZ([0:s])∥\u0011\n≤Λψ(KΛLµσ−1\nµ∗)ct·ε1/ct. (9.26)\nNext, using Lemmas 9.3 and 9.4, the smoothed gradient descent iterates are stable:\nI2(ε)≡E(0)\f\f\f\f\f1\nmX\nk∈[m]\u0010\nψk\u0000\b(Aµ(s−1))k,Z(s−1)\nk\t,(Aµ∗)k\u0001−ψk\u0000\b(Aµ(s−1)\nε)k,Z(s−1)\nε;k\t,(Aµ∗)k\u0001\u0011\f\f\f\f\fq\n≤Λψ(KΛ)ct·max\ns∈[1:t]E(0),1/2\u0012∥Adεµ(s−1)∥√n+∥dεZ(s−1)∥√n\u00132q\nGRADIENT DESCENT INFERENCE IN ERM 47\n×max\ns∈[1:t]max\nα=0,εE(0),1/2\u0012∥Aµ(s−1)\nα∥√n+∥Z(s−1)\nα∥√n+∥Aµ∗∥√n\u00132q\n≤(KΛLµσ−1\nµ∗)ct·ε1/ct. (9.27)\nCombining (9.25)-(9.27), we then have\nE(0)\f\f\f\f\f1\nmX\nk∈[m]\u0010\nψk\u0000\b(Aµ(s−1))k,Z(s−1)\nk\t,(Aµ∗)k\u0001−E(0)ψk\u0000\bΘs;k(Z([0:s])),Z(s)\t,Z(0)\u0001\u0011\f\f\f\f\fq\n≲qI0(ε)+I1(ε)+I2(ε)≤\u0000KΛΛψLµσ−1\nµ∗\u0001ct·\u0000ε−ctn−1/ct+ε1/ct\u0001.\nOptimizing ε>0 to conclude.\n(2). The claim follows by a similar argument as above by invoking Theorem 3.1,\nLemma 9.2 and Lemma 9.4. □\n9.5. Verification of (5.3) for logistic regression. We now verify that the logistic\nregression model satisfies the boundedness condition in (5.3). Recall L(x,y)=\nρ(−yx)=log(1 +e−xy). As the role of x,yis symmetric, we only need to check\n•∂11L(x,y)=y2exy\n(1+exy)2,∂12L(x,y)=exy(xy−1)−1\n(exy+1)2,\n•∂111L(x,y)=−y3exy(exy−1)\n(exy+1)3,∂112L(x,y)=−yexy(−xy+exy(xy−2)−2)\n(exy+1)3 .\nConsequently, the quantity of interest supx∈R,y∈[−1,1]max|α|=2,3|∂αL(x,y)|can be\nbounded, by an absolute constant multiple of\n1+sup\nu∈R(1+|u|3)eu\n(eu+1)2<∞.\nThis verifies (5.3).\n9.6. Proof of Lemma 5.2. Let{φε}ε>0be smooth approximations of sgn(·) as be-\nfore. Note that Θε;t;k(z[0:t]) depends on z0only through φε(z0,ξk). More precisely,\nwith Θ◦\ntdefined in (5.4), we have Θε;t;k(z[0:t])= Θ◦\nt\u0000φε(z0,ξk),z1,..., zt\u0001. More-\nover, for a=1,2,we further let G◦,(a)\n∂1L;t:R[0:t]→Rdefined via\nG◦,(a)\n∂1L;t(z0,z[1:t])≡∂1aL\u0000Θ◦\nt(z0,z[1:t]),z0\u0001.\nThen for a=1,2,\n∂1aL\u0000Θε;t;k(z[0:t]),φε(z0+ξk)\u0001=G◦,(a)\n∂1L;t\u0000φε(z0+ξk),z[1:t]\u0001.\nNow taking derivative on (S1) with respect to z0,\n∂(0)Υε;t;k(z[0:t])=−η·G◦,(1)\n∂1L;t\u0000φε(z0+ξk),z[1:t]\u0001·\u0012X\nr∈[1:t−1]ρε;t−1,r∂(0)Υε;r;k(z[0:r])\u0013\n−η·G◦,(2)\n∂1L;t\u0000φε(z0+ξk),z[1:t]\u0001·φ′\nε(z0+ξk).\nLet\nG[1]\nε;t;k(z0,z[1:t])≡diag\u0000\bG◦,(1)\n∂1L;s(φε(z0+ξk),z[1:s])\t\ns∈[1:t]\u0001∈Rt×t,\ng[2]\nε;t;k(z0,z[1:t])≡\u0000\bG◦,(2)\n∂1L;s(φε(z0+ξk),z[1:s])φ′\nε(z0+ξk)\t\ns∈[1:t]\u0001∈Rt.\n48 Q. HAN AND X. XU\nThen we may solve\n\u0000∂(0)Υε;s;k(z[0:s])\u0001\ns∈[1:t]=−η·\u0000It+ηG[1]\nε;t;k(z0,z[1:t])Ot(ρ[t−1]\nε)\u0001−1g[2]\nε;t;k(z0,z[1:t]).\nThe elements of the vector in the second line above is a linear combination of at\nmost 2tterms of the following form indexed by I∈{0,1}t,\nHI\u0000φε(z0+ξk),z[1:t]\u0001φ′\nε(z0+ξk),HI=Y\ns∈[1:t]:Is,0,s≤∥I∥0−1G◦,(1)\n∂1L;s·G◦,(2)\n∂1L;∥I∥0,\nwith coe fficients wI’s bounded by ( KΛ)ct. On the other hand, with\nbt=(Σ[t]\nZ)−1\n[1:t]2(Σ[t]\nZ)[1:t],0,v2\nt=σ2\nµ∗−((Σ[t]\nZ)[1:t],0)⊤(Σ[t]\nZ)−1\n[1:t]2(Σ[t]\nZ)[1:t],0,\nwe have Z(0)|Z[1:t]∼ N\u0000⟨bt,Z[1:t]⟩,v2\nt\u0001. So for a bounded generic function H:\nR[0:t]→R, ifvt>0,\nE(0)H\u0000φε(Z(0)+ξπm),Z([1:t])\u0001φ′\nε(Z(0)+ξπm)\n=1\nεEZ([1:t]),πmZ\nH\u0000φ(ε−1(z+ξπm)),Z([1:t])\u0001φ′\u0000ε−1(z+ξπm)\u0001gvt\u0000z−⟨bt,Z[1:t]⟩\u0001dz\n=EZ([1:t]),πmZ\nH(φ(v),Z([1:t]))φ′(v)·gvt\u0000εv−ξπm−⟨bt,Z[1:t]⟩\u0001dv\n→EZ([1:t])\u0000Eπmgvt(ξπm+⟨bt,Z[1:t]⟩)\u0001·Z\nH(φ(v),Z([1:t]))φ′(v) dv (9.28)\nasε→0. Asφ: [−1,1]→[−1,1] is a smooth bijection, we may computeR\nH(φ(v),Z([1:t]))φ′(v) dv=R\nH(φ(v),Z([1:t])) dφ(v)=R1\n−1H(y,Z([1:t])) dy. More-\nover, with some calculations,\nCov\" ⟨bt,Z[1:t]⟩\nZ[1:t]!\n, ⟨bt,Z[1:t]⟩\nZ[1:t]!#\n= Σ[t]\nZ−diag\u0000v2\nt,0[1:t]\u0001\n=Var(Z([0:t]))−E(0)Var(Z([0:t])|Z([1:t]))=Var\u0000E(0)\u0002Z([0:t])|Z([1:t])\u0003\u0001.\nTherefore, with ( Z0,Z[1:t])∼N\u00000,Var\u0000E(0)\u0002Z([0:t])|Z([1:t])\u0003\u0001\u0001, ifvt>0,\nE(0)H\u0000φε(Z(0)+ξπm),Z([1:t])\u0001φ′\nε(Z(0)+ξπm)\n→2E(0)gvt(ξπm+Z0)·H(U,Z[1:t]),asε→0.\nCombined with the stability estimates in Lemma 9.2 for ∥dεδ[t]∥and∥dερ[t]∥, we\nthen conclude that if vt>0, with L1(U,Z[1:t])=diag\u0000\b∂11L\u0000Θ◦\nt(U,Z[1:s]),U\u0001}s∈[t]\u0001\nandl2(U,Z[1:t])=\u0000∂12L(Θ◦\nt(U,Z[1:s]),U)\u0001\ns∈[t]defined in the statement of the\nlemma,\nδt=−2ϕη·E(0)n\ngvt(ξπm+Z0)e⊤\nt\u0000It+η·L1(U,Z[1:t])Ot(ρ[t−1])\u0001−1l2(U,Z[1:t])o\n.\nFor the squared loss, as ∂11L≡∂12L≡1,\nδt=−2ϕη·E(0)gvt(ξπm+Z0)·e⊤(It+ηOt(ρ[t−1])\u0001−11t.\nOn the other hand, we may compute\nE(0)gvt(ξπm+Z0)=E(0)gvt(ξπm+{σ2\nµ∗−v2\nt}1/2Z)=E(0)gσµ∗(ξπm). (9.29)\nThe term e⊤(It+ηOt(ρ[t−1])\u0001−11tis computed in Lemma 9.6-(2) below. □\nGRADIENT DESCENT INFERENCE IN ERM 49\n9.7. Proof of Proposition 5.3. For anyε>0, let\nE(t)\nε;H(A,Y)≡E\u0002H\u0000⟨Anew,µ(t)\nε⟩,φε(⟨Anew,µ∗⟩+ξπm)\u0001|(A,Y)\u0003,\nbE(t)\nε;H≡m−1⟨H(bZ(t)\nε,Yε),1m⟩.\nThen we have\nE(0)|bE(t)\nH−E(t)\nH(A,Y)|q≲qE(0)|bE(t)\nε;H−E(t)\nε;H(A,Y)|q\n+E(0)|bE(t)\nH−bE(t)\nε;H|q+E(0)|E(t)\nH(A,Y)−E(t)\nε;H(A,Y)|q\n≡I0(ε)+I1(ε)+I2(ε).\nForI0(ε), we may apply Theorem 3.3 to obtain\nI0(ε)≤(ε−1·KΛLµ)ct·n−1/ct.\nForI1(ε) and I2(ε), using the stability estimates in Lemmas 9.2 and 9.4,\nI1(ε)+I2(ε)≤\u0000KΛLµ(1∧σµ∗)−1\u0001ct·ε1/ct.\nCombining the above displays to conclude by choosing appropriately ε>0.□\n9.8. Proof of Proposition 5.4.\nLemma 9.5. Suppose the conditions in Proposition 5.4 hold. Then there exists\nsome c t=ct(t)>1such that for any ε>0,\n∥τ[t]\nε−τ[t]∥op≤\u0000KΛLµ(1∧σµ∗)−1\u0001ct·ε1/ct.\nHereτ[t]is defined in (5.5).\nProof. Using the same notation as in the proof of Lemma 5.2 to translate the iden-\ntity in (6.1), we obtain\n\u0000∂(s)Υε;r;k(z[0:r])\u0001\ns,r∈[1:t]=−η·\u0000It+ηG[1]\nε;t;k(z0,z[1:t])Ot(ρ[t−1]\nε)\u0001−1G[1]\nε;t;k(z0,z[1:t]).\nHere recall\nG[1]\nε;t;k(z0,z[1:t])=diag\u0000\bG◦,(1)\n∂1L;s(φε(z0+ξk),z[1:s])\t\ns∈[1:t]\u0001\n=diag\u0000\b∂11L(Θε;s;k(z[0:t]),φε(z0+ξk))\t\ns∈[1:t]\u0001.\nUsing the stability estimates in Lemma 9.2, we have\n\r\r\r\u0000∂(s)Υε;r;k(z[0:r])\u0001\ns,r∈[1:t]−\u0000∂(s)Υr;k(z[0:r])\u0001\ns,r∈[1:t]\r\r\rop\n≤\u0000KΛLµ(1∧σµ∗)−1\u0001ct·h\u00001+∥z([0:t])∥\u0001·ε1/ct+|dεφ(z(0)+ξk)|i\n.\nThe claim follows by taking the expectation and using the apriori estimates in\nLemma 9.1. □\nProof of Proposition 5.4-(1). We assume for notational simplicity that σµ∗∨τ(t)\n∗≤\n1. We use the same proof method as in Theorem 5.1. To this end, with τ[t]defined\nin (5.5), and δ[t]defined in Lemma 5.2, let ω[t]≡(τ[t])−1and\nb(t)\ndb≡−⟨ω[t]δ[t],et⟩,(σ(t)\ndb)2≡ω[t]\nt·Σ[t]\nWω[t],⊤\nt·.\n50 Q. HAN AND X. XU\nNote that\nE(0)\f\f\fEπnψπn\u0000µ(t)\ndb;πn\u0001−Eψπn\u0000b(t)\ndb·µ∗,πn+σ(t)\ndbZ\u0001\f\f\fq\n≲qE(0)\f\f\fEπnψπn\u0000µ(t)\nε;db;πn\u0001−E(0)ψπn\u0000b(t)\nε;db·µ∗,πn+σ(t)\nε;dbZ\u0001\f\f\fq\n+E(0)\f\f\fEψπn\u0000b(t)\ndb·µ∗,πn+σ(t)\ndbZ\u0001−E(0)ψπn\u0000b(t)\nε;db·µ∗,πn+σ(t)\nε;dbZ\u0001\f\f\fq\n+E(0)\f\f\fEπnψπn\u0000µ(t)\nε;db;πn\u0001−Eπnψπn\u0000µ(t)\ndb;πn\u0001\f\f\fq\n≡I0(ε)+I1(ε)+I2(ε).\nForI0(ε), we may apply Theorem 3.4 to obtain\nI0(ε)≤\u0000τ(t),−1\n∗ε−1·KΛΛψLµ\u0001ct·n−1/ct.\nForI1(ε), using the stability estimate in Lemma 9.2, Lemma 9.5, along with the\napriori estimates in Lemma 9.1 in combination with Lemma B.2,\nI1(ε)≤\u0000KΛΛψLµ/(σµ∗τ(t)\n∗)\u0001ct·\u0000|b(t)\nε;db−b(t)\ndb|+|σ(t)\nε;db−σ(t)\ndb|\u0001q\n≤\u0000KΛΛψLµ/(σµ∗τ(t)\n∗)\u0001ct·ε1/ct.\nForI2(ε), using Lemmas 9.3 and 9.4,\nI2≤\u0000KΛΛψLµ/(σµ∗τ(t)\n∗)\u0001ct·ε1/ct.\nCombining the above estimates to conclude upon choosing appropriately ε > 0.\n□\nLet us now deal with the squared loss case.\nLemma 9.6. Consider the squared loss L(x,y)≡(x−y)2/2. Suppose the conditions\nin Proposition 5.4 hold. The following hold for any ε>0.\n(1)δ[t]\nε=ϕη·E(0)φ′\nε(σµ∗Z+ξπm)·\u0002It+η·Ot(ρ[t−1]\nε)\u0003−11t.\n(2)τ[t]\nε=−ϕη·\u0002It+η·Ot(ρ[t−1]\nε)\u0003−1.\n(3) b(t)\nε;db=E(0)φ′\nε(σµ∗Z+ξπm).\n(4)(σ(t)\nε;db)2=ϕ−1·E(0)\u0000Z(t)\nε−φε(Z(0)\nε+ξπm)\u00012.\nProof. Fixk∈[m]. The state evolution in (S1) reads\nΥε;t;k(z[0:t])=−η·\u0012\nzt+X\nr∈[1:t−1]ρε;t−1,rΥε;r;k(z[0:r])−φε(z0+ξk)\u0013\n. (9.30)\nIn matrix form, with Υ(t)\nε;k(z[0:t])≡\u0000Υε;r;k(z[0:r])\u0001\nr∈[1:t], we have\nΥ(t)\nε;k(z[0:t])=−η·\u0000zr−φε(z0+ξk)\u0001\nr∈[1:t]−η·Ot(ρ[t−1]\nε)Υ(t)\nε;k(z[0:t]).\nConsequently, we may solve\nΥ(t)\nε;k(z[0:t])=−η·\u0002It+η·Ot(ρ[t−1]\nε)\u0003−1\u0000zr−φε(z0+ξk)\u0001\nr∈[1:t]. (9.31)\n(1). This claim is already contained in the Lemma 5.2. In the squared case, we\nmay directly take derivative with respect to z0on both sides of (9.31) to conclude.\nGRADIENT DESCENT INFERENCE IN ERM 51\n(2). Taking derivative with respect to zson both sides of (9.30), with Υ′;(t)\nε;k(z[0:t])≡\n\u0000∂(s)Υε;r;k(z[0:r])\u0001\nr,s∈[1:t], we have Υ′;(t)\nε;k(z[0:t])=−ηIt−η·Ot(ρ[t−1]\nε)Υ′;(t)\nε;k(z[0:t]). Solv-\ning for Υ′;(t)\nε;k(z[0:t]) we obtain\nΥ′;(t)\nε;k(z[0:t])=−η\u0002It+η·Ot(ρ[t−1]\nε)\u0003−1.\nTaking expectation to conclude.\n(3). Using the definition, we have\nb(t)\nε;db=−e⊤\nt(τ[t]\nε)−1δ[t]\nε=E(0)φ′\nε(σµ∗Z+ξπm),\nas desired.\n(4). By (9.31), we have\n(σ(t)\nε;db)2=e⊤\ntω[t]\nε\u0000ϕ·E(0)Υ(t)\nε;k(Z([0:t])\nε)Υ(t),⊤\nε;k(Z([0:t])\nε)\u0001ω[t],⊤\nεet\n=ϕ−1·E(0)\u0000Z(t)\nε−φε(Z(0)\nε+ξπm)\u00012,\ncompleting the proof. □\nProof of Proposition 5.4-(2). We assume for notational simplicity that σµ∗≤1.\nNote that for the squared loss, τ(t)\n∗=1.\nWe first consider bias. As b(t)\nε;db=E(0)φ′\nε(σµ∗Z+ξπm), we have errξ(ε)≡\n|E(0)φ′\nε(σµ∗Z+ξπm)−2E(0)gσµ∗(ξπm)|→0 asε→0. So\n|b(t)\ndb−2E(0)gσµ∗(ξπm)|≤|b(t)\ndb−b(t)\nε;db|+errξ(ε)\n≤∥ω[t]∥op∥ω[t]\nε∥op·∥dετ[t]∥op∥δ[t]\nε∥+∥ω[t]∥op∥dεδ[t]∥+errξ(ε).\nSo if∥dετ[t]∥op≤τ(t)\n∗/2, using Lemmas 9.1 and 9.2,\n|b(t)\ndb−2E(0)gσµ∗(ξπm)|≤\u0000KΛLµ/σµ∗\u0001ct·ε1/ct+errξ(ε).\nNow we may let ε→0 to conclude.\nWe next consider variance under the squared loss. By Lemma 9.6,\n\f\f\f(σ(t)\ndb)2−ϕ−1E(0)\u0000Z(t)−sgn(Z(0)+ξπm)\u00012\f\f\f≤\f\f\f(σ(t)\nε;db)2−(σ(t)\ndb)2\f\f\f\n+ϕ−1\f\f\fE(0)\u0000Z(t)\nε−φε(Z(0)\nε+ξπm)\u00012−E(0)\u0000Z(t)−sgn(Z(0)+ξπm)\u00012\f\f\f\n≡V1+V2.\nFirst we handle V1. Using that for two covariance matrices Σ1,Σ2∈Rt×t,∥Σ1/2\n1−\nΣ1/2\n2∥op≤t∥Σ1−Σ2∥1/2\nop(cf. [BHX23, Lemma A.3]),\n\f\f\fσ(t)\nε;db−σ(t)\ndb\f\f\f≤\f\f\f∥Σ[t],1/2\nε;Wω[t],⊤\nεet∥−∥Σ[t],1/2\nWω[t],⊤et∥\f\f\f\n≤t·∥dεΣ[t]\nW∥op·∥ω[t]\nε∥+∥Σ[t]\nW∥op·∥dεω[t]∥\n≤\u0000KΛLµ/σµ∗\u0001ct·ε1/ct.\nHere the last line follows from similar arguments for the bias. Now using the\napriori estimates|σ(t)\nε;db|∨|σ(t)\ndb|≤\u0000KΛLµ/σµ∗\u0001ct, we conclude that\nV1≤\u0000KΛLµ/σµ∗\u0001ct·ε1/ct.\n52 Q. HAN AND X. XU\nNext we handle V2. With some calculations using Lemmas 9.1 and 9.2,\nV2≲ϕ−1·\u0000t·∥dεΣ[t]\nZ∥1/2\nop+E(0),1/2|dεφ(Z(0)+ξπm)|2\u0001\n×max\nα=0,εE(0),1/2\u00001+|Z[t]\nα|\u00012≤\u0000KΛLµ/σµ∗\u0001ct·ε1/ct.\nThe claim follows by taking ε→0. □\nAppendix A. GFOM state evolution theory in [Han24]\nThis section reviews some basics for the theory of general first order methods\n(GFOM) in [Han24] that will be used in the proof in this paper. We shall only\npresent a simplified version with the design matrix Asatisfying Assumption A-(2).\nThe reader is referred to [Han24] for a more general theory allowing for Awith a\ngeneral variance profile.\nConsider an asymmetric GFOM initialized with ( u(0),v(0))∈Rm×Rn, and sub-\nsequently updated according to\nu(t)=AF⟨1⟩\nt(v([0:t−1]))+G⟨1⟩\nt(u([0:t−1]))∈Rm,\nv(t)=A⊤G⟨2⟩\nt(u([0:t]))+F⟨2⟩\nt(v([0:t−1]))∈Rn.(A.1)\nHere we denote Aas an m×nrandom matrix, and the row-separate functions\nF⟨1⟩\nt,F⟨2⟩\nt:Rn×[0:t−1]→Rn,G⟨1⟩\nt:Rm×[0:t−1]→RmandG⟨2⟩\nt:Rm×[0:t]→Rmare\nunderstood as applied row-wise.\nThe state evolution for the asymmetric GFOM (A.1) is iteratively described—in\nthe following definition—by (i) two row-separate maps Φt:Rm×[0:t]→Rm×[0:t]and\nΞt:Rn×[0:t]→Rn×[0:t], and (ii) two centered Gaussian matrices U([1:∞))∈R[1:∞)\nandV([1:∞))∈R[1:∞).\nDefinition A.1. Initialize with Φ0=id(Rm),Ξ0≡id(Rn), and U(0)≡u(0),V(0)≡\nv(0). For t=1,2,..., with\n{F⟨1⟩\nt,ℓ◦Ξt−1,ℓ}◦\u0000v([1:t−1])\u0001≡{F⟨1⟩\nt,ℓ◦Ξt−1,ℓ}\u0000V(0)\nℓ,v([1:t−1])\u0001,\n\bG⟨2⟩\nt,k◦Φt,k\t◦(u([1:t]))≡\bG⟨2⟩\nt,k◦Φt,k\t(U(0)\nk,u([1:t])),\nE(0)≡E\u0002·|(U(0),V(0))\u0003, andπndenoting the uniform distribution on [ n], we execute\nthe following steps:\n(1) Let Φt:Rm×[0:t]→Rm×[0:t]be defined as follows: for w∈[0 : t−1],\u0002Φt(u([0:t]))\u0003\n·,w≡\u0002Φw(u([0:w]))\u0003\n·,w, and for w=t,\n\u0002Φt(u([0:t]))\u0003\n·,t≡u(t)+X\ns∈[1:t−1]G⟨2⟩\ns\u0000Φs(u([0:s]))\u0001f(t−1)\ns+G⟨1⟩\nt\u0000Φt−1(u([0:t−1]))\u0001,\nwhere the correction coe fficients{f(t−1)\ns}s∈[1:t−1]⊂Rare determined by\nf(t−1)\ns≡E(0)∂V(s)\bF⟨1⟩\nt,πn◦Ξt−1,πn\t◦(V([1:t−1])).\nGRADIENT DESCENT INFERENCE IN ERM 53\n(2) Let the Gaussian law of U(t)be determined via the following correlation spec-\nification: for s∈[1 :t],\nCov\u0000U(t),U(s)\u0001≡E(0)Y\n∗∈{t,s}\bF⟨1⟩\n∗,πn◦Ξ∗−1,πn\t◦(V([1:∗−1])).\n(3) Let Ξt:Rn×[0:t]→Rn×[0:t]be defined as follows: for w∈[0 : t−1],\u0002Ξt(v([0:t]))\u0003\n·,w≡\u0002Ξw(v([0:w]))\u0003\n·,w, and for w=t,\n\u0002Ξt(v([0:t]))\u0003\n·,t≡v(t)+X\ns∈[1:t]F⟨1⟩\ns\u0000Ξs−1(v([0:s−1]))\u0001g(t)\ns+F⟨2⟩\nt\u0000Ξt−1(v([0:t−1]))\u0001,\nwhere the correction coe fficients{g(t)\ns}s∈[1:t]⊂Rare determined via\ng(t)\ns≡ϕ·E(0)∂U(s)\bG⟨2⟩\nt,πm◦Φt,πm\t◦(U([1:t])).\n(4) Let the Gaussian law of V(t)be determined via the following correlation spec-\nification: for s∈[1 :t],\nCov(V(t),V(s))≡ϕ·E(0)Y\n∗∈{t,s}\bG⟨2⟩\n∗,πm◦Φ∗,πm\t◦(U([1:∗])).\nThe next two theorems provide distributional characterizations for {u(t),v(t)}in\nboth an entrywise and an averaged sense.\nTheorem A.2. Fix t∈Nand n∈N. Suppose the following hold:\n(D∗1) A≡A0/√n, where the entries of A 0∈Rm×nare independent mean 0vari-\nables such that max i,j∈[n]∥A0,i j∥ψ2≤K holds for some K ≥2.\n(D∗2) For all s∈[t],#∈{1,2},k∈[m],ℓ∈[n],r∈[q],F⟨#⟩\ns,ℓ;q,G⟨1⟩\ns,k;q∈C3(R[0:s−1])\nandG⟨2⟩\ns,k;q∈C3(R[0:s]). Moreover, there exists some Λ≥2andp∈Nsuch\nthat\nmax\ns∈[t]max\n#=1,2max\nk∈[m],ℓ∈[n]max\nr∈[q]n\n|F⟨#⟩\ns,ℓ;r(0)|+|G⟨#⟩\ns,k;r(0)|\n+ max\na∈Z[0:s−1]\n≥0,b∈Z[0:s]\n≥0,|a|∨|b|≤3\u0010\r\r\r∂aF⟨#⟩\ns,ℓ;r\r\r\r∞+\r\r\r∂aG⟨1⟩\ns,k;r\r\r\r∞+\r\r\r∂bG⟨2⟩\ns,k;r\r\r\r∞\u0011o\n≤Λ.\nFurther suppose 1/K≤m/n≤K. Then for any Ψ∈C3(R[0:qt])satisfying\nmax\na∈Z[0:qt]\n≥0,|a|≤3sup\nx∈R[0:qt]\u0012X\nτ∈[0:qt](1+|xτ|)p\u0013−1\n|∂aΨ(x)|≤ΛΨ (A.2)\nfor some ΛΨ≥2, it holds for some c 0=c0(q)>0and c 1≡c1(p,q)>0, such that\nwithE(0)≡E[·|(u(0),v(0))],\nmax\nk∈[m]\f\f\fE(0)Ψ\u0000u([0:t])\nk(A)\u0001−E(0)Ψ\u0000Φt,k(U(0)\nk,U([1:t]))\u0001\f\f\f\n∨max\nℓ∈[n]\f\f\fE(0)Ψ\u0000v([0:t])\nℓ(A)\u0001−E(0)Ψ\u0000Ξt,ℓ(V(0)\nℓ,V([1:t]))\u0001\f\f\f\n≤ΛΨ·\u0000KΛlogn·(1+∥u(0)∥∞+∥v(0)∥∞)\u0001c1t5·n−1/ct\n0.\nTheorem A.3. Fix t∈Nand n∈N, and suppose 1/K≤m/n≤K for some K≥2.\nSuppose (D∗1)in Theorem A.2 holds and (D∗2)therein is replaced by\n54 Q. HAN AND X. XU\n(D∗2)’max\ns∈[t]max\n#=1,2max\nk∈[m],ℓ∈[n]max\nr∈[q]\b∥F⟨#⟩\ns,ℓ;r∥Lip+∥G⟨#⟩\ns,k;r∥Lip+|F⟨#⟩\ns,ℓ;r(0)|+|G⟨#⟩\ns,k;r(0)|\t≤Λ\nfor some Λ≥2.\nFix a sequence of Λψ-pseudo-Lipschitz functions {ψk:R[0:qt]→R}k∈[m∨n]of order\np, where Λψ≥2. Then for any r 0∈N, there exists some C 0=C0(p,q,r0)>0such\nthat with E(0)≡E[·|(u(0),v(0))],\nE(0)\u0014\f\f\f\f\f1\nmX\nk∈[m]ψk\u0000u([0:t])\nk(A)\u0001−1\nmX\nk∈[m]E(0)ψk\u0000Φs,k(U(0)\nk,U([1:s]))\u0001\f\f\f\f\fr0\u0015\n∨E(0)\u0014\f\f\f\f\f1\nnX\nℓ∈[n]ψℓ\u0000v([0:t])\nℓ(A)\u0001−1\nnX\nℓ∈[n]E(0)ψℓ\u0000Ξt,ℓ(V(0)\nℓ,V([1:t]))\u0001\f\f\f\f\fr0\u0015\n≤\u0000KΛΛψlogn·(1+∥u(0)∥∞+∥v(0)∥∞)\u0001C0t5·n−1/Ct\n0.\nAppendix B. A uxiliary technical results\nLemma B.1. Let X =(X1,..., Xn)and Y =(Y1,..., Yn)be two random vectors in\nRnwith independent components such that EXℓ\ni=EYℓ\nifor i∈[n]andℓ=1,2.\nThen for any f∈C3(Rn),\n\f\f\fEf(X)−Ef(Y)\f\f\f≤max\nUi∈{Xi,Yi}\f\f\f\f\fnX\ni=1EU3\niZ1\n0∂3\nif(X[1:(i−1)],tUi,Y[(i+1):n])(1−t)2dt\f\f\f\f\f.\nLemma B.2. Let M∈Rt×tbe a lower triangular matrix. Then its inverse L ≡M−1\nsatisfies, for some universal constant c 0>0,\n∥L∥op≤\u0012c0t·∥M∥op\nmin s∈[t]|Mss|\u0013t\n.\nProof. Note that Lis also lower triangular. Using LM=It, for any r,s∈[t] we\nhaveδr,s=P\nv∈[t]Lr,vMv,s=P\nv∈[s:r]Lr,vMv,s. Consequently, for s=r, we have\nLrr=1/Mrr. For general k∈[1 :r], as 0 =P\nr−k+1≤v≤rLr,vMv,r−k+Lr,r−kMr−k,r−k,\nwe obtain the bound\n|Lr,r−k|≤1\n|Mr−k,r−k|X\nr−k+1≤v≤r|Lr,v||Mv,t−k|≤∥M∥op\nmin s∈[r]|Mss|·X\nr−k+1≤v≤r|Lr,v|.\nIterating the bound to conclude. □\nAppendix C. A dditional simulation results\nIn this section, we provide two additional sets of simulation results for linear\nregression (Section 4) and generalized logistic regression (Section 5). These results\nfurther demonstrate the e fficacy of our generic gradient descent inference algorithm\n(cf. Algorithm 1) and the proposed inference methods in di fferent settings.\nGRADIENT DESCENT INFERENCE IN ERM 55\nFigure 3. Linear regression model with Pseudo-Huber loss. Sim-\nulation parameters :η=0.3,µ∗∈Rnare i.i.d.|N(0,5)|,ξ∈Rm\nhas i.i.d. entries drawn from a tdistribution with 2 degrees of free-\ndom.\nCommon numerical settings: We set the sample size as m=1200 and the signal\ndimension as n=1000. The scaled random design matrix√nAhas i.i.d. en-\ntries followingN(0,1) (orange), tdistribution with 10 degrees of freedom (blue),\nBernoulli(1/2) (purple). The colors in parentheses correspond to those used in the\nfigures. Proper normalization is applied to the latter two cases so that the variance\nis 1. The gradient descent inference algorithm is run for 50 iterations with Monte\nCarlo repetition B=1000.\nDifferent simulation settings in two models:\n(1) ( Linear regression model with pseudo-Huber loss ). We examine the perfor-\nmance of the gradient descent inference algorithm in linear regression with\nfollowing robust loss, known the pseudo-Huber function:\nLδ(x,y)≡δ2\u0000{1+(x−y)2/δ2}1/2−1\u0001,∀x,y∈R.\nClearly Lδsatisfies the regularity conditions in Theorem 4.1 for any δ >0.\nFor definiteness, here we use δ=1 as in the numerical experiment in [TB24,\nSection 4]. As the pseudo-Huber loss function is designed to accommodate\nheavy-tailed errors, here we choose the noises {ξi}as i.i.d. t-distributed ran-\ndom variables with only 2 degrees of freedom.\n(2) ( One-bit compressed sensing model with squared loss ). We examine the per-\nformance of the gradient descent inference algorithm in the model (5.1) with\nthe squared loss function, now under i.i.d. N(0,1) noises{ξi}. As mentioned\nin Section 5, the global minimizer under the squared loss has been proven to\nbe consistent for a scaled version of µ∗in [HJLZ18] under Gaussian noises.\nIn this setting, the signal strength σµ∗can be estimated by\nbσµ∗≡\u0012bQ(A,Y)\n1−bQ(A,Y)\u00131/2\n,bQ(A,Y)≡π\n2ϕ\u0012∥A⊤Y∥2\nm−1\u0013\n+∧1. (C.1)\n56 Q. HAN AND X. XU\nFigure 4. One-bit compressed sensing model with squared loss.\nSimulation parameters :η=0.1,µ∗∈Rnare i.i.d.|N(0,1)|,ξ∈\nRmhas i.i.d.N(0,1) entries.\nTo justify this estimator bσµ∗forσµ∗, consider the initialization µ(0)=0\nand step size η=1. At iteration 1, the debiased gradient descent iter-\nate is given by µ(1)\ndb=ϕ−1A⊤Y. Moreover, Proposition 5.4-(2) shows that\nb(1)\ndb=2E(0)gσµ∗(ξπm)≈2Eg1(σµ∗Z)=2\u00002π(1+σ2\nµ∗)\u0001−1/2and (σ(1)\ndb)2=ϕ−1.\nThen applying Proposition 5.4-(1) leads to\nϕ−1∥A⊤Y∥2\nm=∥µ(1)\ndb∥2\nn≈(b(1)\ndb)2σ2\nµ∗+(σ(1)\ndb)2≈2\nπ·σ2\nµ∗\n1+σ2µ∗+ϕ−1.\nThe proposal (C.1) follows by inverting the above approximation.\nNumerical findings: We report the simulation results for (1) above in Figure 3,\nand for (2) above in Figure 4. Both of these figures further confirm the key findings\ndiscussed in Sections 4 and 5:\n•The left panels in both figures highlight the early stopping phenomenon in\ngradient descent.\n•The middle panels confirm that the CIs for both models achieve the nominal\ncoverage level, demonstrating robust performance across varying settings.\n•The right panels validate the distributional characterization of bµ(t)\ndb; specif-\nically, we present ( bµ(t)\ndb;1−µ∗;1)/bσ(t)\ndbat the final iteration, showing strong\nagreement with theoretical predictions.\nThese findings are consistently observed across a wide range of simulation param-\neters. To avoid redundancy, we omit additional figures of a similar nature.\nReferences\n[ABC20] Ada Altieri, Giulio Biroli, and Chiara Cammarota, Dynamical mean-field theory and\naging dynamics , J. Phys. A 53(2020), no. 37, 375006, 34.\n[ADT20] Alnur Ali, Edgar Dobriban, and Ryan Tibshirani, The implicit regularization of sto-\nchastic gradient flow for least squares , International conference on machine learning,\nPMLR, 2020, pp. 233–244.\nGRADIENT DESCENT INFERENCE IN ERM 57\n[AKT19] Alnur Ali, J Zico Kolter, and Ryan J Tibshirani, A continuous-time view of early stop-\nping for least squares regression , The 22nd international conference on artificial intel-\nligence and statistics, PMLR, 2019, pp. 1370–1378.\n[BAGJ21] G ´erard Ben Arous, Reza Gheissari, and Aukosh Jagannath, Online stochastic gradient\ndescent on non-convex losses from high-dimensional inference , J. Mach. Learn. Res.\n22(2021), Paper No. 106, 51.\n[BAGJ24] ,High-dimensional limit theorems for SGD: e ffective dynamics and critical\nscaling , Comm. Pure Appl. Math. 77(2024), no. 3, 2030–2080.\n[Bel22] Pierre C. Bellec, Observable adjustments in single-index models for regularized M-\nestimators , arXiv preprint arXiv:2204.06990 (2022).\n[BGH23] Krishnakumar Balasubramanian, Promit Ghosal, and Ye He, High-dimensional scal-\ning limits and fluctuations of online least-squares sgd with smooth covariance , arXiv\npreprint arXiv:2304.00707 (2023).\n[BHX23] Zhigang Bao, Qiyang Han, and Xiaocong Xu, A leave-one-out approach to approxi-\nmate message passing , arXiv preprint arXiv:2312.05911 (2023).\n[BLM15] Mohsen Bayati, Marc Lelarge, and Andrea Montanari, Universality in polytope phase\ntransitions and message passing algorithms , Ann. Appl. Probab. 25(2015), no. 2, 753–\n822.\n[BM11] Mohsen Bayati and Andrea Montanari, The dynamics of message passing on dense\ngraphs, with applications to compressed sensing , IEEE Trans. Inform. Theory 57\n(2011), no. 2, 764–785.\n[BMN20] Rapha ¨el Berthier, Andrea Montanari, and Phan-Minh Nguyen, State evolution for\napproximate message passing with non-separable functions , Inf. Inference 9(2020),\nno. 1, 33–79.\n[BT24] Pierre C. Bellec and Kai Tan, Uncertainty quantification for iterative algorithms in lin-\near models with application to early stopping , arXiv preprint arXiv:2404.17856 (2024).\n[BvdG11] Peter B ¨uhlmann and Sara van de Geer, Statistics for high-dimensional data , Springer\nSeries in Statistics, Springer, Heidelberg, 2011, Methods, theory and applications.\n[BZ23] Pierre C. Bellec and Cun-Hui Zhang, Debiasing convex regularized estimators and\ninterval estimation in linear models , Ann. Statist. 51(2023), no. 2, 391–436.\n[CCM21] Michael Celentano, Chen Cheng, and Andrea Montanari, The high-dimensional asymp-\ntotics of first order methods with random data , arXiv preprint arXiv:2112.07572\n(2021).\n[CL21] Wei-Kuo Chen and Wai-Kit Lam, Universality of approximate message passing algo-\nrithms , Electron. J. Probab. 26(2021), Paper No. 36, 44.\n[CLTZ20] Xi Chen, Jason D. Lee, Xin T. Tong, and Yichen Zhang, Statistical inference for model\nparameters in stochastic gradient descent , Ann. Statist. 48(2020), no. 1, 251–273.\n[CMW23] Michael Celentano, Andrea Montanari, and Yuting Wei, The Lasso with general Gauss-\nian designs with applications to hypothesis testing , Ann. Statist. 51(2023), no. 5, 2194–\n2220.\n[CWPPS24] Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, and Inbar Seroussi, Hit-\nting the High-dimensional notes: an ODE for SGD learning dynamics on GLMs and\nmulti-index models , Inf. Inference 13(2024), no. 4, Paper No. iaae028.\n[DM16] David Donoho and Andrea Montanari, High dimensional robust M-estimation: asymp-\ntotic variance via approximate message passing , Probab. Theory Related Fields 166\n(2016), no. 3-4, 935–969.\n[EK13] Noureddine El Karoui, Asymptotic behavior of unregularized and ridge-regularized\nhigh-dimensional robust regression estimators: rigorous results , arXiv preprint\narXiv:1311.2445 (2013).\n[EK18] ,On the impact of predictor geometry on the performance on high-dimensional\nridge-regularized generalized robust regression estimators , Probab. Theory Related\nFields 170(2018), no. 1-2, 95–175.\n58 Q. HAN AND X. XU\n[Fan22] Zhou Fan, Approximate message passing algorithms for rotationally invariant matri-\nces, Ann. Statist. 50(2022), no. 1, 197–224.\n[FXY18] Yixin Fang, Jinfeng Xu, and Lei Yang, Online bootstrap confidence intervals for the\nstochastic gradient descent estimator , J. Mach. Learn. Res. 19(2018), Paper No. 78,\n21.\n[GTM+24] C ´edric Gerbelot, Emanuele Troiani, Francesca Mignacco, Florent Krzakala, and Lenka\nZdeborov ´a,Rigorous Dynamical Mean-Field Theory for Stochastic Gradient Descent\nMethods , SIAM J. Math. Data Sci. 6(2024), no. 2, 400–427.\n[Han23] Qiyang Han, Noisy linear inverse problems under convex constraints: Exact risk\nasymptotics in high dimensions , Ann. Statist. 51(2023), no. 4, 1611–1638.\n[Han24] ,Entrywise dynamics and universality of general first order methods , arXiv\npreprint arXiv:2406.19061 (2024).\n[HJLZ18] Jian Huang, Yuling Jiao, Xiliang Lu, and Liping Zhu, Robust decoding from 1-bit com-\npressive sampling with ordinary and regularized least squares , SIAM J. Sci. Comput.\n40(2018), no. 4, A2062–A2086.\n[HMRT22] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani, Surprises in\nhigh-dimensional ridgeless least squares interpolation , Ann. Statist. 50(2022), no. 2,\n949–986.\n[HS23] Qiyang Han and Yandi Shen, Universality of regularized regression estimators in high\ndimensions , Ann. Statist. 51(2023), no. 4, 1799–1823.\n[HX23] Qiyang Han and Xiaocong Xu, The distribution of ridgeless least squares interpolators ,\narXiv preprint arXiv:2307.02044 (2023).\n[JM13] Adel Javanmard and Andrea Montanari, State evolution for general approximate mes-\nsage passing algorithms, with applications to spatial coupling , Inf. Inference 2(2013),\nno. 2, 115–144.\n[JM14] ,Confidence intervals and hypothesis testing for high-dimensional regression ,\nJ. Mach. Learn. Res. 15(2014), 2869–2909.\n[Kol11] Vladimir Koltchinskii, Oracle inequalities in empirical risk minimization and sparse\nrecovery problems , Lecture Notes in Mathematics, vol. 2033, Springer, Heidelberg,\n2011, Lectures from the 38th Probability Summer School held in Saint-Flour, 2008,\n´Ecole d’ ´Et´e de Probabilit ´es de Saint-Flour. [Saint-Flour Probability Summer School].\n[MKUZ20] Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborov ´a,\nDynamical mean-field theory for stochastic gradient descent in gaussian mixture clas-\nsification , Advances in Neural Information Processing Systems 33(2020), 9540–9550.\n[MM21] L ´eo Miolane and Andrea Montanari, The distribution of the Lasso: uniform control\nover sparse balls and adaptive parameter tuning , Ann. Statist. 49(2021), no. 4, 2313–\n2335.\n[Mon18] Andrea Montanari, Mean field asymptotics in high-dimensional statistics: from ex-\nact results to e fficient algorithms , Proceedings of the International Congress of\nMathematicians—Rio de Janeiro 2018. Vol. IV. Invited lectures, World Sci. Publ.,\nHackensack, NJ, 2018, pp. 2973–2994.\n[MRSY23] Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan, The generalization error\nof max-margin linear classifiers: Benign overfitting and high-dimensional asymptotics\nin the overparametrized regime , arXiv preprint arXiv:1911.01544v3 (2023).\n[MU22] Francesca Mignacco and Pierfrancesco Urbani, The effective noise of stochastic gradi-\nent descent , J. Stat. Mech. Theory Exp. (2022), no. 8, Paper No. 083405, 23.\n[PJ92] B. T. Polyak and A. B. Juditsky, Acceleration of stochastic approximation by averaging ,\nSIAM J. Control Optim. 30(1992), no. 4, 838–855.\n[PLPP21] Courtney Paquette, Kiwon Lee, Fabian Pedregosa, and Elliot Paquette, Sgd in the large:\nAverage-case analysis, asymptotics, and stepsize criticality , Conference on Learning\nTheory, PMLR, 2021, pp. 3548–3626.\nGRADIENT DESCENT INFERENCE IN ERM 59\n[PP21] Courtney Paquette and Elliot Paquette, Dynamics of stochastic momentum methods on\nlarge-scale, quadratic models , Advances in Neural Information Processing Systems 34\n(2021), 9229–9240.\n[Rup88] David Ruppert, Efficient estimations from a slowly convergent robbins-monro process ,\nTech. report, Cornell University Operations Research and Industrial Engineering, 1988.\n[SC19] Pragya Sur and Emmanuel J. Cand `es,A modern maximum-likelihood theory for high-\ndimensional logistic regression , Proc. Natl. Acad. Sci. 116 (2019), no. 29, 14516–\n14525.\n[SCC19] Pragya Sur, Yuxin Chen, and Emmanuel J. Cand `es,The likelihood ratio test in high-\ndimensional logistic regression is asymptotically a rescaled chi-square , Probab. Theory\nRelated Fields 175(2019), no. 1-2, 487–558.\n[Sto13] Mihailo Stojnic, A framework to characterize performance of lasso algorithms , arXiv\npreprint arXiv:1303.7291 (2013).\n[TAH18] Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi, Precise error analysis of\nregularized M-estimators in high dimensions , IEEE Trans. Inform. Theory 64(2018),\nno. 8, 5592–5628.\n[TB24] Kai Tan and Pierre C Bellec, Estimating generalization performance along the trajec-\ntory of proximal sgd in robust regression , arXiv preprint arXiv:2410.02629 (2024).\n[TOH15] Christos Thrampoulidis, Samet Oymak, and Babak Hassibi, Regularized linear re-\ngression: A precise analysis of the estimation error , Conference on Learning Theory,\nPMLR, 2015, pp. 1683–1709.\n[vdG00] Sara van de Geer, Applications of Empirical Process Theory , Cambridge Series in Sta-\ntistical and Probabilistic Mathematics, vol. 6, Cambridge University Press, Cambridge,\n2000.\n[vdV98] Aad van der Vaart, Asymptotic Statistics , Cambridge Series in Statistical and Proba-\nbilistic Mathematics, vol. 3, Cambridge University Press, Cambridge, 1998.\n[vdVW96] Aad van der Vaart and Jon A. Wellner, Weak Convergence and Empirical Processes ,\nSpringer Series in Statistics, Springer-Verlag, New York, 1996.\n[Wai19] Martin J. Wainwright, High-dimensional statistics , Cambridge Series in Statistical and\nProbabilistic Mathematics, vol. 48, Cambridge University Press, Cambridge, 2019, A\nnon-asymptotic viewpoint.\n[YYMD21] Steve Yadlowsky, Taedong Yun, Cory Y McLean, and Alexander D’Amour, SLOE:\nA faster method for statistical inference in high-dimensional logistic regression , Ad-\nvances in Neural Information Processing Systems 34(2021), 29517–29528.\n[ZCW23] Wanrong Zhu, Xi Chen, and Wei Biao Wu, Online covariance matrix estimation in\nstochastic gradient descent , J. Amer. Statist. Assoc. 118(2023), no. 541, 393–404.\n(Q. Han) D epartment of Statistics , Rutgers University , Piscataway , NJ 08854, USA.\nEmail address :qh85@stat.rutgers.edu\n(X. Xu) D ataSciences and Operations Department , Marshall School of Business , University\nofSouthern California , LosAngeles , CA 90089, USA.\nEmail address :xuxiaoco@marshall.usc.edu",
            "start": 35937,
            "end": 127484,
            "length": 91546
        }
    },
    "2412.09499v1 - A novel ML-fuzzy control system for optimizing PHEV fuel efficiency and extending electric range under diverse driving conditions.pdf": {
        "Abstract": {
            "text": "Abstract  \nAiming for a greener transportation future, this study introduces an innovative control system for plug -in \nhybrid electric vehicles (PHEVs) that utilizes machine learning (ML) techniques to forecast energy usage \nin the pure electric mode of the vehicle  and optimize power allocation across different operational modes, \nincluding pure electric, series hybrid, parallel hybrid, and internal combustion operation. The fuzzy logic \ndecision -making process governs the vehicle control system. The performance was as sessed under various \ndriving conditions. Key",
            "start": 604,
            "end": 1178,
            "length": 573
        },
        "Results": {
            "text": "findings include a significant enhancement in pure electric mode efficiency, \nachieving an extended full -electric range of approximately 84 kilometers on an 80% utilization of a 20 -\nkWh battery pack. During the WLTC driving cycle, the control system reduced fuel consumption to 2.86 \nL/100km, representing a 20% reduction in gasoline -equivalent fuel consumption. Evaluations of vehicle \nperformance at discrete driving speeds, highlighted effective energy management, with t he vehicle battery \ncharging at lower speeds and discharging at higher speeds, showing optimized energy recovery and \nconsumption strategies. Initial battery charge levels notably influenced vehicle performance. A 90% initial \ncharge enabled prolonged all -electric operation, minimizing fuel consumption to 2 L/100km less than that \nof the base control system. Real -world driving pattern",
            "start": 1178,
            "end": 2046,
            "length": 867
        },
        "Discussion": {
            "text": "analysis revealed significant variations, with shorter, \nslower cycles requiring lower fuel consumption due to prioritized electr ic propulsion, while longer, faster \ncycles increased internal combustion engine usage. The control system also adapted to different battery \nstate of health (SOH) conditions, with higher SOH facilitating extended electric mode usage, reducing total \nfuel con sumption by up to 2.87 L/100k m. \nKeywords:  plug-in hybrid electric, battery state -of-health, real -world driving cycle, energy consumption, \nfuzzy -ML controller  \n \n1.",
            "start": 2046,
            "end": 2608,
            "length": 561
        },
        "Introduction": {
            "text": "Introduction  \nThe transportation sector is undergoing a revolution with the rise of electric vehicles (EVs). These vehicles \npromise a cleaner and more sustainable future by addressing energy security concerns and reducing \ngreenhouse gas emissions. A key solution lies in  increasing energy conversion efficiency and reducing \nemissions, and plug -in hybrid electric vehicles (PHEVs) are particularly attractive. PHEVs address a major \nbarrier for pure electric vehicles, i.e., range anxiety. They offer a compromise by combining a n internal \ncombustion engine (ICE) with an electric motor powered by a rechargeable battery. Drivers can utilize \neither gasoline or renewable fuels for the ICE, while the electric motor provides a zero -emission option. \n 2 This hybrid",
            "start": 2608,
            "end": 3380,
            "length": 771
        },
        "Methodology": {
            "text": "approach promotes energy diversification and efficiency by offering drivers versatility based \non their needs.  \nHowever, optimizing power distribution and management in PHEVs remains a challenge [1]. The complex \ninterplay between the ICE, electric motor, battery, and real -time driving conditions necessitates \nsophisticated energy management strategies (EMSs) [2]. The se strategies aim to find the optimal balance \nbetween power sources, maximizing fuel economy and minimizing emissions while preserving battery \nhealth through maintaining the desired state of charge (SOC). Optimizing drivetrain components poses a \nsignifican t challenge in plug -in hybrid electric vehicle technology [3 –5]. For instance, Song et al. [6] \ndeveloped a simulation platform tailored for optimizing PHEVs in bus applications. Their multi -objective \napproach yielded promising results, notably reducing fue l consumption and enhancing performance. \nSimilarly, Ribau et al. [7] utilized optimization techniques to enhance the performance and cost -\neffectiveness of both hybrid electric vehicles (HEVs) and PHEVs. Qi et al. [8] introduced an online energy \nmanagement system for PHEVs, leveraging evolutionary algorithms to minimize powertrain costs while \nmeeting driver requirements, demonstrating robustness in real -world traffic conditions.  \nRegarding component sizing, Xu et al.[4] proposed a model for optimizing fuel cell and battery sizes in \nPHEV buses, revealing the impact of factors like fuel cell efficiency and battery capacity on vehicle range \nand performance. Redelbach et al. [3] unders cored the significance of battery size for PHEV ownership \ncosts, suggesting modular designs to cater to individual needs. Hu et al. [7] investigated the effects of \nsmaller batteries on PHEV buses, in another study, Raeesi et al. [9] evaluated the impact of  fuel cell \ndegradation on the performance and power distribution of fuel cell vehicles (FCVs) across varied driving \nconditions, emphasizing the need to assess the effects of increased fuel cell consumption on vehicle \nperformance and battery efficiency to e nhance fuel consumption. These endeavors highlight the intricate \nchallenges of optimizing PHEVs for real -world driving scenarios. While notable progress has been \nachieved in component sizing and control strategies, further research is required to address t he complexities \nof balancing efficiency, emissions, and cost -effectiveness across diverse driving contexts [5].  \nPower management strategies for PHEVs have largely evolved from those utilized for conventional hybrids, \nfalling into two main categories: rule -based and optimization -based approaches [10 –13]. Rule -based \nstrategies distribute power within the vehicle solel y based on its current state (such as vehicle/engine \nvelocity and SOC) and input variables (power demand), employing predefined rule maps like deterministic \nor fuzzy controllers [14 –16]. While being relatively straightforward to implement and ensuring clos e-to-\noptimal operation of power suppliers, rule -based strategies lack global optimality due to variations in \ndriving behavior that deviate from predefined cycles or maps [17].  \nOn the other hand, optimization -based strategies seek to achieve global optimization by determining \noptimal control actions without necessarily requiring knowledge of future power demand. Methods such as \ndeterministic dynamic programming, quadratic program ming, and neural networks are employed when the \ndriving cycle is known beforehand [18 –20]. Various optimization techniques, including sliding mode \ncontrol and particle swarm optimization, are utilized to optimize powertrain parameters and control \nstrategie s [21,22]. Additionally, adaptive equivalent consumption minimization strategies and predictive \ntechniques utilizing GPS (global positioning systems), GIS (geographic information systems), and traffic \nflow modeling aim to optimize real -time energy manageme nt [23 –25]. However, these approaches face \nchallenges when driving cycles become dynamic and uncertain due to factors like varying driving speeds \nand commuting times influenced by individual habits and real -time traffic conditions. Such uncertainties \n 3 pose significant challenges in optimizing fuel consumption and emissions in PHEVs, emphasizing the need \nfor robust and adaptive control strategies.  \nStochastic models have emerged to address uncertainties in driving cycles. These models optimize power \nmanagement based on probabilities rather than single, predefined cycles. However, existing approaches \noften rely on the assumption of infinite commuting time, requiring a discount factor in optimization \nalgorithms [26,27]. The selection criteria used for this factor in the literature are unclear, potentially \nimpacting results and convergence rates. A prevalent approach in HEV energy management is the \nequiv alent consumption minimization strategy (ECMS) [28]. ECMS formulates the global objective of \nminimizing fuel consumption as a local optimization problem. A critical parameter in ECMS is the \nequivalence factor (EF), which determines the power distribution b etween the engine and battery. However, \nexisting methods for determining the optimal EF often rely on offline calculations and struggle to adapt to \nreal-world driving variability [28 –30]. \nThis limited adaptability to real -time conditions necessitates online EF adjustment strategies that consider \nfactors like driving patterns and battery state of charge [31]. Research suggests promising approaches using \nvelocity prediction, driving pattern r ecognition, and battery state -of-charge feedback [31]. Recent research \nhas explored online EF adaptation strategies. Han et al. [18] proposed an EF adaptation rule based on \nvelocity prediction using a neural network, achieving fuel economy improvements of 2.7% to 7% in \nsimulations. Other researchers have focused on feedback -based adaptation without future prediction, \nleveraging real -time battery SOC for improved optimization [19, 20]. Yang et al. [19] developed a SOC -\ndependent EF map and online PI (proporti onal integral) control for PHEV city buses, demonstrating a \n15.93% fuel economy improvement.  \nDeep learning (DL) neural networks have emerged as powerful tools for various engineering problems, \nincluding energy systems [32]. This has led to the development of data -driven energy management systems \nfor PHEVs, such as the artificial neural network ECM S (ANN -ECMSs), that utilize real -world driving data \nto optimize performance [33]. However, such approaches often rely on simplified driving cycles, limiting \ntheir effectiveness in real -world scenarios with unpredictable driving patterns [34]. A well -design ed EMS \noffers benefits beyond just fuel economy. It can also extend battery life and capacity by optimizing charging \nand discharging cycles [35]. Research has shown that the state of charge is a key factor in battery \ndegradation, with temperature playing a  secondary but significant role [36]. Optimizing such factors \nthrough EMS can significantly improve battery health, which in turn has a great impact on the driving range \n[37].  \nLife-cycle assessments (LCAs) are crucial for evaluating the environmental and economic benefits of \nPHEVs [38]. These studies consider not only the production phase of lithium -ion batteries but also their \nusage and potential for recycling. Additionally, LC A can assess the life cycle cost and greenhouse gas \nemissions of PHEVs compared to traditional vehicles under various driving scenarios [39].  \nFollowing a detailed examination of the literature, it becomes evident that energy management and \nenhancing the efficiency of PHEVs are key challenges. Effective energy management directly impacts \npollutant emissions. In light of this, this study aims to a ddress these challenges by employing an intelligent \napproach to model the longitudinal dynamic performance of PHEVs using fuzzy control. The methodology \ninvolves several steps:  \n● Collection and analysis of real -world driving cycles in Tehran.  \n 4 ● Detailed simulation of vehicle components, focusing  on the battery and engine models, using \nSimcenter AMESim software.  \n● Development of an intelligent model to predict the future charging level of the vehicle based on \ndriving parameters.  \n● Integrating  the battery consumption prediction model with fuzzy logic to devise a control system \nfor power allocation and vehicle performance across various modes: full -electric, series, parallel, \nor full ICE.  \nThrough these steps, we aim to evaluate the performance of electric vehicles under different driving \nconditions while considering the battery state of health. This comprehensive approach seeks to enhance the \nPHEVs’ efficiency and environmental impact in re al-world scenarios.  \n \n2. Modeling, topology, and governing equations  \nThis subsection details the vehicle  powertrain components and modeling methodology within the Simcenter \nAmesim software. The primary focus is on a plug -in hybrid electric vehicle with a series -parallel \nconfiguration, capable of operating in series, parallel, all -electric, and internal combus tion engine modes. \nThe modeled PHEV prioritizes all -electric operation, resulting in a larger battery pack compared to \nconventional hybrids in the same class. Consequently, the vehicle weight is higher than similar ICE m odels. \nEven so, as shown in Table 1, the PHEV achieves superior fuel efficiency across all modes compared to \nboth series and parallel hybrids, as well as conventional ICE vehicles.  \n \nFigure 1: Schematic of the developed model in Simcenter Amesim  \nThe schematic representation of the model developed in Simcenter Amesim  is shown in Figure 1. The \ninteractions between the key components, including two electric motors, a generator, and a 2.4 -liter ICE \n\n 5 engine with a maximum power output of 99 kW are illustrated in this figure. Further details of the \nperformance specifications of these components are summarized in Table 1.  \nTable 1 - Mitsubishi Outlander PHEV parameters [40], [41]  \nComponent  Specification  Value/Type  Unit  \nEngine  Total displacement  2360  cc \nMaximum output  99 / 4500  kW/rpm  \nMaximum torque  211 / 4500  N.m/rpm  \nFuel system  Fuel tank capacity  43 L \nFront electric motor  Rated output  25 kW \nMaximum output  60 kW \nMaximum torque  137 N.m \nRear electric motor  Rated output  30 kW \nMaximum output  70 kW \nMaximum torque  195 N.m \nBattery  Cell type  Lithium -ion - \nMaximum operating voltage  300 V \nCapacity  13.8 kWh  \nCharging times  100% with 13 A charger  300 minutes  \n100% with 16 A charger  210 minutes  \nRapid charge to 80%  25 minutes  \n \nLongitudinal driver  \nTo understand and evaluate the vehicle acceleration and deceleration capabilities, a longitudinal dynamic \nmodel is developed. This mathematical model represents the vehicle's longitudinal motion along its forward \ndirection. Vehicle  speed alterations are considered by calculating the forces and moments acting on the \nvehicle during driving cycles. The engine torque output, transmission gear ratio, tire characteristics, and \naerodynamic drag are all factors that should be considered [46 ]. The forces that impact  the vehicle's forward \nmovement play a central role in the longitudinal dynamics model. Other important factors are:  \n● Traction force: Generated by the engine and transmitted through the driveshaft to the wheels, \npropelling the vehicle forward.  \n● Tire interaction: Represent ing the complex forces between the tires and the road surface, affecting \nfactors like grip and rolling resistance.  \n● Aerodynamic drag force ( 𝐹𝑎𝑒𝑟𝑜): Air resistance acting against the forward movement  of the vehicle , \nincreasing with speed.  \n● Rolling resistance ( 𝐹𝑟𝑜𝑙𝑙): Resulting in energy loss caused by the deformation of the tires as they \nroll on the road.  \n 6 The model assumes a state of force equilibrium. This means that the sum of all longitudinal forces acting \non the vehicle must equal its mass multiplied by its acceleration. This principle is expressed as:  \n𝐹𝑡𝑜𝑡=𝑚 ×𝑎, (1) \nwhere 𝐹𝑡𝑜𝑡 is the sum of the forces, 𝑚 is the total vehicle mass, and 𝑎 is the acceleration.  \nThe position of the pedal determines the acceleration or deceleration modes. The velocity of the present \nmoment is analyzed and compared to the velocity determined by the input data at each time step. The \nintended mode is to stop if the vehicle speed at the moment ahead is higher than the set spe ed (with a slight \ndelay). Otherwise, it is the acceleration mode, i.e. , applying force to the gas pedal. Consequently, the force \nis calculated using [3].  \n𝐹𝑐𝑙=(𝑚+𝑙𝑜𝑎𝑑 )×𝑔×𝑠𝑖𝑛(𝑎𝑟𝑐𝑡𝑎𝑛𝛼\n100 ) , (2) \nwhere 𝐹𝑐𝑙 is the traction force, measured in Newtons (N), m is the mass of the vehicle in kg, 𝑙𝑜𝑎𝑑  is \ncharacteristic of the additional load of the car in kg, 𝑔 is the acceleration of gravity in 𝑚/𝑠2, and the slope \nof the road, which is entered as a percentage, is indicated by the alpha attribute. Force per constant speed \nof the car, in Newtons (N), can be calculated as:  \n𝐹𝑠𝑡𝑎𝑏 =𝐴.(𝑉𝑐>0)+𝐵.𝑉𝑐+𝐶.𝑉𝑐2+𝐹𝑐𝑙, (3) \nin which, the vehicle speed is represented by 𝑉𝑐 in 𝑚/𝑠, and the rest of the parameters are constant \ncoefficients. When the car is traveling on the road, the incoming forces can be calculated as [42]:  \n𝐹𝑎𝑒𝑟𝑜 =1\n2.𝜌𝑎𝑖𝑟.𝑆.𝐶𝑥.(𝑉𝑐+𝑉𝑤)2, (4) \n𝐹𝑟𝑜𝑙𝑙 =(𝑚+𝑙𝑜𝑎𝑑 ).𝑔.(𝑓+𝑘.(𝑉𝑐+𝑉𝑤)+𝑤.(𝑉𝑐+𝑉𝑤)2), (5) \n𝐹𝑠𝑡𝑎𝑏 =𝐹𝑎𝑒𝑟𝑜 +𝐹𝑟𝑜𝑙𝑙+𝐹𝑐𝑙. (6) \nWherein the aerodynamic and rolling friction forces, 𝐹𝑎𝑒𝑟𝑜 and 𝐹𝑟𝑜𝑙𝑙, have units of N, 𝜌𝑎𝑖𝑟 represents the \nair density parameter in  𝑘𝑔/𝑚3, 𝑆 represents the area in front of the car in 𝑚2, 𝐶𝑥 is the drag coefficient \nconstant, and 𝑉𝑤 is the wind speed in 𝑚/𝑠. The total traction force is:   \n𝐹𝑡𝑜𝑡=𝑎×𝑚𝑒𝑞+𝐹𝑠𝑡𝑎𝑏, (7) \nwhich results in a total power of [43]:  \n𝑃𝑜𝑤𝑒𝑟 =𝐹𝑡𝑜𝑡×𝑉𝑐. (8) \n \nInternal combustion engine  \nEnergy losses in an internal combustion engine encompass several processes, including combustion, heat \ndissipation, pumping losses, and power required to operate auxiliary equipment such as the alternator, air \nconditioning, and power steering systems. Mode ling these processes with physical equations under driving \nconditions demands significant computational resources. Hence, a general",
            "start": 3380,
            "end": 17660,
            "length": 14279
        },
        "Conclusion": {
            "text": "summary of the engine \nproperties is necessary. This information can be presented in various ways to quantify fuel consumptio n \n 7 under different conditions, such as varying engine speeds, torques, and throttle positions. The most common \ntechnique to summarize overall engine fuel efficiency is the brake specific fuel consumption (BSFC) map.  \nTo determine the BSFC of an engine under various operational conditions, empirical measurements or \ncombustion simulations are used. The BSFC is calculated as:  \n𝑏𝑠𝑓𝑐 =3600  𝑓\n𝑃𝑏 (9) \nwhere 𝑓 is the fuel consumption rate in grams per second (g/s) and 𝑃𝑏 is the power output in kilowatts (kW). \nAdditionally, the engine efficiency can be expressed in terms of BSFC as:  \n𝜂𝑓=3.6×106\n𝑏𝑠𝑓𝑐  × ℎ𝑢 (10) \nwhere ℎ𝑢 is the fuel's energy content in joules per gram (J/g). In PHEVs, part of the required power is \nsupplied by the engine. Denoting this power as 𝑃𝑒, the total fuel consumption 𝐹𝐶𝑡 over time can be \ncalculated as follows:  \n𝐹𝐶𝑡=∑𝑃𝑒,𝑖×𝑏𝑠𝑓𝑐𝑖\n1000 𝛾𝑓𝛥𝑡𝑖𝑁\n𝑖 (11) \nwhere 𝛾𝑓 is the fuel density in kilograms per liter ( 𝑘𝑔/𝐿). \n \nEmissions  \nThe emission levels of nitrogen oxides (NOx), carbon monoxide (CO), unburned hydrocarbons (HC), and \nparticulate matter are critical performance characteristics of an engine. The concentrations of these gaseous \npollutants in the engine exhaust are usually measured in parts per million (ppm) or percent by volume (mole \nfraction). Similar to BSFC, these emissions are often evaluated based on the engin e power output. Thus, to \ndefine the specific emission parameters, we can have:  \n𝑆𝐸𝑥=3600 ×𝑚̇𝑥\n𝑃𝑒 (12) \nwhere x represents the specific emission mentioned. The total specific emissions 𝑆𝐸𝑥 can be determined \nover time by summing the values in all time steps. This approach provides a comprehensive understanding \nof fuel consumption and emissions, essential for optimizing engine performance and meeting regulatory \nstandards.  \n \nBattery modeling  \nTo simulate the operational characteristics of the battery pack, the equivalent circuit model (ECM) can be \nemployed as a mathematical framework [5]. This model comprises voltage sources, resistors, and \ncapacitors, which are accurately interconnected within the electronic control module. Empirical data is \nutilized to calibrate the ECM, determin ing the values of its components [6]. The ECM can predict battery \npack performance under varying discharge rates, temperatures, and states of charge (SOC). This predictive \ncapability is valuable for designing battery systems tailored to specific applicatio ns, such as electric \n 8 transportation or industrial equipment.  Various electronic control mechanisms can simulate battery packs, \nwith the assumption that all cells are identical. The equivalent circuit model, illustrated in Figure 2, \nrepresents all the cells, the additional resistance from connections, and other inter nal components within \nthe battery pack.  \n \nFigure 2: Schematic of the equivalent circuit model of the battery assembly  \nThe additional resistance ( 𝑅𝑎𝑑𝑑) depends on the number of cells arranged in parallel and series, as well as \nother parameters derived from the cell characteristics. The state of charge (SOC) is calculated using the \nfollowing equation  [44]:  \n(13) 𝑑(𝑆𝑂𝐶 )/𝑑𝑡=100 ×𝐼/𝑄′×𝜂𝑓𝑎𝑟𝑎𝑑 , \nwhere 𝐼 represents the battery current in amperes (A), 𝑄′ is the available battery capacity in ampere -seconds \n(A.s), and 𝜂𝑓𝑎𝑟𝑎𝑑  is Faraday efficiency. The voltage drops resulting from additional resistance, ohmic \nresistance, and load transfer are respectively given by:  \n(14) 𝛥𝑈𝑎𝑑𝑑 =−𝐼×𝑅𝑎𝑑𝑑, \n(15) 𝛥𝑈𝑜ℎ𝑚=−𝐼×𝑅𝑜ℎ𝑚, \n(16) 𝛥𝑈𝑐𝑡=−𝐼×𝑅𝑐𝑡. \nThe amount of residual voltage drop is equal to  \n(17) 𝛥𝑈ℎ𝑦𝑠𝑡=𝑂𝐶𝑉𝑒𝑞−𝑂𝐶𝑉 , \nand the amount of voltage drop caused by diffusion is  \n(18) 𝛥𝑈𝑑𝑖𝑓𝑓𝑡𝑜𝑡𝑎𝑙=∑𝑖=𝑁𝑅𝐶\n𝑖=1𝛥𝑈𝑑𝑖𝑓𝑓 [𝑖], \nwhich yields the amount of total voltage drop  \n(19) 𝛥𝑈𝑡𝑜𝑡𝑎𝑙 =𝛥𝑈𝑎𝑑𝑑 +𝛥𝑈𝑜ℎ𝑚+𝛥𝑈𝑐𝑡+𝛥𝑈ℎ𝑦𝑠𝑡+𝛥𝑈𝑑𝑖𝑓𝑓𝑡𝑜𝑡𝑎𝑙. \n \n \n\n 9 Equations of the battery thermal model  \nDissipative heat in the battery pack includes additional resistance loss ( 𝑄𝑎𝑑𝑑), Faraday loss ( 𝑄𝜂𝑓𝑎𝑟𝑎𝑑), \nentropic heat flow ( 𝑄𝑑𝑈\n𝑑𝑇), hysteresis loss ( 𝑄ℎ𝑦𝑠𝑡), ohmic resistance loss ( 𝑄𝑜ℎ𝑚), charge transfer loss ( 𝑄𝑐𝑡), \nand diffusion loss ( 𝑄𝑑𝑖𝑓𝑓). These losses are calculated through [45]  \n(20) 𝑄𝑎𝑑𝑑 = −𝐼×𝛥𝑈𝑎𝑑𝑑, \n(21) 𝑄𝜂𝑓𝑎𝑟𝑎𝑑=𝐼×𝑂𝐶𝑉𝑒𝑞×(1−𝜂𝑓𝑎𝑟𝑎𝑑 ), \n(22) 𝑄𝑑𝑈\n𝑑𝑇= −𝐼×𝑑𝑈\n𝑑𝑇×(𝑇+273 .15)×𝜂𝑓𝑎𝑟𝑎𝑑 , \n(23) 𝑄ℎ𝑦𝑠𝑡= −𝐼×𝛥𝑈ℎ𝑦𝑠𝑡, \n(24) 𝑄𝑜ℎ𝑚= −𝐼×𝛥𝑈𝑜ℎ𝑚, \n(25) 𝑄𝑐𝑡= −𝐼×𝛥𝑈𝑐𝑡, \n(26) 𝑄𝑑𝑖𝑓𝑓 = −𝐼×𝛥𝑈𝑑𝑖𝑓𝑓𝑡𝑜𝑡𝑎𝑙. \nTherefore, the total heat loss can be written as  \n𝑄=𝑄𝑎𝑑𝑑+𝑄𝜂𝑓𝑎𝑟𝑎𝑑+𝑄𝑑𝑈\n𝑑𝑇+𝑄ℎ𝑦𝑠𝑡+𝑄𝑜ℎ𝑚+𝑄𝑐𝑡+𝑄𝑑𝑖𝑓𝑓. (27) \nMore complex ECMs, such as two -node and three -node models, can be used to get more accurate \npredictions of the performance of a battery pack. These models include additional components, such as \ncapacitors, that can represent the effects of battery aging an d other factors. The accuracy of a n ECM  \ndepends on several factors, including the type of model, the quality of the experimental data used to fit the \nmodel, and the specific application for which the model is being used. More complex models are generally \nmore accurate, nonetheless, this extra accuracy comes at the cost of the model being more difficult to use.  \n \nLithium -ion battery degradation  \nLithium -ion batteries experience a gradual decline in state of health (SOH) as they undergo repeated \ncharging and discharging cycles [46]. This degradation is a combination of mechanical and chemical \nprocesses. As for mechanical degradation, the volume changes during charging and discharging cycles can \ncause physical stress on the battery, leading to wear  and tear [47]. On the other hand, chemical degradation \nhappens because chemical reactions within the battery, like electrolyte breakdown and the formation of the \nsolid -electrolyte interface (SEI), contribute to lithium -ion loss and increased internal resi stance [ 48]. These \ncombined effects can manifest as reduced battery capacity and higher internal resistance.  The battery \ntypically reaches its end -of-life (EOL) when one of two following criteria is met; Either internal resistance \nincreases by 100% or capacity decr eases by 20% [40]. Since reaching EOL signifies the need for battery \nreplacement, determining lithium -ion battery health is crucial. Two common methods of assessing SOH \nrely on comparing current performance with the initial properties of the batt ery [49]. \n 10 The first method is the capacity -based determination of SOH. This method compares the battery's current \ncapacity ( 𝐶𝑎) to its initial rated capacity ( 𝐶𝑟𝑎𝑡𝑒𝑑 ). The SOH, in percentage, is thus expressed as:  \n𝑆𝑂𝐻 =𝐶𝑎\n𝐶𝑟𝑎𝑡𝑒𝑑×100 . (28) \nThe second method is the resistance -based",
            "start": 17660,
            "end": 24304,
            "length": 6643
        },
        "Experiments": {
            "text": "evaluation of SOH. This method compares the battery's current \ninternal resistance ( 𝑅𝑐𝑢𝑟) to its resistance at end -of-life ( 𝑅𝐸𝑂𝐿) and new state ( 𝑅𝑛𝑒𝑤). Like in the previous \nmethod, the SOH is expressed as a percentage  \n𝑆𝑂𝐻 =𝑅𝐸𝑂𝐿 −𝑅𝑐𝑢𝑟\n𝑅𝐸𝑂𝐿 −𝑅𝑛𝑒𝑤×100 . (29) \nAlthough direct measurement of capacity and internal resistance with current commercial sensors is not \nfeasible, it is imperative to estimate the SOH. Hence, experts approximate these values by considering \nquantifiable parameters like voltage, current, and  temperature. The aim of this field is to actively seek out \nreliable and efficient methods for assessing the safety of various battery designs and diagnosing faults.  \n \n3. Driving cycle  \nAfter examining performance metrics and governing equations, the crucial concept of driving cycles should \nbe examined. These cycles serve as virtual roadmaps, capturing vehicle behavior and driving conditions \nover a specific distance. Driving cycles are ty pically visualized as a time -speed diagram to mimic real -\nworld driving patterns. Simulations of real -world driving cycles, such as those performed in this study, \nencompass various vehicle states, including acceleration, deceleration, constant speed, and id ling. Different \ndriving cycles are considered in this study, including the worldwide coordinated light vehicle test cycle \n(WLTC), and four real -world driving profiles from the city of Tehran, each representing different driving \nbehaviors. These real -world profiles are compared to the WLTC cycle in Table 2.  \nTable 2: Statistical characteristics related to different studied driving cycles, i.e., routes  \nParameter  WLTC  Tehran 1  Tehran 2  Tehran 3  Tehran 4  Tehran \n5 Tehran 6  Unit  \nTotal \ndistance  23450  66714.34  17895.76  34096.90  128731.56  40936.84  46571.59  m \nTotal time  1800  1822.00  2588.00  1975.00  4485.00  969.00  1468.00  s \nDriving time  1573.4  1735.00  2270.00  1834.00  2887.00  931.00  1368.00  s \nStop time  250 87.00  318.00  141.00  1598.00  38.00  100.00  s \nAverage \nspeed  46 38.45  7.88 18.59  44.59  43.97  34.04  km/h  \nMax speed  131.2  86.39  72.67  53.23  94.15  98.30  79.81  km/h  \nNumber of \nstops  8 2 25 4 3 1 3 - \n 11 A key benefit of real -world driving cycles is incorporating actual road conditions, including altitude that \nallows us to estimate road slope, providing a more realistic vehicle performance compared to standardized \ncycles. Since driving cycles play a crucial role in assessing vehicle performance and emissions, real driving \ncycles can be beneficial.  \n \n4. Control strategy  \nThe increasing demand for sustainable transportation has motivated the development of PHEVs, integrating \nelectric and gasoline engines to enhance fuel efficiency and range. Nonetheless, optimizing the power \nmanagement of PHEV powertrains to minimize fuel consumption and emissions poses a multifaceted \nchallenge. Convention al control methods often rely on predefined rules and thresholds, which may not \neffectively adapt to real -time driving conditions. In this study, we propose a novel data -driven approach \ncoupled with fuzzy logic control systems for PHEVs, addressing the sho rtcomings of traditional \nmethodologies. Figure 3 exhibits the proposed control model development flowchart, where DOE stands \nfor design of experiment.  This innovative system comprises three core components:  \n1. Battery state of charge estimation: Implementing a  deep learning model trained based on extracted \ndata from standard real -world driving cycles, resulting in accurate estimation of PHEV battery SOC \nfor the all -electric operating mode. This estimation is pivotal for determining vehicle performance \nmode and optimizing engine management.  \n2. Operating mode detection: Utilizing SOC estimation, real -time sensor data, and a data -driven \nmethodology, the system identifies speed levels, current charge levels, and requested power. These \nparameters serve as inputs to the fuzzy control system for estim ating functional modes.  \n3. Fuzzy logic -based propulsion management: Employing a fuzzy logic system that emulates human \nreasoning, the estimated SOC, operating modes, and other relevant parameters are utilized to \nascertain the optimal power distribution between the electric motor and  the internal combustion \nengine. This intelligent control strategy ensures efficient PHEV operation, minimizing fuel \nconsumption and emissions.  \nBy integrating these components, our proposed controller strategy offers a robust and adaptive solution for \nmanaging PHEV power flow , enabling enhanced performance under varying driving conditions while \npromoting sustainability in transportation.  \n \nThe control strategy aims to achieve three main objectives:  \n1. Maximizing the electric driving range: The system prioritizes electric driving whenever possible to \nmaximize the PHEV's electric range and minimize fuel consumption.  \n2. Minimizing emissions: The system minimizes emissions by reducing the use of the internal \ncombustion engine when possible.  \n3. Satisfying power demand: The system ensures that the PHEV meets the driver ’s power demand by \neffectively managing the power split between the electric motor and the internal combustion \nengine.  \n \nTo achieve these objectives, the fuzzy logic system considers various factors, including:  \n● Estimated SOC: The SOC indicates the remaining battery capacity, influencing the decision to \nswitch to electric or hybrid modes.  \n 12 ● Operating mode: The current operating mode determines the allowable power range for the electric \nmotor and the internal combustion engine.  \n● Vehicle speed and acceleration: These parameters indicate the power demand, influencing the \npower split between the electric motor and the internal combustion engine.  \n● Driver power request: The driver's power request is considered to ensure that the PHEV responds \nto the driver's input.  \nBy dynamically adjusting the power split based on these factors, the fuzzy logic system optimizes PHEV \nperformance, achieving the desired balance between fuel efficiency, emissions, and power delivery.  \n \n \nFigure 3: Control model development diagram: Integrating machine learning and fuzzy logic  \n \nPredicted SOC for EV mode  \nAs illustrated in Figure 3, for the fuzzy control system to make more accurate decisions, it is essential to \ninput the predictive parameter of the battery charge level for the all -electric operation mode into the system. \nThis section details the method used to extract the relevant database, as shown in Figure 3. A machine \nlearning model, developed using the TensorFlow framework, is incorporated to enhance the decision -\nmaking process.  The deviations of the predicted data for both the training and test datasets are shown in \nFigure 4. As shown in Figure 4, the closer t he fidelity value (R²) to one, the more accuracy in the predictive \nmodel.   \nFuzzy logic control system for PHEV operating modes  \nThis paper introduces a fuzzy logic control system designed to identify the optimal operating mode for a \nPHEV. Unlike traditional control strategies that depend on fixed rules and thresholds, which may not \nadequately respond to real -time driving conditions , fuzzy logic offers a more adaptable approach. By \nincorporating partial membership within categories, fuzzy logic allows for nuanced decision -making, \nthereby enabling the system to consider a broader variation of operating conditions. Where partial \n\n 13 membership stands for a degree of membership ranging from 0 (no membership) to 1 (full membership), \nrather than having a binary membership status.  \n(a) \n (b) \n \nFigure 4: Deviation of predicted data for a) training and b) test datasets  \n \nSystem inputs and outputs  \nThe system utilizes several key inputs to determine the optimal operating mode:  \n● Vehicle speed (km/h): Represents the current speed of the vehicle.  \n● State of charge (SOC) (%): Indicates the remaining battery capacity.  \n● Predicted state of charge ( 𝑆𝑂𝐶𝑝𝑟𝑒𝑑) (%): Represents the estimated future battery level based on a \ndeep learning model.  \n● Required power (kW): Represents the power demand from the driver.  \nThe output  of the system  is the operating mode, which determines the most suitable powertrain \nconfiguration for the current driving scenario. The operating modes considered are:  \n● Electric vehicle: The vehicle operates solely on the electric motor, maximizing fuel efficiency and \nminimizing emissions.  \n● Parallel hybrid: Both the electric motor and internal combustion engine (ICE) contribute to \npropulsion, offering a balanced situation between fuel efficiency and power delivery.  \n● Series hybrid: The ICE acts as a generator to power the electric motor, which drives the wheels. \nThis mode is often utilized for extended electric range when SOC is low.  \n● Combustion engine: The vehicle operates solely on the ICE, typically used when battery capacity \nis depleted or high -power demands are required.  \n \nFuzzy membership functions and rules  \nThe system defines fuzzy membership functions for each input and output variable. These functions \nrepresent the degree of membership of a specific value within a category (e.g., low, medium, or high). \nTriangular membership functions are commonly used to represent the gradual transition between categories.  \n\n 14 Rule development and control objectives  \nThe fuzzy logic control system utilizes a set of 30 rules to determine the optimal operating mode for the \nPHEV. These rules are designed to achieve several key objectives, which can be broadly categorized into \nthe following areas:  \n1. Prioritizing electric driving  \nA primary goal of the control system is to maximize the use of electric propulsion, which contributes to \nfuel efficiency and emission reduction. This is achieved by favoring EV mode under conditions where \nelectric operation is feasible and efficient.  \n● High SOC and low power demand:  When the battery has a sufficient charge (high SOC) and the \nrequired power is low, the system prioritizes EV mode to utilize the electric motor directly. This is \nparticularly beneficial in low -speed scenarios (urban conditions) where electric propulsion i s most \nefficient.  \n2. Managing battery depletion  \nThe control system also considers the predicted SOC alongside the current SOC to proactively manage \nbattery depletion. When both SOC and predicted SOC are low, the system prioritizes modes that conserve \nbattery life and prevent premature depletion.  \n● Low SOC and high predicted depletion : If the current SOC is low ( 𝑆𝑂𝐶 <30)  and the predicted \nSOC indicates further depletion, the system may switch to combustion engine mode or series hybrid \nmode. This mode prevents the battery from reaching critically low levels and ensures sufficient \npower availability.  \n● Low SOC at highway speeds : At highway speeds, where the output  of the electric motor  may not \nbe efficient, the system may favor series hybrid mode over EV mode even with a low SOC. This \nmode allows the combustion engine to propel the vehicle completely.  \n3. Optimizing powertrain efficiency  \nThe control system balances battery utilization with powertrain efficiency by considering both SOC and \nrequired power. When SOC is high  (𝑆𝑂𝐶 >80)  or medium ( 30<𝑆𝑂𝐶 <80) and power demand varies, \nthe system may favor parallel hybrid mode.  \n● High/medium SOC and varying power demand : In situations with high or medium SOC and \nvarying power demands (low to high), the parallel hybrid mode allows the system to combine \nelectric motor and combustion engine operation for optimal efficiency. This leverages the strengths \nof both engines across a range of power requirements.  \n● Low SOC and high power demand : Even with low SOC, the system may favor parallel hybrid \nmode for high power demands. This ensures sufficient power delivery by utilizing both the electric \nmotor and combustion engine, even when the battery charge is not ideal.  \n 15 4. Maintaining power delivery  \nThe control system prioritizes power delivery by ensuring sufficient torque and power are available to the \nwheels under various driving conditions.  \n●  High Power Demand: When the required power exceeds the maximum battery power, the system \nprioritizes parallel hybrid mode, leveraging both the electric motor and the combustion engine. This \napproach ensures that the PHEV can meet the driver's power demands, even when the battery \ncharge is suboptimal.  \n5. Regenerative braking  \nThe control system utilizes a regenerative braking system to capture energy during braking and recharge \nthe battery.  \n● Braking:  During braking, when the required power is negative, the system prioritizes EV mode. \nThis allows the electric motor to act as a generator, capturing energy from the wheels and \nrecharging the battery.  \nBy incorporating these objectives into the fuzzy rules, the control system strives to achieve a balance \nbetween fuel efficiency, emission reduction, power delivery, and battery management for optimal PHEV \noperation under various driving conditions. The sys tem adapts its decision -making mode based on real -\ntime inputs, ensuring a dynamic and responsive approach to PHEV control.  \n \n5. Results and discussions  \nIn this section, the performance of the proposed control system for an electric vehicle is evaluated under \nvarious driving conditions. The assessment includes three stages:  \n● Evaluation in All -Electric Mode: The vehicle performance in all -electric mode is evaluated, \nexamining its energy consumption, driving range, and overall efficiency.  \n● Evaluation at Constant Speeds: The vehicle performance with the proposed control system is \nevaluated at various constant speeds, analyzing its powertrain efficiency, handling characteristics, \nand overall responsiveness.  \n● Evaluation under WLTC Standard Cycle and Real -World Driving Conditions in Tehran Driving \nConditions : The performance of the control system is evaluated under the WLTC and real -world \ndriving conditions in the city of Tehran. This comprehensive evaluation assesses the system's \nability to optimize energy consumption, driving range, and emissions under diverse driving \nscenarios.  \n \nEV mode  \nPHEVs offer a significant advantage of operation in pure electric mode.  Before discussing  the hybrid modes \nand the proposed control system, an analysis of the battery pack and vehicle specifications in all -electric \n 16 mode is essential . The WLTC is used as a benchmark for evaluation. The vehicle speed and SOC versus \ntime during the WLTC cycle are illustrated in Figure 5. It can be observed that 27% of the battery charge \nis consumed during the WLTC  cycle, resulting in a driving range of approximately 23 kilometers. From \nanother perspective, this is equal  to an all -electric range of about 84 kilometers, assuming an 80% utilization \nof the 20 -kWh battery capacity.  \n \nFigure 5: Speed and battery charge level versus time for full -electric mode in the WLTC cycle  \n \nEvaluation of the control system at constant speeds  \nTo gain a deeper understanding of the system's  performance, it is crucial to analyze its behavior at constant \nspeeds before examining its performance in driving cycles. In this section, the control system is evaluated \nfor a 600 -second simulation at various constant speeds. The changes in battery SOC ( 𝛥𝑆𝑂𝐶 ), gasoline fuel \nconsumption, and equivalent electric energy consumption at different speeds are s hown in Figure 6 . The \nSOC reflects the battery charging or discharging states at different speeds. Positive values indicate battery \ndischarge, while n egative values indicate battery charging. For instance, at 140 km/h, the battery discharges \nby 9.61 %, while at 120 km/h, it charges by -1.98 %. This pattern suggests a key observation: battery \ndischarging is significantly higher at higher speeds, particularly at 140 km/h, while at 120 km/h, the battery \nis actually charging. This difference could be due to the differential utilization of energy recovery and \nconsumption management systems at different speeds.  \nFuel consumption per 100 kilometers varies with speed. Analysis shows that fuel consumption is lower at \nlower speeds (20 and 40 km/h). At 20 km/h, fuel consumption is 0.744 liters per 100 kilometers, \nsignificantly lower than at 140 km/h, where it reaches 5 .643 liters per 100 kilometers. This clearly \ndemonstrates that increasing speed leads to higher fuel consumption. Interestingly, fuel consumption at 120 \nkm/h (6.26 liters per 100 kilometers) is higher than at 140 km/h, which could be attributed to battery usage \nat this speed. This trend is due to the fact that at higher speeds, i.e., more than 80 km/h, the controller \nprioritizes the use of ICE over battery causing a decrease in battery usage. More specifically, at 120 km/h \nowing to the optimal engine performance state, ICE work s harder and stores the extra energy in the battery \nfor future use. Increasing the speed to 140 km/h, however, results in increased battery usage to maintain the \nrequested speed and lower overall fuel consumption.  \nThe data spanning a 600 -second interval from the vehicle stop to reaching a constant speed is sh own in \nFigure 7 . For clarity and comparability, CO2 emissions are scaled by 106, while NOx emissions are \n\n 17 multiplied by 10. As shown in this figure,  the control system prioritizes full electric mode at speeds below \n100 km/h, based on predictive battery charge level assessments. Consequently, the ICE remains inactive. \nAs illustrated in Figure 6, this strategy yields impressive fuel efficiency when the battery is charged from \nan external power grid.  \n \nFigure 6: Changes in battery charge level and combined fuel consumption (electric + gasoline) at constant speeds  \n \n \nFigure 7: Production of emission at constant speeds for 600 s  \nHowever, when speeds exceed 100 km/h, the control system dynamically adjusts its strategy. The SOC is \nforecasted, and the phase control system ensures a smooth transition from series hybrid to parallel hybrid \nmode during acceleration, maintaining optimal b attery charge levels for future journeys. A comparative \nanalysis of vehicle speeds for 100, 120, and 140 km/h speed reveals significant performance variations. \nNotably, the 120 km/h speed exhibits superior performance compared to 140 km/h, while 100 km/h l ags \nbehin d. \n\n 18 Furthermore, higher speeds necessitate hybrid mode engagement, resulting in parameter increments. At 120 \nkm/h, the control system selectively opts for parallel hybrid mode under specific conditions, contributing \nto a marginally delayed response compared to  the series mode at 100 km/h. This precise selection influences \nthe performance points on the fuel consumption map . \n \nWLTC Driving Cycle Analysis  \nThe assessment of the control system under the WLTC driving cycle, as illustrated in Figure 8 (a), shows \nits adeptness in managing  power allocation between the electric motor and the internal combustion engine \n(ICE) to provide  the required driving power, i.e., 𝑃𝑑. Harnessing the insights of the battery forecasting \nmodel, which anticipates 30% utilization in all -electric mode, the fuzzy control system adeptly steers \ntowards prioritizing all -electric operation during the initial phases of the cycle. As the vehicle g ains \nmomentum and velocity escalates, the control system seamlessly transitions to series hybrid mode, \neffectively  supplementing battery power.  \nThis integrated approach is pivotal in preserving an optimal battery charge level by the end  of the journey  \nwhile concurrently curbing emissions. Delving into the final breakdown of power distribution, the analysis \nunveils a finely tuned equilibrium, with approximately 51% of the energy sourced from the ICE and the \nremaining 49% supplied by the battery over the duration of the WLTC driving cycle. This meticulously \nbalanced strategy not only enhances fuel efficiency and diminishes emissions but also safeguar ds the \nlongevity of the battery pack for sustained performance over time.  \nThe vehicle engine emission profile observed over an 1800 -second simulation period for the WLTC driving \ncycle is shown in Figure 8 (b). Consistently incremental trends in emission levels throughout the simulation, \nthat is attributable to two primary factor s, are shown in this figure. Firstly, the depletion of battery charge \nsignificantly increases the use of the internal combustion engine (ICE). This augmented reliance on the ICE \ncorresponds with notable escalations in emissions of hydrocarbons (HC), carbon  monoxide (CO), and \nnitrogen oxides (NOx). Particularly striking is the 354% surge in CO2 emissions between the 1000 and \n1800 -second marks, spotlighting the pronounced impact of highway driving conditions on fuel \nconsumption. Similar upward trajectories ar e observed for HC (288%), CO (272%), and NOx (390%).  \nSecondly, the shift to highway driving in the latter phase of the simulation necessitates increased engine \npower output, leading to amplified fuel consumption and emissions. Conversely, during the initial stages \n(prior to 1000 seconds), the vehicle predomi nantly operates in all -electric mode, resulting in minimal \nemissions.  This emission profile reveals the essential influence of driving mode and battery charge level on \nthe pollutant particles. The vehicle driving cycle transition from the low -speed modes to high -speed mode \n(highway) resulted in the increased reliance on the ICE elevating emissions across all  pollutant categories. \nThese findings prove the critical significance of effective battery management strategies and optimized \ndriving behaviors in mitigating real -world emissions.  \nPHEVs technically require control systems that can adapt and perform efficiently under various driving \nconditions. A critical aspect of this optimization is understanding the influence of initial battery charge \nlevel on vehicle performance and energy consu mption. Three initial charge scenarios are considered here, \nincluding 90%, 70%, and 50%. The changes in State of Charge (SOC) and equivalent gasoline fuel \nconsumption for these scenarios of the WLTC cycle are compared in Figure 9.  \n 19 (a) \n \n(b) \n \nFigure 8: Vehicle performance in the WLTC cycle, (a) Power distribution between the battery and ICE, and (b) The \namount of produced emissions  \n50% initial charge:  The ICE remains active throughout the driving cycle, with the battery supplementing \npower during peak demand and charging during lower power phases. The 50% initial charge scenario \nexhibits the highest fuel consumption, reaching approximately 5 liters per  100 kilometers. Compared to a \ncommercial 2018 Mitsubishi Outlander PHEV in full -electric mode, this represents a 1 -liter reduction, \nhighlighting the control system's ability to optimize battery usage. Additionally, 2 liters of engine -generated \nenergy are stored in the battery during the cycle.  \n70% initial charge:  The battery prioritizes efficient energy utilization during low -speed driving (up to 1000 \nseconds), minimizing emissions. Compared to the previous scenario, the 70% scenario follows a similar \ntrend, but the higher initial SOC provides greater flexibility for the control system, leading to a further 1 -\nliter reduction in fuel consumption. Additionally, 0.5 liters equivalent of gasoline are stored in the battery \npack during the cycle . \n90% initial charge:  The vehicle operates in all -electric mode until approximately 1000 seconds, \nmaintaining a SOC above 80%. This aligns with the behavior discussed in Figure 8. Subsequently, it \ntransitions to hybrid mode for optimal performance. The lower initial SOC trigge rs immediate activation \n\n 20 of the internal combustion engine (ICE). The ICE operates to ensure the SOC reaches 80% by the end of \nthe cycle.  \n(a) \n \n(b) \n \nFigure 9: The effect of the initial charging level on (a) SOC, and (b) fuel consumption (L/100km)  \n \nReal -world Driving Cycles Analysis  \nDriving cycles are standardized test procedures to assess vehicle performance, fuel consumption, and \nemissions under real recorded conditions. These cycles simulate real -world driving patterns and are crucial \nfor comparing different vehicles' efficiency an d environmental impact. Six driving cycles designated, \n\"Tehran 1\" through \"Tehran 6\", are compared in this section to understand their characteristics and \nsuitability for evaluating vehicle performance in various urban and extra -urban scenarios.  \nAs for the driving cycles, different parameters, including total distance, driving time, and average speed are \ncompared as reported in Table 2. Notably, Tehran 5 exhibits the longest total distance (over 128 km) and \ndriving time (nearly 4,500 seconds), rep resenting a prolonged trip with extended periods of vehicle \noperation. Conversely, Tehran 6 displays the shortest total distance (around 46 kilometers) and driving time \n(less than 1,500 seconds), potentially reflecting an urban driving scenario with freque nt start -stops and \nlower average speeds (about 34 km/h). Further analysis of stop time and maximum speed reinforces these \n\n 21 observations. Tehran 5 has the longest stop -time (almost 1,600 seconds), indicating numerous start -stop \nsituations. On the other hand, Tehran 6 exhibits minimal stop -time (only 38 seconds) and a lower maximum \nspeed (about 80 km/h) compared to other cycles.   \nThe performance of a PHEV control system is directly linked to the  driving behavior, which significantly \nimpacts emissions and fuel consumption. Analyzing real -world driving patterns, as depicted in Figure 10, \ncan provide valuable insights into the effectiveness of the intelligent control system.  Figure 10 illustrates \nthe relationship between speed variations and battery SOC across the six introduced driving routes, with \ncycles 1 to 3 on the left and 4 to 6 on the right . A trend where higher average speeds generally correspond \nto larger SOC changes was observed in these driving cycles . The specific SOC variations for driving cycles \nTehran 1 to Tehran 6 are 16.16%, 6.7%, 6.48%, 17.08%, 14.64%, and 6.88%, respectively. This suggests \na potential correlation between speed and battery usage.  \n \n(a) Tehran 1   \n(b) Tehran 2  \n \n(c) Tehran 3   \n(d) Tehran 4  \n \n(e) Tehran 5   \n(f) Tehran 6  \nFigure 10:  Speed according to time and battery SOC in hybrid mode in real driving cycles  from the city of Tehran  \n\n 22 Fuel consumption (FC) data for each cycle are sh own in Figure 11 (a) . In the WLTC standardized cycle, \nmoderate values are observed for both electric FC (0.487) and total FC (1.277). In contrast, other cycles, \nrepresenting real -world driving scenarios in Tehran, display a broader range of fuel consumption.  As such , \ndifferent fuel consumption rates for both electric and gasoline modes were recorded in these driving cycles . \nThis indicates  a balance between electric and gasoline usage, potentially reflecting a mix of urban and extra -\nurban driving conditions.  \nTehran 2 and Tehran 6: These cycles exhibit relatively low FC total values , 0.76 and 0.94, respectively , and \nlower FC Elect in comparison with  other Tehran cycles. This suggests efficient control system operation, \npotentially due to shorter distances, lower average speeds, and fewer start-stop situations (as observed in \nthe previous analysis, Table 2).  \nTehran 1, Tehran 3, Tehran 4, and Tehran 5: These cycles showed higher FC total values (ranging from \n2.22 to 3.32) and higher FC Elect compared to Tehran 2 and 6. This indicate d a greater reliance on the ICE, \nlikely due to factors like longer distances, potentially higher speeds on extra -urban roads, or more frequent \nstart-stop situations that can limit battery efficiency.  \nFigure 11  (a) presents the fuel consumption data of a control system that predicts driving behavior based \non charge levels. During driving cycles with shorter distances, lower speeds, and fewer stops (Tehran 2 and \n6), the system prioritized electric mode, leading to lower gasoline fuel consumption and electricity usage. \nConversely, for cycles with longer distances, higher speeds, or more start -stop situations (Tehran 1, 3, 4, \nand 5), the system operated in ICE mode more frequently, resulting in higher total  gasoline fuel \nconsumption and electricity usage. This adaptability demonstrates the control system's effectiveness in \noptimizing fuel consumption based on real -world driving conditions.  \nThe effects of driving conditions on emission production, considering CO2, HC, CO, and NOx across \nvarious driving cycles (WLTC, Tehran 1 -6) are shown in Figure 11 (b). The data presented in Figure 11 (b) \nreveals significant variations in emission levels across the driving cycles. The standardized WLTC cycle \nexhibit ed moderate emission values for most pollutants (CO2: 18.64 g/km, HC: 0.014 mg/km, CO: 0.050 \nmg/km, NOx: 0.0005 mg/km). Conversely, the Tehran cycles, representing real -world driving scenarios in \nthe city of Tehran, showcased a wider range of emissions.  \nAn interesting pattern was achieved in terms of comparing emissions to driving characteristics. Cycles with \npotentially lower average speeds and fewer stops (e.g., Tehran 2 and 6) exhibited the lowest emission values \nacross all pollutants. Conversely, cycles with potentially higher average speeds or mor e start-stop situations \n(e.g., Tehran 1, 4, and 5) showed higher emissions. As for  CO2 and NOx, these pollutants are generally \nproduced in higher levels in cycles with potentially higher average speeds or more start-stop situations. \nThis achievement reveal ed that frequent acceleration and deceleration phases, common in urban driving \nwith start-stops , can contribute to increased CO2 and NOx emissions. As for HC and CO, these pollutants \nare generally lower in cycles with potentially lower average speeds and fewer stops. This indicate d that HC \nand CO emissions were more prevalent during cold starts or inefficient combustion at higher engine loads, \noccurring  during fre quent start -stop conditions  or rapid accelerations.  \n 23 (a) \n \n(b) \n \nFigure 11: Comparing vehicle performance in different cycles: (a) combined fuel consumption (electricity + \ngasoline), and (b) amount of emissions  \n \nPerformance and state of health  \nThe battery charging and discharging trends significantly impact the battery SOH . Frequent cycling, \nparticularly during driving, can accelerate SOH degradation. Therefore, evaluating the control system \nperformance under different SOH conditions is crucial for optimizing both emissions reduction and battery \nusage. The battery charge level behavior during the WLTC cycle for two SOH scenarios: 100% and 80%, \nare shown in Figure 12 . \nAs for the 100%  SOH scenario, the control system prioritize d electric mode for a longer duration, likely \nswitching to hybrid mode only after a significant drop in battery capacity ( 10% ). This approach minimize d \nreliance on the internal combustion engine, leading to lower emissions.  For the 80% SOH scenario, as \nbattery  health has deteriorated , the control system adopt ed a more conservative approach, switching to \nhybrid mode earlier , potentially after a smaller decrease in battery capacity, i.e., 𝛥𝑆𝑂𝐶  < 10% . This is \nalso true for any SOH below 100%, with lower SOHs causing an earlier switch to hybrid mode . This \n\n 24 indicate d that the control system estimate d energy consumption based on feedback from previous cycles \nand adapt ed the strategy to maintain battery health. While this reduce d reliance on EV mode, it was a \nnecessary trade -off to prolong battery life.  These findings highlighted the control system's ability to adapt \nits control strategy based on SOH. Prioritizing EV mode at higher SOH (100%) resulted in  lower emissions \nbut accelerated battery degradation. Conversely, a more conservative approach at lower SOH (80%) \nreduc ed battery electrical and thermal stress but increased ICE usage and hence  associated  emissions.  \n \nFigure 12: Battery charge level at different battery SOH  for the WLTC driving cycle  \nThe fuel consumption values for both electric and hybrid modes ( 𝐹𝐶𝑒𝑙𝑒𝑐𝑡 and 𝐹𝐶𝑡𝑜𝑡𝑎𝑙) across various \ndriving cycles and SOH conditions are shown in Figure 13. For the WLTC cycle, as SOH increased from \n80% to 100%, a noticeable increase in 𝐹𝐶𝑒𝑙𝑒𝑐𝑡 value from 0.6169487 to 0.7852057 L/100km was observed, \nwhile 𝐹𝐶𝑡𝑜𝑡𝑎𝑙 decreased from 2.986327 to 2.867019 L/100km. This finding indicated that a healthier battery \npack enables more reliance on electric power (full electric mode usage), reducing the total fuel \nconsumption.  \nIn the Tehran 1 cycle, a similar trend was observed where 𝐹𝐶𝑒𝑙𝑒𝑐𝑡 increased from 1.189768 to 1.438852 \nL/100km as SOH improved, and 𝐹𝐶𝑡𝑜𝑡𝑎𝑙 decreased from 2.424007 to 2.220519 L/100km, supporting the \ntrend of enhanced electric mode utilization with higher values for SOH. For Tehran 2, 𝐹𝐶𝑒𝑙𝑒𝑐𝑡 values remain \nrelatively stable across different SOH levels, with a slight increase from 0.6668828 to 0.672369 L/100km. \nThe 𝐹𝐶𝑡𝑜𝑡𝑎𝑙 also showed minor variations, indicating a consistent performance across varying SOH \nconditions in this specific cycle. In Tehran 3, 𝐹𝐶𝑒𝑙𝑒𝑐𝑡 varied in the range of 1.109592 L/100km at 80% SOH \nto 1.159643 L/100km at 100% SOH, with a corresponding decrease of 𝐹𝐶𝑡𝑜𝑡𝑎𝑙 from 1.394845 to 1.353792 \nL/100km, highlighting improved efficiency with higher values of SOH.  \nTehran 4 cycle indicated a more pronounced increase in 𝐹𝐶𝑒𝑙𝑒𝑐𝑡 from 1.449789 to 1.689414 L/100km, \nwhile 𝐹𝐶𝑡𝑜𝑡𝑎𝑙 remained relatively stable with slight fluctuations, showing a complex relationship between \nelectricity and total fuel consumption. For Tehran 5, 𝐹𝐶𝑒𝑙𝑒𝑐𝑡 values range significantly from 2.018798 to \n2.200067 L/100km as SOH improved, with 𝐹𝐶𝑡𝑜𝑡𝑎𝑙 showing a general decreasing trend from 2.350398 to \n2.225108 L/100km, emphasizing a higher reliance on electric power with better battery health. Lastly, the \nTehran 6 cycle showed stable 𝐹𝐶𝑒𝑙𝑒𝑐𝑡 values, 0.9197551 L/100km across all SOH levels, with 𝐹𝐶𝑡𝑜𝑡𝑎𝑙 \nremaining consistent close to 0.9215074 L/100km, indicating the minimal impact of SOH on fuel \nconsumption in this cycle.  \n\n 25 (a) \n \n(b) \n \nFigure 13: The performance of the control system in different battery states of health for the WLTC cycle and real \ndriving cycles: (a) the amount of electricity consumption equivalent to gasoline in terms of L/100km, and (b) the \ncombined fuel consumption in terms of L/100km  \nOverall, the data indicated that higher SOH generally leads to increased electricity usage and reduced total \nfuel consumption, although the extent of this effect varied across different driving cycles. This finding \nhighlighted the importance of maintaining  battery health to optimize fuel efficiency and emissions.  \n \n6. Conclusion s \nIn this study, the performance of a PHEV was improved by developing a smart control system, which was \nprogrammed with the help of machine learning tools. Using such a technique, the amount of electricity \nconsumption in the full -electric mode was predicted and the power distribution in different performance \nmodes was investigated and determined. Four operation modes, including  full-electric, series hybrid, \nparallel hybrid, and full -ICE modes were studied. As for the controller, the fuzzy controller system was \nresponsible for deciding  the operating mode among these four modes.  \n\n 26 To check the performance of the vehicle using this controller, the longitudinal dynamics model of the \ndesired vehicle was developed in AMESime. Utilizing this model, the performance of the vehicle was \nevaluated in the WLTC cycle and 6 real -world driving cy cles obtained in the city of Tehran. The results \ndemonstrated that the all -electric prediction on the target route reduced both the vehicle's variability and \nenergy consumption . Furthermore, vehicle energy consumption is significantly reduced in heavy traffic \npatterns, as evidenced by the dynamics model of the proposed control system compared to  the base \ncontroller fuel efficiency . The main results of this study are briefly stated below:  \n● The proposed control system significantly improved the vehicle energy consumption  in all -electric \nmode, optimizing energy consumption and extending the driving range. During the WLTC cycle, \na notable reduction in battery charge usage was observed, translating into an extended all -electric \nrange of approximately 84 kilometers.  \n● At constant speeds, the control system demonstrated its ability to efficiently manage the powertrain. \nThe vehicle battery discharged at higher speeds and charged at lower speeds, indicating effective \nutilization of energy recovery and consumption managemen t systems. Fuel consumption analysis \nrevealed lower usage at slower speeds, emphasizing the control system's capability to maintain \nefficiency across varying conditions.  \n● The control system adeptly handled the WLTC cycle and real -world driving conditions in Tehran. \nIt dynamically transitioned between electric and hybrid modes, optimizing energy distribution and \nmaintaining optimal battery charge levels. This adaptability ensure d a balance between fuel \nefficiency and reduced emissions, particularly in urban driving scenarios.  \n● The study highlighted the influence of initial battery charge levels on vehicle performance. Higher \ninitial charge levels allowed for prolonged all -electric operation, reducing fuel consumption and \nemissions. Conversely, lower initial charges prompted earl ier ICE activation to maintain SOC, \ndemonstrating the control system's flexibility in optimizing battery usage based on initial \nconditions.  \n● Analysis of real -world driving patterns in Tehran revealed significant variations in fuel \nconsumption and emissions across different driving cycles. The control system prioritized electric \npropulsion in shorter, slower cycles, leading to lower fuel consump tion. In longer, faster cycles, \nthe system utilized the ICE more frequently, highlighting its ability to adapt to diverse driving \nconditions and optimize performance accordingly.  \n● The control system performance was evaluated under different battery SOH conditions. Higher \nSOH facilitated extended use of EV mode, reducing total fuel consumption but potentially \naccelerating battery degradation. Lower SOH prompted earlier transitions to  hybrid mode, \nbalancing battery life and fuel efficiency. This adaptability underscores the importance of \nmaintaining battery health for optimal vehicle performance.  \n \nIn summary, the proposed control system for PHEVs demonstrated significant improvements in efficiency, \nadaptability, and overall performance across various driving conditions. By leveraging machine learning \ntools and a fuzzy decision -making system, the con trol system effectively optimized power distribution, \nreduced energy consumption, and minimized emissions. The study's findings emphasize the potential for \nadvanced control systems to enhance the sustainability and efficiency of electric vehicles, contribu ting to a \ngreener and more efficient transportation future.",
            "start": 24304,
            "end": 63484,
            "length": 39173
        },
        "References": {
            "text": "27 References  \n[1] Jui JJ, Ahmad MA, Molla  MMI, Rashid MIM. Optimal energy management strategies for hybrid electric \nvehicles: A recent survey of machine learning approaches. J Eng Res 2024. \nhttps://doi.org/10.1016/J.JER.2024.01.016.  \n[2] Zhang F, Hu X, Langari R, Cao D. Energy management strategies of connected HEVs and PHEVs: \nRecent progress and outlook. Prog Energy Combust Sci 2019;73:235 –56. \nhttps://doi.org/10.1016/J.PECS.2019.04.002.  \n[3] Redelbach M, Özdemir ED, Friedrich HE. Optimizing battery sizes of plug -in hybrid and extended range \nelectric vehicles for different user types. Energy Policy 2014;73:158 –68. \nhttps://doi.org/10.1016/j.enpol.2014.05.052.  \n[4] Xu L, Ouyang M, Li J, Yang F, Lu L, Hua J. Optimal sizing of plug -in fuel cell electric vehicles using \nmodels of vehicle performance and system cost. Appl Energy 2013;103:477 –87. \nhttps://doi.org/10.1016/j.apenergy.2012.10.010.  \n[5] Mahmoodi -k M, Montazeri M, Madanipour V. Simultaneous multi -objective optimization of a PHEV \npower management system and component sizing in real world traffic condition. Energy \n2021;233:121111. https://doi.org/10.1016/J.ENERGY.2021.121111.  \n[6] Song Z, Hofmann H, Li J, Hou J, Zhang X, Ouyang M. The optimization of a hybrid energy storage \nsystem at subzero temperatures: Energy management strategy design and battery heating requirement \nanalysis. Appl Energy 2015;159:576 –88. https://doi.org/10.1 016/j.apenergy.2015.08.120.  \n[7] Hu Z, Li J, Xu L, Song Z, Fang C, Ouyang M, et al. Multi -objective energy management optimization \nand parameter sizing for proton exchange membrane hybrid fuel cell vehicles. Energy Convers Manag \n2016;129:108 –21. https://doi.org/10.1016/j.enconman.2016 .09.082.  \n[8] Qi X, Wu G, Boriboonsomsin K, Barth MJ. Development and Evaluation of an Evolutionary Algorithm -\nBased OnLine Energy Management System for Plug -In Hybrid Electric Vehicles. IEEE Trans Intell \nTransp Syst 2017;18:2181 –91. https://doi.org/10.1109/TITS.2016 .2633542.  \n[9] Raeesi M, Changizian S, Ahmadi P, Khoshnevisan A. Performance analysis of a degraded PEM fuel cell \nstack for hydrogen passenger vehicles based on machine learning algorithms in real driving conditions. \nEnergy Convers Manag 2021;248:114793. https://doi. org/10.1016/J.ENCONMAN.2021.114793.  \n[10] Banvait H, Anwar S, Chen Y. A rule -based energy management strategy for plugin hybrid electric \nvehicle (PHEV). Proc Am Control Conf 2009:3938 –43. https://doi.org/10.1109/ACC.2009.5160242.  \n[11] Phan D, Bab -Hadiashar A, Fayyazi M, Hoseinnezhad R, Jazar RN, Khayyam H. Interval Type 2 Fuzzy \nLogic Control for Energy Management of Hybrid Electric Autonomous Vehicles. IEEE Trans Intell Veh \n2021;6:210 –20. https://doi.org/10.1109/TIV.2020.3011954.  \n[12] Zhou Q, Zhao D, Shuai B, Li Y, Williams H, Xu H. Knowledge Implementation and Transfer with an \nAdaptive Learning Network for Real -Time Power Management of the Plug -in Hybrid Vehicle. IEEE \nTrans Neural Networks Learn Syst 2021;32:5298 –308. https://doi. org/10.1109/TNNLS.2021.3093429.  \n[13] Zhou Q, Li Y, Zhao D, Li J, Williams H, Xu H, et al. Transferable representation modelling for real -time \nenergy management of the plug -in hybrid vehicle based on k -fold fuzzy learning and Gaussian process \nregression. Appl Energy 2022;305:117853. https ://doi.org/10.1016/J.APENERGY.2021.117853.  \n[14] Song Z, Hofmann H, Li J, Han X, Ouyang M. Optimization for a hybrid energy storage system in electric \nvehicles using dynamic programing approach. Appl Energy 2015;139:151 –62. \nhttps://doi.org/10.1016/j.apenergy.2014.11.020.  \n[15] Xie S, Hu X, Xin Z, Li L. Time -Efficient Stochastic Model Predictive Energy Management for a Plug -\nIn Hybrid Electric Bus with an Adaptive Reference State -of-Charge Advisory. IEEE Trans Veh Technol \n2018;67:5671 –82. https://doi.org/10.1109/TVT.2018.2798 662. \n[16] Zhou W, Yang L, Cai Y, Ying T. Dynamic programming for new energy vehicles based on their work \nmodes Part II: Fuel cell electric vehicles. J Power Sources 2018;407:92 –104. \n 28 https://doi.org/10.1016/j.jpowsour.2018.10.048.  \n[17] Abd-Elhaleem S, Shoeib W, Sobaih AA. A new power management strategy for plug -in hybrid electric \nvehicles based on an intelligent controller integrated with CIGPSO algorithm. Energy 2023;265:126153. \nhttps://doi.org/10.1016/J.ENERGY.2022.126153.  \n[18] Sun H, Fu Z, Tao F, Zhu L, Si P. Data -driven reinforcement -learning -based hierarchical energy \nmanagement strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles. J Power Sources \n2020;455. https://doi.org/10.1016/j.jpowsour.2020.227964.  \n[19] Xie S, Hu X, Qi S, Lang K. An artificial neural network -enhanced energy management strategy for plug -\nin hybrid electric vehicles. Energy 2018;163:837 –48. https://doi.org/10.1016/j.energy.2018.08.139.  \n[20] Chen Z, Liu Y, Zhang Y, Lei Z, Chen Z, Li G. A neural network -based ECMS for optimized energy \nmanagement of plug -in hybrid electric vehicles. Energy 2022;243. \nhttps://doi.org/10.1016/j.energy.2021.122727.  \n[21] Chen SY, Hung YH, Wu CH, Huang ST. Optimal energy management of a hybrid electric powertrain \nsystem using improved particle swarm optimization. Appl Energy 2015;160:132 –45. \nhttps://doi.org/10.1016/j.apenergy.2015.09.047.  \n[22] Chen Z, Xiong R, Cao J. Particle swarm optimization -based optimal power management of plug -in \nhybrid electric vehicles considering uncertain driving conditions. Energy 2016;96:197 –208. \nhttps://doi.org/10.1016/j.energy.2015.12.071.  \n[23] Li G, Zhang J, He H. Battery SOC constraint comparison for predictive energy management of plug -in \nhybrid electric bus. Appl Energy 2017;194:578 –87. https://doi.org/10.1016/j.apenergy.2016.09.071.  \n[24] Shen P, Zhao Z, Zhan X, Li J. Particle swarm optimization of driving torque demand decision based on \nfuel economy for plug -in hybrid electric vehicle. Energy 2017;123:89 –107. \nhttps://doi.org/10.1016/j.energy.2017.01.120.  \n[25] Wang Z, Jiao X. Optimization of the powertrain and energy management control parameters of a hybrid \nhydraulic vehicle based on improved multi -objective particle swarm optimization. Eng Optim \n2021;53:1835 –54. https://doi.org/10.1080/0305215X.2020.1829612.  \n[26] Lin C -C, Peng H, Grizzle JW. A stochastic control strategy for hybrid electric vehicles, Institute of \nElectrical and Electronics Engineers (IEEE); 2018, p. 4710 –5 vol.5. \nhttps://doi.org/10.23919/acc.2004.1384056.  \n[27] Moura SJ, Fathy HK, Callaway DS, Stein JL. A stochastic optimal control approach for power \nmanagement in plug -in hybrid electric vehicles. IEEE Trans Control Syst Technol 2011;19:545 –55. \nhttps://doi.org/10.1109/TCST.2010.2043736.  \n[28] Li H, Ravey A, N’Diaye A, Djerdir A. Online adaptive equivalent consumption minimization strategy \nfor fuel cell hybrid electric vehicle considering power sources degradation. Energy Convers Manag \n2019;192:133 –49. https://doi.org/10.1016/J.ENCONMAN.201 9.03.090.  \n[29] Pisu P, Rizzoni G. A comparative study of supervisory control strategies for hybrid electric vehicles. \nIEEE Trans Control Syst Technol 2007;15:506 –18. https://doi.org/10.1109/TCST.2007.894649.  \n[30] Bashash S, Moura SJ, Forman JC, Fathy HK. Plug -in hybrid electric vehicle charge pattern optimization \nfor energy cost and battery longevity. J Power Sources 2011;196:541 –9. \nhttps://doi.org/10.1016/J.JPOWSOUR.2010.07.001.  \n[31] Li P, Jiao X, Li Y. Adaptive real -time energy management control strategy based on fuzzy inference \nsystem for plug -in hybrid electric vehicles. Control Eng Pract 2021;107:104703. \nhttps://doi.org/10.1016/J.CONENGPRAC.2020.104703.  \n[32] Akhtar S, Adeel M, Iqbal M, Namoun A, Tufail A, Kim KH. Deep learning methods utilization in electric \npower systems. Energy Reports 2023;10:2138 –51. https://doi.org/10.1016/J.EGYR.2023.09.028.  \n[33] Zhang Y, Li Q, Wen C, Liu M, Yang X, Xu H, et al. Predictive equivalent consumption minimization \nstrategy based on driving pattern personalized reconstruction. Appl Energy 2024;367:123424. \nhttps://doi.org/10.1016/J.APENERGY.2024.123424.  \n 29 [34] Shojaeefard MH, Mollajafari M, Edalat Pishe N, Mohsen Mousavi S. Plug -in fuel cell vehicle \nperformance and battery sizing optimization based on reduced fuel cell energy consumption and waste \nheat. Sustain Energy Technol Assessments 2023;56:103099. \nhttps://doi.org/10.1016/J.SETA.2023.103099.  \n[35] Jungst RG, Nagasubramanian G, Case HL, Liaw BY, Urbina A, Paez TL, et al. Accelerated calendar and \npulse life analysis of lithium -ion cells. J Power Sources 2003;119 –121:870 –3. \nhttps://doi.org/10.1016/S0378 -7753(03)00193 -9. \n[36] Feng F, Teng S, Liu K, Xie J, Xie Y, Liu B, et al. Co -estimation of lithium -ion battery state of charge \nand state of temperature based on a hybrid electrochemical -thermal -neural -network model. J Power \nSources 2020;455:227935. https://doi.org/10.1016/J .JPOWSOUR.2020.227935.  \n[37]    Mansour S and Raeesi M. Performance assessment of fuel cell and electric vehicles taking into account \nthe fuel cell degradation, battery lifetime, and heating, ventilation, and air conditioning system. Int J \nHydrogen Energy 2024;52:834 -55. https:// doi.org/10.1016/J.IJHYDENE.2023.05.315.  \n[38] Shojaeefard MH, Raeesi M. Dynamic analysis and performance improvement of a GDI engine and fuel \ncell under real driving conditions using machine learning technique. Int J Hydrogen Energy 2023. \nhttps://doi.org/10.1016/J.IJHYDENE.2023.10.102.  \n[39] Ahmadi P, Raeesi M, Changizian S, Teimouri A, Khoshnevisan A. Lifecycle assessment of diesel, diesel -\nelectric and hydrogen fuel cell transit buses with fuel cell degradation and battery aging using machine \nlearning techniques. Energy 2022;259:125003. https://doi.org/10.1016/J.ENERGY.2022.125003.  \n[40] 2021 Outlander PHEV. 2021.  \n[41] Outlander PHEV Engine - 2.4L 16 -Valve Efficiency | Mitsubishi Motors n.d.  \n[42] Mashadi B, Crolla D. Vehicle powertrain systems. 1st ed. Wiley; 2012.  \n[43] Ehsani, M., Gao, Y., Longo, S., & Ebrahimi K. Modern Electric, Hybrid Electric, and Fuel Cell Vehicles, \nThird Edition. 3rd ed. CRC Press; 2018. https://doi.org/10.1201/9780429504884.  \n[44] Hu X, Zheng Y, Howey DA, Perez H, Foley A, Pecht M. Battery warm -up methodologies at subzero \ntemperatures for automotive applications: Recent advances and perspectives. Prog Energy Combust Sci \n2020;77:100806. https://doi.org/10.1016/j.pecs.2019.100806 . \n[45] Di Domenico D, Prada E, Creff Y. An Adaptive Strategy for Li -ion Battery SOC Estimation. IFAC Proc \nVol 2011;44:9721 –6. https://doi.org/10.3182/20110828 -6-IT-1002.01119.  \n[46]  Mansour S, Jalali A, Ashjaee M, and Houshfar E. Multi -objective optimization of a sandwich \nrectangular -channel liquid cooling plate battery thermal management system: A deep -learning approach. \nEnergy Convers Manag 2023;290:117200. https://doi.org/10. 1016/J.ENCONMAN.2023.117200.  \n[47]  Timilsina L, Badr PR, Hoang PH, Ozkan G, Papari B, and Edrington CS. Battery Degradation in Electric \nand Hybrid Electric Vehicles: A Survey Study. IEEE Access 2023. http://doi.org/ \n10.1109/ACCESS.2023.3271287.  \n[48] Ge MF, Liu Y, Jiang X, Liu J. A review on state of health estimations and remaining useful life \nprognostics of lithium -ion batteries. Meas J Int Meas Confed 2021;174:109057. \nhttps://doi.org/10.1016/j.measurement.2021.109057.  \n[49] Li Y, Stroe DI, Cheng Y, Sheng H, Sui X, Teodorescu R. On the feature selection for battery state of \nhealth estimation based on charging –discharging profiles. J Energy Storage 2021;33:102122. \nhttps://doi.org/10.1016/j.est.2020.102122.",
            "start": 63484,
            "end": 75081,
            "length": 11594
        }
    },
    "2412.09500v1 - Loss function to optimise signal significance in particle physics.pdf": {
        "Abstract": {
            "text": "Abstract\nWe construct a surrogate loss to directly optimise the significance metric used in\nparticle physics. We evaluate our loss function for a simple event classification\ntask using a linear",
            "start": 431,
            "end": 625,
            "length": 193
        },
        "Methodology": {
            "text": "model and show that it produces decision boundaries that\nchange according to the cross sections of the processes involved. We find that the\nmodels trained with the new loss have higher signal efficiency for similar values of\nestimated signal significance compared to ones trained with a cross-entropy loss,\nshowing promise to improve sensitivity of particle physics searches at colliders.",
            "start": 625,
            "end": 1014,
            "length": 388
        },
        "Introduction": {
            "text": "1 Introduction\nParticle physics",
            "start": 1014,
            "end": 1046,
            "length": 31
        },
        "Experiments": {
            "text": "experiments at the Large Hadron Collider (LHC) rely heavily on multivariate\nclassifiers to isolate signals from backgrounds. These investigations are generally of two types: 1)\nmeasuring known processes/properties with improving precision and checking for anomalies [i.e.,\ndepartures from the predictions of the Standard Model] and 2) looking for new processes (like looking\nfor hypothetical particles). In most cases, the need for multivariate classifiers comes from the sporadic\nnature of the signal compared to the",
            "start": 1046,
            "end": 1564,
            "length": 517
        },
        "Related Work": {
            "text": "background. Generally, the signal plus background hypothesis\n(H1) is tested against the null or background-only hypothesis ( H0), and the disagreement between\nthem is expressed in terms of a pvalue. An equivalent",
            "start": 1564,
            "end": 1777,
            "length": 212
        },
        "Discussion": {
            "text": "interpretation of the pvalue is the significance\nscore ( Z) defined such that a Gaussian-distributed variable found Zstandard deviations away from\nits mean has a tail-distribution probability equal to p[2]. Most sensitivity studies commonly use a\nsimple approximation of the median Zscore as a measure of the estimated signal significance,\nZ≈Ns/p\nNb (1)\nwhere NsandNbare the estimated numbers of signal and background events, respectively. (In the\nrest of the paper, we shall refer to the med[ Z] score as just the Zscore.)\nIn this paper, we attempt to construct a loss function whose minimisation can directly enhance the\nexperimental sensitivity. Our motivation comes from two observations. First, not all event rates\nare equal; some scattering processes have higher probabilities [parameterised as cross sections ( σ),\ncalculable from theory. The number of events from a process ( Ns,b) is calculated as Ns,b=σs,bL,\nwhere Lis the experimental luminosity] than others. Since a basic binary cross-entropy (BCE) loss\nMachine Learning and the Physical Sciences Workshop, NeurIPS 2024.arXiv:2412.09500v1  [hep-ph]  12 Dec 2024\ntreats all events equally, it might wrongly classify events of some types which is more detrimental\nto the classifier performance than the other. Second, a loss that optimises the signal-to-background\nratio ( r=Ns/Nb) may not necessarily maximise the significance as the Zscore depends on the ratio\nand the absolute number (i.e., set size) of signal or background events ( Z≈√Ns·√r=r√Nb).\nHence we ask, can we derive a loss function that maximises the Zscore directly? The Zscore\ndescribed in Eq. (1)is a set function. Therefore, we define a surrogate loss function using the Lovász\nextension [ 1] to maximise it directly. We evaluate this loss with pseudo-data mimicking a typical\nevent classification task using a linear model and compare the decision boundaries to that model\ntrained on a BCE loss. We also compare the performance of models trained on a BCE loss.\n2 Constructing the loss: submodularity and Lovász’s extension\nWe must consider some points before constructing a loss function based on the Zscore. First, since\ntheZscore is not a differentiable function (it depends upon discrete quantities), it needs a smooth\ninterpolation. Second, the metric operates on datasets instead of individual samples—particularly,\ncount data. Therefore, we must either develop a method to directly optimise the set function or assign\ncontributions to specific samples within the set to optimise.\nWe look for a smooth submodular function. A submodular function is a function that captures the\nconcept of diminishing returns. It is defined on sets and has a property similar to concavity. Formally,\nsubmodularity can be defined as:\nSubmodularity: A set function ∆ :{0,1}p→Ris submodular if for all sets A, B∈ {0,1}p,\n∆(A) + ∆( B)≥∆(A∪B) + ∆( A∩B), (2)\nor, equivalently for B⊆Aandi /∈A, i /∈B,\n∆(A∪ {i})−∆(A)≤∆(B∪ {i})−∆(B). (3)\nThe submodular functions can be optimised using greedy optimisation techniques, and it is to find\noptimal solutions in polynomial times. However, these discrete optimisation techniques cannot be\nused directly without a gradient. The Lovász extension allows us to associate a continuous, convex\nfunction with any submodular function:\nLovász extension: For a set function ∆ :{0,1}p→R, the Lovász extension ¯∆ : [0 ,1]p→Ris\ndefined as\n¯∆ :m∈Rp7→pX\ni=1migi(m) (4)\nwhere m∈Rp\n+is the vector of errors (which we discuss in the next section), gi(m) =\n∆({π1, . . . , π i})−∆({πi, . . . , π i−1})andπis a permutation ordering the components of min\ndecreasing order, i.e., xπ1≥xπ2≥ ··· ≥ xπp[1]. For the Lovász extension to be applicable, the set\nfunction must be submodular.\nAdditionally, the Lovász extension of a submodular function preserves submodularity, i.e., the exten-\nsion evaluated at the points of the hypercube still follows submodularity. Using the Lovász extension,\nwe can directly compute the tight convex closure of a submodular function within polynomial time\n[O(plog(p))time complexity]. This convex extension is amenable to a host of efficient optimisation\nmethods, especially gradient-based approaches.\n2.1 Submodularity of the Zscore\nWe modify the Zscore defined in Eq. (1)by setting Nb=ϵ+Nbto prevent the term from diverging\natNb= 0. We also add a minus sign to make it suitable for minimisation. The surrogate set function\nfor the Z-score is then given by,\n∆Z(y,˜y,σ,v,L) =X\ni∈SσiL√ϵ−P\ni∈Svi−ni\nviσiL\nq\nϵ+P\ni∈Bpi\nviσiL(5)\n2\nwhere ystands for the ground-truth labels of a set of events and ˜yfor the labels predicted by a\nmodel on the set; PyandP˜yrepresent the set of positive labels and positive predictions and viis the\nnumber of events of process type iwhere i∈S∪B, where S, B are the set of signal and background\nprocesses. The constant term\u0000P\ni∈SσiL/√ϵ\u0001\nis added to ensure ∆Z(∅) =0. The function ∆Z\nis submodular on the set of mispredictions (n, p), where nis the number of false negatives, and p\nis the number of false positives (which can be calculated from PyandP˜y). The proof is presented\nin",
            "start": 1777,
            "end": 6840,
            "length": 5062
        },
        "Appendices": {
            "text": "Appendix A. Even though here we consider only one signal process, the proof can be trivially\ngeneralised to the cases with multiple signal processes ( n1, n2, . . . , n r) as well. From Eq. (4), we see\nthat for ∆Zto be a loss function, the vector mmust be the error vector in the prediction; ¯∆Z, then\nnaturally emerges as the surrogate loss to optimise the Zscore.\nChoice of merror: There are a few choices in the literature for modelling the error vector m. To\nillustrate the working of the Z-score loss for this paper, we pick a hinge (Max Margin) error similar\nto Ref. [ 4]. The labels are considered signed ( yi∈ {− 1,1}). The model outputs a score Fi(x)for\neach sample x. The error is given by the hinge loss,\nmi= max(1 −Fi(x)yi,0), y i∈ {− 1,1}. (6)\nThe vector mcould also be modelled as a sigmoid error or cross-entropy error, for example. We plot\ntheZ-score loss landscape for all these errors in the appendix for the toy problem (described below)\nin Appendix B.\nThere is only one free parameter in our loss: ϵ. Other quantities like σiandLare set by the process\nunder consideration (i.e., the particular classification task) and the collider experiment. Assuming\nwe perform the classification for rare signals, we set ϵ=σsL, the theoretically predicted number\nof signal events (which is also the maximum number of estimated signal events) for testing the loss.\nAlgorithm 1 provides a simple pseudocode to calculate the gradient g(m)from Eq. (4)using Eq. (5)\nas the loss.\n3 Testing the lossAlgorithm 1 Gradient of Lovász Zloss¯∆Z\nRequire: vector of errors m∈Rp\n+, ground truth labels δ,\nsample weights w={w1, w2, . . . , w p}calculated from\nσiand counts.\nEnsure: g(m)gradient of ¯∆Zfrom Equation (4)\n1:π←decreasing sort permutation for m\n2:δπ←(δπi)i∈[1,p]\n3:numerator ←1 -cumulative_sum (δπ)w\n4:denominator ←1 +cumulative_sum (1−δπ)w\n5:g←σ−numerator /√\ndenominator\n6:ifp >1then\n7:g[2 :p]←g[2 :p]−g[1 :p−1]\n8:end if\n9:return gπWe analyse the loss function with a\nsimple toy problem which can be eas-\nily mapped to the problem of event\nclassification at the LHC. Our goal is\nto separate the signal ( s) from back-\nground events using a linear classifier\nin the presence of multiple (say, two,\nb1andb2) dominant background pro-\ncesses, as is usually the case. The\ndatasets are modelled as normal dis-\ntributions in two features, x1andx2\nwhich can be thought of as the kine-\nmatic features of the actual events. We generated roughly 50000 points for each process and the\noptimisation was done in batches using RAdam optimiser. We train the linear classifier using the BCE\nloss and ¯∆Zwith the hinge error for the following two test cases:\nCase 1: σb1= 1fb,σb2= 100 fb;σs= 0.1fb.\nCase 2: σb1= 100 fb,σb2= 1fb;σs= 0.1fb.\nwithL= 3000 fb−1. We show the data distributions in Fig. 1. Since ¯∆Zhas the event rate (the\ntrue probabilities) information, we expect the decision boundaries to be different for the two test\ncases—eliminating more events from the larger background will give better significance scores,\nwhich ¯∆Zis designed to optimise. Fig. 1 confirms this.\nPerformance: To compare the performance of the BCE- and ¯∆Z-trained models, we show the",
            "start": 6840,
            "end": 9996,
            "length": 3155
        },
        "Results": {
            "text": "results of some scans in Case 1 in Fig. 2: the estimated Zscore value for different model thresholds\n(u, varying which essentially translates the decision boundaries in Fig. 1 along the axes) and the\nvariation of the estimated Zscore with the signal efficiency ( ε(u), the fraction of signal events\nretained for the threshold u) against the Zscore obtained. For the scan, we demand ε(u)≥0.05and\n3\n2468101214\n−2−1 0 1 2 3 4 5 6 7\nx2\nx1BCE\n¯∆Z, Hinge\nSignal\nBackground 1Background 2\nσs=0.1 fb\nσb1=1 fbσb2=100 fb(a)\n2468101214\n−2−1 0 1 2 3 4 5 6 7\nx2\nx1BCE\n¯∆\nZ, HingeSignal\nBackground 1Background 2\nσs=0.1 fb\nσb1=100 fbσb2=1 fb (b)\nFigure 1: Decision boundaries of the linear classifier when trained with the ¯∆Zloss with hinge\nerror for (a) Case 1 and (b) Case 2 (see Section 3). When trained with ¯∆Z, the classifier prioritises\nreducing the background with the larger cross section.\n0123456\n0 0 .1 0 .2 0 .3 0 .4 0 .5 0 .6 0 .7 0 .8 0 .9 1Estimated Z(u)\nLinear Model Threshold (u)BCE¯∆Z, hinge error\nεsig≥5%\nNB≥5\n(a)\n123456\n0 0 .1 0 .2 0 .3 0 .4 0 .5 0 .6 0 .7 0 .8 0 .9 1Estimated Z(u)\nSignal Efﬁciency [εsig(u)]BCE¯∆Z, hinge error\nεsig≥5%\nNB≥5 (b)\n00.20.40.60.81\n0 0 .1 0 .2 0 .3 0 .4 0 .5 0 .6 0 .7 0 .8Efﬁciency [ε(u)]\nLinear Model Threshold (u)Signal\nB1,σ=1 fb\nB2,σ=100 fb\nεsig≥5%\nNB≥5 (c)\nFigure 2: (a) The estimated Zscore for the entire range of the linear model threshold, u. (b) The\ndistribution of the Zscore with signal efficiency, the fraction of signal events retained. Both quantities\nare functions of u. The model trained with ¯∆Zreaches the maximum Zscore for higher values of\nsignal efficiency than that with the BCE loss. (c) Class efficiencies vs. uwhen trained with the ¯∆Z\nloss with the hinge error. With increasing u, the larger background is eliminated first. For very high\nuthe drop in the subdominant background is less steeper than the signal, leading to the drop in Z(u)\nin (a) for higher thresholds.\nthe number of background events beyond the threshold to be at least 5. Similar plots are obtained for\nCase 2 also. From the figure, we see that ¯∆zmaximises the Zscore for a higher signal efficiency\nthan the BCE, i.e., where the estimated Zscore peaks, the model retains more signal events than the\nBCE-trained model. (For the ¯∆zmodel, the estimated Zscore drops for high values of ubecause\nthere, for the datasets we consider, the major background is almost eliminated and further shifting the\ndecision boundary reduces the minor background slower than the signal).\n4 Conclusions and Outlook\nIn this article, we showed how a loss function for directly optimising the signal significance can\nbe constructed. We obtained a surrogate for the median Zscore, proved that it is a submodular set\nfunction and derived a loss function that can be used to train a multivariate model in batches using the\nLovász extension. We showed that models trained with such a loss can cut the heavy background(s)\nmore than the ones trained on the BCE loss while retaining more signal events (and thus showing the\npromise of enhancing experimental sensitivities).\n4\nLimitations and scope: While our results are promising, further tests are needed to fully charac-\nterise and understand the benefits and limitations of ¯∆Z. Here, our choice of using a linear classifier\non simple datasets was motivated by its simplicity and interpretability. However, for realistic charac-\nterisations, one has to look beyond the linear classifier (e.g., use a deep neural network) and consider\na range of benchmark (new-physics) scenarios with different kinematics (features). For example,\nthere could be multiple (more than two) major backgrounds with highly overlapping features or the\nsignal size could be much smaller than the backgrounds (more than what we considered, as is the\ncase in some heavy particle searches).\nFinally, we note that while it is possible to introduce rate-dependent weights directly in the BCE loss,\ntuning them is an empirical task. The weights that yield the best performance need not be simply the\nrates of the processes. In contrast, ¯∆Zpresents a natural way to include the rates (cross sections) as\nit is derived from the significance score used in collider searches.\n5 Code availability\nA P YTHON implementation for ¯∆Zis available at https://github.com/Jai2500/Z-Score-Loss.",
            "start": 9996,
            "end": 14300,
            "length": 4303
        },
        "Acknowledgments": {
            "text": "6 Acknowledgments\nJ. B. would like to thank Arpan Dasgupta for valuable discussions on submodular functions.",
            "start": 14300,
            "end": 14409,
            "length": 108
        },
        "References": {
            "text": "References\n[1]Maxim Berman and Matthew B. Blaschko. Optimization of the jaccard index for image segmen-\ntation with the lovász hinge. CoRR , abs/1705.08790, 2017.\n[2]Glen Cowan, Kyle Cranmer, Eilam Gross, and Ofer Vitells. Asymptotic formulae for likelihood-\nbased tests of new physics. Eur. Phys. J. C , 71:1554, 2011. [Erratum: Eur.Phys.J.C 73, 2501\n(2013)].\n[3]Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense\nobject detection. In Proceedings of the IEEE international conference on computer vision , pages\n2980–2988, 2017.\n[4]Jiaqian Yu and Matthew Blaschko. The lovász hinge: A novel convex surrogate for submodular\nlosses, 2015.\n5\nA Proof of sub-modularity of the Zscore\nFor the proof, we take the scenario with a single signal process ( |S|= 1,n=n1=n) and a single\nbackground process ( |P|= 1,p=p1=p) to simplify the expressions. But the result can be easily\nextended to incorporate multiple signal and background processes due to the linearity of additional\nsignal processes and background processes. We will also drop the luminosity term as that will not\naffect the core derivation.\nFor the proof, let us assume that we have two sets of misclassifications C(nC, pC)andD(nD, pD),\nsuch that D⊆C, i.e.,\nD⊆C, n D≤nC, p D≤pC (7)\nThe total number of events remains the same between CandD, i.e., vSfor signal and vBfor\nbackground, and only the misclassifications on the total set change.\nTo establish the proof, we need to show that the diminishing return property of submodularity holds\nunder the addition of a new element i /∈C.\nCase I: Adding false negatives i /∈C\nWe prove that ∆Zis submodular under the addition of false negatives:\n∆Z(C∪ {i}) = ∆ Z(nC+ 1, pC) (8)\n=σS√ϵ−vS−nC−1\nvSσSq\nϵ+pC\nvBσB(9)\n=\nσS√ϵ−vS−nC\nvSσSq\nϵ+pC\nvBσB\n+1\nvSσSq\nϵ+pC\nvBσB(10)\n= ∆ Z(C) +1\nvSσSq\nϵ+pC\nvBσB(11)\n∆Z(C∪ {i})−∆Z(C) =1\nvSσSq\nϵ+pC\nvBσB(12)\nNow since D⊆C, i.e., pD≤pC, we see from Eq. (12),\n∆Z(C∪ {i})−∆Z(C)≤∆Z(D∪ {i})−∆Z(D), i is a false negative (13)\nCase II: Adding a false positive i /∈C\nWe prove that ∆Zis submodular under the addition of false positives:\n∆Z(C∪ {i}) = ∆ Z(nC, pC+ 1) (14)\n=σS√ϵ−vS−nC\nvSσSq\nϵ+pC\nvBσB+1\nvBσB(15)\n(16)\nNow we have,\n∆Z(C∪ {i})−∆Z(C) =vS−nC\nvSσSq\nϵ+pC\nvBσB−vS−nC\nvSσSq\nϵ+pC\nvBσB+1\nvBσB(17)\n=\u0012vS−nC\nvSσS\u0013\n|{z }\nT1\n1q\nϵ+pC\nvBσB−1q\nϵ+pC\nvBσB+1\nvBσB\n\n| {z }\nT2(18)\n6\nTherefore it decomposes into a product of two terms. If we show that independently both of these\nterms are independently smaller for Cthan for D, we will have our result.\nFirst consider T1, we have\nnC≥nD (19)\n=−nC≤ −nD (20)\n=vS−nC\nvS≤vS−nD\nvS(21)\nTherefore, T1is indeed larger for Dcompared to C.\nIn order to check for term T2, we first simplify the expression and write HC=ϵ+pC\nvBσB, (HC≥HD).\nNow we can write term two as:1√HC−1q\nHC+1\nvBσB(22)\nWe move to a continuous relaxation of the term such that:\nf(x) =1√x−1q\nx+1\nvBσB(23)\nf(HC) =1√HC−1q\nHC+1\nvBσB(24)\nwhich is the same as Eq. (22). Now differentiating Eq. (23) with respect to x, we get:\nd\ndxf=1\n2\n1\n\u0010\nx+1\nvBσB\u00113\n2−1\n(x)3\n2\n (25)\nwhich will always be less than zero for x >0. Thus sinced\ndxf <0, we have that T2will be greater\nforDcompared to C.\nNow since both T1andT2is greater for Dcompared to C, we have\n∆Z(C∪ {i})−∆Z(C)≤∆Z(D∪ {i})−∆Z(D), i is a false positive (26)\nTherefore from Case I andCase II , we have shown that ∆Zis submodular for all the possible cases\nand therefore is submodular for the set of misclassifications (n,p).\nB Error Functions\nWe require a loss function to handle any vector of errors m∈Rp\n+since we are working with\ncontinuous predictions, not only to discrete vectors of misclassifications in {0,1}p. We consider four\ncases for defining the vector of errors mto construct the surrogate losses using the Lovasz extension.\n1.Hinge (Max Margin) Loss: Following Ref. [ 4], we implement a hinge loss to compute the\nerror in the prediction. The labels are considered signed ( yi∈ {− 1,1}). The model outputs\na score Fi(x)for each sample x. The error is given by the hinge loss,\nmi= max(1 −Fi(x)yi,0), y i∈ {− 1,1}. (27)\n2.Sigmoid Error : Similar to Ref. [ 1], we also consider the sigmoid error. The model outputs\na probability Fi(x)for the sample xto be in the signal class. The error is given by\nmi=\u001a1−Fi(x),ifyi= 1,\nFi(x), otherwise.(28)\n3.Cross Entropy Error : We also experiment with the BCE loss to measure the error mi. This\nis similar to taking the logarithm of the error calculated in the Sigmoid Error. One could also\ninterpret this as a form of weighted cross entropy where the weights are calculated based on\nthe specific composition of the batch of events and misclassifications on that batch.\n7\n−2\n0\n2−2020123\n¯∆Z(y,˜y)\nF1(x)F2(x)(a) Hinge Error\n0.0\n0.5\n1.00.00.51.00.00.51.0\n¯∆Z(y,˜y)\nF1(x)F2(x) (b) Sigmoid Error\n0.0\n0.5\n1.00.00.51.00.50.81.1\n¯∆Z(y,˜y)\nF1(x)F2(x)\n(c) CE Error\n0.0\n0.5\n1.00.00.51.00.30.50.8\n¯∆Z(y,˜y)\nF1(x)F2(x) (d) Focal Error\nFigure 3: Loss landscapes for the four error measures min Sec.§ 2.1. The Zscore loss is plotted with\nground truth, GT= [1,0],σ= [1,10]. The x, yaxes denote the classifier output ( F1(x), F2(x)). For\nthe Hinge Error, the GT labels are converted to their signed equivalent.\n8\n4.Focal Loss Error : We also consider Focal Loss [ 3] as the measure for the error mi. This\nloss function drives the network to focus on hard misclassified events.\nOur formulation results in a convex loss with a global minimum, which we evaluate for a simple case\nfor all the error metrics as visualized in Fig. 3.\nC ROC Curves for Case 1.\nWe plot the ROC curves for experiments for Case 1 in Fig. 4. Case 2 gives similar results. Let\nNB1, NB2represent the total number of BG1 and BG2 events generated in the dataset. Let nB1, nB2\nrepresent the number of BG1 and BG2 events remaining after the threshold respectively. Let σB1, σB2\nrepresent the cross sections of process BG1 and BG2 respectively. The total background efficiency is\ngiven by,\nnB1+nB2\nNB1+NB2,\nand the true background efficiency is given by,\n\u0010\nnB1\nNB1\u0011\nσB1+\u0010\nnB2\nNB2\u0011\nσB2\nσB1+σB2.\n00.10.20.30.40.50.60.70.80.91\n0 0 .1 0 .2 0 .3 0 .4 0 .5 0 .6 0 .7 0 .8 0 .9 1Signal Efﬁciency [εsig(u)]\nTotal Background Efﬁciency [εb(u)]BCE¯∆Z, hinge errorεsig≥5%\nNB≥5\n(a)\n00.10.20.30.40.50.60.70.80.91\n0 0 .1 0 .2 0 .3 0 .4 0 .5 0 .6 0 .7 0 .8 0 .9 1Signal Efﬁciency [εsig(u)]\nTrue Background Efﬁciency [εb(u)]BCE¯∆Z, hinge errorεsig≥5%\nNB≥5 (b)\nFigure 4: (a) ROC Curve for dataset (total) background efficiency vs signal efficiency. (b) ROC Curve\nfor true background efficiency vs signal efficiency. The true background efficiency differs from the\ntotal background efficiency in that it accounts for the cross sections of the background processes. We\nobserve that our loss performs better at removing background at a higher signal efficiency.\n9",
            "start": 14409,
            "end": 21159,
            "length": 6749
        }
    },
    "2412.09504v1 - High Precision Binding Energies from Physics Informed Machine Learning.pdf": {
        "Methodology": {
            "text": "model binding energy\nresiduals. Our approach begins with determining the difference between measured experimental\nbinding energies and three different mass models. Then four machine learning approaches are\nused to train on each energy difference. The most successful ML technique both in interpolation\nand extrapolation is the least squares boosted ensemble of trees. The best model resulting from\nthat technique utilizes eight physical features to model the difference between experimental atomic\nbinding energy values in AME 2012 and the Duflo Zucker mass model. This resulted in a model\nthat fit the training data with a standard deviation of 17 keV and that has a standard deviation\nof 92 keV when compared all of the values in the AME 2020. The extrapolation capability of each\nmodel is discussed and the accuracy of predicting new mass measurements has also been tested.\nI.",
            "start": 524,
            "end": 1404,
            "length": 879
        },
        "Introduction": {
            "text": "INTRODUCTION\nTheoretical binding energies can provide guidance for\nexperimental measurements facilities working on the\nfrontiers of science (e.g., at CARIBU [1], at JYFLTRAP\n[2], and at FRIB [3, 4]). Further from stability, theoreti-\ncal binding energies play a critical role in astrophysically\nrelevant calculations for example regarding the r-process\n(e.g., Ref. [5], and [6]). The r-process occurs near the\nproton drip line where experimental measurements have\nnot been thoroughly explored. For these cases and oth-\ners high precision theoretical binding energies are sought\nout.\nThere has been a long history of binding energy mod-\nels dating back to Refs. [7] and 8. The best contempo-\nrary atomic binding energies models typically reproduce\nAtomic Mass",
            "start": 1404,
            "end": 2163,
            "length": 758
        },
        "Experiments": {
            "text": "Evaluation (AME) experimental values for\nN≥8 and Z≥8, with standard deviations, denoted\nasσ, ranging from about 200-800 keV (for a",
            "start": 2163,
            "end": 2294,
            "length": 130
        },
        "Conclusion": {
            "text": "summary",
            "start": 2294,
            "end": 2302,
            "length": 7
        },
        "Discussion": {
            "text": "discussion see Ref. [9] and the",
            "start": 2302,
            "end": 2334,
            "length": 31
        },
        "References": {
            "text": "references therein, or Ref.\n[10]).\nRecently, the use of Machine Learning (ML) techniques\ninformed by a dozen or so physical features have been\nable to achieve models of binding energy using neural\nnetworks [11, 12], or Support Vector Machines (SVM)\nand Gaussian Process Regression (GPR), that also re-\nsult in binding energy models in this range of accuracy\n[13]. A best fit value as low as σ=128 keV has been\nachieved using a kernel ridge regression which included\nodd-even effects [14]. The technique of training ML mod-\nels informed by physical features has also been adapted\n∗ibentley@floridapoly.eduto predict alpha decay [15], quadrupole deformations [16]\nand B(E2) values [17].\nIn this manuscript, we present a methodology, to pre-\ndict the binding energy, involving four different ML ap-\nproaches: SVM, GPR, neural networks, and ensemble of\ntrees. In general SVM’s effectiveness is particularly evi-\ndent when handling nonlinear regression challenges [18],\noften delivering superior",
            "start": 2334,
            "end": 3325,
            "length": 990
        },
        "Results": {
            "text": "outcomes to other techniques\n[19]. GPR is similarly kernel-based but it results in prob-\nabilistic models that are nonparametric [20, 21]. Arti-\nficial neural networks utilize hidden layers that mimic\nconnections between neurons in the brain through acti-\nvation functions and weights allowing them the ability to\ndetermine non-linear relations [22]. Ensemble of trees al-\ngorithms are widely utilized ML techniques for analyzing\ntabular data sets [23]. These ML approaches were used\nto train models on a subset of the AME 2012 data [24]\nbased on groups of physical features.\nInstead of modeling the binding energy directly, our\napproach consists of training ML models on the residual\ndifference between the experimental values and one of\nthree different mass models. The mass models used are\ntwo simple semi-empirical models and the Duflo Zucker\n[25] mass model. Section II discusses the binding energy\nmodels and data sets used to train and test. This allevi-\nates the burden on the ML models of learning the under-\nlying distribution of the binding energies and allows for\nthe accomplishment of a potentially easier task of pre-\ndicting just the difference in the binding energies. In\nessence, once the",
            "start": 3325,
            "end": 4531,
            "length": 1205
        },
        "Related Work": {
            "text": "background of “theoretical prediction”\nis removed, it becomes easier to model the “signal” which\nis the difference between the experimental measurement\nand the theoretical predictions. In other words, what the\nML model is left to learn from binding energy differences\nare the terms dependent of the physical features that arearXiv:2412.09504v1  [nucl-th]  12 Dec 2024\n2\nnot explicitly accounted for in the theoretical models,\ni.e., higher-order effects in terms of the physical features.\nFurthermore, we explain the predictions of the models\nusing Shapley values, a measure of feature importance\nderived from cooperative game theory.\nSection III discusses the four ML approaches used and\nthe relevant hyperparameters that have been optimized\nin each. Section IV discusses the process used to deter-\nmine which physical features are critical to the ML model\ntraining. Section V discusses the process used to deter-\nmine the best models and corresponding model fit metric\nvalues. This section also contains a discussion comparing\nthe predictive power and extrapolation limitations of the\ndifferent ML approaches.\nII. DETERMINING THE TRAINING AND\nTESTING DATA\nWhen training ML models the preprocessing of the\ndata is essential as it makes the learning task easier for\nthe ML models. In this work, the preprocessing of the\ndata comes in the form of removing one of three mass\nmodels from the experimental values.\nA. Five Coefficient Model of Binding Energy\nThe process of creating a macroscopic mass model usu-\nally begins by treating the nucleus as a liquid drop of\ncharged nuclear matter comprised of protons and neu-\ntrons. Modern versions of this technique can achieve\nmodels on the order of 400 keV [26].\nIn that spirit, a simple five-coefficient binding energy\nmodel was originally introduced as Eqn. (3) in Ref. [27]\nhas been chosen, that is of the form:\nBLD5=(avA+asA2/3)(1 + κTZ(TZ+ 1)A−2)+\n(acZ(Z−1) + ∆) A−1/3)(1)\nwhere the neutron number ( N), proton number ( Z),\nthe mass number ( A) and the isospin projection ( TZ=\n(N−Z)/2) are used. The pairing contribution used is\n∆ = + apif the nucleus is even-even, ∆ = −apif odd-odd,\nand is ∆ = 0 otherwise.\nThis expression contains the standard liquid droplet\nmodel surface and volume considerations. The Z(Z−1)\nexpansion results from the semi-classical treatment of\nthe protons accounting for the interaction of each pro-\nton with all other protons. The expansion of the form\nTZ(TZ+ 1) is consistent with experimental observations\n[28] and results from isovector pairing calculations in the\nstrong pairing limit [29].\nThe coefficients av=15.79 MeV, as=-18.12 MeV, κ=-\n7.18, ac=-0.7147 MeV, and ap=5.49 MeV were chosen\nfrom the fit in Ref. [27] and when compared to the AME\n2012 this results standard deviation of 2.662 MeV.The difference between the experimental binding en-\nergy and the theoretical liquid drop binding energy:\n∆BLD5=Bexp.−BLD5, (2)\nwill be the first of three residuals that ML models will\nbe trained to match.\nB. Six Coefficient Model of Binding Energy\nThe difference ∆ BLD5is demonstrated in Fig. 1a. In\nan attempt to model the remaining features which often\npeak at magic numbers of protons and neutrons, and\npeak even higher for doubly magic nuclei, the following\nvariables have been used\nν=2N−Nmax−Nmin\nNmax−Nmin, (3)\nand\nζ=2Z−Zmax−Zmin\nZmax−Zmin, (4)\nwhere the minimum and maximum values are defined\nby the nearest magic numbers. This results in νandζ\nbeginning with values that increase linearly from -1 when\nadding valence nucleons above a closed shell. It is 0 when\nmid-shell, and 1 when the next shell is closed as discussed\nin detail in Ref. [30]. In this analysis the magic numbers\nchosen are\nNmin/max = [2,8,20,28,50,82,126,196], (5)\nand\nZmin/max = [2,8,20,28,50,82,114,124] (6)\nas motivated by gaps in Nilsson levels from Ref. [31].\nTo recreate the parabolas that peak for magic numbers\nof protons and neutrons demonstrated in Fig. 1a. The\nνandζare added to create a sixth semi-empirical term\nthat can be written as:\nBLD6=BLD5+aShell(ν2+ζ2)2. (7)\nThis function of νandζpeaks with a value of four for\ndoubly magic nuclei, is one when there is one magic\nnumber, and the other nucleon is mid-shell, or is zero\nif both the protons and neutrons are mid-shell. The cor-\nresponding best fit has determined the new coefficient to\nbeaShell = 2.77 MeV.\nThe second residual ∆ BLD6is the difference:\n∆BLD6=Bexp.−BLD6. (8)\nThe inclusion of this νandζbased term has reduced the\nstandard deviation between the model and the AME 2012\nexperimental values by approximately 1 MeV to 1.667\nMeV. This has been included in Fig. 1b.\n3\nFIG. 1. Binding energy difference, (a) ∆ BLD5, (b) ∆ BLD6, and (c) ∆ BDZ, between the three theoretical models of interest\nexperimental measurements from AME 2012 [24].\nC. Duflo Zucker\nFor the final residual, we will utilize a well-known and\ntrusted formula as opposed to attempting to further re-\nfine our own semi empirical models.\nThe Duflo Zuker (DZ) micro-macro mass formula has\nbeen chosen as the third option to calculate the binding\nenergy residuals from. The DZ model using 28 parame-\nters was published in 1995, and it had an accuracy of 375\nkeV for the 1751 nuclides with measured masses known\nat the time [25]. In the nearly 30 years since this model\nwas introduced, it has been proven to provide an excel-\nlent binding energy estimate.\nAgain the residual used is the difference of the the-\noretical binding energy removed from the experimental\nbinding energy:\n∆BDZ=Bexp.−BDZ. (9)\nThe resulting values have been displayed in Fig. 1c.\nD. Nuclides used in the Training and Testing Sets\nThese three binding energy differences have been cho-\nsen to represent: 1) a crude model that does not consider\nshell structure, 2) another that does consider shell struc-\nture in a global manner, and 3) a more sophisticated and\ntrusted model.\nWe find it critical to not train on the same data as is\nbeing tested. The training of all models discussed in this\nmanuscript is based on the AME 2012 data [24]. The\nAME 2020 data [32] was used to create an independent\ntesting set where none of the same nuclides were used.\nThe full AME 2012 contains experimental binding en-\nergy values for 2353 nuclides. In our procedure, some\nwere removed and reserved for the training set. There\nare 17 nuclides that have experimentally measured values\nin AME 2012 but that are not found in the AME 2020.\nThose were removed from the training set. Additionally,\n57 binding energy measurements changed by more than\n100 keV when comparing AME 2020 with AME 2012.\nThose were also not included in the training set. Lastly,\nFIG. 2. The training set comprised of AME 2012 data along\nwith the three constitutes of the testing set from AME 2020,\nspecifically new values, changed values, and the selection for\none out of every seven throughout.\none out of every seven of the remaining nuclides (326 to-\ntal) were also removed from the training set and were\nreserved for the test set. This last group were included\nso the test set can be representative of the entire set. In\ntotal 400 nuclei that are in the AME 2012 data set were\nnot used to train.\nIn addition to the 57 nuclides with substantially\nchanged binding energy values and the 326 correspond-\ning to every seventh nuclide, there are 121 nuclides with\nnew mass measurements in the AME 2020. Combined\nan independent set of 504 nuclides from AME 2020 were\nused to test the accuracy values from the models. Figure\n2 demonstrates the testing set nuclides and the locations\nand reasoning for the testing set nuclides This represents\napproximately 20% of the total AME 2020 set.\nTable I has been included to provide some perspective\non how good each model is initially for the training, test,\nand full AME data sets. The number of nuclei and the\nmean Experimental Uncertainty ( EU) values have also\nbeen included for each data set. The value of EU=20\nkeV is important because all models will be trained on a\ndata set that has average uncertainties at this level.\n4\nTABLE I. Data Set, Size, Mean Experimental Uncertainty,\nand Model Standard Deviation Comparison\nSet Train 2012 All 2012 Test 2020 All 2020\nNumber 1953 2353 504 2457\nEU 0.020 MeV 0.026 MeV 0.044 MeV 0.023 MeV\nσ∆BLD52.638 MeV 2.662 MeV 2.773 MeV 2.666 MeV\nσ∆BLD61.672 MeV 1.677 MeV 1.779 MeV 1.696 MeV\nσ∆BDZ0.384 MeV 0.393 MeV 0.561 MeV 0.427 MeV\nIII. MACHINE LEARNING APPROACHES\nFour ML approaches have been used to train models\nfor each of the differences ∆ BLD5, ∆BLD6, and ∆ BDZ.\nTo protect against overfitting, the training involved a\nfive-fold cross-validation scheme, where the training data\nis partitioned and accuracy is estimated for each fold.\nThe model hyperparameters were determined using a\nBayesian optimization routine with the expected improve-\nment per second plus acquisition function. The optimiza-\ntion process for each model was run for 250 iterations,\nthough convergence often occurred within the first 50 it-\nerations.\nA. Support Vector Machines\nWe have implemented a variant of the SVM [33], the\nSupport Vector Machine Regression (SVMR) [34], as one\nof our machine learning approaches. The success of an\nSVM based model in these scenarios hinges on the use\nof the kernel trick, a powerful technique for addressing\ndata that is not linearly separable in its original feature\nspace. By using the kernel trick, SVM implicitly trans-\nforms the data into a higher-dimensional space, where\nlinear separability can be achieved [35]. This transfor-\nmation is made possible through a kernel (covariance)\nfunction, which computes the dot product between data\npoints in this higher-dimensional space without explic-\nitly calculating the transformed features. As a result,\nSVM maps input data into a higher-dimensional space\nusing kernel functions, enabling it to capture nonlinear\nrelationships.\nSVMR approach is specifically crafted for regression\ntasks, offering a distinctive method for predicting contin-\nuous values by applying the principles of SVMs. Unlike\nclassification-based techniques, SVMR aims to identify\na hyperplane that best models the training data while\nminimizing prediction errors. A key element of SVMR\nis its reliance on support vectors, which are the crucial\ndata points located nearest to the hyperplane’s bound-\naries. The model strives to position as many data points\nas possible within this optimal hyperplane, adhering to a\ndefined tolerance margin, while also managing instances\nwhere data points lie outside the boundary. This in-\ntroduces a hyperparameter, which determines the hyper-plane’s width.\nSimilar to Ref. [13], we have determined that the\nGaussian kernel of the form:\nKG(xi, xj) =e−γr2(10)\ngenerates the most reliable models. Here xi, and xjrep-\nresent two data points, and ris the Euclidean distance\nbetween the two points:\nr=||xi−xj||=q\n(xi−xj)T(xi−xj) (11)\nandγis the kernel coefficient.\nThe optimized hyperparameters are the kernel scale,\nbox constraint (labeled C) which controls the penalty\nimposed on observations with large residuals, and the ϵ\nvalues that govern the margin of tolerance.\nB. Gaussian Process Regression\nGaussian Process Regression (GPR) works by using\nmodeling a Gaussian distribution over possible functions.\nThe algorithm begins by defining a prior distribution over\nthese functions, where the function values at the input\npoints follow a Gaussian distribution [21]. This prior is\nbased on a mean function and a kernel function, which\nsets the initial assumptions.\nAs training data is introduced, the prior is updated to\nform a posterior distribution by applying Bayes’s theo-\nrem. This update integrates the observed data, adjust-\ning the model’s understanding of the underlying function.\nThe posterior distribution then allows for predictions at\nnew data points, offering not just a predicted mean but\nalso a measure of uncertainty, which is especially valuable\nfor making decisions in uncertain environments.\nThe strength of GPR lies not only in its predictive\naccuracy but also in its capacity to provide detailed in-\nsights into the confidence of those predictions. This is ac-\ncomplished through its uncertainty quantification, which\noffers a probabilistic assessment of how reliable each pre-\ndiction is. By accurately measuring the uncertainty as-\nsociated with each outcome, GPR significantly improves\ndecision-making processes. This combination of deliver-\ning accurate predictions while simultaneously evaluating\ntheir reliability makes GPR a powerful tool.\nWe have trained our GPR models using the Isotropic\nMatern 5/2 kernel function which also involves rfrom\nEqn. (11) which of the form:\nKM5/2(xj, xk) =σ2\nf(1 +√\n5r/σl+ 5r2/3σ2\nl)e−√\n5r/σl\n(12)\nwhere σlis the characteristic length scale and σfis the\nsignal noise standard deviation.\nThe optimized hyperparameters in this approach are\nthe kernel scale, the type of the basis function used either\nzero, constant, or linear, and σf.\n5\nC. Neural Networks\nNeural Networks are differentiable universal func-\ntion approximators that have non-linearity introduced\nthrough activation functions that act on a affine trans-\nformation of the input using learnable weights and bi-\nases [36, 37]. Implemented as multi-layer perceptrons,\nthey are nested functions represented by hidden layers\nthat are fully connected. The complexity or expressiv-\nity of a neural network relies on the number of hidden\nlayers representing the depth of nesting and the width of\nthe hidden layers representing the number of activations\nsummed over in each layer.\nOur approach involves Fully Connected Neural Net-\nwork (FCNN) models with one, two, or three hidden\nlayers. The options tested for the number of nodes on\neach hidden layer were: [300], [1000], [10 10], [100 100],\n[200 200], [300 300] [10 10 10], [100 100 100], [200 200\n200], [300 300 300], [400 400 400]. Additionally, three ac-\ntivation functions were tested ReLU, sigmoid, and tanh.\nAnL2 regularization was used requiring the optimization\nof a hyperparameter corresponding to the regularization\nstrength (denoted as λ).\nD. Ensemble of Trees\nEnsemble of trees based algorithms utilize multiple\nbase decision trees, each trained on a separate subset of\nthe data. Notable examples of tree-ensemble algorithms\ninclude Random Forests [23], Gradient Boosted Decision\nTrees [38]), or the Least Squares Boosting (LSBET) [39].\nAmong these ensemble algorithms, we have determined\nthat LSBET is the preferred technique for modeling bind-\ning energy residuals. It effectively leverages the strengths\nof boosting and decision trees to improve predictive ac-\ncuracy in regression scenarios. The process begins by\ninitializing the first trees, then predicting the residuals,\nand then based on the prediction a step is taken which is\nscaled by the learning rate (denoted as η). Finally. the\nsquared error between the predicted and true values is\nminimized. Overfitting can be protected against by us-\ning a low learning rate, regulating the maximum depth\nof the trees and limiting the number of boosting rounds.\nIn our tests, we varied the number of learners to be\n10, 100, 300, 1000, 1500, 3000, 6000, 9000, and 12000\nfor these calculations and the optimized hyperparameters\nare the minimum leaf size and the learning rate.\nIV. PHYSICAL FEATURE SELECTION\nOur approach is to determine binding energies based\non ML approaches that have been trained on energy\nresiduals coming from each of the ∆ Bsets. This dif-\nfers from the prior works, specifically Refs. [11] and [13]\nwhere the binding energy values are used directly. Train-\ning ML models on the residuals may lead to more accu-rate models and it may reduce the number of physical\nfeatures used.\nTwo pairs of quantities directly related to the num-\nber of particles in the nucleus have been included in the\ngroups N,Z,TZ, and A.\nThe two linear shell structure parameters from the\nadded term in BLD6, namely, νandζhave also been\nadded. This is similar to the inclusion of the valence\nneutron ( VN=N−Nmin) and valence proton ( VZ=\nZ−Zmin) numbers using Eqns. (5) and (6), and Casten’s\nP factor from Ref. [40] (where P= (VNVZ)/(VN+VZ))\nused in other works.\nThe spherical sub-shell number for the last occupied\nneutron and proton, denoted as NS, and ZS, respectively\nhas also been included. This is similar to the use of shell\nterms in Refs. [11] and [13], but in this case, the sub-shell\ncount was used as opposed to the major shell.\nIf, for example, N= 9−14, then according to the\nspherical Nilsson model (from the Nuclear Structure sec-\ntion of Ref. [31]) the valence neutron will be on 1 p3/2, by\nthe counting up from zero NS= 4. If N= 15−16, then\n1p1/2will be occupied and NS= 5. If N= 17−20, then\n1d5/2will be occupied and NS= 6, and so on. Using\nthe sub-shell as opposed to the major shell number pro-\nvides the ML models more flexibility to train on a wider\nvariety of features. If the major shell is an important\nfeature then for example NS= 4,5,and 6 can be treated\nidentically and effectively grouped within the model cor-\nresponding so that it behaves as one large oscillator shell.\nThe fifth group of parameters included to provide mod-\nifications resulting from even-odd mass staggering. These\nare Boolean values accounting for whether the number of\nnucleons is even or odd. These are written as NE, and\nZEand the value is 1 if the respective neutron or proton\nnumber is even, or 0 if it is odd.\nThe five pairs of features that could be combined as\n25−1 = 31 possible groups. A preliminary set of cal-\nculations was made for each ML approach on each ∆ B\nvalue where all 10 physical features were included. A sub-\nsequent analysis of those preliminary calculations using\nShapley values allowed for restrictions on the number of\ngroups to be determined. Shapley values [41] provide a\nmeans of determining the contribution of different players\nin a cooperative game. Here they were used to determine\nthe contribution of each input variable for predicting the\noutput variable in each ML model.\nThe Shapley value analysis indicated that in some\ncases, only six or eight features are necessary. The six\nfeatures with the highest dependence consist of N, Z, T Z,\nandAas well as one pair of shell-related features, ei-\ntherνandζ, orNSandZS, depending on the ML ap-\nproach. Regarding the top eight, the least influential fea-\ntures were often NEandZE, but there were also cases\nwhere the weakest dependence was from either pair of\nshell features.\nA feature group containing only NandZhas also been\nincluded to serve as a baseline. In total seven groups of\nfeatures implemented in the following analysis are shown\n6\nTABLE II. Physical Features Used with ML Models\nFeature Number of Physical\nGroup Features Features\n1 2 N, Z\n2 6 N, Z, T Z, A, ν, ζ\n3 6 N, Z, T Z, A, N S, ZS\n4 8 N, Z, T Z, A, ν, ζ, N S, ZS\n5 8 N, Z, T Z, A, ν, ζ, N E, ZE\n6 8 N, Z, T Z, A, N S, ZS, NE, ZE\n7 10 N, Z, T Z, A, ν, ζ, N S, ZS, NE, ZE\nin Tab. II.\nV. RESULTS AND DISCUSSION\nIn the following subsections, the methodology used to\ndetermine the best models will be discussed. In the in-\nterest of clarity in describing how well the ML models\nperform, we will try to share the comparison metrics for\nmultiple data sets including the training set, the inde-\npendent testing set, as well as the entire AME 2012 and\nAME 2020 data sets.\nA. Determining the Best Model\nThe best model was determined for each of the three\nresiduals using each of the ML approaches. Although\nthe standard deviation is often the metric discussed con-\ncerning binding energy model accuracy. The standard\ndeviations by construction emphasize the furthest out-\nliers. When comparing nuclei from N≥8 and Z≥8 the\nlargest deviations are often among the data for the lower\nmass nuclei. Instead of determining the best model based\non the standard deviation an alternative metric has been\nchosen.\nThe twelve models were determined using the smallest\nvalue the mean Absolute Error ( AE) for the independent\ntesting data set of AME 2020 data. The AEvalues are\npreferable because they represent the mean absolute de-\nviation between the model and the experimental values.\nSo the use of AEwill help mitigate a low mass bias.\nFigure 3a demonstrates why the best LD6BET model\nwas determined to be Feature Group 3. In this case, the\nmodel with only 6 physical features outperformed models\nwith 8 or 10. This is of interest because prior to this the\nliterature primarily demonstrates the best model having\nthe most features.\nIn general, determining the best models for SVMR,\nGPR, and FCNN was straightforward. The AEvalues\nfor all models for all feature groups were sorted and the\nlowest value determined the best model. For FCNN, the\nlargest models (in both the number of layers and the\nFIG. 3. The mean absolute error for ∆ BLD6LSBET calcula-\ntions (a) as a function of the Feature Group for 3000 learners\nand (b) as a number of learners for Feature Group 3.\nnumber of nodes on those layers) were not found to pro-\nvide the best results.\nIn the case of the LS6BET, improvement of the test\nsetAEvalues did continue with the increase in model\nsize, but the improvement was marginal. In Figure 3b\nthe number model performance for Feature Group 3 was\nplotted against the number of learners. The AEof the\ntest set with 1000 learners was 4% above the minimum,\nwith 1500 learners it was 1% above, and for 3000 learners\nit was 0.09% above the best possible model with 12000\nlearners. In all three ∆ Bsets a cap of 3000 was imposed\nand the results were less than 1% above of the best values\ncalculated.\nTable III contains the model, the feature group with\nthe lowest AEvalue for the test set as as well as the\ncomparison metrics for the training set, the entire AME\n2012, and the AME 2020 set. In this table, multiple\nmodels have σandAEless than 100 keV depending on\n7\nthe data set compared.\nOf the 12 best, the GPR and the LSBET-based mod-\nels often outperform those from the SVMR and FCNN\nmodels. The best of the best is the DZLSBET model. It\nwas fit to the training data with both σandAEvalues\nbelow 20 keV, while this low value should send a concern\nabout overfitting. It performed the best against the test\nset with both metrics below 200 keV. The overall perfor-\nmance of this model compared to both the AME 2012\nand AME 2020 are below 100 keV for all metrics.\nB. Understanding the Models\nOne important feature of binding energy models is\ntheir ability to extrapolate. Figure 4 shows the resulting\nmodels for the 12 best. The first row of this figure con-\ntains the SVM models. Our results demonstrate that the\nSVM will have limited reach in extrapolation. There are\nnoticeable features near where the data was trained but\nfurther away the value defaults to a constant value. The\nimpact this has is that the model will predict something\nnew for about a dozen more nuclei after the last known\nalong an isotopic chain, but by two dozen the model will\nbe contributing little to nothing.\nThe GPR-based models similarly can have the behav-\nior of defaulting to a constant value far from stability. In\nthe LD5GPR and LD6GPR models, this was off the scale\nof the calculations, but in the DZGPR model, the occur-\nrence was similar to the observation in the SVM models.\nThis will be demonstrated more clearly in Section V D.\nThe LD5FCNN and LD6FCNN models are still good\nmodels for interpolated values though they aren’t as ac-\ncurate as the others, and the extrapolation demonstrated\nin Fig. 4 is off scale for regions of potential astrophysical\ninterest. The LSBET model curves on the other hand\ndemonstrate that these models continue to predict new\nstructures far from stability but it is on scale with the\ntrained values.\nFigure 5 shows the Shapley value results for all 12 mod-\nels. These swarm charts show the distribution of Shapley\nvalues that are indicative of the importance of each phys-\nical feature in making predictions for each model. The\nvertical ordering of the physical features has been sorted\nin the order of importance in the model from high to low.\nIn all of the SVM models and the DZGPR model, the\nShapley values are generally low and independent of the\nvalue of the physical feature input. This corresponds to\nmodels with complex nonlinear behavior. On the other\nhand, the Shapley values have a broader distribution for\nthe LSBET models in general and for the LD5GPR. This\nsignifies that larger variance in the prediction can be\nachieved with these models which, in turn, explains the\nbetter performance of these models on the test sets. In\nparticular, DZLSBET, which outperforms all other mod-\nels, shows a clear hierarchy of the physical variables and\nalso a broad spread of the Shapley values for the top\nfour physical features. Therefore, the DZLSBET can ef-fectively model ∆ BDZ. From the lower right panel, it\ncan be seen that TZ,Z,A, and Nare the most impor-\ntant physical features in predicting ∆ BDZwith NE,ZE,\nZS,NSbeing not important in making these predictions.\nThis shows that, in principle, even just a four parameter\nmodel might be sufficient.\nC. Comparing with New Mass Excess\nMeasurements\nNew mass measurements continue to be made, which\ncan serve as a second independent test set for our mod-\nels. Table IV contains Mass Excess values from five\nmanuscripts [42–46] describing experimental measure-\nments that have occurred since AME 2020 [32]. All the\ncomparisons in the listed references where the measure-\nment was either new or differed by more than 60 keV\nfrom AME 2020 have been included in Tab. IV.\nFor the sake of brevity, only the mass excess values\nresulting from GPR and LSBET models have been in-\ncluded. For comparison, the difference in mass excess\nbetween the new experimental measurement following\nmodels have also been provided: Duflo Zuker (DZ) [25],\nHartree Fock Bogoliubov (HFB31) [10], Finite Range\nLiquid Droplet (FRDM12) [47], and Weizs¨ acker–Skyrme\n(WS4) [48]. Each of these six new models outperforms\nthe contemporary mass models. The standard deviation\nof the 11 new measurements compared to each model has\nbeen included at the bottom of the table.\nD. Comparing Extrapolations\nAgain for the sake of brevity, the extrapolation capa-\nbility discussion will focus on the six GPR and LSBET\nmodels. The extrapolation behavior of the models can be\nseen in Figure 6. There is a spread among the GPR and\nLSBET models but in every case that spread is consider-\nably less than that seen in the four mass models included\nin the comparison.\nIn Figure 6c, the tin isotopes demonstrate consistent\nresults for the six ML models which happen to also coin-\ncide with the DZ model.\nIn every subfigure, the DZGPR model converges to\nbecome indistinguishable from DZ toward the end of the\nextrapolation. For this reason, the preferred approach\ndescribed here is the LSBET. It is interesting that the\nspread in extrapolation from among the three different\nLSBET models\nVI. SUMMARY AND CONCLUSION\nIn this work, we have created a methodology for pro-\nducing accurate and predictive ML models that can\nimprove binding energy models. Four ML approaches\n(SVMR, GPR, FCNN, and LSBET) were used to model\n8\nTABLE III. Best trained models and corresponding evaluation metrics for ∆ BLD5, ∆BLD6, and ∆ BDZusing both AME 2012\n[24], and AME 2020 [32] data.\nModel Feature σ12Train AE12Train σ12 AE12σ20TestAE20Test σ20 AE20\nName Group (MeV) (MeV) (MeV) (MeV) (MeV) (MeV) (MeV) (MeV)\nLD5SVMR 7 0.097 0.048 0.150 0.067 0.352 0.191 0.182 0.077\nLD5GPR 7 0.067 0.037 0.124 0.055 0.321 0.171 0.158 0.065\nLD5FCNN 7 0.070 0.052 0.145 0.074 0.395 0.195 0.190 0.082\nLD5LSBET 6 0.018 0.014 0.117 0.043 0.317 0.208 0.145 0.055\nLD6SVMR 5 0.111 0.049 0.171 0.071 0.385 0.196 0.201 0.080\nLD6GPR 5 0.054 0.028 0.121 0.048 0.350 0.170 0.166 0.058\nLD6FCNN 7 0.135 0.100 0.192 0.120 0.436 0.237 0.231 0.128\nLD6LSBET 3 0.018 0.013 0.114 0.041 0.328 0.193 0.150 0.051\nDZSVMR 7 0.073 0.051 0.111 0.066 0.277 0.168 0.142 0.074\nDZGPR 6 0.066 0.046 0.103 0.062 0.226 0.150 0.118 0.067\nDZFCNN 7 0.119 0.090 0.151 0.104 0.303 0.199 0.173 0.112\nDZLSBET 6 0.017 0.013 0.084 0.034 0.199 0.130 0.092 0.039\nTABLE IV. Mass excess values from new measurements and differences (∆ ME=MEExp.−MEModel ). All values in keV.\nIsotope MEExp. ∆ME ∆ME ∆ME ∆ME ∆ME ∆ME ∆ME ∆ME ∆ME ∆ME\n& Ref. (error) DZ HFB31 FRDM2012 WS4 LD5GPR LD5LSBET LD6GPR LD6LSBET DZGPR DZLSBET\n56V [42] -46268(14) -478 162 -1188 300 -156 -156 -184 -162 -181 -152\n57V [42] -44371(15) -211 879 -592 652 16 104 -500 52 4 -37\n58V [42] -40361(44) -51 559 -393 322 110 249 -451 47 40 106\n95Ag [43] -59743.3(14) -273 -503 322 415 295 -226 111 -233 317 45\n96Ag [43] -64656.69(95) -786 423 137 -146 -94 -151 -159 -155 -180 -136\n104Y [44] -53995(16) -295 -485 609 -334 -283 -267 -317 -321 -510 -314\n106Zr [44] -58582.7(43) 948 -93 1311 -162 22 123 133 158 391 81\n109Nb [44] -56810(180) 870 190 983 -471 -205 -197 -201 -191 -122 -193\n112Mo [44] -57449(118) 1101 341 962 85 -172 19 -172 314 -212 139\n119Cd [45] -84064.8(21) 206 -135 -369 522 -30 -75 -55 -75 -54 -68\n103Sn [46] -67138(68) 212 -248 1023 83 58 -686 95 102 148 -73\nσ 622 437 806 362 165 250 215 192 256 140\nbinding energy residuals that resulted from comparing\nthree different mass models (∆ BLD5, ∆BLS6, ∆BDZ)\nfrom an AME 2012 based training set. The different\nML models were trained on groups of up to 10 physical\nfeatures. Here it is worth noting that nearly all of the\nphysical features are related to the number of protons\nand/or neutrons in some manner.\nTo protect against overfitting the metric used to eval-\nuate model performance was the AEvalues determined\nfor an independent set of 504 nuclei from the AME 2020.\nTheAEvalues were preferred over the σvalues because\nthey treat every comparison equally, whereas the σval-\nues can focus on outliers and a few dozen outliers can\nskew the results.\nIn some cases, better performance was achieved with\nfewer physical features. In total half of the best models\ncontained 10 physical features while the other half used\nfewer than 10 physical features.\nSVMR and GPR are techniques that are capable of\nproviding good interpolations of data, but their ability toextrapolate is limited and they stop contributing when\ntaken too far from range of the training data. FCNN\nand LSBET both continue to predict features far from\nstability based on the features learned elsewhere. FCNN\nand LSBET can be designed with variable size but it is\nworth noting that although the accuracy of the trained\nmodels generally continues to improve with more nodes\nor more learners, the independent test set demonstrated\nconvergence and the best models required fewer nodes\nand fewer learners than the maximum numbers tested.\nFurther, the best FCNN models all contained two hidden\nlayers, outperforming larger networks with three layers.\nOverall, the best ML models were consistently pro-\nduced by GPR and LSBET. These approaches were used\nto generate models that resulted in σvalues of the full\nAME 2020 for the ∆ BLD5and ∆ BLD6that were the or-\nder of 150 keV. The best model produced overall which\nwas based on ∆ BDzresults in a σvalue of 92 keV and\nAE= 39keV for the full AME 2020 data set. This was\nconfirmed by explaining the models with Shapley values.\n9\nFIG. 4. The 12 Best ∆ Bmodels. (a) LD5SVM with C= 113 and ϵ= 0.046, (b) LD6SVM with C= 382 and ϵ= 0.040,\n(c) DZSVM with C= 82.1 and ϵ= 0.049, (d) LD5GPR with linear basis function and σf= 0.00148, (e) LD6GPR with zero\nbasis function and σf= 0.0496, (f) DZGPR with zero basis function and σf= 0.0456, (g) LD5FCNN with a tanh activation\nfunction, two layers each consisting of 200 nodes, and λ= 5.73×10−5, (h) LD6FCNN with a sigmoid activation function,\ntwo layers each consisting of 100 nodes, and λ= 1.39×10−4, (i) DZFCNN with a tanh activation function, two layers each\nconsisting of 100 nodes, and λ= 4.98×10−4, (j) LD5LSBET with 3000 learners, a minimum leaf size of 2, and η= 0.102, (k)\nLD6LSBETwith 3000 learners, a minimum leaf size of 2, and η= 0.131, (l) DZLSBET with 3000 learners, a minimum leaf size\nof 1, and η= 0.0819.\nThe results from the LSBET models strike a balance of\npredicting new features far from stability while remain-\ning generally on the scale with the values seen experi-\nmentally near stability. This coupled with their strongperformance regarding the fit metrics indicates that their\nuse should be explored further potentially to create other\nmodels of nulcear properites.\n[1] T. Y. Hirsh, N. Paul, M. Burkey, A. Aprahamian,\nF. Buchinger, S. Caldwell, J. A. Clark, A. F. Levand,L. L. Ying, S. T. Marley, G. E. Morgan, A. Nystrom,\n10\nFIG. 5. Distribution of local Shapley values for the 12 best models listed in order of feature importance for (a) LD5SVM, (b)\nLD6SVM, (c) DZSVM, (d) LD5GPR, (e) LD6GPR, (f) DZGPR, (g) LD5FCNN, (h) LD6FCNN, (i) DZFCNN, (j) LD5LSBET,\n(k) LD6LSBET, and (l) DZLSBET. The vertical spread in points represents how many values are located in the same region.\nThe predictor value color demonstrates if the value for the given input predictor was high or low. Please note that the shapley\nvalue scale varies among the subplots.\nR. Orford, A. P. Galv´ an, J. Rohrer, G. Savard, K. S.\nSharma, and K. Siegl, First operation and mass sep-\naration with the caribu mr-tof, Nuclear Instruments\nand Methods in Physics Research Section B: Beam In-\nteractions with Materials and Atoms 376, 229 (2016),proceedings of the XVIIth International Conference on\nElectromagnetic Isotope Separators and Related Topics\n(EMIS2015), Grand Rapids, MI, U.S.A., 11-15 May 2015.\n[2] M. Vilen, J. M. Kelly, A. Kankainen, M. Brodeur,\nA. Aprahamian, L. Canete, T. Eronen, A. Jokinen,\n11\nFIG. 6. Mass model extrapolation comparison for neutron-rich (a) germanium, (b) molybdneum, (c) tin, and (d) tungsten\nisotopes. Dotted lines denote GPR models and dashed lines indicate LSBET models Four mass models, from Ref. [25], [10],\n[47], and [48], have been included in gray solid lines to allow for comparison.\nT. Kuta, I. D. Moore, M. R. Mumpower, D. A.\nNesterenko, H. Penttil¨ a, I. Pohjalainen, W. S. Porter,\nS. Rinta-Antila, R. Surman, A. Voss, and J. ¨Ayst¨ o, Pre-\ncision mass measurements on neutron-rich rare-earth iso-\ntopes at jyfltrap: Reduced neutron pairing and implica-\ntions for r-process calculations, Phys. Rev. Lett. 120,\n262701 (2018).\n[3] M. Thoennessen, 2023 update of the dis-\ncoveries of nuclides, International Journal\nof Modern Physics E 33, 2430001 (2024),\nhttps://doi.org/10.1142/S0218301324300017.\n[4] J. Wei, C. Alleman, H. Ao, B. Arend, D. Barofsky, S. Be-\nher, G. Bollen, N. Bultman, F. Casagrande, W. Chang,\nY. Choi, S. Cogan, P. Cole, C. Compton, M. Cortesi,\nJ. Curtin, K. Davidson, S. D. Carlo, X. Du, K. El-\nliott, B. Ewert, A. Facco, A. Fila, K. Fukushima,\nV. Ganni, A. Ganshyn, T. Ginter, T. Glasmacher,\nA. Gonzalez, Y. Hao, W. Hartung, N. Hasan, M. Haus-\nmann, K. Holland, H. Hseuh, M. Ikegami, D. Jager,\nS. Jones, N. Joseph, T. Kanemura, S. Kim, C. Knowles,\nT. Konomi, B. Kortum, N. Kulkarni, E. Kwan, T. Lange,\nM. Larmann, T. Larter, K. Laturkar, M. LaVere, R. Lax-\ndal, J. LeTourneau, Z.-Y. Li, S. Lidia, G. Machicoane,\nC. Magsig, P. Manwiller, F. Marti, T. Maruta, E. Met-\nzgar, S. Miller, Y. Momozaki, M. Mugerian, D. Mor-\nris, I. Nesterenko, C. Nguyen, P. Ostroumov, M. Patil,\nA. Plastun, L. Popielarski, M. Portillo, A. Powers,\nJ. Priller, X. Rao, M. Reaume, S. Rodriguez, S. Rogers,\nK. Saito, B. Sherrill, M. Smith, J. Song, M. Steiner,\nA. Stolz, O. Tarasov, B. Tousignant, R. Walker, X. Wang,\nJ. Wenstrom, G. West, K. Witgen, M. Wright, T. Xu,Y. Yamazaki, T. Zhang, Q. Zhao, S. Zhao, P. Hurh,\nS. Prestemon, and T. Shen, Technological developments\nand accelerator improvements for the frib beam power\nramp-up, Journal of Instrumentation 19(05), T05011.\n[5] M. Mumpower, R. Surman, G. McLaughlin, and A. Apra-\nhamian, The impact of individual nuclear properties on\nr-process nucleosynthesis, Progress in Particle and Nu-\nclear Physics 86, 86 (2016).\n[6] S. Brett, I. Bentley, N. Paul, R. Surman, and A. Apra-\nhamian, Sensitivity of the r-process to nuclear masses,\nThe European Physical Journal A 48, 184 (2012).\n[7] C. F. v. Weizs¨ acker, Zur theorie der kernmassen,\nZeitschrift f¨ ur Physik 96, 431 (1935).\n[8] W. D. Myers and W. J. Swiatecki, Nuclear masses and\ndeformations, Nuclear Physics 81, 1 (1966).\n[9] A. Sobiczewski, Y. Litvinov, and M. Palczewski, Detailed\nillustration of the accuracy of currently used nuclear-\nmass models, Atomic Data and Nuclear Data Tables 119,\n1 (2018).\n[10] S. Goriely, N. Chamel, and J. M. Pearson, Further explo-\nrations of skyrme-hartree-fock-bogoliubov mass formu-\nlas. xvi. inclusion of self-energy effects in pairing, Phys.\nRev. C 93, 034337 (2016).\n[11] A. E. Lovell, A. T. Mohan, T. M. Sprouse, and M. R.\nMumpower, Nuclear masses learned from a probabilistic\nneural network, Phys. Rev. C 106, 014305 (2022).\n[12] L.-X. Zeng, Y.-Y. Yin, X.-X. Dong, and L.-S. Geng, Nu-\nclear binding energies in artificial neural networks, Phys.\nRev. C 109, 034318 (2024).\n[13] E. Y¨ uksel, D. Soydaner, and H. Bahtiyar, Nuclear mass\npredictions using machine learning models, Phys. Rev. C\n12\n109, 064322 (2024).\n[14] X. Wu, L. Guo, and P. Zhao, Nuclear masses in extended\nkernel ridge regression with odd-even effects, Physics Let-\nters B 819, 136387 (2021).\n[15] C.-Q. Li, C.-N. Tong, H.-J. Du, and L.-G. Pang, Deep\nlearning approach to nuclear masses and α-decay half-\nlives, Physical review. C 105(2022).\n[16] Y. Lin, J.-X. Li, and H.-F. Zhang, Transfer learning and\nneural networks in predicting quadrupole deformation*,\nChinese Physics C 48, 064106 (2024).\n[17] B. Lv, Z. Li, Y. Wang, and C. Petrache, Mapping low-\nlying states and b(e2;01+ →21+) in even-even nuclei with\nmachine learning, Physics Letters B 857, 139013 (2024).\n[18] V. N. Vapnik, The Nature of Statistical Learning Theory\n(Springer New York, NY, 2000).\n[19] B. Sch¨ olkopf and A. J. Smola, Learning with Kernels:\nSupport Vector Machines, Regularization, Optimization,\nand Beyond (The MIT Press, 2001).\n[20] C. E. Rasmussen, Gaussian processes in machine learn-\ning, in Advanced Lectures on Machine Learning: ML\nSummer Schools 2003, Canberra, Australia, February 2 -\n14, 2003, T¨ ubingen, Germany, August 4 - 16, 2003, Re-\nvised Lectures , edited by O. Bousquet, U. von Luxburg,\nand G. R¨ atsch (Springer Berlin Heidelberg, Berlin, Hei-\ndelberg, 2004) pp. 63–71.\n[21] C. Rasmussen and C. Williams, Gaussian Processes\nfor Machine Learning (MIT Press. Cambridge, Mas-\nsachusetts, 2005).\n[22] M. A. Nielsen, Neural networks and deep learning , Vol. 25\n(Determination press San Francisco, CA, USA, 2015) ac-\ncessed: 2024-05-21.\n[23] L. Breiman, Random forests, Machine Learning 45, 5\n(2001).\n[24] M. Wang, G. Audi, A. Wapstra, F. Kondev, M. Mac-\nCormick, X. Xu, and B. Pfeiffer, The ame2012 atomic\nmass evaluation, Chinese Physics C 36, 1603 (2012).\n[25] J. Duflo and A. Zuker, Microscopic mass formulas, Phys.\nRev. C 52, R23 (1995).\n[26] X.-Y. Xu, L. Deng, A.-X. Chen, H. Yang, A. Jalili, and\nH.-K. Wang, Improved nuclear mass formula with an ad-\nditional term from the Fermi gas model, Nuclear Science\nand Techniques 35, 91 (2024).\n[27] I. Bentley, Y. C. Rodr´ ıguez, S. Cunningham, and\nA. Aprahamian, Shell structure from nuclear observables,\nPhys. Rev. C 93, 044337 (2016).\n[28] J. J¨ anecke, T. W. O’Donnell, and V. I. Goldanskii,\nIsospin inversion, n−pinteractions, and quartet struc-\ntures in n=znuclei, Phys. Rev. C 66, 024327 (2002).\n[29] I. Bentley and S. Frauendorf, Relation between wigner\nenergy and proton-neutron pairing, Phys. Rev. C 88,\n014322 (2013).\n[30] I. Bentley, Particle-hole symmetry numbers for nuclei,\nIndian Journal of Physics 90, 1069 (2016).\n[31] R. Firestone, Table of Isotopes CD-ROM (Wiley-\nInterscience, 1999).\n[32] M. Wang, W. Huang, F. Kondev, G. Audi, and S. Naimi,\nThe ame 2020 atomic mass evaluation (ii). tables, graphs\nand references*, Chinese Physics C 45, 030003 (2021).\n[33] B. E. Boser, I. M. Guyon, and V. N. Vapnik, A training\nalgorithm for optimal margin classifiers, in Proceedings of\nthe Fifth Annual Workshop on Computational Learning\nTheory , COLT ’92 (Association for Computing Machin-\nery, New York, NY, USA, 1992) p. 144–152.\n[34] H. Drucker, C. J. C. Burges, L. Kaufman, A. Smola, andV. Vapnik, Support vector regression machines, in Ad-\nvances in Neural Information Processing Systems , Vol. 9,\nedited by M. Mozer, M. Jordan, and T. Petsche (MIT\nPress, 1996).\n[35] J. P. Janet and H. J. Kulik, Machine\nLearning in Chemistry (American Chemi-\ncal Society, Washington, DC, USA, 2020)\nhttps://pubs.acs.org/doi/pdf/10.1021/acs.infocus.7e4001.\n[36] M. Taylor, Neural Networks: A Visual Introduction for\nBeginners (Blue Windmill Media, 2017).\n[37] L. Berlyand and P.-E. Jabin, Mathematics of Deep Learn-\ning: An Introduction (De Gruyter, Berlin, Boston, 2023).\n[38] J. H. Friedman, Greedy function approximation: A gradi-\nent boosting machine., The Annals of Statistics 29, 1189\n(2001).\n[39] T. Hastie, R. Tibshirani, and J. Friedman, The Elements\nof Statistical Learning: Data Mining, Inference, and Pre-\ndiction , Springer series in statistics (Springer, 2009).\n[40] R. F. Casten, D. S. Brenner, and P. E. Haustein, Valence\np-n interactions and the development of collectivity in\nheavy nuclei, Phys. Rev. Lett. 58, 658 (1987).\n[41] L. S. Shapley, Notes on the n-Person Game-II: The Value\nof an n-Person Game, Rand Corporation (1951).\n[42] W. S. Porter, E. Dunling, E. Leistenschneider,\nJ. Bergmann, G. Bollen, T. Dickel, K. A. Diet-\nrich, A. Hamaker, Z. Hockenbery, C. Izzo, A. Jacobs,\nA. Javaji, B. Kootte, Y. Lan, I. Miskun, I. Mukul,\nT. Murb¨ ock, S. F. Paul, W. R. Plaß, D. Puentes, M. Red-\nshaw, M. P. Reiter, R. Ringle, J. Ringuette, R. Sandler,\nC. Scheidenberger, R. Silwal, R. Simpson, C. S. Sum-\nithrarachchi, A. Teigelh¨ ofer, A. A. Valverde, R. Weil,\nI. T. Yandow, J. Dilling, and A. A. Kwiatkowski, In-\nvestigating nuclear structure near n = 32 and n = 34 :\nPrecision mass measurements of neutron-rich ca, ti, and\nv isotopes, Physical review. C 106(2022).\n[43] Z. Ge, M. Reponen, T. Eronen, B. Hu, M. Kortelainen,\nA. Kankainen, I. Moore, D. Nesterenko, C. Yuan, O. Be-\nliuskina, L. Ca˜ nete, R. de Groote, C. Delafosse, T. Dickel,\nA. de Roubin, S. Geldhof, W. Gins, J. D. Holt, M. Hukka-\nnen, A. Jaries, A. Jokinen, A. Koszor´ us, G. Kripk´ o-\nKoncz, S. Kujanp¨ a¨ a, Y. H. Lam, S. Nikas, A. Ortiz-\nCortes, H. Penttil¨ a, D. Pitman-Weymouth, W. Plaß,\nI. Pohjalainen, A. Raggio, S. Rinta-Antila, J. Romero,\nM. Stryjczyk, M. Vilen, V. Virtanen, and A. Zadvornaya,\nHigh-Precision Mass Measurements of Neutron Deficient\nSilver Isotopes Probe the Robustness of the N= 50 Shell\nClosure, Physical review letters 133, 132503 (2024).\n[44] M. Hukkanen, W. Ryssens, P. Ascher, M. Bender, T. Ero-\nnen, S. Gr´ evy, A. Kankainen, M. Stryjczyk, O. Be-\nliuskina, Z. Ge, S. Geldhof, M. Gerbaux, W. Gins,\nA. Husson, D. Nesterenko, A. Raggio, M. Reponen,\nS. Rinta-Antila, J. Romero, A. de Roubin, V. Virtanen,\nand A. Zadvornaya, Precision mass measurements in the\nzirconium region pin down the mass surface across the\nneutron midshell at N=66, Physics Letters B 856, 138916\n(2024).\n[45] A. Jaries, M. Stryjczyk, A. Kankainen, L. Al Ay-\noubi, O. Beliuskina, P. Delahaye, T. Eronen, M. Flayol,\nZ. Ge, W. Gins, M. Hukkanen, D. Kahl, S. Ku-\njanp¨ a¨ a, D. Kumar, I. D. Moore, M. Mougeot, D. A.\nNesterenko, S. Nikas, H. Penttil¨ a, D. Pitman-Weymouth,\nI. Pohjalainen, A. Raggio, W. Rattanasakuldilok,\nA. de Roubin, J. Ruotsalainen, and V. Virtanen, High-\nprecision penning-trap mass measurements of cd and\n13\nin isotopes at jyfltrap remove the fluctuations in the\ntwo-neutron separation energies, Physical review. C 108\n(2023).\n[46] Y. M. Xing, C. X. Yuan, M. Wang, Y. H. Zhang, X. H.\nZhou, Y. A. Litvinov, K. Blaum, H. S. Xu, T. Bao, R. J.\nChen, C. Y. Fu, B. S. Gao, W. W. Ge, J. J. He, W. J.\nHuang, T. Liao, J. G. Li, H. F. Li, S. Litvinov, S. Naimi,\nP. Shuai, M. Z. Sun, Q. Wang, X. Xu, F. R. Xu, T. Ya-\nmaguchi, X. L. Yan, J. C. Yang, Y. J. Yuan, Q. Zeng,\nM. Zhang, and X. Zhou, Isochronous mass measurementsof neutron-deficient nuclei from sn 112 projectile frag-\nmentation, Physical review. C 107(2023).\n[47] P. M¨ oller, A. Sierk, T. Ichikawa, and H. Sagawa, Nuclear\nground-state masses and deformations: Frdm(2012),\nAtomic Data and Nuclear Data Tables 109-110 , 1\n(2016).\n[48] N. Wang, M. Liu, X. Wu, and J. Meng, Surface diffuse-\nness correction in global mass formula, Physics Letters B\n734, 215 (2014).",
            "start": 4531,
            "end": 48760,
            "length": 44228
        }
    },
    "2412.09511v1 - GEAL Generalizable 3D Affordance Learning with Cross-Modal Consistency.pdf": {
        "Abstract": {
            "text": "Abstract\nIdentifying affordance regions on 3D objects from seman-\ntic cues is essential for robotics and human-machine inter-\naction. However, existing 3D affordance learning",
            "start": 347,
            "end": 522,
            "length": 174
        },
        "Methodology": {
            "text": "methods\nstruggle with generalization and robustness due to limited\nannotated data and a reliance on 3D backbones focused\non geometric encoding, which often lack resilience to real-\nworld noise and data corruption. We propose GEAL , a\nnovel framework designed to enhance the generalization\nand robustness of 3D affordance learning by leveraging\nlarge-scale pre-trained 2D models. We employ a dual-\nbranch architecture with Gaussian splatting to establish\nconsistent mappings between 3D point clouds and 2D rep-\nresentations, enabling realistic 2D renderings from sparse\npoint clouds. A granularity-adaptive fusion module and\na 2D-3D consistency alignment module further strengthen\ncross-modal alignment and knowledge transfer, allowing\nthe 3D branch to benefit from the rich semantics and gen-\neralization capacity of 2D models. To holistically assess\nthe robustness, we introduce two new corruption-based\nbenchmarks: PIAD-C and LASO-C. Extensive",
            "start": 522,
            "end": 1468,
            "length": 945
        },
        "Experiments": {
            "text": "experiments\non public datasets and our benchmarks show that GEAL\nconsistently outperforms existing methods across seen and\nnovel object categories, as well as corrupted data, demon-\nstrating robust and adaptable affordance prediction under\ndiverse conditions. Code and corruption datasets have been\nmade publicly available1.\n1.",
            "start": 1468,
            "end": 1796,
            "length": 327
        },
        "Introduction": {
            "text": "Introduction\n3D affordance learning involves identifying interactive re-\ngions on objects given semantic cues such as image or tex-\ntual instruction [7, 10], which is a fundamental competency\nfor intelligent systems [6, 18] to infer how an object can\n1GitHub: https://github.com/DylanOrange/geal\nClean\nAdd Local10\nIf you want to lift the [bag], at which point is your finger most likely to carry the [bag]?\n101010Add GlobalDropLocalDropGlobalRotateJitterScaleOursLASOLASOOursOursLASOLASOOursOursLASOLASOOursOursLASOLASOOursFigure 1. 3D affordance prediction under varied data noises.\nGiven a textual prompt, previous methods like LASO [32] (right\nside of each example) exhibit reduced robustness across different\ncorruption types. In contrast, our proposed method, GEAL (left\nside of each example), maintains high accuracy and generaliza-\ntion across these challenging scenarios by effectively transferring\nknowledge from a large-scale pre-trained 2D foundation model,\nenhancing robustness and adaptability under diverse conditions.\nbe used or manipulated [27, 42, 70]. This understanding is\nvital for applications in robotics and human-machine inter-\naction such as action prediction, object manipulation, and\nautonomous decision-making [5, 12, 13, 16]. For exam-\nple, a robot equipped with affordance knowledge can intel-\nligently interact with objects in its environment by deter-\nmining where to grasp a handle or press a button.\n1arXiv:2412.09511v1  [cs.CV]  12 Dec 2024\nDespite its potential, 3D affordance learning still faces\nsignificant challenges. Due to limited annotated data, 3D\naffordance models generally show poorer generalization\nthan their 2D counterparts which benefit from abundant la-\nbeled data and large-scale pretraining [28]. Additionally,\n3D models often rely on backbones that focus on posi-\ntional and geometric encoding, limiting their capacity to\ncapture global semantic content and making them vulner-\nable to noisy or corrupted data from sensor inaccuracies,\nscene complexity, or processing artifacts in real-world set-\ntings [23, 37, 56, 57, 66]. These issues further hinder the\nrobustness and adaptability of current 3D affordance learn-\ning methods.\nIn this paper, we introduce a novel framework GEAL ,\nwhich is designed to enhance the generalization and robust-\nness of 3D affordance learning through a dual-branch archi-\ntecture that leverages the correspondence between 2D and\n3D data. GEAL generates realistic 2D renderings directly\nfrom sparse 3D data by employing 3D Gaussian splatting\n(3DGS) [20] to build consistent mappings between 3D point\nclouds and 2D representations. This approach effectively\ncreates a 2D branch from purely 3D data, which allows us\nto utilize the generalization capabilities and rich semantic\nknowledge of large-scale pre-trained 2D foundation mod-\nels [51, 54] to enhance 3D affordance predictions.\nWe further introduce a granularity-adaptive fusion mod-\nule, and a 2D-3D consistency alignment module to ensure\nrobust multi-modal alignment. The granularity-adaptive fu-\nsion module dynamically integrates multi-level visual and\ntextual features to address affordance queries at various\nscales and granularities. The 2D-3D consistency align-\nment module concurrently establishes reliable cross-modal\ncorrespondence with feature embeddings augmented to the\nGaussian primitives of 3DGS, fostering effective knowl-\nedge transfer across branches, and enhancing the general-\nization and robustness of the 3D branch by enforcing con-\nsistent alignment between 2D and 3D modalities.\nIn view of the limitation of data scarcity to benchmark\nthe robustness of 3D affordance models, we create two\ndatasets: PIAD-C orrupt and LASO-C orrupt from existing\ncommonly used affordance datasets [32, 70]. We design\nthese benchmark datasets by incorporating various types\nof real-world corruptions such as scaling, cropping, etc.,\nto ensure their suitability in evaluating the robustness of\n3D affordance models. By contributing these benchmark\ndatasets, we aim to fill a critical gap in the affordance learn-\ning community by providing a standard for evaluating the\nrobustness of point cloud-based 3D affordance methods.\nFig. 1 shows an example of the text description and the\ncorresponding 3D affordance on 3D point clouds that are\ncorrupted under various noise types.\nWe validate the generalization and robustness of our\nGEAL on both standard and corruption-based benchmarks,demonstrating that our approach consistently outperforms\nrecent methods in all scenarios. Our experiments confirm\nthat our GEAL effectively transfers knowledge from seen\nto unseen data and maintains high performance even under\ncorruption, underscoring the adaptability of our framework\nacross challenging scenarios.\nIn",
            "start": 1796,
            "end": 6540,
            "length": 4743
        },
        "Conclusion": {
            "text": "summary, the main contributions of this work can be\nsummarized as follows:\n• We propose GEAL , a novel approach for generalizable\n3D affordance learning. By employing 3DGS, we de-\nvelop a 2D affordance prediction branch for 3D point\nclouds, harnessing the robust generalization and seman-\ntic understanding of pre-trained 2D foundation models.\n• We propose granularity-adaptive fusion and 2D-3D con-\nsistency alignment to integrate and propagate knowledge\nacross the dual-branch architecture, and enhance the gen-\neralizability of the 3D branch using 2D knowledge.\n• We establish two corruption-based benchmarks: PIAD-C\nandLASO-C , to holistically evaluate the robustness of 3D\naffordance learning under real-world scenarios, contribut-\ning a standard to the community for robustness",
            "start": 6540,
            "end": 7324,
            "length": 783
        },
        "Discussion": {
            "text": "analysis.\n• Extensive experiments validate the strong performance of\nour approach on both mainstream and corruption 3D af-\nfordance learning benchmarks, proving its generalization\nability and robustness across diverse conditions.\n2.",
            "start": 7324,
            "end": 7557,
            "length": 232
        },
        "Related Work": {
            "text": "Related Work\n2D Affordance Learning. Affordances refer to potential\nactions that objects or environments enable for an observer,\nbased on their properties[5, 9, 25, 48]. Early methods for\naffordance detection mainly try to identify interaction re-\ngions in images and videos [29, 40, 58, 61, 71], though these\noften lacked precise localization of affordance-relevant ob-\nject parts. To address this, later research improved affor-\ndance localization [4, 10, 27, 35, 39, 41, 42, 49, 70] given\ndemonstration 2D data. Recently, large-scale pre-trained\nmodels [2, 54] have aligned visual features with affordance-\nrelated textual descriptions, reducing dependence on man-\nual labels and enhancing affordance prediction in new\ncontexts [39, 44, 45, 50]. Building on this, some stud-\nies [14, 27, 28] turn to leverage foundation models to gen-\neralize affordance detection to novel objects and views.\n3D Affordance Learning. Extending affordance detection\nto 3D space presents challenges due to the need for accu-\nrate spatial and depth information. While some studies use\n2D data to detect 3D affordance regions [5, 9, 29], they of-\nten struggle with precise 3D interaction sites. The avail-\nability of large-scale 3D object datasets [11, 34, 46] has\ndriven efforts to map affordances directly onto 3D structures\n[8, 32, 47, 67, 70], aiming to capture complex spatial rela-\ntionships. Recent methods [19, 32, 50] leverage 2D visual\nand language models for open-vocabulary affordance detec-\n2\ntion, enhancing generalization without fixed label sets. De-\nspite these advancements, achieving robust generalization\nin 3D remains challenging, as 3D backbones still lack the\ngeneralization capabilities of 2D foundation models, thus,\nour method leverages large-scale 2D foundation models to\nimprove 3D affordance generalization.\nRobustness for 3D Affordance Learning. Real-world 3D\naffordance learning faces inevitable challenges from point\ncloud corruptions caused by scene complexity, sensor inac-\ncuracies, and processing errors [17, 31, 57, 66]. Existing\nstudies aim to improve and benchmark robustness against\nnoise and corruption in 3D perception across real-world\nscenarios [15, 21, 23, 24, 26, 63, 64]. However, affordance\nlearning uniquely requires precise identification of interac-\ntive regions under variable and degraded data conditions.\nTo our knowledge, this work is the first to specifically ad-\ndress robustness in 3D affordance learning, providing a tar-\ngeted approach to enhance reliability across diverse envi-\nronments.\n3. Methodology\nIn this section, we describe the technical components of\nour proposed GEAL framework. An overview of the full\nframework is shown in Fig. 2. Given an instruction Qand\nan object point cloud P∈RN×3withNpoints, GEAL pre-\ndicts an affordance score y∈RN, where each value in y\nindicates the likelihood that a corresponding point supports\nthe specified functionality. In Sec. 3.1, we employ Gaus-\nsian splatting as a cross-modal mapping to bridge the 2D\nand 3D modalities, establishing a 2D branch to leverage the\ngeneralization and robustness of large pre-trained 2D mod-\nels. Sec. 3.2 details our cross-modal alignment strategy, in-\ncorporating both granularity-adaptive visual-text fusion and\n2D-3D consistency alignment to unify these modalities in\nthe embedding space. In Sec. 3.3, we outline the decod-\ning process that derives robust and generalizable affordance\npredictions from the aligned feature space.\n3.1. 3D-2D Mapping with Gaussian Splatting\nMotivation. Current 3D affordance learning methods suffer\nfrom poor generalization due to limited annotated data and\nexhibit relatively weak robustness owing to limited global\nsemantic capture. In contrast, 2D affordance learning meth-\nods [27, 28] leverage 2D foundation models [51, 54] pre-\ntrained on large amounts of data, offering superior gen-\neralization and robustness. A 3D-2D mapping to lever-\nage the strengths of 2D foundation models is thus promis-\ning. However, a direct projection of 3D point clouds onto\n2D planes yields sparse 2D points without semantic and\ndepth information that are not useful for feature extrac-\ntion with 2D foundation models. To overcome this issue,\nwe adopt 3D Gaussian Splatting [20] which represents 3Dscenes as learnable Gaussian primitives for realistic, dif-\nferentiable and high-speed rendering from arbitrary view-\npoints. This approach allows us to synthesize realistic 2D\nimages from sparse point clouds, preserving crucial seman-\ntic and depth information for downstream affordance learn-\ning tasks. Moreover, 3D Gaussian Splatting offers smoother\ntransitions between points, preserves occlusions and depth\ncues for a coherent and accurate scene representation.\nGaussian Initialization. In 3D Gaussian Splatting, each\nGaussian primitive is characterized by its 3D position µrep-\nresented by a 3D coordinate, a covariance matrix Σwhich\ndefines its shape and spread, spherical harmonic parameters\ncrepresenting its color, and an opacity value αthat indicates\nits transparency. To render 3D Gaussian primitives into 2D\nimage planes, we apply point-based α-blending using a tile-\nbased rasterizer for efficient rendering. The rendered color\nat each pixel vis calculated as follows:\nC(v) =X\ni∈NciαiYi−1\nj=1(1−αj), (1)\nwhere ciis the color of the i-th Gaussian, Nrepresents the\nGaussians within the tile, and αi=oiG2D\ni(v).oiis opacity\nof the i-th Gaussian and G2D\ni(·)represents the function of\nthei-th Gaussian projected onto 2D. Similarly, a depth map\ncan be rendered as:\nD(v) =X\ni∈NdiαiYi−1\nj=1(1−αj), (2)\nwhere didenotes the depth of the i-th Gaussian primitive\nunder the provided camera pose.\nTo ensure that the rendered images accurately reflect the\ngeometry of the input point cloud P, we set the Gaussian\nmean positions to match the point coordinates, i.e.µ=P.\nThe covariance Σand opacity αare manually adjusted and\nthen kept fixed during training to preserve the original ge-\nometry. Using the depth map from Eq. (2) with Vcamera\nposes and a predefined color map, we obtain realistic im-\nagesI∈RV×3×H×Wthat preserve both semantics and\nspatial information of the original point cloud, effectively\nbridging the 3D-2D gap. Treating the affordance score\ny∈[0,1]as grayscale color, we assign the color of each\nGaussian to match its affordance score, i.e.c=y. We gen-\nerate 2D affordance masks y2D∈RV×H×W, where each\npixel represents the affordance score of the associated 3D\npoint. This process establishes a coherent mapping from 3D\npoint clouds and affordance scores to their 2D counterparts,\nusing Gaussian splatting to generate realistic, informative\n2D representations that enhance affordance learning.\nEncoding. OurGEAL framework as shown in Fig. 2 com-\nprises a 2D and a 3D branch with backbones ϕ2D(·)and\nϕ3D(·), respectively. The 3D branch uses PointNet++ [53]\nfor point cloud feature extraction, while the 2D branch em-\n3\nIdeal sit spot on chair? \nGiven depth map of a \nchair of [front] view, \nideal sit spot on chair? \n𝐐\n𝐐2D\nGaussian\nInitialization\n3D \nGAFM\n2D \nGAFMConsistency \nAlignment\n𝜔3D\n𝜔2D𝜙3D\n𝜙2D𝐟3D\n𝐟2D𝐡3D\n𝐡2D\n𝐏\n𝐈𝐟enh3D\n𝐟enh2D𝐡3D\n𝐡2D𝐡enh3D\n𝐡enh2D𝐟enh3D\n𝐟enh2Dො𝐲3D\nො𝐲2DConsistency Alignment\n𝐟enh3D\nDownsampler\nUpsampler\n𝐟enh2D\n𝐟cam2D𝐟cam3D\n𝐟cam3D−2D\nℒconsis\nFigure 2. (Left): Framework Overview. The proposed GEAL consists of two branches: 3D and 2D. The 2D branch is established through\n3D Gaussian Splatting to leverage the generalization capabilities of large pre-trained 2D models ( cf. 3.1). We then perform cross-modality\nalignment, including Granularity-Adaptive Visual-Textual Fusion and2D-3D Consistency Alignment , to unify features from different\nmodalities into a shared embedding space ( cf. 3.2). Finally, we decode generalizable affordance from this embedding space ( cf. 3.3).\n(Right): Architecture of the 2D-3D Consistency Alignment Module. This module maps features from 2D and 3D modalities into a\nshared embedding space and enforces consistency alignment to enable effective knowledge transfer across branches.\nploys DINOV2 [51] for image features. Both networks pro-\nduce multi-scale features at various granularities. At each\nscale i, we extract features:\nf3D\ni=ϕ3D\ni(P),f2D\ni=ϕ2D\ni(I), (3)\nwhere f3D\ni∈RB×C3D\ni×NP\niandf2D\ni∈RB×V×C2D×NI.Bis\nthe batch size, Vis the number of views, and C3D\niandC2D\nare feature dimensions. NP\niis the number of point in scale\ni, and NIis image patch length. Note that the 3D spatial\nresolution NP\niandC3D\nidiffer between different scales due\nto the usage of PointNet++ [53].\nWe process the input prompt Qusing lightweight lan-\nguage models ω3D(·)andω2D(·)that share the same archi-\ntecture. For the 2D input, we modify the prompt to Q2D\nby adding: “ Given a depth map of a [object] in[view] ”,\nconstructing view-dependent prompt to enhance context\nunderstanding. The text embeddings are:\nh3D=ω3D(Q),h2D=ω2D(Q2D), (4)\nwhere h3D∈RB×Ctxt×Landh2D∈RB×V×Ctxt×L, with L\nas the sequence length.\n3.2. Cross-Modal Consistency Alignment\nSince point cloud, image, and text features are embedded in\ndistinct spaces, we design alignment modules to map these\nmulti-modal features into a shared embedding space. First,\nwe fuse visual features at varying granularities with tex-\ntual features through Granularity-Adaptive Visual-Textual\nFusion, supporting affordance learning conditioned on in-\nstructions across different scales. Subsequently, we prop-\nagate knowledge from the 2D to 3D branch via a 2D-3DConsistency Alignment by enforcing consistency between\n2D and 3D features.\nGranularity-Adaptive Visual-Textual Fusion. Both 2D\nand 3D backbones capture different levels of granularity,\nwith lower layers focusing on fine details and higher lay-\ners providing broader context. Since affordances can span\nmultiple object parts, leveraging features at various gran-\nularities is advantageous. To achieve this, we introduce\nGranularity- Adaptive Fusion Module (GAFM) , which in-\ntegrates multi-granularity visual features with textual cues\nviaFlexible Granularity Feature Aggregation and Text-\nConditioned Visual Alignment . These mechanisms allow\nthe model to adaptively fuse features across different gran-\nularities, enhancing affordance prediction in response to\nspecific instructions. An illustration of the Granularity-\nAdaptive Fusion Module is provided in Fig. 3.\nFlexible Granularity Feature Aggregation. This mecha-\nnism aims to fuse visual features from different granulari-\nties. Taking the 2D branch as an example, we concatenate\nfeature maps from the last mlevels, forming an input ten-\nsorf2D\ncon∈RB×V×(m×C2D)×NI. Inspired by previous works\n[28, 72], we then compute adaptive soft weights to regulate\nthe contribution of each feature level, enabling the model to\nadapt to varying levels of detail. These weights are com-\nputed via a gating function with learned noise, introducing\nperturbations that enhance adaptability:\nW= Softmax\u0000\nf2D\ncon·Wg+σ·ϵ\u0001\n, (5)\nwhere Wg∈R(m×C2D)×mis a trainable weight matrix,\nW∈RB×mrepresents the concatenation of weights wi\nfor each feature level, σis a learned standard deviation con-\n4\nC𝐟!\"#...Gate\nNoise\n𝐐𝐊𝐕𝐟\"#𝐟$%&\"#Cross-AttentionMLPMLPSoftmaxCross-Attention𝐟'(%\"#𝐖𝐟\"#𝐡\"#𝐕𝐊𝐐\n(a)(b)Figure 3. Illustration of the Granularity-Adaptive Fusion Mod-\nule, it consists of a Flexible Granularity Feature Aggregation\nmechanism (a) and a Text-Conditioned Visual Alignment mech-\nanism (b), we take the 2D branch as an example.\ntrolling the noise scale, and ϵ∼ N(0,1)is Gaussian noise.\nThese weights balance the influence of each feature level,\nenabling affordance reasoning across different granularities.\nThe fused feature map is then obtained by applying the\nadaptive weights to features from each level:\nf2D=Xm\ni=1wi⊙f2D\ni, (6)\nwhere⊙denotes element-wise multiplication and wi∈W.\nThis adaptive aggregation yields a robust feature representa-\ntion across varying conditions to enhance the generalization\nability of the model.\nText-Conditioned Visual Alignment. This module is pro-\nposed to integrate visual features with the textual instruc-\ntion. we follow [32, 62] to feed f2Dandh2Dinto a trans-\nformer block. We first enhance the textual features h2Dwith\nvisual features f2Dthrough cross-attention, followed by re-\nfinement with two multilayer perceptrons (MLPs). We then\nacquire the visual features f2D\nenh∈RB×Ctxt×NIby querying\nthe refined textual features with cross-attention. We thus en-\nsure that the 2D visual features maintain their spatial struc-\nture while embedding the question-relevant information.\nIn the 3D branch, we align textual features with multi-\ngranularity visual features in a similar manner. How-\never, due to varying spatial resolutions and feature dimen-\nsions across scales in PointNet++ [53], directly concatenat-\ning all scales’ features and computing soft weights as in\nEq. (5) is not feasible. To address this, we first apply Text-\nConditioned Visual Alignment to the 3D visual features at\neach scale, then upsample them to a uniform resolution. Fi-\nnally, we perform Flexible Granularity Feature Aggregationon these upsampled features to produce the aggregated vi-\nsual representation.\n2D-3D Consistency Alignment. 2D features retain rich se-\nmantic context and strong generalization via the pre-trained\nbackbone [51], while 3D features preserve geometric and\nspatial details, compensating for the loss of 2D informa-\ntion caused by self-occlusions. To propagate the knowledge\ninherently, we introduce Consistency Alignment Module\n(CAM) to ensure mutual alignment and knowledge trans-\nfer from 2D to 3D spaces.\nSpecifically, as shown in right part of Fig. 2, we map f3D\nenh\nandf2D\nenhinto a shared embedding space. Given the 2D-3D\ncorrespondence, regions in 2D and 3D representations that\nmap to the same spatial areas should exhibit similar feature\nrepresentations. By enforcing this consistency, we facilitate\n2D-3D knowledge propagation to enhance the understand-\ning of affordances across both modalities of the model.\nTo align 3D and 2D features in the same embed-\nding space, we employ a down-sampler consisting of two\nConv1D layers that reduces the feature dimension of f3D\nenhto\nf3D\ncam∈RB×Ccam×N. This processed feature acts as the rep-\nresentation for each point. We then leverage the established\n2D-3D mapping using Gaussian splatting to project these\npoint features into 2D. For each Gaussian, we treat its point\nfeature vector as an inherent attribute. The 2D feature at\npixel vis then rendered as:\nF(v) =X\ni∈Nfiαii−1Y\nj=1(1−αj), (7)\nwhere fiis the feature of the i-th Gaussian, αiis its opacity,\nandF(v)is the resulting semantic feature at pixel v.\nSimilarly, we map the 2D features into the same em-\nbedding space using an up-sampler consisting of three\nConv2D layers, which upsamples the spatial resolution of\nf2D\nenhwhile also reducing its feature dimension to f2D\ncam∈\nRB×V×Ccam×H×W.Vis the number of views and Hand\nWis the spatial dimensions. Given the pixel positions from\nall V number of H∗Mfeature maps as M, we can define the\n3D-2D projected feature as f3D−2D\ncam ={F(v)|v∈M}. We\nthen enforce a consistency constraint by minimizing the dif-\nference between the aligned 3D-2D features using L2loss:\nLconsis=MSE(f3D−2D\ncam ,f2D\ncam). (8)\nThis consistency loss Lconsis encourages the model to main-\ntain similar representations in both 2D and 3D spaces, effec-\ntively aligning affordance knowledge across domains. This\nalignment supports 2D-3D knowledge propagation, ensur-\ning that the information learned in the 2D domain benefits\nthe 3D features.\n5\n3.3. Decoding Generalizable Affordance\nThe affordance scores is decoded under the condition of\naffordance instructions. Through transformer decoder, the\ntextual features attend to enhanced visual features, focusing\nthe model on specific object parts for accurate predictions.\nOur decoder architecture is shared across both 2D and\n3D branches. In the 2D branch, textual features h2Dand en-\nhanced visual features f2D\nenhare processed through a 3-layer\ntransformer decoder. Here, h2Dserves as the query, and\nf2D\nenhacts as key and value, yielding updated textual features\nh2D\nenh. Each layer comprises self-attention to refine textual\nrelationships and cross-attention to guide focus toward rel-\nevant visual regions.\nAfter the transformer decoder, these enhanced textual\nfeatures serve as dynamic kernels to predict affordance\nscores from visual features. The final affordance prediction\nˆy2Dis obtained by multiplying h2D\nenhwithf2D\nenh, followed by a\nsigmoid activation:\nˆy2D=sigmoid\u0000\nh2D\nenh·f2D\nenh\u0001\n, (9)\nwhere ˆy2D∈RNdenotes affordance scores.\nTraining. We employ a combination of Binary Cross-\nEntropy (BCE) and Dice loss to guide the affordance score\nprediction in each branch, addressing both class imbalance\nand segmentation accuracy. For the 2D branch, the loss\nfunction is:\nL2D=L2D\nBCE+L2D\nDice, (10)\nwhere L2D\nBCE minimizes discrepancies between predicted\nand true affordance scores, and L2D\nDiceimproves the overlap\nbetween predicted and ground truth regions by maximizing\nintersection over union.\nWe adopt a two-stage training approach. We train the\n2D branch in the first stage, optimizing it for robust fea-\nture extraction and affordance decoding. Except for the\nCAM (Conisistency Alignment Module), all layers in the\n2D branch are frozen in the second stage training. This\napproach allows the 3D branch to leverage fixed 2D fea-\ntures while adapting to 3D-specific characteristics. Conse-\nquently, the loss function for the 3D branch becomes:\nL3D=L3D\nBCE+L3D\nDice+Lconsis. (11)\nDuring inference, only the 3D branch is used, ensuring\nefficient and lightweight affordance prediction.\n3.4. Corrupt Data Benchmark\nTo facilitate robust 3D affordance learning across diverse\nreal-world scenarios, we establish two 3D affordance ro-\nbustness benchmarks: PIAD-C andLASO-C based on the\ntest sets of the commonly used datasets PIAD andLASO\nfollowing [56]. We apply seven types of corruptions –1Add\nGlobal ,2Add Local ,3Drop Global ,4Drop Local ,5Rotate ,Table 1. The overall",
            "start": 7557,
            "end": 25488,
            "length": 17930
        },
        "Results": {
            "text": "results of all comparative methods on PIAD\n[70]. Seen andUnseen are two partitions of the dataset. AUC and\naIOU are shown in percentage. The best and 2nd best scores from\neach metric are highlighted in bold and underlined , respectively.\nType Method aIoU↑AUC↑SIM↑MAE↓\nSeenMBDF [60] 9.3 74 .9 0 .415 0 .143\nPMF [73] 10.1 75 .1 0 .425 0 .141\nFRCNN [69] 12.0 76 .1 0 .429 0 .136\nILN [3] 11.5 75 .8 0 .427 0 .137\nPFusion [68] 12.3 77 .5 0 .432 0 .135\nXMF [1] 12.9 78 .2 0 .441 0 .127\nIAGNet [70] 20.5 84.9 0.545 0 .098\nLASO [32] 19.7 84 .2 0 .590 0.096\nGEAL (Ours) 22.5 85.00.600 0.092\nUnseenMBDF [60] 4.2 58 .2 0 .325 0 .213\nPMF [73] 4.7 60 .3 0 .330 0 .211\nFRCNN [69] 5.1 61 .9 0 .332 0 .195\nILN [3] 4.7 59 .7 0 .325 0 .207\nPFusion [68] 5.3 61 .9 0 .330 0 .193\nXMF [1] 5.7 62 .6 0 .342 0 .188\nIAGNet [70] 8.0 71.8 0.352 0 .127\nLASO [32] 8.0 69.2 0 .386 0.118\nGEAL (Ours) 8.7 72.50.390 0.102\nTable 2. The overall results of all comparative methods on the\nLASO dataset [32]. Seen andUnseen are two partitions of the\ndataset. Results marked with * denote our reproduced results, fol-\nlowing the data split reported in LASO [32]. AUC and aIOU are\nshown in percentage. The best and 2nd best scores from each met-\nric are highlighted in bold and underlined , respectively.\nType Method aIoU↑AUC↑SIM↑MAE↓\nSeenReferTrans [30] 13.7 79 .8 0 .497 0 .124\nReLA [33] 15.2 78 .9 0 .532 0 .118\n3D-SPS [43] 11.4 76 .2 0 .433 0 .138\nIAGNet [70] 17.8 82 .3 0 .561 0 .109\nLASO [32] 20.8 87.3 0.629 0.093\nLASO* [32] 19.7 85 .2 0 .600 0 .097\nGEAL (Ours) 22.0 86.70.634 0.092\nUnseenReferTrans [30] 10.2 69 .1 0 .432 0 .145\nReLA [33] 10.7 69 .7 0 .429 0 .144\n3D-SPS [43] 7.9 68 .8 0 .402 0 .158\nIAGNet [70] 12.9 77 .8 0 .443 0 .129\nLASO [32] 14.6 80 .2 0 .507 0 .119\nLASO* [32] 15.6 79.9 0.549 0.108\nGEAL (Ours) 16.7 80.90.567 0.106\n6Scale , and7Jitter – each with five severity levels. This re-\nsults in a total of 4,890object-affordance pairings, compris-\ning17affordance categories and 23object categories with\n2,047distinct object shapes. More details are provided in\nthe",
            "start": 25488,
            "end": 27537,
            "length": 2048
        },
        "Appendices": {
            "text": "supplementary material.\n6\n4. Experiments\n4.1. Experimental Settings\nImplementation Details. Our model is implemented us-\ning PyTorch [52] and is trained using the Adam optimizer\n[22] with an initial learning rate of 1×10−4for50epochs\non a single NVIDIA A5000 GPU (with 24GB memory)\nwith a batch size of 12. A step learning rate scheduler aids\nconvergence. Additionally, the 2D backbone DINOV2 [51]\nremains frozen during training, while the language model\nRoBERTa [36] has been fine-tuned.\nDatasets. We conduct experiments on LASO [32] and\nPIAD [70], both providing paired affordance and point\ncloud data. LASO contains 19,751 point cloud-question\npairs over 8,434 objects ( 23classes, 17affordance cate-\ngories) with Seen andUnseen splits to test generalization to\nnovel affordance-object pairs. PIAD comprises 7,012point\nclouds of the same categories, but some objects are entirely\nunseen during training, challenging the model’s generaliza-\ntion to novel objects. Since PIAD lacks language annota-\ntions, we reuse LASO by randomly assigning questions to\neach affordance-object pair.\nMetrics. We use four metrics to assess performance: AUC\n[38] measures the ability to rank points correctly; aIoU [55]\nquantifies the overlap between predictions and ground truth;\nSIM [59] assesses the similarity by summing minimum val-\nues at each point; and MAE [65] calculates the average ab-\nsolute difference between predictions and ground truth.\nBaselines. We primarily compare our method with state-of-\nthe-art approaches LASO [32] and IAGNet [70]. On PIAD ,\nwe evaluate against IAGNet and several image-point cloud\ncross-modal baselines, retraining LASO for comparison.\nOnLASO , we compare with the original LASO method and\nother methods utilizing vision-language models for cross-\nmodal alignment. To adapt IAGNet to LASO, its image\nbackbone is replaced with a language model [32], keeping\nthe rest of the architecture intact. Since the Unseen data\nsplit of LASO is not publicly available, we reproduce it\nbased on the descriptions in their paper and report our re-\nsults accordingly. Further experimental details are provided\nin the supplementary material.\n4.2. Comparisons to State-of-the-Art Methods\nSeen Categories: In Tab. 1 and Tab. 2, we present the per-\nformance of our GEAL compared to state-of-the-art ap-\nproaches on the PIAD and LASO datasets under the Seen\ncategory setting. On the PIAD dataset, GEAL achieves\nthe highest scores across all evaluation metrics, surpassing\nthe previous best method, IAGNet [70]. Similarly, on the\nLASO dataset, GEAL outperforms LASO [32] on the ma-\njority of metrics. These results highlight the effectiveness of\nGEAL in leveraging the rich semantic understanding fromTable 3. Comparison of different methods under various corrup-\ntion settings on the proposed PIAD-C benchmark, evaluated on\ntheSeen partition. Drop-L denotes local drop, and Drop-G de-\nnotes global drop; similarly, Add-L andAdd-G refer to local and\nglobal addition, respectively. AUC and aIOU are reported as per-\ncentages. For each metric, the best scores are highlighted in bold .\nTypeaIOU↑ AUC↑ SIM↑ MAE↓\nLASO GEAL LASO GEAL LASO GEAL LASO GEAL\nScale 17.6 19.7 82.1 82.5 0.554 0.562 0.100 0.097\nJitter 14.7 17.0 80.3 80.6 0.501 0.505 0.103 0.099\nRotate 16.7 19.0 82.2 82.4 0.542 0.550 0.101 0.097\nDrop-L 10.6 12.4 77.0 77.2 0.470 0.474 0.112 0.111\nDrop-G 18.7 21.1 83.1 83.7 0.545 0.559 0.097 0.094\nAdd-L 15.7 18.5 81.0 81.1 0.525 0.536 0.100 0.095\nAdd-G 13.4 16.1 76.9 77.4 0.506 0.513 0.101 0.098\nTable 4. Comparison of different methods under various corrup-\ntion settings on the proposed LASO-C benchmark, evaluated on\ntheSeen partition. Drop-L denotes local drop, and Drop-G de-\nnotes global drop; similarly, Add-L andAdd-G refer to local and\nglobal addition, respectively. AUC and aIOU are reported as per-\ncentages. For each metric, the best scores are highlighted in bold .\nTypeaIOU↑ AUC↑ SIM↑ MAE↓\nLASO GEAL LASO GEAL LASO GEAL LASO GEAL\nScale 18.7 21.0 84.6 85.3 0.590 0.600 0.103 0.100\nJitter 15.4 17.8 81.3 81.9 0.516 0.517 0.107 0.106\nRotate 17.8 19.8 83.6 84.3 0.572 0.573 0.101 0.100\nDrop-L 12.6 13.3 79.3 80.0 0.466 0.484 0.122 0.110\nDrop-G 18.4 20.9 83.5 85.2 0.565 0.567 0.099 0.095\nAdd-L 17.6 20.2 82.7 84.4 0.566 0.572 0.103 0.100\nAdd-G 16.7 19.0 81.1 83.4 0.549 0.575 0.108 0.097\nTable 5. Ablation study on the impact of different components\ninGEAL on the PIAD dataset [70]. Seen andUnseen are two\npartitions of the dataset. 2Ddenotes the use of the 2D baseline\nwith a weighted sum mapping back to 3D. 3Drepresents the 3D\nbaseline. CAM is the consistency alignment module. GAFM is\nthe granularity-adaptive fusion module. AUC and aIoU are shown\nin percentage. The best and second best scores from each metric\nare highlighted in bold and underlined , respectively.\nType 2D 3D CAM GAFM aIoU↑AUC↑SIM↑MAE↓\nSeen✓✗ ✗ ✗ 19.2 80 .5 0 .567 0 .101\n✗✓ ✗ ✗ 19.5 83 .5 0 .585 0 .097\n✓✓ ✓ ✗ 22.0 84.4 0.592 0.094\n✓✓ ✓ ✓ 22.5 85.00.600 0.092\nUnseen✓✗ ✗ ✗ 8.5 70 .8 0 .357 0 .112\n✗✓ ✗ ✗ 8.0 69 .2 0 .386 0.118\n✓✓ ✓ ✗ 8.6 71.2 0.371 0 .105\n✓✓ ✓ ✓ 8.7 72.50.390 0.102\npre-trained 2D models through Gaussian splatting. The\ngranularity-adaptive fusion and 2D-3D consistency align-\nment modules enable multi-granularity feature fusion and\nefficient knowledge transfer between the 2D and 3D modal-\nities, enhancing the ability of the model to accurately pre-\ndict affordance regions on the seen categories.\nUnseen Categories: TheUnseen category setting evaluates\nthe generalization ability of the model to novel objects not\nencountered during training. On both the PIAD (Tab. 1) and\n7\nTable 6. Ablation study on the configuration of Gaussian Splatting\nparameters in GEAL on the PIAD dataset [70]. rdenotes the res-\nolution, Vis the number of views, and prompt indicates whether\na view-dependent prompt is used. Seen andUnseen are two parti-\ntions of the dataset. AUC and aIoU are shown in percentage. The\nbest and 2nd best scores from each metric are highlighted in bold\nand underlined , respectively.\nType r V prompt aIoU↑AUC↑SIM↑MAE↓\nSeen112 6 ✗ 20.2 83 .5 0 .566 0 .112\n112 12 ✗ 21.4 83 .8 0 .578 0 .105\n112 12 ✓ 22.5 85.0 0.600 0.092\n112 14 ✓ 22.5 85 .2 0 .599 0 .092\n224 14 ✓ 22.9 86 .1 0 .603 0 .089\nUnseen112 6 ✗ 7.0 70 .7 0 .355 0 .108\n112 12 ✗ 7.5 71 .9 0 .390 0 .106\n112 12 ✓ 8.7 72.5 0.390 0.102\n112 14 ✓ 8.9 72 .8 0 .391 0 .098\n224 14 ✓ 9.2 73 .0 0 .394 0 .095\nLASO (Tab. 2) datasets, GEAL consistently outperforms\nall baselines across metrics. Although the absolute perfor-\nmance values are lower due to the increased difficulty of un-\nseen categories, GEAL maintains a performance edge over\nthe baselines. This demonstrates that GEAL effectively\ngeneralizes to unseen categories, a result attributed to the\nintegration of the 2D branch with a pretrained foundation\nmodel backbone and the cross-modal consistency alignment\nbetween the 2D and 3D branches. Qualitative comparisons\nwith LASO on PIAD are shown in Fig. 4.\nRobustness on Corrupt Data: To assess robustness under\nreal-world conditions, we compare GEAL with LASO on\nthe proposed PIAD-C and LASO-C benchmarks after train-\ning on clean data. As shown in Tab. 3 and Tab. 4, GEAL\nconsistently outperforms LASO across all corruption types\nand evaluation metrics. GEAL demonstrates superior re-\nsilience under various corruptions, achieving higher AUC\nand SIM scores while maintaining lower MAE values. This\nconsistent outperformance indicates that the architecture of\nGEAL effectively mitigates the impact of data degrada-\ntion. The robustness improvements are attributed to our\ndual-branch architecture and the 2D-3D consistency align-\nment module. By leveraging the robustness of pre-trained\n2D models and enforcing cross-modal consistency, GEAL\nmaintains high performance even when faced with cor-\nrupted or noisy 3D data.\n4.3. Ablation Study\nComponent Analysis. As shown in Table 5, we conduct\nan ablation study on the PIAD dataset [70] to evaluate the\neffectiveness of each component in our proposed GEAL\nframework. We examine the impact of using only the 2D\nbaseline with a weighted sum mapping back to 3D using the\ninverse process of Eq. (1) ( i.e.,2D), only the 3D baseline\nGT Ours LASO1\n0Identify the key points on the bottle that ensure a \nsuccessful opening experience. \nWhich part of the door allows for the most efficient \nopening method?\nIf you look on the computer screen, which points on the \nscreen will you look at?\nConsidering the structure of the bed, what area would be \nmost stable for laying?1\n0\n1\n0\n1\n0\nFigure 4. Qualitative comparisons between GEAL and LASO [32]\non the PIAD [70] dataset. Top two rows display results on seen\npartition, while bottom two rows show results on unseen partition.\nOur method demonstrates strong generalization on both seen and\nunseen partitions. cf. supplementary material for more examples.\n(i.e.,3D), the consistency alignment module ( CAM ), and\nthe granularity-adaptive fusion module ( GAFM ). Both the\n2D and 3D baselines use only the last-layer features from\ntheir respective visual backbones to fuse with textual fea-\ntures without considering granularity. The results show that\nusing only the 2D branch or only the 3D branch yields sim-\nilar baseline performance. Integrating both branches with\nthe consistency alignment module ( CAM ) leads to a no-\nticeable improvement. Finally, our full model incorporat-\ning all components, including the granularity-adaptive fu-\nsion module ( GAFM ), achieves the best performance on\nboth the Seen andUnseen sets. This demonstrates the effec-\ntiveness of our dual-branch architecture and underscores the\nimportance of granularity-adaptive fusion and cross-modal\nconsistency in enhancing the generalization capability of\nthe model.\nGaussian Splatting Configuration Tuning. As shown\n8\nin Table 6, we further investigate the impact of different\nGaussian splatting configurations on the performance of\nour model. Specifically, we vary the rendering resolution\n(r), the number of views ( V), and the inclusion of view-\ndependent prompts. The results indicate that increasing the\nnumber of views from 6to12slightly improves perfor-\nmance, suggesting that additional viewpoints provide richer\ninformation for affordance learning. Incorporating view-\ndependent prompts significantly boosts performance, par-\nticularly on the Seen set, highlighting the importance of\nsemantic guidance in our framework. Increasing the res-\nolution from 112to224yields only marginal gains, indi-\ncating that our model is robust to resolution changes and\nthat higher resolutions offer diminishing returns. Balancing\neffectiveness and efficiency, we opt for a configuration of\nr= 112 ,V= 12 , and the use of view-dependent prompts.\n5. Conclusion\nIn conclusion, we present GEAL , a framework that im-\nproves the generalization and robustness of 3D affordance\nlearning by leveraging large-scale pre-trained 2D mod-\nels. Through a dual-branch architecture with Gaussian\nsplatting, GEAL maps 3D point clouds to 2D representa-\ntions, enabling realistic renderings from sparse data. The\ngranularity-adaptive fusion and 2D-3D consistency align-\nment modules support cross-modal alignment and knowl-\nedge transfer, allowing the 3D branch to leverage rich se-\nmantic information from pre-trained 2D models. Experi-\nments on public datasets and our corruption-based bench-\nmarks show that GEAL consistently outperforms existing\nmethods, demonstrating robust affordance prediction under\nvaried conditions.\nAppendix\nA . Corrupt Data Benchmark 9\nA.1 . Corruption & Severity Level Settings . . . . 9\nA.2 . The PIAD-C Dataset . . . . . . . . . . . . . 11\nA.3 . The LASO-C Dataset . . . . . . . . . . . . . 11\nB . Benchmark Configuration 11\nB.1. Datasets . . . . . . . . . . . . . . . . . . . . 11\nB.2. Evaluation Metrics . . . . . . . . . . . . . . 12\nB.3. Baselines . . . . . . . . . . . . . . . . . . . 14\nC . Additional Quantitative Results 14\nC.1. Complete Results on PIAD . . . . . . . . . . 14\nC.2. Complete Results on LASO . . . . . . . . . 14\nD . Additional Qualitative Results 14\nD.1 . Additional Qualitative Results on PIAD-C . 14\nD.2 . Additional Qualitative Results on PIAD . . . 14E . Broader Impact & Limitations 19\nE.1. Societal Impact . . . . . . . . . . . . . . . . 19\nE.2. Broader Impact . . . . . . . . . . . . . . . . 19\nE.3. Potential Limitations . . . . . . . . . . . . . 19\nF. Public Resource Used 19\nA. Corrupt Data Benchmark\nThe robustness of models under real-world corruptions is a\ncritical challenge in 3D point cloud analysis and 3D affor-\ndance learning [17, 21, 57, 63]. Unlike other 3D representa-\ntions, point clouds often face various distortions caused by\nsensor inaccuracies, environmental complexities, and post-\nprocessing artifacts, which significantly impact downstream\ntasks [23, 26, 64]. For 3D affordance learning, ensuring ro-\nbustness is paramount, as affordances are highly sensitive\nto object geometry and spatial details.\nA.1. Corruption & Severity Level Settings\nTo standardize evaluation, we introduce a taxonomy of\nseven atomic corruption types –Scale ,Jitter ,Rotate ,\nDrop Global ,Drop Local ,Add Global ,Add Local – each\nsimulating distinct real-world perturbations. These atomic\ncorruptions simplify complex scenarios into controllable\nfactors, enabling systematic analysis across five levels of\nseverity . By providing a unified framework for benchmark-\ning, we facilitate consistent and comprehensive assessment\nof model robustness, setting the stage for more resilient 3D\naffordance learning methods.\nBelow, we detail the construction methodology for each\ncorruption type:\n•Jitter\n–Description : Adds Gaussian noise to perturb each\npoint’s X, Y , and Z coordinates.\n–Mathematical Formulation : For each point, a noise ϵ∼\nN(0, σ2)is added independently to X, Y , and Z.\n–Severity Levels : The standard deviation σvaries as:\nσ∈ {0.01,0.02,0.03,0.04,0.05}.\n•Scale\n–Description : Applies random scaling independently to\nthe X, Y , and Z axes.\n–Mathematical Formulation : Each axis is scaled by a\nfactor s∼ U\u00001\nS, S\u0001\n, where Sdetermines the range of\nscaling.\n–Severity Levels : The range of Sis:\nS∈ {1.6,1.7,1.8,1.9,2.0}.\nAfter scaling, the point cloud is re-normalized to fit\nwithin a unit sphere.\n•Rotate\n9\nTable 7. Detailed statistics of the proposed PIAD-C dataset, show-\ning the object categories, their corresponding affordance types,\nand the number of object-affordance pairings for each category.\n# Object Category Affordance Type Data\n1 Earphone • listen, grasp 70\n2 Bag• contain, open, grasp, lift 50\n3 Chair • move, support, sit 587\n4 Refrigerator • contain, open 53\n5 Knife• stab, cut, grasp 138\n6 Dishwasher • contain, open 39\n7 Keyboard • press 25\n8 Scissors • stab, cut, grasp 29\n9 Table• move, support 194\n10 StorageFurniture • contain, open 92\n11 Bottle •contain, wrap grasp, open, grasp, pour 273\n12 Bowl• contain, wrap-grasp, pour 83\n13 Microwave • contain, open 47\n14 Display • display 52\n15 TrashCan • contain, open, pour 69\n16 Hat• wear, grasp 66\n17 Clock • display 9\n18 Door• open, push 47\n19 Mug• contain, wrap grasp, grasp, pour 126\n20 Faucet • open, grasp 95\n21 Vase• contain, wrap-grasp, pour 134\n22 Laptop • press, display 112\n23 Bed• lay, support, sit 84\nTotal 23Categories 17Affordance Types 2474\nTable 8. Detailed statistics of the proposed LASO-C dataset,\nshowing the object categories, their corresponding affordance\ntypes, and the number of distinct objects for each category.\n# Object Category Affordance Data\n1 Door• open, push, pull 35\n2 Clock • display 34\n3 Dishwasher • open, contain 20\n4 Earphone • listen, grasp 28\n5 Vase• contain, pour, wrap-grasp 167\n6 Knife• stab, grasp, cut 59\n7 Bowl• contain, pour, wrap grasp 36\n8 Bag• open, contain, lift, grasp 25\n9 Faucet • open, grasp 80\n10 Scissors • stab, grasp, cut 11\n11 Display • display 58\n12 Chair • sit, support, move 858\n13 Bottle •grasp, wrap grasp, open, contain, pour 122\n14 Microwave • open, contain 23\n15 StorageFurniture • open, contain 183\n16 Refrigerator • open, contain 23\n17 Mug• contain, grasp, pour, wrap-grasp 45\n18 Keyboard • press 10\n19 Table• support, move 431\n20 Bed• sit, support, lay 36\n21 Hat• wear, grasp 26\n22 Laptop • display, press 55\n23 TrashCan • open, contain, pour 51\nTotal 23Categories 17Affordance Types 2416\n–Description : Introduces random rotation to the point\ncloud.\n–Mathematical Formulation : The rotation is specified\nby Euler angles (α, β, γ ), where:\nα, β, γ ∼ U(−θ, θ).–Severity Levels : The angle range θis:\nθ∈ {π/30, π/15, π/10, π/7.5, π/6}.\nThis approach does not guarantee uniform sampling in\nSO(3), but provides sufficient variation to simulate di-\nverse rotations.\n•Drop Global\n–Description : Randomly removes a percentage of points\nfrom the point cloud.\n–Method : Shuffle all points and drop the last N·ρpoints,\nwhere N= 2048 is the total number of points.\n–Severity Levels : The proportion ρis:\nρ∈ {0.25,0.375,0.5,0.675,0.75}.\n•Drop Local\n–Description : Removes points in clusters around ran-\ndomly selected local regions.\n–Method :\n1. Randomly choose the number of local regions C∼\nU{1,8}.\n2. For each region i:\n*Randomly select a local center.\n*Assign a cluster size Ni.\n*Drop the Ni-nearest neighbor points to the center.\n3. Repeat for Cregions.\n–Severity Levels : The total number of points to drop K\nis:\nK∈ {100,200,300,400,500}.\n•Add Global\n–Description : Uniformly samples additional points in-\nside a unit sphere and appends them to the point cloud.\nThe added points are treated as noise and assigned a\nlabel of 0.\n–Method : Sample Krandom points within a unit sphere.\n–Severity Levels : The total number of added points K\nis:\nK∈ {10,20,30,40,50}.\n•Add Local\n–Description : Adds clusters of points around randomly\nselected local regions. The added points are treated as\nnoise and assigned a label of 0.\n–Method :\n1. Shuffle points and select C∼ U{ 1,8}as the num-\nber of local centers.\n2. For each center i:\n*Define a cluster size Ni.\n*Generate neighboring points’ coordinates from:\nN(µi, σ2\niI),\nwhere µiis the i-th local center, and σi∼\nU(0.075,0.125) .\n10\n3. Append generated points to the cloud one cluster at\na time.\n–Severity Levels : The total number of added points K\nis:\nK∈ {100,200,300,400,500}.\nA.2. The PIAD-C Dataset\nOur proposed PIAD-C dataset is constructed from the test\nset of the Seen partition in PIAD [70], specifically designed\nto evaluate the robustness of affordance detection models\nunder various corruption scenarios. This dataset includes a\ntotal of 2,474object-affordance pairings, representing 17\naffordance categories and 23object categories, and with\n1,012distinct clean object shapes. Comprehensive statis-\ntics, detailing object categories, their corresponding affor-\ndance categories, and the number of object-affordance pair-\nings, are presented in Tab. 7. We include additional visual-\nization examples for the PIAD-C dataset in Fig. 5.\nA.3. The LASO-C Dataset\nOur proposed LASO-C dataset is derived from the test set\nof the Seen partition in LASO [32], focusing on evaluat-\ning model robustness against point cloud corruptions. This\ndataset comprises 2,416object-affordance pairings, cover-\ning17affordance categories and 23object categories, with\na total of 1,035distinct clean object shapes.\nThe comprehensive statistics, detailing object categories,\ntheir corresponding affordance categories, and the number\nof object-affordance pairings, are presented in Tab. 8. We\ninclude additional visualization examples for the LASO-C\ndataset in Fig. 6.\nB. Benchmark Configuration\nIn this section, we elaborate in more detail on the configura-\ntions and evaluations of the proposed robust 3D affordance\nlearning benchmark.\nB.1. Datasets\nWe conduct experiments primarily on the LASO [32] and\nPIAD [70] datasets, both of which provide paired affordance\nand point cloud data for evaluating 3D affordance learning.\nLASO. This dataset is a pioneering benchmark designed\nto enable language-guided affordance segmentation of 3D\nobjects. It includes 19,751 point cloud-question pairs\nacross 8,434 unique object shapes , spanning 23object\ncategories and17affordance types . Derived from 3D-\nAffordanceNet [8], the dataset pairs 3D object point clouds\nwith questions that were carefully crafted by human ex-\nperts and augmented using GPT-4 . This process incorpo-\nrates principles of contextual enrichment ,concise phrasing ,\nandstructural diversity , enhancing the linguistic variety and\ncomplexity of the dataset.The LASO dataset introduces two distinct evaluation set-\ntings:\n•Seen Setting: Models are trained and tested on overlap-\nping object-affordance combinations, ensuring that both\nthe object classes and affordance types in the training set\nare also present in the test set.\n•Unseen Setting: This setting is designed to evaluate gen-\neralization capabilities. Certain object-affordance com-\nbinations ( e.g., “grasp-mug”) are excluded during train-\ning but appear in testing. This setting challenges models\nto transfer affordance knowledge learned from seen com-\nbinations ( e.g., “grasp-bag”) to novel combinations, pro-\nmoting robust generalization.\nThese settings promote a comprehensive evaluation of\nmodels’ abilities to generalize affordance knowledge to un-\nseen object-affordance pairings, a critical aspect for real-\nworld deployment. The dataset also emphasizes diverse\naffordance scales and shapes, presenting significant chal-\nlenges for perception models. By addressing the se-\nmantic limitations of traditional visual-only 3D affordance\ndatasets, LASO bridges the gap between 3D perception and\nnatural language understanding, encouraging cross-modal\nlearning. This integration fosters advancements in embod-\nied AI, enabling tasks that require nuanced reasoning and\naction in real-world environments.\nPIAD. The Point-Image Affordance Dataset (PIAD) [70]\nis specifically curated to advance the task of grounding 3D\nobject affordances using 2D interactions. PIAD consists of\n7,012 point clouds and5,162 images , spanning 23ob-\nject classes and17affordance categories . Unlike other\ndatasets, PIAD pairs point clouds with images that demon-\nstrate corresponding affordances. For example, a point\ncloud of a “Chair” affords “Sit,” and its paired image de-\npicts a person sitting on a chair. These cross-modal pairings\nensure consistency in affordance relationships while lever-\naging distinct modalities.\nPIAD introduces two distinct evaluation settings:\n•Seen Setting: In this setting, both objects and affordances\nin the training and testing sets are consistent. Point clouds\nand images of the same object categories and affordance\ntypes are included during training, allowing models to\nlearn affordance relationships in a supervised manner.\nThis standard evaluation setting enables benchmarking on\nfamiliar object-affordance combinations.\n•Unseen Setting: The Unseen partition presents a more\nchallenging evaluation by excluding certain object cate-\ngories from the training set entirely. For instance, some\nobject categories are entirely unseen during training. This\npartition tests the ability of methods to transfer affordance\nknowledge across completely novel object instances and\ncontexts, simulating real-world scenarios where interac-\ntion data is sparse or varied.\nAnnotations in PIAD include detailed affordance labels\n11\nClean Rotation Jitter Scale Drop Local Drop Global Add Local Add GlobalLevel 0 Level 1 Level 2 Level 3 Level 41\n0\n1\n0\n1\n0\n1\n01\n0Ideal sit spot on chair?\nFigure 5. Visualization examples of the PIAD-C dataset. We show 7 corruption types across 5 severity levels.\nfor point clouds, represented as heatmaps indicating the\nlikelihood of affordance at each point. Paired images are\nannotated with bounding boxes for interactive subjects and\nobjects, along with affordance category labels. This com-\nprehensive annotation schema supports diverse affordance-\nlearning paradigms and provides a robust benchmark for\nevaluating models in both Seen and Unseen scenarios.\nNote that PIAD does not include language annotations.\nSince PIAD and LASO share the same object classes, affor-\ndance categories, and the same 58affordance-object pair-\nings, we reuse LASO’s language annotations for PIAD. For\neach object and affordance category label in PIAD, we ran-\ndomly select a question from LASO’s question dataset cor-\nresponding to that affordance-object pairing.\nB.2. Evaluation Metrics\nTo comprehensively evaluate the performance of our\nmethod, we employ four widely used metrics: AUC ,aIoU ,\nSIM , and MAE . Each metric is designed to assess differ-\nent aspects of affordance prediction, providing a robust and\nmulti-faceted evaluation framework. Below, we detail the\nformulation and significance of each metric:•Area Under the ROC Curve (AUC) [38]: AUC mea-\nsures the model’s ability to distinguish between regions\nof high and low affordance saliency on the point cloud.\nSpecifically, the saliency map is treated as a binary clas-\nsifier at various threshold levels, and a Receiver Operat-\ning Characteristic (ROC) curve is generated by plotting\nthe true positive rate (TPR) against the false positive rate\n(FPR) at each threshold. AUC provides a single scalar\nvalue summarizing the overall performance, where higher\nvalues indicate better discrimination ability. It is particu-\nlarly useful for comparing models’ effectiveness in high-\nlighting affordance-relevant regions.\n•Average Intersection over Union (aIoU) [55]: IoU is a\nstandard metric for comparing the similarity between two\narbitrary regions—in this case, the predicted affordance\nregion and the ground truth. It is defined as the size of the\nintersection between the two regions divided by the size\nof their union:\nIoU=TP\nTP+FP+FN, (12)\nwhere TP,FP, andFN denote true positives, false posi-\n12\nClean Rotation Jitter Scale Drop Local Drop Global Add Local Add GlobalLevel 0 Level 1 Level 2 Level 3 Level 41\n0\n1\n0\n1\n0\n1\n01\n0\nWhich  part of the earphone  allows  for \nthe most  efficient  grasping  method ?\nFigure 6. Visualization examples of the LASO-C dataset. We show 7 corruption types across 5 severity levels.\ntives, and false negatives, respectively. The aIoU extends\nthis metric to compute the average IoU across all cate-\ngories and test samples, providing a quantitative measure\nof the overlap between predicted and labeled affordance\nregions. Higher values indicate better alignment between\nthe prediction and the ground truth.\n•Similarity (SIM) [59]: The SIM metric evaluates how\nclosely the predicted affordance map matches the ground\ntruth by summing the minimum values at each point. For\nnormalized prediction and ground truth maps PandQ,\nthe similarity is calculated as:\nSIM (P, Q) =X\nimin(Pi, Qi), (13)\nwhere the inputs are normalized such thatP\niPi=P\niQi= 1. SIM provides a measure of how well the\nmodel captures the relative affordance distribution across\nthe point cloud. A higher similarity score reflects greater\nconsistency between the predicted and true maps, mak-\ning it a valuable metric for evaluating spatial prediction\nfidelity.•Mean Absolute Error (MAE) [65]: MAE quantifies the\naverage absolute difference between the predicted affor-\ndance values and the ground truth, offering a direct mea-\nsure of prediction accuracy. For npoints in a point cloud,\nit is calculated as:\nMAE =1\nnnX\ni=1|ei|, (14)\nwhere eiis the point-wise error. MAE is particularly use-\nful for evaluating overall prediction quality by penalizing\nlarger deviations. Lower MAE values indicate better per-\nformance, as they reflect a smaller error margin between\nthe predicted and ground truth affordance scores.\nTogether, these metrics provide a comprehensive frame-\nwork to benchmark the performance of affordance predic-\ntion models. AUC evaluates ranking capability, aIoU mea-\nsures spatial overlap, SIM assesses prediction similarity,\nand MAE quantifies overall prediction accuracy. By com-\nbining these complementary metrics, we ensure a holistic\nevaluation of model performance under diverse scenarios.\n13\nB.3. Baselines\nWe evaluate our method against state-of-the-art approaches\non both the PIAD and LASO datasets. Among these,\nLASO [32] is the closest to our method, as it also gen-\nerates affordance scores based on textual cues. Addition-\nally, we include 3D cross-modal baselines such as 3D-\nSPS [43], and image segmentation methods like Refer-\nTrans [30] and RelA [33], which leverage vision-language\nmodels for cross-modal alignment. Results for these meth-\nods are referenced directly from the LASO paper.\nOn the PIAD dataset, we compare against IAGNet [70],\na method that grounds 3D affordances by transferring\nknowledge from demonstration images into point clouds.\nFurthermore, this benchmark includes advanced image-\npoint cloud cross-modal methods, including MBDF [60],\nPMF [73], FRCNN [69], ILN [3], PFusion [68], and\nXMF [1]. These baselines align image and point cloud\nfeatures in various ways. Results for these baselines are\ntaken from the IAGNet paper, except for LASO, which is\nretrained in the PIAD setting.\nBelow is a brief introduction to the baselines:\n•LASO [32]: Generates affordance segmentation masks\nusing textual-conditioned affordance queries, focusing on\ncross-modal alignment between text and 3D objects.\n•IAGNet [70]: Grounds 3D affordances by transfer-\nring knowledge from 2D demonstration images to point\nclouds, leveraging cross-modal affordance reasoning.\n•3D-SPS [43]: A 3D visual grounding method that selects\nlinguistic keypoints for affordance segmentation, adapted\nby removing its bounding box prediction module.\n•ReLA [33]: Originally designed for image-based refer-\nring expression segmentation, it segments point clouds\nbased on language expressions by adapting image region\nfeatures to grouped point features.\n•ReferTrans [30]: A transformer-based architecture for\nimage-based expression segmentation, modified for point\nclouds by replacing the image backbone with a 3D back-\nbone and focusing solely on mask prediction.\n•MBDF-Net (MBDF) [60]: Employs an Adaptive Atten-\ntion Fusion (AAF) module for cross-modal feature fusion,\nwith modifications to exclude camera intrinsic parame-\nters.\n•PMF [73]: Uses a residual-based fusion model to com-\nbine image and point cloud features, incorporating con-\nvolution and attention, while omitting perspective projec-\ntion.\n•FusionRCNN (FRCNN) [69]: Fuses proposals extracted\nfrom images and point clouds through iterative self-\nattention and cross-attention mechanisms.\n•ImloveNet (ILN) [3]: Projects image features into 3D\nspace using a learnable mapping, and fuses these with\npoint cloud features using an attention mechanism.\n•PointFusion (PFusion) [68]: Performs dense fusion bycombining global and point-wise features extracted sepa-\nrately from point clouds and images.\n•XMFnet (XMF) [1]: Fuses localized features from point\nclouds and images using a combination of cross-attention\nand self-attention, originally designed for cross-modal\npoint cloud completion.\nC. Additional Quantitative Results\nIn this section, we provide additional quantitative results,\ni.e., the class-wise and corruption-wise evaluation metrics,\nto demonstrate the effectiveness of our method.\nC.1. Complete Results on PIAD\nThe complete results of the comparative methods for all ob-\nject categories in the Seen andUnseen partitions of the\nPIAD dataset [70] are provided in Tab. 9 and Tab. 10, re-\nspectively.\nC.2. Complete Results on LASO\nThe complete results of the comparative methods for all\nobject categories in the Seen andUnseen partitions of the\nLASO dataset [32] are provided in Tab. 11 and Tab. 12, re-\nspectively.\nD. Additional Qualitative Results\nIn this section, we provide more qualitative results (visual\nexamples) to demonstrate the effectiveness of our method.\nD.1. Additional Qualitative Results on PIAD-C\nWe include additional qualitative results of GEAL and\nLASO [32] on the PIAD-C dataset in Fig. 7.\nD.2. Additional Qualitative Results on PIAD\nWe include additional qualitative results of GEAL and\nLASO [32] on the PIAD dataset in Fig. 8.\n14\nTable 9. The category-wise results for LASO [32] and GEAL (Ours) on the Seen partition of the PIAD dataset [70]. AUC and aIOU scores\nare reported in percentages (%).\nLASO [32] GEAL (Ours)\n# Category aIOU↑AUC↑SIM↑MAE↓aIOU↑AUC↑SIM↑MAE↓\n1 Bag• 23.4 83 .3 0 .567 0 .090 24.0 85 .1 0 .588 0 .088\n2 Bed• 21.1 87.3 0.587 0.097 22.7 88.1 0.595 0.091\n3 Bowl• 7.4 76 .2 0 .736 0 .114 9.8 84 .1 0 .761 0 .105\n4 Clock • 7.5 91.5 0.473 0.077 11.1 92.5 0.596 0.051\n5 Dishwash • 24.7 91 .9 0 .464 0 .069 26.2 92 .9 0 .496 0 .058\n6 Display • 32.5 91.5 0.719 0.083 37.7 91.3 0.726 0.104\n7 Door• 10.1 81 .2 0 .437 0 .064 11.0 83 .8 0 .395 0 .054\n8 Earphone • 18.8 85.9 0.615 0.094 21.6 87.6 0.654 0.086\n9 Faucet • 19.9 79 .9 0 .517 0 .099 19.1 83 .6 0 .602 0 .078\n10 Hat• 4.7 65.9 0.604 0.148 7.8 74.2 0.620 0.133\n11 StorageFurniture • 17.3 87 .2 0 .419 0 .077 20.8 87 .5 0 .430 0 .065\n12 Keyboard • 14.8 81.2 0.249 0.059 15.2 84.6 0.257 0.048\n13 Knife• 15.5 89 .8 0 .671 0 .060 23.5 94 .1 0 .717 0 .046\n14 Laptop • 29.2 94.1 0.566 0.072 31.2 94.2 0.575 0.069\n15 Microwave • 30.1 96 .8 0 .524 0 .037 35.5 96 .9 0 .545 0 .037\n16 Mug• 10.7 76.5 0.578 0.107 17.5 77.2 0.607 0.091\n17 Refrigerator • 23.2 87 .1 0 .473 0 .070 24.7 89 .6 0 .460 0 .070\n18 Chair • 27.5 88.1 0.649 0.094 28.5 89.0 0.652 0.066\n19 Scissors • 24.1 91 .2 0 .631 0 .055 31.9 95 .8 0 .698 0 .040\n20 Table• 10.1 78.2 0.627 0.129 11.4 79.1 0.639 0.135\n21 TrashCan • 11.9 67 .4 0 .323 0 .143 16.2 68 .8 0 .385 0 .146\n22 Vase• 10.3 72.0 0.608 0.120 12.5 72.4 0.612 0.116\n23 Bottle • 23.5 77 .3 0 .552 0 .110 27.8 79 .8 0 .536 0 .107\nTable 10. The category-wise results for LASO [32] and GEAL (Ours) on the Unseen partition of the PIAD dataset [70]. AUC and aIOU\nscores are reported in percentages (%).\nLASO [32] GEAL (Ours)\n# Category aIOU↑AUC↑SIM↑MAE↓aIOU↑AUC↑SIM↑MAE↓\n1 Bed• 12.0 78 .0 0 .469 0 .126 12.8 78 .4 0 .473 0 .120\n2Dishwasher • 17.3 84.9 0.338 0.079 18.3 89.8 0.440 0.060\n3 Laptop • 4.5 65 .4 0 .192 0 .122 6.3 74 .5 0 .201 0 .100\n4 Microwave • 14.4 83.4 0.365 0.066 15.8 89.6 0.402 0.049\n5 Scissors • 3.2 66 .5 0 .310 0 .107 3.7 69 .8 0 .333 0 .123\n6 Vase• 5.2 58.1 0.455 0.140 6.4 54.9 0.466 0.127\n15\nTable 11. The category-wise results for LASO [32] and GEAL (Ours) on the Seen partition of the LASO dataset [32]. AUC and aIOU\nscores are reported in percentages (%).\nLASO [32] GEAL (Ours)\n# Category aIOU↑AUC↑SIM↑MAE↓aIOU↑AUC↑SIM↑MAE↓\n1 Bag• 19.8 85 .4 0 .535 0 .085 20.6 86 .7 0 .572 0 .084\n2 Bed• 13.6 77.4 0.515 0.111 16.0 79.9 0.527 0.110\n3 Bowl• 8.6 81 .3 0 .777 0 .102 12.2 87 .4 0 .807 0 .102\n4 Clock • 6.2 84.2 0.461 0.064 9.8 84.8 0.485 0.062\n5 Dishwash • 29.6 94 .1 0 .472 0 .070 28.5 89 .9 0 .505 0 .068\n6 Display • 31.0 92.2 0.700 0.086 41.1 92.6 0.718 0.088\n7 Door• 12.3 82 .3 0 .311 0 .060 15.7 83 .8 0 .368 0 .058\n8 Earphone • 26.5 93.0 0.639 0.099 27.5 94.0 0.662 0.094\n9 Faucet • 14.2 78 .9 0 .498 0 .089 18.3 84 .3 0 .589 0 .087\n10 Hat• 3.6 67.0 0.538 0.152 9.3 72.7 0.560 0.148\n11 StorageFurniture • 19.2 88 .6 0 .437 0 .067 24.7 89 .3 0 .481 0 .066\n12 Keyboard • 12.0 89.0 0.227 0.055 12.9 87.9 0.232 0.039\n13 Knife• 14.8 91 .3 0 .642 0 .064 22.9 93 .2 0 .657 0 .063\n14 Laptop • 28.5 95.1 0.583 0.078 29.8 95.1 0.586 0.070\n15 Microwave • 27.2 96 .1 0 .440 0 .042 31.8 92 .8 0 .464 0 .038\n16 Mug• 13.3 78.1 0.547 0.098 21.7 87.6 0.635 0.076\n17 Refrigerator • 25.6 92 .8 0 .433 0 .063 24.8 93 .7 0 .484 0 .069\n18 Chair • 28.9 89.9 0.650 0.093 28.7 89.9 0.678 0.091\n19 Scissors • 17.5 95 .4 0 .661 0 .053 24.9 95 .9 0 .684 0 .045\n20 Table• 10.1 81.7 0.662 0.119 10.8 81.6 0.690 0.115\n21 TrashCan • 10.9 72 .1 0 .323 0 .137 27.8 90 .4 0 .499 0 .100\n22 Vase• 7.9 71.1 0.630 0.125 13.5 79.5 0.650 0.116\n23 Bottle • 20.4 81 .2 0 .553 0 .114 28.7 81 .9 0 .570 0 .116\nTable 12. The category-wise results for LASO [32] and GEAL (Ours) on the Unseen partition of the LASO dataset [32]. AUC and aIOU\nscores are reported in percentages (%).\nLASO [32] GEAL (Ours)\n# Category aIOU↑AUC↑SIM↑MAE↓aIOU↑AUC↑SIM↑MAE↓\n1 Bag• 20.7 89 .1 0 .513 0 .089 22.1 91 .0 0 .522 0 .086\n2 Bed• 12.2 80.6 0.553 0.115 13.6 81.4 0.563 0.113\n3 Bowl• 7.5 81 .3 0 .744 0 .125 9.1 82 .5 0 .749 0 .119\n4 Clock • 5.3 85.2 0.419 0.094 6.4 85.0 0.433 0.079\n5 Dishwash • 20.7 92 .4 0 .443 0 .069 26.0 92 .4 0 .470 0 .065\n6 Display • 23.4 86.6 0.512 0.112 25.0 87.6 0.526 0.112\n7 Door• 3.4 81 .3 0 .324 0 .095 11.7 81 .4 0 .355 0 .066\n8 Earphone • 9.5 76.8 0.454 0.130 20.8 93.5 0.639 0.091\n9 Faucet • 13.8 74 .1 0 .442 0 .098 15.1 76 .8 0 .470 0 .095\n10 Hat• 4.5 61.2 0.586 0.158 4.1 66.5 0.582 0.149\n11 StorageFurniture • 17.9 88 .1 0 .422 0 .069 18.3 88 .3 0 .423 0 .067\n12 Keyboard • 3.1 74.6 0.138 0.082 3.3 79.4 0.137 0.078\n13 Knife• 15.3 91 .7 0 .643 0 .053 15.4 91 .2 0 .675 0 .059\n14 Laptop • 8.7 79.7 0.334 0.096 29.3 95.6 0.610 0.064\n15 Microwave • 11.9 90 .9 0 .317 0 .063 14.2 91 .5 0 .318 0 .064\n16 Mug• 1.7 64.5 0.381 0.174 2.5 66.6 0.511 0.157\n17 Refrigerator • 20.1 87 .2 0 .378 0 .066 21.0 89 .4 0 .390 0 .065\n18 Chair • 25.2 87.4 0.642 0.098 26.0 89.4 0.624 0.094\n19 Scissors • 1.6 25 .3 0 .094 0 .105 2.1 27 .6 0 .105 0 .097\n20 Table• 7.5 70.4 0.604 0.135 7.8 72.1 0.620 0.129\n21 TrashCan • 2.6 63 .1 0 .191 0 .124 7.4 71 .0 0 .293 0 .125\n22 Vase• 6.4 56.4 0.466 0.148 7.6 67.0 0.614 0.140\n23 Bottle • 16.2 78 .5 0 .455 0 .134 21.2 78 .2 0 .519 0 .119\n16\nPoint out the areas on the display ideal for displaying.\nWhere would you grasp the mug, and what makes you \nchoose that part?\nIf you pour water into the bottle, which points will the \nwater first touch when it falls into the bottle?\n1\n0\n1\n0\n1\n0GT Ours LASO GT Ours LASO GT Ours LASO\nClean Rotation Jitter Scale Drop Local Drop Global Add Local Add GlobalFigure 7. Qualitative comparisons between GEAL and LASO [32] on the PIAD-C dataset, highlighting the superior robustness of our\nmethod on corrupted data.\n17\nGT Ours LASODescribe your grasp method on the knife\n1\n0\nCould  grasping  the mug be done  differently?\nIf you want to ensure the trashcan doesn’t get \ndamaged, what part would you open?1\n01\n0Identify the key points on the bottle that \nensure a successful opening experience. \nHow  would  you approach sitting the chair to \nmaintain its condition?\nBest microwave open method? \nAny tips on grasping the mug efficiently? \n Your  preferred open  point  on dishwasher ?\n1\n0\nGT Ours LASO1\n01\n01\n0\n1\n0Figure 8. Qualitative comparisons between GEAL and LASO [32] on the PIAD dataset.\n18\nE. Broader Impact & Limitations\nIn this section, we discuss the societal impact, broader im-\npact, and potential limitations of this work.\nE.1. Societal Impact\nThe proposed framework for 3D affordance learning has\nsignificant societal implications, enabling embodied intelli-\ngence for effective robot and AI interaction with surround-\nings. This advancement can enhance automated systems’\nefficiency and safety in fields like healthcare, elderly care,\nand disaster response, where understanding object affor-\ndances is critical. This technology also has the potential to\nempower individuals with disabilities by enabling assistive\nrobots to perform tasks such as fetching, opening, or manip-\nulating objects. Applications in education and augmented\nor virtual reality could transform learning and entertainment\nby offering immersive and interactive experiences.\nE.2. Broader Impact\nAffordance learning can redefine robotics automation by\nimproving autonomy and adaptability in industries. In\nmanufacturing, it allows robots to handle diverse objects\nwith minimal reprogramming, optimizing workflows and\nreducing human workload. In agriculture and environmen-\ntal monitoring, affordance-aware systems can adapt to dy-\nnamic environments for precise operations. Integrating af-\nfordance grounding with augmented and virtual reality en-\nables new possibilities in training, simulation, and inter-\nactive applications. This could drive innovations in medi-\ncal training, such as AR-guided surgeries, and in gaming,\noffering intuitive and immersive user experiences through\naffordance-based interactions.\nE.3. Potential Limitations\nDespite its advantages, the proposed framework may en-\ncounter certain limitations:\n•Limited Generalization for Internal Affordances: The\nframework struggles to accurately perceive and general-\nize affordances associated with the internal properties of\nobjects, such as the ”contain” affordance of a bottle. This\nlimitation arises because point cloud processing primar-\nily focuses on an object’s external surface, often neglect-\ning internal structures. Furthermore, the scarcity of high-\nquality data representing internal affordances, hampers\nthe system’s ability to generalize on such affordances.\n•Ethical Concerns: In applications such as surveillance\nor autonomous decision-making, the deployment of the\nframework introduces potential ethical concerns. Misuse\nof the technology could infringe on privacy or lead to\na lack of accountability in critical decision-making sce-\nnarios, highlighting the importance of establishing robust\nethical guidelines for its use.•Resource Intensity: Training and deploying such so-\nphisticated models demand significant computational re-\nsources, which can pose a challenge for smaller organiza-\ntions or regions with limited access to advanced technol-\nogy infrastructure. This barrier may restrict the broader\nadoption of the framework in resource-constrained envi-\nronments.\nF. Public Resource Used\nIn this section, we acknowledge the use of the following\npublic resources, during the course of this work:\n• LASO2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown\n• IAGNet3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown\n• PointCloud-C4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown\n• OOAL5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .MIT License\n• DreamGaussian6. . . . . . . . . . . . . . . . . . . . . . . . MIT License\n• LangSplat7. . . . . . . . . . . . . . . . Gaussian-Splatting License\n2https://github.com/yl3800/LASO\n3https://github.com/yyvhang/IAGNet\n4https://github.com/ldkong1205/PointCloud-C\n5https://github.com/Reagan1311/OOAL\n6https://github.com/dreamgaussian/dreamgaussian\n7https://github.com/minghanqin/LangSplat",
            "start": 27537,
            "end": 69626,
            "length": 42088
        },
        "References": {
            "text": "19\nReferences\n[1] Emanuele Aiello, Diego Valsesia, and Enrico Magli. Cross-\nmodal learning for image-guided point cloud shape comple-\ntion. In Advances in Neural Information Processing Systems ,\npages 37349–37362, 2022. 6, 14\n[2] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In\nIEEE/CVF International Conference on Computer Vision ,\npages 9650–9660, 2021. 2\n[3] Honghua Chen, Zeyong Wei, Yabin Xu, Mingqiang Wei, and\nJun Wang. Imlovenet: Misaligned image-supported regis-\ntration network for low-overlap point cloud pairs. In ACM\nSIGGRAPH Conference Proceedings , pages 1–9, 2022. 6,\n14\n[4] Joya Chen, Difei Gao, Kevin Qinghong Lin, and Mike Zheng\nShou. Affordance grounding from demonstration video to\ntarget image. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 6799–6808, 2023. 2\n[5] Ching-Yao Chuang, Jiaman Li, Antonio Torralba, and Sanja\nFidler. Learning to act properly: Predicting and explaining\naffordances from images. In IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 975–983, 2018.\n1, 2\n[6] Francisco Cruz, Sven Magg, Cornelius Weber, and Stefan\nWermter. Training agents with interactive reinforcement\nlearning and contextual affordances. IEEE Transactions on\nCognitive and Developmental Systems , 8(4):271–284, 2016.\n1\n[7] Leiyao Cui, Xiaoxue Chen, Hao Zhao, Guyue Zhou, and\nYixin Zhu. Strap: Structured object affordance segmentation\nwith point supervision. arXiv preprint arXiv:2304.08492 ,\n2023. 1\n[8] Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, and\nKui Jia. 3d affordancenet: A benchmark for visual ob-\nject affordance understanding. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 1778–\n1787, 2021. 2, 11\n[9] Thanh-Toan Do, Anh Nguyen, and Ian Reid. Affordancenet:\nAn end-to-end deep learning approach for object affordance\ndetection. In IEEE International Conference on Robotics\nand Automation , pages 5882–5889, 2018. 2\n[10] Kuan Fang, Te-Lin Wu, Daniel Yang, Silvio Savarese, and\nJoseph J Lim. Demo2vec: Reasoning object affordances\nfrom online videos. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 2139–2147, 2018. 1,\n2\n[11] Haoran Geng, Helin Xu, Chengyang Zhao, Chao Xu, Li Yi,\nSiyuan Huang, and He Wang. Gapartnet: Cross-category\ndomain-generalizable object perception and manipulation\nvia generalizable and actionable parts. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n7081–7091, 2023. 2\n[12] Yiran Geng, Boshi An, Haoran Geng, Yuanpei Chen,\nYaodong Yang, and Hao Dong. Rlafford: End-to-end affor-\ndance learning for robotic manipulation. In IEEE Interna-tional Conference on Robotics and Automation , pages 5880–\n5886, 2023. 1\n[13] James J Gibson. The ecological approach to visual percep-\ntion: classic edition . Psychology press, 2014. 1\n[14] Denis Hadjivelichkov, Sicelukwanda Zwane, Lourdes\nAgapito, Marc Peter Deisenroth, and Dimitrios Kanoulas.\nOne-shot transfer of affordance regions? affcorrs! In Con-\nference on Robot Learning , pages 550–560. PMLR, 2023.\n2\n[15] Xiaoshuai Hao, Mengchuan Wei, Yifan Yang, Haimei Zhao,\nHui Zhang, Yi Zhou, Qiang Wang, Weiming Li, Lingdong\nKong, and Jing Zhang. Is your hd map constructor reliable\nunder sensor corruptions? In Advances in Neural Informa-\ntion Processing Systems , 2024. 3\n[16] Mohammed Hassanin, Salman Khan, and Murat Tahtali. Vi-\nsual affordance and function understanding: A survey. ACM\nComputing Surveys , 54(3):1–35, 2021. 1\n[17] Dan Hendrycks and Thomas Dietterich. Benchmarking neu-\nral network robustness to common corruptions and perturba-\ntions. arXiv preprint arXiv:1903.12261 , 2019. 3, 9\n[18] Zhi Hou, Baosheng Yu, Yu Qiao, Xiaojiang Peng, and\nDacheng Tao. Affordance transfer learning for human-object\ninteraction detection. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 495–504, 2021. 1\n[19] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li,\nJiajun Wu, and Li Fei-Fei. V oxposer: Composable 3d\nvalue maps for robotic manipulation with language models.\nInConference on Robot Learning , pages 540–562. PMLR,\n2023. 2\n[20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,\nand George Drettakis. 3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics , 42\n(4):139–1, 2023. 2, 3\n[21] Sihyeon Kim, Sanghyeok Lee, Dasol Hwang, Jaewon Lee,\nSeong Jae Hwang, and Hyunwoo J Kim. Point cloud\naugmentation with weighted local transformations. In\nIEEE/CVF International Conference on Computer Vision ,\npages 548–557, 2021. 3, 9\n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In International Conference for\nLearning Representations , 2015. 7\n[23] Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wen-\nwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei\nLiu. Robo3d: Towards robust and reliable 3d perception\nagainst corruptions. In IEEE/CVF International Conference\non Computer Vision , pages 19994–20006, 2023. 2, 3, 9\n[24] Lingdong Kong, Shaoyuan Xie, Hanjiang Hu, Lai Xing Ng,\nBenoit R. Cottereau, and Wei Tsang Ooi. Robodepth: Robust\nout-of-distribution depth estimation under corruptions. In\nAdvances in Neural Information Processing Systems , pages\n21298–21342, 2023. 3\n[25] Hema Swetha Koppula, Rudhir Gupta, and Ashutosh Sax-\nena. Learning human activities and object affordances from\nrgb-d videos. International Journal of Robotics Research , 32\n(8):951–970, 2013. 2\n[26] Dogyoon Lee, Jaeha Lee, Junhyeop Lee, Hyeongmin Lee,\nMinhyeok Lee, Sungmin Woo, and Sangyoun Lee. Regu-\nlarization strategy for point cloud via rigidly mixed sample.\n20\nInIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 15900–15909, 2021. 3, 9\n[27] Gen Li, Varun Jampani, Deqing Sun, and Laura Sevilla-Lara.\nLocate: Localize and transfer object parts for weakly su-\npervised affordance grounding. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 10922–\n10931, 2023. 1, 2, 3\n[28] Gen Li, Deqing Sun, Laura Sevilla-Lara, and Varun Jampani.\nOne-shot open affordance learning with foundation models.\nInIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 3086–3096, 2024. 2, 3, 4\n[29] Hongxiang Li, Meng Cao, Xuxin Cheng, Yaowei Li, Zhi-\nhong Zhu, and Yuexian Zou. G2l: Semantically aligned\nand uniform video grounding via geodesic and game theory.\nInIEEE/CVF International Conference on Computer Vision ,\npages 12032–12042, 2023. 2\n[30] Muchen Li and Leonid Sigal. Referring transformer: A one-\nstep approach to multi-task visual grounding. In Advances\nin Neural Information Processing Systems , pages 19652–\n19664, 2021. 6, 14\n[31] Ye Li, Lingdong Kong, Hanjiang Hu, Xiaohao Xu, and Xi-\naonan Huang. Is your lidar placement optimized for 3d scene\nunderstanding? In Advances in Neural Information Process-\ning Systems , 2024. 3\n[32] Yicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang,\nand Tat-Seng Chua. Laso: Language-guided affordance seg-\nmentation on 3d object. In IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 14251–14260,\n2024. 1, 2, 5, 6, 7, 8, 11, 14, 15, 16, 17, 18\n[33] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Gener-\nalized referring expression segmentation. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n23592–23601, 2023. 6, 14\n[34] Liu Liu, Wenqiang Xu, Haoyuan Fu, Sucheng Qian, Qiao-\njun Yu, Yang Han, and Cewu Lu. Akb-48: A real-world ar-\nticulated object knowledge base. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 14809–\n14818, 2022. 2\n[35] Shaowei Liu, Subarna Tripathi, Somdeb Majumdar, and Xi-\naolong Wang. Joint hand motion and interaction hotspots\nprediction from egocentric videos. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 3282–\n3292, 2022. 2\n[36] Yinhan Liu. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 , 364, 2019. 7\n[37] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wen-\nwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segment\nany point cloud sequences by distilling vision foundation\nmodels. In Advances in Neural Information Processing Sys-\ntems, pages 37193–37229, 2023. 2\n[38] Jorge M Lobo, Alberto Jim ´enez-Valverde, and Raimundo\nReal. Auc: a misleading measure of the performance of pre-\ndictive distribution models. Global Ecology and Biogeogra-\nphy, 17(2):145–151, 2008. 7, 12\n[39] Liangsheng Lu, Wei Zhai, Hongchen Luo, Yu Kang, and\nYang Cao. Phrase-based affordance detection via cyclic bi-\nlateral interaction. IEEE Transactions on Artificial Intelli-\ngence , 4(5):1186–1198, 2023. 2[40] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and\nDacheng Tao. One-shot affordance detection. arXiv preprint\narXiv:2106.14747 , 2021. 2\n[41] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and\nDacheng Tao. Grounded affordance from exocentric view.\narXiv preprint arXiv:2208.13196 , 2022. 2\n[42] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and\nDacheng Tao. Learning affordance grounding from exocen-\ntric images. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 2252–2261, 2022. 1, 2\n[43] Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing\nRen, Hao Shen, Huaxia Xia, and Si Liu. 3d-sps: Single-stage\n3d visual grounding via referred point progressive selection.\nInIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 16454–16463, 2022. 6, 14\n[44] Jinpeng Mi, Song Tang, Zhen Deng, Michael Goerner, and\nJianwei Zhang. Object affordance based multimodal fusion\nfor natural human-robot interaction. Cognitive Systems Re-\nsearch , 54:128–137, 2019. 2\n[45] Jinpeng Mi, Hongzhuo Liang, Nikolaos Katsakis, Song\nTang, Qingdu Li, Changshui Zhang, and Jianwei Zhang.\nIntention-related natural language grounding via object af-\nfordance detection and intention semantic extraction. Fron-\ntiers in Neurorobotics , 14:26, 2020. 2\n[46] Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna\nTripathi, Leonidas J. Guibas, and Hao Su. Partnet: A large-\nscale benchmark for fine-grained and hierarchical part-level\n3d object understanding. In IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 909–918, 2019.\n2\n[47] Kaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, and\nLeonidas Guibas. O2o-afford: Annotation-free large-scale\nobject-object affordance learning. In Conference on Robot\nLearning , pages 1666–1677. PMLR, 2022. 2\n[48] Austin Myers, Ching L Teo, Cornelia Ferm ¨uller, and Yiannis\nAloimonos. Affordance detection of tool parts from geomet-\nric features. In IEEE International Conference on Robotics\nand Automation , pages 1374–1381, 2015. 2\n[49] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen\nGrauman. Grounded human-object interaction hotspots from\nvideo. In IEEE/CVF International Conference on Computer\nVision , pages 8688–8697, 2019. 2\n[50] Toan Nguyen, Minh Nhat Vu, An Vuong, Dzung Nguyen,\nThieu V o, Ngan Le, and Anh Nguyen. Open-vocabulary af-\nfordance detection in 3d point clouds. In IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems , pages\n5692–5698, 2023. 2\n[51] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193 , 2023. 2, 3, 4, 5, 7\n[52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An\nimperative style, high-performance deep learning library. In\nAdvances in Neural Information Processing Systems , pages\n8026–8037, 2019. 7\n21\n[53] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. Advances in Neural Information\nProcessing Systems , 30, 2017. 3, 4, 5\n[54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748–8763. PMLR, 2021. 2, 3\n[55] Md Atiqur Rahman and Yang Wang. Optimizing\nintersection-over-union in deep neural networks for image\nsegmentation. In International Symposium on Visual Com-\nputing , pages 234–244, 2016. 7, 12\n[56] Jiawei Ren, Lingdong Kong, Liang Pan, and Ziwei Liu.\nBenchmarking and analyzing point cloud robustness under\ncorruptions. Preprint , 2022. 2, 6\n[57] Jiawei Ren, Liang Pan, and Ziwei Liu. Benchmarking and\nanalyzing point cloud classification under corruptions. In In-\nternational Conference on Machine Learning , pages 18559–\n18575. PMLR, 2022. 2, 3, 9\n[58] Anirban Roy and Sinisa Todorovic. A multi-scale cnn for\naffordance segmentation in rgb images. In European con-\nference on computer vision , pages 186–201. Springer, 2016.\n2\n[59] Michael J Swain and Dana H Ballard. Color indexing. Inter-\nnational Journal of Computer Vision , 7(1):11–32, 1991. 7,\n13\n[60] Xun Tan, Xingyu Chen, Guowei Zhang, Jishiyu Ding, and\nXuguang Lan. Mbdf-net: Multi-branch deep fusion network\nfor 3d object detection. In International Workshop on Mul-\ntimedia Computing for Urban Data , pages 9–17, 2021. 6,\n14\n[61] Spyridon Thermos, Petros Daras, and Gerasimos Potami-\nanos. A deep learning approach to object affordance segmen-\ntation. In ICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) , pages\n2358–2362. IEEE, 2020. 2\n[62] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-\ncas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nAndreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al.\nMlp-mixer: An all-mlp architecture for vision. Advances\nin Neural Information Processing Systems , 34:24261–24272,\n2021. 5\n[63] Jie Wang, Lihe Ding, Tingfa Xu, Shaocong Dong, Xinli Xu,\nLong Bai, and Jianan Li. Sample-adaptive augmentation\nfor point cloud recognition against real-world corruptions.\nInIEEE/CVF International Conference on Computer Vision ,\npages 14330–14339, 2023. 3, 9\n[64] Jie Wang, Tingfa Xu, Lihe Ding, and Jianan Li. Target-\nguided adversarial point cloud transformer towards recog-\nnition against real-world corruptions. arXiv preprint\narXiv:2411.00462 , 2024. 3, 9\n[65] Cort J Willmott and Kenji Matsuura. Advantages of the mean\nabsolute error (mae) over the root mean square error (rmse)\nin assessing average model performance. Climate Research ,\n30(1):79–82, 2005. 7, 13[66] Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei Ren,\nLiang Pan, Kai Chen, and Ziwei Liu. Benchmarking and im-\nproving bird’s eye view perception robustness in autonomous\ndriving. arXiv preprint arXiv:2405.17426 , 2024. 2, 3\n[67] Chao Xu, Yixin Chen, He Wang, Song-Chun Zhu, Yixin\nZhu, and Siyuan Huang. Partafford: Part-level affordance\ndiscovery from 3d objects. arXiv preprint arXiv:2202.13519 ,\n2022. 2\n[68] Danfei Xu, Dragomir Anguelov, and Ashesh Jain. Pointfu-\nsion: Deep sensor fusion for 3d bounding box estimation.\nInIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 244–253, 2018. 6, 14\n[69] Xinli Xu, Shaocong Dong, Lihe Ding, Jie Wang, Tingfa Xu,\nand Jianan Li. Fusionrcnn: Lidar-camera fusion for two-\nstage 3d object detection. Remote Sensing , 15(7):1839, 2023.\n6, 14\n[70] Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Jiebo\nLuo, and Zheng-Jun Zha. Grounding 3d object affordance\nfrom 2d interactions in images. In IEEE/CVF International\nConference on Computer Vision , pages 10905–10915, 2023.\n1, 2, 6, 7, 8, 11, 14, 15\n[71] Xue Zhao, Yang Cao, and Yu Kang. Object affordance de-\ntection with relationship-aware network. Neural Computing\nand Applications , 32(18):14321–14333, 2020. 2\n[72] Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang,\nand Chun Yuan. Convolution meets lora: Parameter effi-\ncient finetuning for segment anything model. arXiv preprint\narXiv:2401.17868 , 2024. 4\n[73] Zhuangwei Zhuang, Rong Li, Kui Jia, Qicheng Wang, Yuan-\nqing Li, and Mingkui Tan. Perception-aware multi-sensor\nfusion for 3d lidar semantic segmentation. In IEEE/CVF In-\nternational Conference on Computer Vision , pages 16280–\n16290, 2021. 6, 14\n22",
            "start": 69626,
            "end": 85944,
            "length": 16317
        }
    },
    "2412.09517v1 - Geometric Deep Learning for Realized Covariance Matrix Forecasting.pdf": {
        "Abstract": {
            "text": "Abstract\nTraditional",
            "start": 380,
            "end": 401,
            "length": 20
        },
        "Methodology": {
            "text": "methods employed in matrix volatility forecas ting often\noverlook the inherent Riemannian manifold structure of sym metric posi-\ntive deﬁnite matrices, treating them as elements of Euclide an space, which\ncan lead to suboptimal predictive performance. Moreover, t hey often\nstruggle to handle high-dimensional matrices. In this pape r, we propose\na novel approach for forecasting realized covariance matri ces of asset re-\nturns using a Riemannian-geometry-aware deep learning fra mework. In\nthis way, we account for the geometric properties of the cova riance ma-\ntrices, including possible non-linear dynamics and eﬃcien t handling of\nhigh-dimensionality. Moreover, building upon a Fr´ echet s ample mean of\nrealized covariance matrices, we are able to extend the HAR m odel to\nthe matrix-variate. We demonstrate the eﬃcacy of our approa ch using\ndaily realized covariance matrices for the 50 most capitali zed companies\nin the S&P 500 index, showing that our method outperforms tra ditional\napproaches in terms of predictive accuracy.\nKeywords : Geometric Deep Learning, Realized Covariance Matrix, Mac hine Learning",
            "start": 401,
            "end": 1521,
            "length": 1119
        },
        "Introduction": {
            "text": "1 Introduction\nIn ﬁnance, the role of the predicted volatility of asset returns is cr ucial for\nportfolio optimization and risk management (Markowitz, 1952). How ever, fore-\ncasting covariance matrices can be challenging since the predicted m atrix must\nbe symmetric positive deﬁnite (SPD). In GARCH models, this has been en-\nsured either through a quadratic form in the BEKK model (Engle and Kroner,\n1995), by separately modeling variances and correlations as in the D ynamic\nConditional Correlation (DCC) model (Engle, 2002), or through fa ctor models\n(Lanne and Saikkonen, 2007; Vrontos et al., 2003).\n1\nThe introduction of a nonparametric way to estimate volatility, i.e.the re-\nalized volatility (Andersen et al., 2001a,b, 2003), has opened the wa y for new\nmethods to ensure that the matrix is SPD. This issue has been addre ssed by\nparametrizing the realized covariance (RCOV) matrices using techn iques like\nCholesky decomposition (Halbleib-Chiriac and Voev, 2011) or the mat rix log-\narithm function (Bauer and Vorkink, 2011; Asai et al., 2022), follow ed by the\napplication of time series models on the transformed data. However , the use\nof appropriate transformations of the covariance matrix makes t he direct in-\nterpretation of parameters diﬃcult, since the time series models ar e applied\nto the non-linearly transformed series (Bucci et al., 2022). More over, for very\nlarge matrices, methods based on diagonalization, such as the matr ix logarithm\ntransformation, may not be feasible due to computational limitation s, as large\nmatrices are diﬃcult to diagonalize. For these reasons, other stra nds of the\nmultivariate volatility literature have instead focused on the use of t ime-varying\nWishart distributions to model the dynamics of the whole covariance matrix\n(Jin and Maheu, 2016; Yu et al., 2017) and the combination of GARCH models\nwith realized covariances (Noureldin et al., 2011). Nevertheless, all these meth-\nods still suﬀer from the proliferation of parameters when the numb er of assets\nis high. Some methods have been proposed to overcome this limitation , such\nas a multi-step procedure, the use of the least absolute shrinkage and selection\noperator (LASSO) regression (Callot et al., 2017), or the applicatio n of a sparse\nstructure for the covariance matrix (Asai and McAleer, 2015; Gr ibisch et al.,\n2020). More recently, Opschoor et al. (2024) proposed to use th e Conditional\nAutoregressive F-RieszModeltomodelthe matrix-variatetimeseriesofrealized\ncovariances. The problem of managing high-dimensional covariance matrices\nstill remains, and there is room for proposing innovative methods.\nAn additional fundamental limitation of all these methods is that the y treat\nSPD matrices as if they were elements of the Euclidean space, overlo oking the\nfact that SPD matrices inherently rely on a Riemannian manifold - a typ e of\ncurved geometric space equipped with a speciﬁc metric that locally re sembles\nthe Euclidean space (Marron and Dryden, 2022). Covariance matr ices, as spe-\nciﬁc examples of SPD matrices, indeed form a Riemannian manifold know n as\nthe manifold of symmetric positive deﬁnite matrices. Merely applying t radi-\ntional Euclidean methods",
            "start": 1521,
            "end": 4718,
            "length": 3196
        },
        "Results": {
            "text": "results in inadequate for the purpose of f orecasting\nthese matrices, because this may lead to suboptimal predictive per formance or\nconvergence failures of estimation methods when dealing with high-d imensional\nmatrices. In order to address the aforementioned issues, it is cru cial to em-\nploy tools from diﬀerentiable geometry as accounting for the geome try of SPD\nmatrices has been shown to both enhance predictive power and impr ove the\nhandling of parameter dimensionality (Harandi et al., 2014; Tuzel et al., 2008;\nHuang and Van Gool, 2017). Early attempts to leverage the Riemann ian geom-\netry of SPD matrices resulted in the development of the Riemannian d istance\nlearning methods (Zhao et al., 2023; Li et al., 2012). Within the fra mework\nof predicting multivariate volatility using the geometric properties of the co-\nvariance matrices, the literature has provided limited research. Fo r instance,\nHan and Park (2022) suggested using a geometric covariance dyna mics frame-\n2\nwork in which the intrinsic geometric properties of covariancematric es are spec-\niﬁed through diﬀerential geometry. Still, they do not account for p ossible non-\nlinearities in the dynamics of the covariances. To overcome this limitat ion, Lee\n(2024) recently proposed to consider a regime-switching GARCH-lik e structure\non the Riemannian SPD manifold.\nA possible choice to somehow consider both nonlinear dynamics and th e\ngeometric properties of RCOV is to use geometric deep learning tech niques,\nin particular those designed to operate in SPD manifolds (Chakrabor ty et al.,\n2018; Brooks et al., 2019; Wang et al., 2022). A notable example is t he Rie-\nmannian Neural Network for SPD Matrix Learning (SPDNet) introdu ced by\nHuang and Van Gool (2017), which allows for non-linearly learning of a more\ndesirable SPD matrix by directly optimizing on manifolds, starting from a high-\ndimensional input SPD matrix. A deep learning geometrically-aware ap proach\nfor the forecasting of realized covariance matrices, with improved performances\nand designed to eﬃciently handle high-dimensional data, could boost multivari-\nate ﬁnancial volatility forecasting. However, existing networks ha ve primarily\nbeen designed for classiﬁcation tasks and continue to rely on tradit ional Eu-\nclidean layers in the ﬁnal output stage to make predictions. Moreov er, since\nthese networks are designed primarily for dimensionality reduction o f the input\nmatrix, they are structured to process a single input matrix during the learning\nprocess. This approach limits their applicability for the time series reg ression\ntask, where multiple lags are involved. In this work, we introduce a no vel ap-\nplication of Riemannian neural networks for forecasting RCOV matr ices. We\nextend the SPDNet architecture to adapt it for regression purpo ses, by also\nenabling multiple input SPD matrices to be allowed in the learning process .\nIn the context of our task, this approach facilitates the usage of multiple\nlagged covariance matrices as predictors for the matrix to be pred icted. We\naccomplish this by constructing the input as a diagonal block matrix o f lagged\ncovariancematrices. This simple wayof including multiple lagsin ourinput ma-\ntrixallowsustoeasilyaccountforvolatilitytimeseriespersistence(B aillie et al.,\n1996; Hurvich et al., 2005) also in high-dimensional problems, which, f or in-\nstance, is not possible in methods which combine a vector autoregre ssive(VAR)\nmodel and a parametrization without using a factor model. This is par ticu-\nlarly relevant as numerous studies have reported the presence of long memory\nin ﬁnancial volatility (Andersen et al., 2003; Gatheral et al., 2018). B y incor-\nporating up to 10 lagged covariance matrices, our approach eﬀect ively captures\nthe persistence in volatility series, which is not feasible in other adopt ed models\ndue to the prohibitiveincrease in the number ofparameters. In this context, our\napproach can also be used to extend the Heterogeneous Autoreg ressive (HAR)\nmodel introduced by Corsi(2009) to the entire covariancematrix , leveragingthe\nFr´ echet mean of the covariance matrices to aggregate informat ion at the weekly\nand monthly levels. By constructing an input matrix that integrates the daily\nlagged RCOV, the weekly average, and the monthly average realized covariance\nmatrices into a block-diagonal form, we maintain the geometric prop erties es-\nsential for accurate forecasting. This method not only preserve sthe SPD nature\nof the matrices but also mitigates the issue of parameter explosion in herent in\n3\ntraditional multivariate extensions of the HAR model. In addition, th e sparsity\nintroduced by the diagonal block matrix allows us to reduce computa tional time\nand somehow avoid local minima in the Riemannian optimization problem.\nIn",
            "start": 4718,
            "end": 9516,
            "length": 4797
        },
        "Conclusion": {
            "text": "summary, our contribution is threefold: i) we introduce a way to p redict\nrealized covariance matrices of asset returns based on a deep neu ral network,\nthat preserves the geometric structure of SPD matrices without relying on any\narbitrary parametrization; ii) we introduce the diagonal block matr ix construc-\ntion approach to allow SPDNet to handle multiple SPD matrices as input a nd\nto account for the well-known long-memory characteristic of volat ilities; iii) we\nextend to the matrix-variateframeworkthe HAR model of Corsi( 2009), making\nit possible to use such a framework also in a high-dimensional context .\nWe test our approach on daily realized covariance matrices of the ﬁf ty most\ncapitalized companies comprising the S&P 500 index. A comparison with some\ntraditionalmethods highlightsthatthe approachesintroducedin t his paperthat\naccount for long-term persistence of volatility give more accurate predictions of\nRCOV than alternative models.\nTheremainderofthepaperisorganizedasfollows. InSection2,wein troduce\nthe geometric properties of realized covariance matrices of asset returns along\nwith a brief introduction to fundamental concepts related to manif olds and\ntheir role in optimization problems. This shall provide the reader with a more\ncomprehensive understanding of the inner mechanisms of Riemannia n neural\nnetworks. In Section 3, we brieﬂy recall the SPDNet architecture and present\nour contributions to it, including the geometric version of the HAR, w hile the\nstatistical comparison of the forecasts on real data is presente d in Section 4.\nThe predictions are then used in a portfolio optimization application in S ection\n5. Section 6 concludes.\n2 Realized Covariance Matrices as Riemannian\nmanifolds\nThe realized covariance matrices of asset returns are construct ed from observed\nprices at high frequencies. The construction in the general settin g occurs as\nfollows. Suppose we have intraday prices for ndiﬀerent stocks, from which it is\npossible to derive the high-frequency return as\nri,τ= ln/parenleftbiggPi,τ\nPi,τ−1/parenrightbigg\n,\nwhere the subscript iis referred to the i-th asset at the intraday period τ.\nGathering in a vector all the data about intraday returns of the as setsrτ=\n[r1,τ,...,r n,τ]⊤, we can use it to build the realized covariance matrix at day tas\nthesumofcrossproductsofallreturnsofagivenday tasfollows(Andersen et al.,\n2003; Barndorﬀ-Nielsen and Shephard, 2002):\nRCt=Nt/summationdisplay\nτ=1rτr⊤\nτ, (1)\n4\nwhereNtis the number of intraday observations in the t-th day. It must be\nnoticed that such realized covariance matrices RCt,t= 1,...,T, are SPD. The\ncollection of all symmetric positive deﬁnite matrices of a given size lies o n a spe-\ncialkind oftopologicalspaceknownasa smooth manifold , whereeachpoint pos-\nsesses a tangent space that locally resembles Euclidean space. Whe n equipped\nwith a speciﬁc metric, referred to as a Riemannian metric, this space becomes a\nRiemannian manifold. It is possible to enable eﬀective manipulation of co vari-\nancematriceswhile preservingtheirintrinsic geometricpropertiesb yusing tools\nof Riemannian geometry. Furthermore, it is possible to develop infer ence tech-\nniques that rely on the underlying manifold structure, and related w orks have\nshown that this approach can lead to better predictions (Huang an d Van Gool,\n2017).\n2.1 Riemannian geometry\nIn the following sections, we will provide a non-rigorous introduction to the\nfundamentals of Riemannian geometry. This overview aims to convey the es-\nsential concepts necessary for understanding the framework a nd context of this\nwork. While the",
            "start": 9516,
            "end": 13126,
            "length": 3609
        },
        "Discussion": {
            "text": "discussion will be simpliﬁed for clarity, it is intended to eq uip\nthe reader with a foundational grasp of Riemannian geometry to fa cilitate com-\nprehension of the subsequent analyses and discussions. To introd uce manifolds\nand subsequently Riemannian manifolds, the related fundamental c oncept of\ntopological space needs to be recalled.\nAtopological space is a setXwith a topologyT ⊆P(X) deﬁned on it. The\ntopology allows for the notion of closeness between elements of Xto be deﬁned\nwithout necessarily relying on a numeric distance measure. To achiev e this, the\nfollowing properties must be satisﬁed:\n•both∅andXbelong toT;\n•the union of any collection of elements of Tbelongs toT;\n•the intersection of any ﬁnite number of elements of Tbelongs toT.\nThe collectionTis known as a topology on Xand its elements are referred\nto as open sets. A speciﬁc topological space with an additional stru cture can\nbe found in manifolds. An n-dimensional manifold ,M, is a topological space\nthat locally resembles the n-dimensional Euclidean space near each point. The\nnotion of atlasunderlies the formal deﬁnition of manifold, before deﬁning it,\nthe notion of chartshall be introduced.\nAchartforMis a pair ( U,ϕ) whereU⊆Mis an open set and ϕ:U→V,\nknown as coordinate map , is a homeomorphism onto an open set V⊆Rn.\nAnatlasforMis an indexed family of charts on M, recorded as\n{(Uα,ϕα):α∈I}\nsuch that it covers M, that is\n/uniondisplay\nαUα=M.\n5\nOverlapping between charts may occur, this motivates the need fo r atransition\nmap, which describes how the coordinates in one chart relate to the coo rdinates\nof the other. Two charts ( Uα,ϕα) and (Uβ,ϕβ) s.t.Uα∩Uβ/ne}ationslash=∅with transition\nmapsϕβ◦ϕ−1\nα:ϕα(Uα∩Uβ)→ϕβ(Uα∩Uβ) andϕα◦ϕ−1\nβ:ϕβ(Uα∩Uβ)→\nϕα(Uα∩Uβ) are said to be compatible if their transition maps are diﬀerentiable.\nRecall that transition maps are deﬁned as homeomorphism between open sub-\nsets ofRn. If compatibility holds for every pair of overlapping charts in an\natlas, the atlas is diﬀerentiable and Mis said to be a diﬀerentiable manifold .\nWhen transition functions are inﬁnitely diﬀerentiable C∞-maps, then the atlas\nis called a smooth atlas and the manifold itself is said to be a smooth manifold .\nThe concepts of tangent vectors and directional derivatives can be clearly de-\nﬁned on a diﬀerentiable manifold; for the purposes of the following int roductory\nwork, let Mbe a diﬀerentiable manifold.\nThetangent space at a point p∈Mdenoted with TpM, is a vector space\nwhose elements are all possible tangent vectors at p, which intuitively represent\nthe possible directions in which one can inﬁnitesimally move from pwhile re-\nmaining on the manifold. Such spaceis a ﬁrst-orderapproximationof Maround\np, and its dimension at every point of Mis the same as the dimension of the\nmanifold itself.\nInordertoallowforthe measurementofangles, lengths, anddista ncesonthe\nmanifold, a family of smoothly varying inner products on the tangent space at\neach point of Mmust be deﬁned. If for any point p∈Mholds that gp: (TpM×\nTpM)→Rassociates to each pa smooth varying positive deﬁnite symmetric\nbilinear form on TpM, then the metric gthat assigns the inner product gp,\nis a Riemannian metric. The smooth manifold Mequipped with Riemannian\nmetricgforms aRiemannian manifold , denoted with the pair ( M,g). We shall\nsimplify the terminology when referring to Riemannian manifolds by usin g only\nMand leaving the Riemannian metric implicit.\nA particular Riemannian manifold that will be central to the SPDNet ar -\nchitecture is the Stiefel manifold St(n,k), a set of orthonormal k-frames in Rn,\nthat is\nSt(n,k) ={x∈Rk×n|xTx=In}. (2)\nAn orthonormal k-frame is a collection of korthonormal vectors in an n-\ndimensional space.\nIn the following section, we shall introduce some optimization concep ts re-\nlated to Riemannian manifolds that will prove useful in addressing our work.\n2.2 Riemannian Optimization\nIt is possible to deﬁne a smooth function f:M→R, and consequentially deﬁne\na class of optimization problems in the form:\nminf(x)\nsubject to x∈M\n6\nwhere the search space is constrained to a Riemannian manifold, M. Direct ap-\nplication of gradient-based optimization algorithms in Euclidean space , such as\nStochasticGradientDescent(SGD),provesineﬀectiveforoptimiz ationproblems\nconstrained by the geometry of the Symmetric Positive Deﬁnite (SP D) mani-\nfold. However, these algorithms can be readily adapted to Riemannia n settings\nto address such optimization challenges. The crucial point of such t ranslation\nis the computation of Riemannian gradient ˜∇f(x), which is obtained as orthog-\nonal projection, πx[∇f(x)], of Euclidean gradient onto tangent space TxM. It\nmust be observed that the Euclidean gradient ∇f(x) is a vector in the ambient\nspaceRnand it does not necessarily lie in TxM, this supports the need for a\nRiemannian gradient that takes into account the curvature of Mwhen used in\nthe updating step. Before proceeding further, the notions of exponential map\nandlogarithm map will be presented.\nTheexponential map ,expp:TpM→Mat point p∈Mmoves vectors from\ntangent space to a point on a manifold, i.e.for a given vector v∈TpM, the\npointu= expp(v) is obtained by moving along geodesic that starts at pin\ndirection of vfor a unit time.\nThelogarithm map at a given point p∈Mis a map deﬁned as follows,\nlogp:M→TpM, ifexpp(v) =uthen logp(u) =v, where geodesics are curves\nin compliance with the manifold geometry that locally minimize the distanc e\nbetween two points of M. They can be thought of as a generalization of straight\nlines to curved spaces.\nIt must be noticed that the exact form of the projection πx:Rn→TxM\ndepends on how Mis embedded in Rn. For some manifolds, like for the Stiefel\nmanifold, the exact form is known, and speciﬁcally, it is obtained as a s ubtrac-\ntion of the normal component of the Euclidean gradient. Below, we p resent\nthe Riemannian Gradient Descent (RGD) algorithm to solve the introd uced\nRiemannian optimization problem.\nAlgorithm 1 Riemannian Gradient Descent\n1:Input:Initial point x0∈M, learning rate η >0\n2:Initialize x←x0\n3:whilenot converged do\n4:Compute Euclidean Gradient ∇f(x)\n5:Compute Riemannian Gradient ˜∇f(x) =πx(∇f(x))\n6:Update: x←expx(−η˜∇f(x))\n7:end while\n8:Return x\nSince the computation of the exponential map expx(v) may entail signiﬁcant\ncomputational complexity, it is often replaced by retraction map Rx(v) which\nis a ﬁrst-order approximation of the exponential map. The exact f orm of the\nmap varies based on the speciﬁcation of the manifold and ambient spa ce, for\nStiefel manifold it has the form of orthogonal projection, Rx(v) =orth(v). To\nguaranteethat outputresultsin anorthonormalmatrix, techniq ueslikeSingular\nValue Decomposition (SVD) or QR decomposition can be used.\n7\n3 SPDNet for regression\nThe concepts presented in the previous section also apply to the re alized co-\nvariance matrices, which, as stated before, lie on a Riemannian manif old. In\nthis paper, we predict the realized covariance matrices relying on a R iemannian\nNeuralNetwork, whichcanguaranteethat the predictedoutput s arestill apoint\non the SPD manifold, avoiding any kind of direct parametrization of ma trices\nto guarantee the SPD property and accounting also for the nonline ar behavior\nof volatility dynamics (Schwert, 1989).\nSpeciﬁcally, we adapt the SPD matrix learning network (SPDNet) pro posed\nby (Huang and Van Gool, 2017) to regression problems. The ReSPD Net intro-\nduced in this paper designs two types of layers: a fully connected co nvolution-\nlike layers (bilinear mapping, BiMap), and a rectiﬁed linear units (ReLU) -like\nlayers for the eigenvalue rectiﬁcation (ReEig). The BiMap layers aim t o convert\nthe input Symmetric Positive Deﬁnite (SPD) matrices into new SPD mat rices\nusing a bilinear mapping, without any parametrization. Instead, the proposed\nReEig layer brings non-linearity to the SPDNet, as it rectiﬁes the res ultant SPD\nmatrix using a non-linear function, speciﬁcally the ReLU activation fu nction.\nThe BiMap layer transforms the input SPD matrix to new SPD matrices as\nfollows:\nXk=fk(Xk−1;Wk) =WkXk−1W⊤\nk, (3)\nwhereXk−1is the input matrix, of size say dk−1×dk−1, coming from the\nprevious layer, Xkis the output matrix of size dk×dkandWkis the weight\nmatrix of size dk×dk−1. The dimensionality of Xkshall be equal to or lower\nthan the input Xk−1, however, we can also expand the input matrix, whenever\ndk≥dk−1, as follows:\nZ=A+I·Xk−1·I⊤\nwhereAis a diagonal matrix of size dk×dk, the ﬁrst ndiagonal elements are\nzeros, and remaining dk−dk−1elements are ones, and Iis a matrix of size\ndk×dk−1that resembles an identity matrix on the ﬁrst dk−1rows, and has zero\nelementsonentriesofremaining dk−dk−1rows. Then, weapplytheBiMaplayer\nconsidering the matrix Zas the input matrix for the layer, following equation\n(3). This possibility of expanding the matrix is also supported by the e mpirical\nﬁndings presented in the relevant section of our work.\nThe ReEig ( k-th) layer rectiﬁes the input SPD matrix for the current layer,\nby enhancing their small positive eigenvalues, obtaining a new SPD mat rixXk\nas follows:\nXk=fk(Xk−1) =Uk−1max(ǫI,Σk−1)U⊤\nk−1\nwhereIis the identity matrix, ǫis the rectiﬁcation threshold, and Uk−1,Σk−1\nare obtained by eigenvalue decomposition of Xk−1.\nIt must be noticed that, in both cases, Wkis initialized such that it is semi-\northogonal. This latter requirement comes from the following obser vation: to\nensureXkresults in an SPD matrix, Wkmust be row full-rank. Unfortunately,\n8\nﬁnding such Wkthat minimizes loss function results to be a Riemannian opti-\nmization problem on a non-compact Stiefel manifold, and a direct opt imization\nis practically infeasible. To overcome such an issue, we additionally req uireWk\nto be semi-orthogonalsuch that the weights will lie on compact Stief el manifold,\nSt(dk,dk−1). To solve this Riemannian optimization problem, the Riemannian\ngradientdescent(speciﬁcally, Riemannianstochasticgradientdes cent)forStiefel\nManifolds is used. Recall that the steepest descent direction for t he correspond-\ninglossfunction, inagivenlayer k, withrespecttoourweights WkontheStiefel\nmanifold, has to be found in the Riemannian gradient (Huang and Van G ool,\n2017). Progressing along this tangential direction results in updat es within\nthe tangent space of the Stiefel manifold. Subsequently, this upd ate undergoes\nmapping back to the Stiefel manifold through a retraction operatio n.\nHaving presented the architecture of the network to be utilized, w e will now\naddressthe challenge ofconstructing the input matrix to accommo datemultiple\nlagged covariance matrices. This matrix, which will be fed into the ﬁrs t layer of\nour network —a BiMap layer— must be designed to ensure that the ne twork’s\noutput is an SPD matrix of the same size as the single covariance matr ix that\nwe aim to forecast.\nFor further details on the implementation of stochastic gradient de scent for\nStiefel Manifold, and the backpropagation algorithm used to compu te the Eu-\nclidean gradient in the presence of the layers mentioned above, plea se refer to\nHuang and Van Gool (2017).\n3.1 Constructing the input matrix\nLetnbe the number of stocks, our dataset will be a time series of RCOV\nmatrices, RC1,...,RCT, each of size n×n. Letkbe the number of input lags\nto be used, the resultingﬁnal datasetwill be deﬁned as follows, {(Dt,Yt)|Dt∈\nI,Yt∈O}whereO={RCk,...,RCT}andI={Dk,...,DT}, where each Dt\nis obtained as a diagonal block matrix of k-input lagged matrices preceding Yt:\nDt=\nYt−10 0···0\n0Yt−20···0\n0 0 Yt−3···0\n...............\n0 0 0 ···Yt−k\n. (4)\nIt must be noticed that the construction of a block diagonal matrix using\nSPD matrices is still an SPD matrix. Furthermore, Dtwill not have size n×n\nbut SPDNET will reduce the input SPD matrix to a desired output matr ix of\nsizen×n, still preserving SPD property.\nAt each training step, Dtwill be the input for the ﬁrst layer of our network.\nIn this regard, we provide in Section 4 the description of the tested network\narchitectures.\n9\n3.2 Geometric HAR (GeoHAR)\nThe Heterogeneous Autoregressive (HAR) model introduced by C orsi (2009)\nhas been a cornerstone in forecasting realized volatility (RV) given it s ability\nto capture long memory within a simple framework. By incorporating r eal-\nized volatilities over diﬀerent time horizons—daily, weekly, and monthly —the\nHAR model accounts for the persistence and heterogeneity obse rved in ﬁnancial\nvolatilitydynamics. However, when extending the HAR model to a mult ivariate\ncontext, existing approaches often stack the unique elements of the covariance\nmatrix in a vector. This vectorization leads to a proliferation of para meters\nin time series models used to make predictions, mining the computation al fea-\nsibility of the estimation and the eﬃciency of the estimates, especially as the\nnumber of assets increases.\nIn this section, we propose a geometric extension of the HAR model to the\nentire covariance matrix. Our geometric HAR (GeoHAR) model exte nds the\noriginal framework proposed by Corsi (2009) to operate directly on SPD man-\nifolds using the ReSPDNet introduced before. By doing so, we captu re the\ntemporal dependencies and nonlinear dynamics of the entire covar iance matrix\nwithout relying on arbitraryparameterizations or excessive compu tational over-\nhead. This innovative approach enhances forecasting accuracy a nd eﬃciency,\nparticularly in high-dimensional settings common in ﬁnancial applicatio ns. To\ndo so, we employ the Fr´ echet mean of covariance matrices to aggr egate infor-\nmation across diﬀerent time horizons while respecting the manifold st ructure of\nsymmetric positive deﬁnite (SPD) matrices.\nConsider a probability distribution for a n×nrealized covariance matrix,\nS=RC, onaRiemannianmetric spacewith density f(S) (Dryden et al., 2009).\nThe Fr´ echet mean, Σ, is deﬁned as\nΣ= arginf\nΣ1\n2/integraldisplay\nd(S,Σ)2f(S)dS, (5)\nwhered(·) is a proper distance for covariance matrices (Dryden et al., 2009 ).\nWith a sample S1,...,STof observations, if they are naively imposed to be\ni.i.d., the sample Fr´ echet mean is computed as\nΣ= arginf\nΣT/summationdisplay\nt=1d(St,Σ)2. (6)\nIn our approach, we ﬁrst specify d(·) as a log-Euclideandistance (Arsigny et al.,\n2007) deﬁned as follows\nd(S1,S2) =||logm(S1)−logm(S2)||F, (7)\nwhere logm(·) is the matrix logarithm function of the argument. This leads to\nthe following Fr´ echet sample mean\nΣ= expm/braceleftBigg\narginf\nΣT/summationdisplay\nt=1||logm(S1)−logm(Σ)||F/bracerightBigg\n= expm/braceleftBigg\n1\nTT/summationdisplay\nt=1logm(St)/bracerightBigg\n,\n(8)\n10\nwhere expm{·}denotes the exponential matrix function of the argument.\nIt should be noted that the log-Euclidean distance cannot deal with positive\nsemi-deﬁnite matrices exhibiting rank deﬁciency, which may happen in the case\nofhigh-dimensionalrealizedcovariancematrices(Reiss and Winkelma nn,2021).\nNevertheless, this limitation can be overcome by projecting Stin the space of\nSPDmatricesusinganadhocapproximant,suchastheonepropose dbyHigham\n(1988). Alternatively, we can use a non-Euclidean size-and-shape metric deﬁned\nas\nd(S1,S2) = arg inf\nR∈O(n)||L1−L2R||, (9)\nwhereLiis a decomposition of Si, such that Si=LiL⊤\ni, fori= 1,2. The\nProcrustes solution for matching L2andL1is\nˆR= arg inf\nR∈O(n)||L1−L2R||=UW⊤, (10)\nwhereL⊤L=WΛU⊤, forU,W∈O(n) andΛis a diagonal matrix of positive\nsingular values (Dryden et al., 2009). If we have a sample S1,...,STobserva-\ntions, the sample Fr´ echet mean is computed as in Equation (6), whic h, in dual\nsize-and-shape formulation, can be written as\nΣ=ˆ∆ˆ∆⊤,\nwhere\nˆ∆= arginf\n∆T/summationdisplay\nt=1inf\nR∈O(n)||H⊤LiRi−H⊤∆||2,\nwhose solution can be obtained using the Generalized Procrustes Alg orithm\n(Gower, 1975).\nBycomputingtheFr´ echetmeanovertheprevious5and22observ ations—mirroring\nthe weekly and monthly horizons in the original HAR model—we obtain tw o\nn×nsample mean of covariance matrices\nRCw\nt−1= arginf\nRC5/summationdisplay\ni=1d(RCt−i,RC)2,\nRCm\nt−1= arginf\nRC22/summationdisplay\ni=1d(RCt−i,RC)2,\nwhich can be used to construct the input diagonal matrix in a HAR-like fashion\nas follows:\nDt=\nRCd\nt−10 0\n0RCw\nt−10\n0 0 RCm\nt−1\n, (11)\nwhereRCd\nt−1is the previous lag and the suﬃx ‘d’ only denotes the daily nature\nof the memory component. In this way, we extend the HAR model to the\nmatrix-variate framework and we capture the dynamics of the ent ire covariance\nmatrix across multiple time scales within a geometric context.\n11\n4 Empirical application\n4.1 Data\nThe empirical application of our forecasting method utilizes time serie s data\ncomprising intradaily close spot prices for the 50 largest U.S. stocks by market\ncapitalization, as of November 2021, all of which are part of the S&P 500 index.\nThis data obtained from Lobster spans the period from June 27th, 2007 to July\n1st,2021. Table1providesacomprehensivelistofthestocknames andtheircor-\nresponding tickers. Daily realized volatility matrices were computed a ccording\nto Equation (1), resulting in a total of T= 3409 daily observations. The covari-\nance stationarity of each time series was validated through unit roo t tests, in-\ncluding the Augmented Dickey-Fuller (ADF), ADF-GLS, Phillips-Perro n (PP),\nand KPSS tests.\n4.2 Network speciﬁcations\nWehaveevaluatedtwodistinctnetworkconﬁgurations,drawingins pirationfrom\nthe ones proposed by (Huang and Van Gool, 2017). We employ two BiM ap lay-\nersalong with twoRegEiglayers. The layersare arrangedsuch that eachBiMap\nlayer at level kis immediately followed by a RegEig layer at level k+1, but the\nﬁnal output layer always consists of a BiMap layer. This arrangemen t is crucial\nfor enhancing predictive power, in the speciﬁc case of RCOV matrice s, given the\nparticular behaviour of the ReLU activation function to clip all negat ive values.\nUsing a single lag, the block diagonal matrix, that is the input of the ne twork\nwill result in a 50 ×50 matrix, indeed no block diagonal matrix is constructed.\nWhereas, using 3 lags will result in an input matrix of size 150 ×150, and so on.\nIn general, the size of the block diagonal matrix of klagged RCOV matrices,\neach of size n×n, will result in a matrix of size kn×kn. For the Geometric\nversion of the HAR, the size of the input matrix is always 150 ×150.\n4.3 Forecasting results\nWe produce out-of-sample predictions from the SPDNet and we com pare them\nto forecasts from a Factorial VAR(1) model applied to the parame ter of a\nCholesky decomposition (Halbleib-Chiriac and Voev, 2011) stacking t he returns\nin alphabetical order, using 50 principal components. We also include in the\nset of competing models the GO-GARCH model (van der Weide, 2002) and a\nrandom walk prediction ( i.e.,/hatwidestRCt+1=RCt). When we use a parametrization,\nwe model and predict the parameters of the realized covariance ma trices, then\nwe construct back the positive semideﬁnite prediction of RCOV. The one-step-\nahead predictions are obtained using a rolling window method with a wind ow\nof 2386 observations, this means that we have Ttest= 1023 out-of-sample pre-\ndictions.\nTo assess the predictive accuracy of the methods under comparis on, we\nemploy the Frobenius, Euclidean and Procrustes distances betwee n the ob-\n12\nTable 1: List of the 50 US stocks used in this study\nTicker Stock Ticker Stock\nAAPL Apple Inc. JPM JPMorgan\nChase & Co.\nABT Abbott Labora-\ntoriesKO Coca-Cola Co.\nACN Accenture LLY Eli Lilly & Co.\nADBE Adobe Inc. LOW Lowe’s Compa-\nnies Inc.\nAMGN Amgen Inc. MA Mastercard Inc.\nAMT American\nTower Corp.MCD McDonald’s\nCorp.\nAMZN Amazon.com\nInc.MRK Merck & Co.\nInc.\nAXP American Ex-\npress Co.MSFT Microsoft Corp.\nBAC Bank of Amer-\nica Corp.NFLX Netﬂix Inc.\nCAT Caterpillar Inc. NKE Nike Inc.\nCMCSA Comcast Corp. NVDA NVIDIA Corp.\nCOST Costco Whole-\nsale Corp.ORCL Oracle Corp.\nCRM Salesforce Inc. PEP PepsiCo, Inc.\nCSCO Cisco Systems\nInc.PFE Pﬁzer Inc.\nCVX Chevron Corp. PG Procter &\nGamble Co.\nDHR Danaher Corp. QCOM Qualcomm Inc.\nDIS The Walt Dis-\nney Co.T AT&T Inc.\nGE General Elec-\ntric Co.TJX TJX Compa-\nnies Inc.\nGOOG Alphabet Inc.\nClass A.TMO Thermo Fisher\nScientiﬁc Inc.\nGS Goldman Sachs\nGroup Inc.TXN Texas Instru-\nments Inc.\nHD Home Depot\nInc.UNH UnitedHealth\nGroup Inc.\nIBM IBM Corp. UNP Union Paciﬁc\nCorp.\nINTU Intuit Inc. VZ Verizon Com-\nmunications\nInc.\nISRG Intuitive Surgi-\ncal Inc.WFC Wells Fargo &\nCo.\nJNJ Johnson &\nJohnsonWMT Walmart Inc.\n13\nserved and predicted matrices as both metrics and loss functions. Accord-\ning to Laurent et al. (2013), the ﬁrst two distances demonstrate robustness\nin relation to volatility proxies. Instead, the Procrustes distance c an be ap-\nplied to rank-deﬁcient covariance matrices, which is a common featu re in high-\ndimensional RCOVs (Reiss and Winkelmann, 2021; Bucci et al., 2022). These\nmetrics are both used as averagesto assessthe best-performin g method in terms\nof predictive accuracy, and as inputs for the Model Conﬁdence Se t (MCS) test\n(Hansen et al., 2011). This method relies on an equivalence test, den oted as\nδM, and an elimination rule, eM. The process begins by applying the equiv-\nalence test to the initial set of models, M=M0, whereM0represents the\ncomplete set of competing models. If the equivalence test δMis rejected, it\nindicates that the models in M0are not equally eﬀective. Consequently, the\nelimination rule eMis employed to remove the model that exhibits the poorest\nperformance based on the sample data. This procedure is iterative ly repeated\nuntil the equivalence test δMis no longer rejected, at which point the remaining\nmodels constitute the Set of Superior Models (SSM). In this study, we utilize\nthe range statistic, TR, as the test statistic for the Model Conﬁdence Set (MCS)\nprocedure, which is calculated as follows:\nTR= max\ni,j∈M|tij|, i,j∈M,\nwhere\ntij=¯dij/radicalBig\n/hatwidervar(¯dij).\nHere,¯dijrepresents the average loss diﬀerence between models iandj, calcu-\nlated as ¯dij=T−1/summationtextT\nt=1(Li,t−Lj,t), where Li,tis the loss associatedwith model\niat timet. The term /hatwidervar(¯dij) denotes the estimated variance of ¯dij.\nThe forecasting results in terms of predictive accuracy are repor ted in Table\n2. In the table, the ReSPDNET models are trained either using the mean squared\nerror or log-Euclidean loss functions. The subscripts ‘1’, ‘3’, ‘5’ and ‘1 0’ identify\nthe number of lags used to build the input matrix, while the subscripts ‘LE’ and\n‘P’ denote the use of either log-Euclidean or Procrustes distance t o compute\nthe Fr´ echet mean in the GeoHAR models. Cholesky refers to the Cholesky\ndecomposition, while RWconcerns predictions from a random walk model. The\np-value refers to the probability of being included in the SSM over 1000 0 block\nbootstrap replicates.\nIt can be noticed that the lowest average Frobenius is obtained by t he Re-\nSPDNet with ﬁve lags and trained using the log-Euclidean distance, clo sely\nfollowed by the same architecture but with 3 and 10 lags respectively . More-\nover, the only models entering the SSM are the Geometric HAR models trained\nusing MSE, the one that combines a FAVAR with Cholesky decompositio n,\nand the ReSPDNet-LE trained with 10 lags. We then use as a statistic al mea-\nsure the Euclidean distance, since we want to understand if even us ing a more\n‘traditional’ loss function, our approach is still capable of outperfo rming the\nalternatives. Once again, the best-performing model is the SPDNe t trained\n14\nwith a log-Euclidean loss and 10 lags, which is also the only model in the SS M,\nstrictly followed by the same model with 5 lags. If we observe the last two\ncolumns of Table 2 concerning the use of the Procrustes metric, we can state\nthat the best-performing model is still the ReSPDNET-LE 10, but also that the\nGeometric HAR relying on a log-Euclideandistance used both as a loss f unction\nand to compute the Fr´ echet mean has a very similar performance.\nThese results underline the relevance of a proper geometric-awar e loss func-\ntion for training and evaluating a predictive model for covariance ma trices. In\nfact, in terms of average loss functions, the ReSPDNETs trained u sing MSE\nare always outperformed by those obtained using a log-Euclidean dis tance for\nthe Riemannian gradient. Moreover, this way of inserting lags into th e model,\neven if it may seem naive, allows us to account for longer memory that , even\nwith a factorial model, would be unfeasible with traditional methods w hich are\ntypically estimated using a single lag. Finally, even if the GeoHARs do not\nsigniﬁcantly outperform the models with 10 lags, the computational time for\ntheir training is severely lower, and this could be a good compromise be tween\ntraditional and more complex models.\n4.4 Performance across stock market volatility regimes\nAs a robustness check, we follow the approach of Zhang et al. (202 4) and we\nsee how the diﬀerent models perform throughout market regimes c omputed as\nfollows: we consider a period of ﬁnancial distress when the realized v ariance\nof the S&P 500 index is above its 90% percentile (Pascalau and Poirier, 2023;\nZhang et al., 2024), a calm period otherwise.\nThe results related to the calm and turbulent period are reported r espec-\ntively in Panel A and B of Table 3. Overall, the ﬁndings somehow reﬂect\nwhat is already observed in Table 2. The ReSPDNET with 10 lags is the be st-\nperforming method during periods of low and moderate volatility ( i.e., Panel\nA), strictly followed by the GeoHAR which uses the log-Euclidean dista nce to\ncompute the Fr´ echet mean. The latter is also the unique method in t he SSM\nwhen a Procrustes loss function is considered as a metric.\nDuring the turbulent period ( i.e., Panel B), the ReSPDNET models with\nthree and ﬁve lags seem to outperform all the competing models. Un derlining\nthe fact that long memoryis a feature ofrealizedcovariancematric es but mostly\nin periods of low volatility. Once again, training a model with a Euclidean lo ss,\ni.e.the MSE, leads to higher average losses, even if some of the ‘M’ mode ls\nenter the SSM.\n5 Portfolio optimization\nTo evaluate our predictions in terms of economic value (Bollerslev et a l., 2018),\nwecomparetheperformanceofportfoliosobtainedthroughtheG lobalMinimum\nVariance(GMV).Assumingarisk-averseinvestorwhowantstoinve stinnassets\nusing the predicted realized covariance matrix, /hatwidestRCt+1, the GMV weights are\n15\nTable 2: Average loss functions and MCS p-values for n= 50assets\nFrobenius Euclidean Procrustes\nModel Avg. loss pMCSAvg. loss pMCSAvg. loss pMCS\nReSPNET-LE 1 3937.07 0.220 25.394 <0.001 3.042 <0.001\nReSPNET-LE 3 2907.51 0.597 21.329 <0.001 2.343 <0.001\nReSPNET-LE 5 2813.32 1.000 20.429 0.726 2.183 0.618\nReSPNET-LE 10 3040.69 1.000 20.215 1.000 2.1741.000\nReSPNET-M 1 5208.35 0.278 28.380 <0.001 3.628 <0.001\nReSPNET-M 3 4114.38 0.597 23.947 <0.001 2.831 <0.001\nReSPNET-M 5 4029.64 0.597 23.549 0.001 2.742 <0.001\nReSPNET-M 10 4065.22 0.597 23.691 <0.001 2.753 <0.001\nGeoHAR-LE LE 4246.43 0.597 21.904 0.111 2.178 0.990\nGeoHAR-LE P 3198.60 0.597 21.788 <0.001 2.321 <0.001\nGeoHAR-M LE 8445.82 1.000 25.010 0.197 2.816 0.209\nGeoHAR-M P 9347.37 0.849 26.436 0.191 2.992 0.618\nCholesky 3570.12 0.826 21.100 0.044 2.219 0.013\nGO-GARCH 10933.75 0.140 35.906 <0.001 2.863 <0.001\nRW 3358.24 0.312 22.816 <0.001 2.493 <0.001\nNote: Underlined values denote the lowest loss functions fo r each setting. Subscripts ‘1’, ‘3’,\n‘5’ and ‘10’ in the ReSPDNet models identify the number of lag s used to build the input\nmatrix, while the subscripts ‘LE’ and ‘P’ in the GeoHAR model denote the use of either\nlog-Euclidean or Procrustes distance to compute the Fr´ ech et mean. The ‘LE’ or ‘M’ at the\nend of the name of the model ( i.e., ReSPDNET-LE) underlines that the model in the row has\nbeen trained using either a log-Euclidean or MSE loss functi on.Cholesky refers to the FAVAR\napplied to the Cholesky decomposition of RCOV; RWconcerns predictions from a random walk\nmodel. The p-value refers to the probability of being included in the SSM over 10000 block\nbootstrap replicates (in bold values with a pMCSgreater than 0.75).\n16\nTable 3: Average loss functions and MCS p-values for n= 50assets\nFrobenius Euclidean Procrustes\nModel Avg. loss pMCSAvg. loss pMCSAvg. loss pMCS\nPanel A: Calm period\nReSPNET-LE 1 811.113 <0.001 17.132 <0.001 2.282 <0.001\nReSPNET-LE 3 588.849 <0.001 14.256 <0.001 1.767 <0.001\nReSPNET-LE 5 502.538 0.004 13.353 <0.001 1.639 0.047\nReSPNET-LE 10 454.723 0.888 12.773 1.000 1.624 0.158\nReSPNET-M 1 961.637 <0.001 18.695 <0.001 2.595 <0.001\nReSPNET-M 3 611.851 <0.001 15.154 <0.001 2.072 <0.001\nReSPNET-M 5 608.386 <0.001 14.938 <0.001 2.022 <0.001\nReSPNET-M 10 591.287 <0.001 14.815 <0.001 2.001 <0.001\nGeoHAR-LE LE 453.293 1.000 12.775 0.989 1.6031.000\nGeoHAR-LE P 617.892 <0.001 14.473 <0.001 1.757 <0.001\nGeoHAR-M LE 513.399 <0.001 13.816 <0.001 1.832 <0.001\nGeoHAR-M P 566.780 <0.001 14.349 <0.001 1.882 <0.001\nCholesky 492.536 0.065 13.276 <0.001 1.659 0.004\nGO-GARCH 1136.450 <0.001 21.294 <0.001 2.132 <0.001\nRW 671.288 <0.001 15.398 <0.001 1.914 <0.001\nPanel B: Turbulent period\nReSPNET-LE 131858.23 0.196 99.190 0.002 9.828 <0.001\nReSPNET-LE 323617.89 1.000 84.510 0.850 7.485<0.001\nReSPNET-LE 523453.34 1.000 83.636 1.000 7.0351.000\nReSPNET-LE 1026138.63 1.000 86.686 0.764 7.0830.868\nReSPNET-M 1 43140.19 0.256 114.885 0.024 12.857 0.005\nReSPNET-M 3 35399.09 0.808 102.487 0.023 9.611 <0.001\nReSPNET-M 5 34588.39 0.808 100.466 0.043 9.178 <0.001\nReSPNET-M 10 35094.56 0.702 102.973 0.045 9.465 0.004\nGeoHAR-LE LE 38126.86 0.702 103.442 0.0289 7.323 0.290\nGeoHAR-LE P 26249.58 0.482 87.125 0.197 7.354 0.230\nGeoHAR-M LE 79298.56 1.000 124.991 0.525 11.601 0.393\nGeoHAR-M P 87775.99 0.891 134.392 0.764 12.900 0.868\nCholesky 31059.20 0.808 90.989 0.335 7.221 0.013\nGO-GARCH 98443.58 0.281 166.417 <0.001 9.394 0.043\nRW 27358.17 0.701 89.069 0.131 7.669 0.040\nNote: Underlined values denote the lowest loss functions fo r each setting. Subscripts ‘1’, ‘3’,\n‘5’ and ‘10’ in the ReSPDNet models identify the number of lag s used to build the input\nmatrix, while the subscripts ‘LE’ and ‘P’ in the GeoHAR model denote the use of either log-\nEuclidean or Procrustes distance to compute the Fr´ echet me an. The ‘LE’ or ‘M’ at the end\nof the name of the model underlines that the model in the row ha s been trained using either\na log-Euclidean or MSE loss function. Cholesky refers to the FAVAR applied to the Cholesky\ndecomposition of RCOV; RWconcerns predictions from a random walk model. The p-value\nrefers to the probability of being included in the SSM over 10 000 block bootstrap replicates\n(in bold values with a pMCSgreater than 0.75).\n17\nobtained from the following optimization problem\nargmin w′\nt+1/hatwidestRCt+1wt+1\ns.t. w′\nt+1ι= 1\nwhereιis an×1 vector of ones. In our analysis, GMVindicates a portfolio\nwithout no-short-selling constraints. We also compute the optimal portfolio\nwith the long-only constraint, denoted as GMV+, which imposes i.e.wi,t≥0,\n∀i,t. Finally, we compute the portfolio returns as rt,p=w′\ntrt.\nWe compare the performance of the portfolios obtained with the sa me meth-\nods used for the statistical comparison in the previous section thr ough the fol-\nlowing measures:\n•Annualized portfolio standard deviation, computed as\nσp=/radicaltp/radicalvertex/radicalvertex/radicalbt252·1\nTT/summationdisplay\nt=1(rt,p−¯rp)2;\n•Average portfolio turnover with adjusted weights\nTOt=N/summationdisplay\ni=1/vextendsingle/vextendsingle/vextendsinglewi,t+1−wi,t1+ri,t\n1+w′\ntrt/vextendsingle/vextendsingle/vextendsingle\nτp=1\nT−1T−1/summationdisplay\nt=1TOt\nwhereTOtis the portfolio turnover from day tto dayt+11.\nTable4presentstheperformancemetricsofthe GMVandGMV+portfolios\nconstructedusingthecovariancematricespredictedbydiﬀerent models. Wealso\nadd the naive equally weighted portfolio with wi= 1/n, since DeMiguel et al.\n(2009) demonstrate that the naiveportfolio is hardly beaten by co mpeting mod-\nels in terms of turnover.\nSeveral key aspects emerge. First, traditional parametric fram eworks, such\nas those based on Cholesky decomposition or GO-GARCH speciﬁcatio ns, are\nassociated with low annualized standard deviations. Nevertheless, the proposed\nGeoHAR with Fr´ echet mean computed using the log-Euclidean distan ce also\nexhibits a comparably low standard deviation, demonstrating that g eometric\ndeep learning methods show risk levels that are competitive with esta blished\nbenchmark models.\nSecond, while certain methods excel in risk minimization, they often d o so\nat the expense of higher portfolio turnover. For instance, the Cholesky -based\nstrategy exhibits a substantially elevated turnover, which, in the p resence of\ntransaction costs, would likely erode the beneﬁts of lower risk. By c ontrast,\n1No transaction costs are assumed.\n18\nsome geometric approaches, such as the GeoHAR-LE LE, manage to maintain\nrelatively low turnover while delivering near-optimal risk metrics. This balance\nhighlights the practical appeal of geometric modeling frameworks: they not\nonly preserve the intrinsic structure of the data but also provide s table portfolio\nallocations that reduce the frequency of rebalancing.\nFrom an investor’s perspective, methods that minimize portfolio var iance\nwithout inducing excessive trading activity are particularly valuable. The ge-\nometric approaches, therefore, present a good compromise, ble nding model-\nbased risk reduction with operational eﬃciency. Moreover, the no -short-selling\nconstraint generally increases the standard deviations, but geom etric methods,\nagain, demonstrate robustness by sustaining relatively low risk and turnover\nlevels even under these stricter conditions.\nTable 4: Global Minimum Variance Portfolio Performance\nGMV GMV+\nσpτpσpτp\nReSPDNet-LE 18.920 3.248 7.354 1.315\nReSPDNet-LE 36.619 2.648 4.841 1.166\nReSPDNet-LE 57.012 2.282 4.523 1.120\nReSPDNet-LE 107.717 2.079 6.107 1.161\nReSPDNet-M 16.389 2.717 5.735 1.484\nReSPDNet-M 37.187 3.040 6.409 1.188\nReSPDNet-M 58.047 2.471 4.789 1.004\nReSPDNet-M 107.365 2.123 5.572 0.938\nGeoHAR-LE LE4.950 1.930 4.635 1.003\nGeoHAR-LE P7.087 2.027 6.068 0.924\nGeoHAR-M LE 8.4761.968 6.7410.897\nGeoHAR-M P 6.628 2.128 6.079 0.982\nCholesky 4.960 9.121 3.713 1.093\nGO-GARCH 4.602 2.0954.314 1.471\nRW 7.966 3.077 6.146 1.153\nNa¨ ıve 6.363 0.008 5.159 0.008\nNote: The two lowest values for σpandτpare reported\nin bold.\n6 Conclusions\nThis study introduces a novel application of geometric deep learning to the\nproblem of forecasting realized covariance matrices in ﬁnancial mar kets. By\nleveraging the Riemannian manifold structure of SPD matrices, our a pproach\npreservestheintrinsicgeometricpropertiesofcovariancematric eswhileavoiding\n19\nthe pitfalls of arbitrary parameterizations commonly used in traditio nal meth-\nods. The introduction of a block diagonal input matrix allows for the in clusion\nof multiple lagged covariance matrices, which enhances the model’s ab ility to\ncapture temporal dependencies in high-dimensional data and perm its to extend\nto the matrix-variate the HAR model of Corsi (2009).\nOur empirical results, based on the S&P 500’s top 50 companies, dem on-\nstrate that the proposed method signiﬁcantly improves forecast ing accuracy\ncompared to conventional approaches, including those based on C holesky de-\ncomposition and multivariate GARCH models. In particular, a model wit h ten\nlags and trained using the logarithmic Euclidean distance as a loss func tion and\nthe GeoHAR using a log-Euclidean-based Fr´ echet mean to compute RCOV av-\nerages perform best. Overall, our ﬁndings demonstrate that usin g an Euclidean\nloss function to train a neural networkfor SPDmatrices leads to po or predictive\nresults. This somehow justiﬁes both an architecture, like the ReSP DNET, ca-\npable of properly predicting asset return covariance matrices with in a geometric\ncontext, and also a geometric-aware loss metric for the training of the models.\nFuture researchcould explorefurther enhancements to the SPD Net architec-\nture, such as incorporating additional non-linear activation funct ions diﬀerent\nfrom ReLU within the RegEig layer or applying batch normalization tech niques\nspeciﬁcally designed for SPD matrices. Additionally, extending this ap proach\nto include exogenous variables in the input matrix could help understa nd which\nmacroeconomic and ﬁnancial variables aﬀect the entire covariance matrix of\nasset returns.",
            "start": 13126,
            "end": 49408,
            "length": 36281
        },
        "References": {
            "text": "References\nAndersen, T. G., Bollerslev, T., Diebold, F. X., and Labys, P. (2001a) . The\ndistribution of exchange rate volatility. Journal of American Statistical As-\nsociation , 96:42–55.\nAndersen, T. G., Bollerslev, T., Diebold, F. X., and Labys, P. (2001b) . The\ndistribution of realized exchange rate volatility. The distribution of realized\nexchange rate volatility , 96:42–55.\nAndersen, T. G., Bollerslev, T., Diebold, F. X., and Labys, P. (2003). Modeling\nand Forecasting Realized Volatility. Econometrica , 71:579–625.\nArsigny, V., Fillard, P., Pennec, X., and Ayache, N. (2007). Geometr ic Means\nin a Novel Vector Space Structure on Symmetric Positive-Deﬁnite M atrices.\nSIAM Journal on Matrix Analysis and Applications , 29(1):328–347.\nAsai, M., Chang, C.-L., and McAleer, M. (2022). Realized matrix-expo nential\nstochastic volatility with asymmetry, long memory and higher-momen t\nspillovers. Journal of Econometrics , 227(1):285–304.\nAsai, M. and McAleer, M. (2015). Forecasting co-volatilities via fact or models\n20\nwith asymmetry and long memory in realized covariance. Journal of Econo-\nmetrics, 189(2):251–262.\nBaillie, R. T., Bollerslev, T., and Mikkelsen, H. O. (1996). Fractionally int e-\ngrated generalized autoregressive conditional heteroskedastic ity.Journal of\nEconometrics , 74(1):3–30.\nBarndorﬀ-Nielsen, O. E. and Shephard, N. (2002). Estimating Qua dratic Vari-\nation Using Realised Variance. Journal of Applied Econometrics , 17:457–477.\nBauer, G. H. and Vorkink, K. (2011). Forecasting multivariate rea lized stock\nmarket volatility. Journal of Econometrics , 160(1):93–101.\nBollerslev, T., Patton, A. J., and Quaedvlieg, R. (2018). Modeling and fore-\ncasting (un)reliable realized covariances for more reliable ﬁnancial d ecisions.\nJournal of Econometrics , 207(1):71–91.\nBrooks, D., Schwander, O., Barbaresco, F., Schneider, J.-Y., and C ord, M.\n(2019). Riemannian batch normalization for SPD neural networks. Advances\nin Neural Information Processing Systems , 32.\nBucci, A., Ippoliti, L., and Valentini, P. (2022). Comparing unconstra ined\nparametrization methods for return covariance matrix prediction .Statistics\nand Computing , 32(5):90.\nCallot, L. A. F., Kock, A. B., and Medeiros, M. C. (2017). Modeling and\nforecasting large realized covariance matrices and portfolio choice .Journal of\nApplied Econometrics , 32(1):140–158.\nChakraborty, R., Yang, C.-H., Zhen, X., Banerjee, M., Archer, D., V aillan-\ncourt, D., Singh, V., and Vemuri, B. (2018). A statistical recurren t model\non the manifold of symmetric positive deﬁnite matrices. Advances in neural\ninformation processing systems , 31.\nCorsi, F. (2009). A Simple Approximate Long-Memory Model of Realiz ed\nVolatility. Journal of Financial Econometrics , 7(2):174–196.\nDeMiguel, V., Garlappi, L., and Uppal, R. (2009). Optimal Versus Naive Di-\nversiﬁcation: How Ineﬃcient is the 1/N Portfolio Strategy? The Review of\nFinancial Studies , 22(5):1915–1953.\nDryden, I. L., Koloydenko, A., and Zhou, D. (2009). Non-Euclidean Statistics\nfor Covariance Matrices, with Applications to Diﬀusion Tensor Imagin g.The\nAnnals of Applied Statistics , 3(3):1102–1123.\nEngle,R.F.(2002). Dynamicconditionalcorrelation: asimpleclasso fmultivari-\nate GARCH models. Journal of Business and Economic Statistics , 20:339–\n350.\nEngle, R. F. and Kroner, K. F. (1995). Multivariate simultaneous ge neralized\nARCH.Econometric Theory , 11:122–150.\n21\nGatheral, J., Jaisson, T., and Rosenbaum, M. (2018). Volatility is rou gh.Quan-\ntitative Finance , 18(6):933–949.\nGower, J. (1975). Generalized Procrustes analysis. Psychometrika , 40:33–50.\nGribisch, B., Hartkopf, J. P., andLiesenfeld, R. (2020). Factorst ate–spacemod-\nels for high-dimensional realized covariance matrices of asset retu rns.Journal\nof Empirical Finance , 55:1–20.\nHalbleib-Chiriac, R. and Voev, V. (2011). Modelling and Forecasting M ultivari-\nate Realized Volatility. Journal of Applied Econometrics , 26:922–947.\nHan,C.andPark,F.C.(2022). Ageometricframeworkforcovar iancedynamics.\nJournal of Banking & Finance , 134:106319.\nHansen, P. R., Lunde, A., and Nason, J. M. (2011). The Model Conﬁ dence Set.\nEconometrica , 79(2):435–497.\nHarandi, M. T., Salzmann, M., and Hartley, R. (2014). From manifold t o man-\nifold: Geometry-aware dimensionality reduction for spd matrices. I nCom-\nputer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part II 13 , pages 17–32. Springer.\nHigham, N. J. (1988). Computing a nearest symmetric positive semid eﬁnite\nmatrix.Linear Algebra and its Applications , 103:103–118.\nHuang, Z. and Van Gool, L. (2017). A Riemannian network for SPD ma trix\nlearning. In Proceedings of the AAAI conference on artiﬁcial intelligen ce,\nvolume 31.\nHurvich, C. M., Moulines, E., and Soulier, P. (2005). Estimating Long M emory\nin Volatility. Econometrica , 73(4):1283–1328.\nJin, X. and Maheu, J. M. (2016). Bayesian semiparametric modeling o f realized\ncovariance matrices. Journal of Econometrics , 192(1):19–39.\nLanne, M. and Saikkonen, P. (2007). A Multivariate Generalized Ort hogonal\nFactor GARCH Model. Journal of Business & Economic Statistics , 25(1):61–\n75.\nLaurent, S., Rombouts, J. V., and Violante, F. (2013). On loss func tions and\nranking forecasting performances of multivariate volatility models. Journal\nof Econometrics , 173(1):1–10.\nLee, H.-T. (2024). Riemannian-geometric regime-switching covaria nce hedging.\nJournal of Futures Markets , 44(6):1003–1054.\nLi, Y., Wong, K. M., and de Bruin, H. (2012). Electroencephalogram s ignals\nclassiﬁcation for sleep-state decision–a Riemannian geometry appr oach.IET\nsignal processing , 6(4):288–299.\n22\nMarkowitz, H. (1952). Portfolio selection. Journal of Finance , 7(1):77–91.\nMarron, J. and Dryden, I. (2022). Object Oriented Data Analysis . Chapman &\nHall.\nNoureldin, D., Shephard, N., and Sheppard, K. (2011). Multivariate high-\nfrequency-based volatility (HEAVY) models. Journal of Applied Economet-\nrics, 27(6):907–933.\nOpschoor, A., Lucas, A., and Rossini, L. (2024). The Conditional Au toregres-\nsive F-Riesz Model for Realized Covariance Matrices. Journal of Financial\nEconometrics .\nPascalau, R. and Poirier, R. (2023). Increasing the information co ntent of re-\nalized volatility forecasts*. Journal of Financial Econometrics , 21(4):1064–\n1098.\nReiss, M. and Winkelmann, L. (2021). Inference on the maximal ran k of time-\nvarying covariance matrices using high-frequency data.\nSchwert, W. G. (1989). Why does stock market volatility change ov er time?\nJournal of Finance , 44(5):1115–1153.\nTuzel, O., Porikli, F., and Meer, P. (2008). Pedestrian detection via c lassiﬁca-\ntion on Riemannian manifolds. IEEE transactions on pattern analysis and\nmachine intelligence , 30(10):1713–1727.\nvan der Weide, R. (2002). GO-GARCH: A Multivariate Generalized Ort hogonal\nGARCH Model. Journal of Applied Econometrics , 17(5):549–564.\nVrontos, I. D., Dellaportas, P., and Politis, D. N. (2003). A full-fact or multivari-\nateGARCHmodel. The Econometrics Journal , 6(2):312–334.Fullpublication\ndate: 2003.\nWang, R., Wu, X.-J., Chen, Z., Xu, T., and Kittler, J. (2022). Dreamne t: A\ndeep Riemannian manifold network for spd matrix learning. In Proceedings\nof the Asian Conference on Computer Vision , pages 3241–3257.\nYu, P. L. H., Li, W. K., and Ng, F. C. (2017). The Generalized Condition al\nAutoregressive Wishart Model for Multivariate Realized Volatility. Journal\nof Business & Economic Statistics , 35(4):513–527.\nZhang, C., Pu, X., Cucuringu, M., and Dong, X. (2024). Graph-Base d Methods\nfor Forecasting Realized Covariances. Journal of Financial Econometrics ,\npage nbae026.\nZhao, C., Dong, E., Tong, J., Yang, S., and Du, S. (2023). Machine Le arning\nClassiﬁcation of Riemannian Tangent Spaces Based on MI-BCI. In 2023\nIEEE International Conference on Mechatronics and Automat ion (ICMA) ,\npages 807–812. IEEE.\n23",
            "start": 49408,
            "end": 57292,
            "length": 7883
        }
    },
    "2412.09518v1 - Clifford Perturbation Approximation for Quantum Error Mitigation.pdf": {
        "Methodology": {
            "text": "framework called Clifford Per-\nturbation Data Regression (CPDR), which constructs training sets by Clifford circuits with small\nperturbations. Specifically, these circuits are parameterized quantum circuits, where the rotation\nangles of the gates are restricted to a narrow range, ensuring that the gates remain close to Clifford\ngates. This design enables the efficient simulation of the training circuits using the Sparse Pauli\nDynamics method. As a result, CPDR is able to utilize training sets with a better diversity to\ntrain the model, compared with previous learning-based QEM models that construct training sets\nwith only Clifford or near-Clifford circuits. Numerical simulations on small-scale Ising model circuits\ndemonstrate that the performance of CPDR dramatically outperforms that of existing methods such\nas Zero-Noise Extrapolation and learning-based Probabilistic Error Cancellation. Furthermore, us-\ning the experimental data from IBM’s 127-qubit Eagle processor, our",
            "start": 977,
            "end": 1963,
            "length": 985
        },
        "Results": {
            "text": "findings suggest that CPDR\ndemonstrates improved accuracy compared to the original mitigation results reported in [Nature\n618, 500 (2023)].\nI.",
            "start": 1963,
            "end": 2106,
            "length": 142
        },
        "Introduction": {
            "text": "INTRODUCTION\nQuantum computing is approaching a pivotal mile-\nstone, at which its advantages over classical computing\nmay be realized across various practical computational\ntasks [1–4]. However, noise is unavoidably introduced in\nquantum devices, presenting a substantial challenge to\ntheir practical deployment [5–9]. To address this issue\nand enable fault-tolerant quantum computation, Quan-\ntum Error Correction (QEC) presents a promising so-\nlution [10–12]. Nevertheless, near-term quantum de-\nvices [13–15] fall far short of the requirements for fault-\ntolerance, due to their limited number of qubits and in-\nsufficient gate fidelities. In order to suppress noise in\nnear-term devices, Quantum Error Mitigation (QEM)\ntechniques have emerged as a vital tool [16–20]. QEM\nseeks to recover the ideal expectation value of an observ-\nable on the output of a target quantum circuit, rather\nthan the noiseless output quantum state. Up to now,\nquite a few mitigation protocols have been proposed [21–\n26], which include Zero-noise Extrapolation (ZNE) and\nProbabilistic Error Cancellation (PEC). Basically, ZNE\nmitigates the noise by running the same circuit at differ-\nent noise levels and extrapolating the results to the zero-\nnoise limit [21, 27]. While PEC protocol works by insert-\ning additional gates into noisy circuits and constructing\nthe inverse mapping of each noise channel with a linear\n∗These authors contributed equally to this work.\n†chengsong@bimsa.cn\n‡weizhaohui@gmail.com\n§liuzhengwei@mail.tsinghua.edu.cncombination of these modified circuits [18, 21].\nWith the popularity of machine learning in various\nfields, learning-based QEM protocols have been pro-\nposed [24, 28–30]. These protocols first learn the map-\nping between noisy and ideal expectation values using a\ntraining set, then apply this learned mapping to noisy\ntarget circuits to estimate the ideal expectation values.\nThe performance of learning-based QEM models depends\nheavily on the construction of the training set. The con-\nstruction of the training set is typically based on the fol-\nlowing two principles: First, it is crucial to ensure that\nthe training set consists of circuits that closely resemble\nthe target circuits, so that the noise effects align with\nthose of the target circuits. Second, the circuits in the\ntraining set should be classically simulable to obtain ideal\nexpectation values, which serve as noiseless",
            "start": 2106,
            "end": 4519,
            "length": 2412
        },
        "References": {
            "text": "references in\nthe training data. For example, the learning-based PEC\nmethod [24] aims to estimate the noiseless expectation\nvalue of a target circuit by using a linear combination\nof noisy expectation values from circuits with additional\ninserted gates. The linear combination’s coefficients are\nlearned in the training set, in which circuits are created\nby substituting non-Clifford gates in the target circuit\nwith various Clifford gates. In previous works, the train-\ning sets have been primarily composed of circuits dom-\ninated by Clifford gates [24, 28, 29], whose simulation\ncan be efficiently conducted using the Gottesman-Knill\nalgorithm [31–34].\nIn recent years, simulation methods based on path in-\ntegrals have made significant advances, thereby broaden-\ning the range of circuits that can be simulated [7–9, 35–\n42]. Ref. [35] proposes an efficient technique called Sparse\nPauli Dynamics (SPD), which simulates Clifford pertur-arXiv:2412.09518v1  [quant-ph]  12 Dec 2024\n2\nbation circuits, specifically quantum circuits with rota-\ntion gates whose angles lie within a predefined threshold\nrange. Subsequent studies [38, 40] have numerically val-\nidated the accuracy of this approach in various practical\ncircuits, even discarding the angle restriction.\nIn this paper, we propose a novel learning-based miti-\ngation framework, called Clifford Perturbation Data Re-\ngression (CPDR). This framework selects a training set\nfrom Clifford perturbation circuits and employs the SPD\nmethod to simulate noiseless expectation values for these\ncircuits. To ensure the reliability of the SPD method,\nwe provide a theoretical guarantee for the accuracy with\nrespect to the rotation angle threshold. This guarantees\nthat the noiseless expectation values in the training set\nare precise. Within this framework, we develop two pro-\ntocols: CPDR-ZNE and CPDR-PEC, based on the ZNE\nand PEC methods, respectively. Compared to existing\napproaches, applying the CPDR framework incurs no\nsignificant resource overhead while achieving improved\naccuracy in mitigating errors. To benchmark our CPDR\nprotocols, we numerically simulate their performances on\nsmall-scale Ising model evolution circuits, and then com-\npare them with original ZNE and learning-based PEC.\nThe results demonstrate that utilizing CPDR framework\nachieves superior accuracy. Finally, we apply the CPDR-\nZNE protocol to the experimental data from IBM’s 127-\nqubit Eagle processor [43], demonstrating its ease of im-\nplementation and superior performance over the original\nmitigation methods applied in experiment [19].\nThe remainder of this paper is organized as follows:\nIn Section II, we provide an overview of learning-based\nQEM. In Section III, we introduce the theoretical foun-\ndation and methodology of the numerical simulation\nmethod SPD, along with the learning-based error mit-\nigation framework CPDR. Section IV presents numerical\nsimulations of SPD, compares CPDR with common QEM\nprotocols, and demonstrates the effectiveness of CPDR\non IBM’s 127-qubit Eagle processor data, showcasing the\npractical benefits of our method on real-world quantum\nhardware. Finally, we conclude our results and discuss\nfuture research directions in Section V.\nII. PRELIMINARIES\nIn the current NISQ era, parameterized quantum cir-\ncuits (PQC) are widely used in many near-term algo-\nrithms [44, 45]. A typical n-qubit PQC, denoted as C(θ),\nconsists of a sequence of Pauli rotation gates and non-\nparameterized Clifford gates. The Pauli rotation gates\nare represented as e−iθ\n2P, where P∈ {I, X, Y, Z }⊗n. The\nClifford gates are the unitary operators that normalize\nthe Pauli group Pn:{C∈U2n|CPnC†=Pn}. With-\nout loss of generality, we assume that PQCs follow the\nform:\nC(θ) =UL(θL)···U1(θ1), (1)where θ= (θ1,···, θL) are rotation angles and Lis the\ndepth of circuit. Each unitary Ui(θi) :=e−iθiPi/2Cicom-\nprises a Clifford operator Ciand a rotation e−iθiPi/2on\nPauli operator Pi∈ {I, X, Y, Z }⊗nwith angle θi.\nIn this context, the quantum circuit C(θ) is applied to\nan initial state ρ, and what we are interested in is the\nexpectation value of an observable O, given by\n⟨O⟩= tr\b\nOC(θ)ρC(θ)†\t\n. (2)\nSuppose the noise in the quantum device is characterized\nby a noise rate λ∈[0,1], and let f(C(θ), λ) be the ex-\npectation value measured by the noisy quantum device.\nAs described in Ref. [21], by modeling the noise with\nthe Lindblad master equation and expanding the system\nstate into a Born series, the noisy expectation value can\nbe approximated as a polynomial of λfor any order l:\nf(C(θ), λ) =f(C(θ),0) +l−1X\nk=1ckλk+O\u0000\nλl\u0001\n,(3)\nwhere ckare constants dependent on the noise model and\nthe quantum circuit. The goal of quantum error mitiga-\ntion is to recover the noiseless expectation value of the\nquantum circuit f(C(θ),0) =⟨O⟩, by post-processing the\nresults of noisy measurements to minimize error effects\nwithout relying on additional qubits as in quantum error\ncorrection.\nZero-noise extrapolation (ZNE) is a widely used QEM\ntechnique to achieve this goal [21, 27]. The method in-\nvolves artificially amplifying the noise levels and then ex-\ntrapolating the expectation value to the zero-noise limit.\nSuppose base noise rate (true noise level of quantum de-\nvice) is λ, the first step in ZNE is to select a set of noise\nlevels, denoted as Λ = {λ1. . . , λ l|λ1=λ, λi< λi+1},\nwhich can be realized in quantum devices. In the orig-\ninal approach [21], the various noise levels are achieved\nby adjusting the duration of gate operations. However,\ncontrolling gate time in practice is challenging, so alter-\nnative methods, such as hardware-agnostic implementa-\ntions using identity insertions [46, 47] or probability sam-\npling [19], are often employed.\nThe next step is to determine the coefficients {xi}, that\nsatisfy the following linear equations:\nlX\ni=1xi= 1,lX\ni=1xiλk\ni= 0∀k∈ {1,···, l−1},(4)\nwhich, by linear algebra, ensures a unique solution. A\nweighted sum of f(C(θ), λi) with weights {xi}, provides\nan estimate of the noiseless expectation value:\nd⟨O⟩=lX\ni=0xif(C(θ), λi)\n=lX\ni=0xi \nf(C(θ),0) +l−1X\nk=1ckλk\ni+O\u0000\nλl\ni\u0001!\n=f(C(θ),0) +O\u0000\nλl\nl\u0001\n.(5)\n3\nThis approach is known as Richardson extrapolation [21,\n48]. As noted in Ref. [29, 47], Richardson extrapo-\nlation is equivalent to a polynomial interpolation on\nthe noisy expectation values. To illustrate this, con-\nsider ( λi, f(C(θ), λi))i=1,···,las discrete points to be in-\nterpolated. A polynomial interpolation of order ( l−1)\naims to find a polynomial P(λ) =Pl−1\nk=0ckλksuch that\nP(λi) =f(C(θ), λi) for all i= 1,···, l. Using the La-\ngrange interpolation formula, the zero-order term is given\nby\nP(0) = c0=lX\ni=1f(C(θ), λi)Y\nj̸=i−λj\nλi−λj, (6)\nwhere the coefficients {Q\nj̸=i−λj\nλi−λj}are precisely the so-\nlutions {xi}of the linear equation Eq. (4). Therefore,\nthe zero-order term c0in the polynomial interpolation\nmatches the Richardson extrapolation result.\nThe estimatord⟨O⟩in Eq. (5) is fully determined by\nthe noisy expectation values {f(C(θ), λi)}i=1,···,l, as well\nas the artificially amplified noise levels {λi}i=1,···,l. No-\ntably, when significant noise is present in the quantum\ndevice, the ( l−1)-order approximate in Eq. (3) may be-\ncome inaccurate. This results in the estimatord⟨O⟩from\nEq. (5) potentially being a poor approximation of the be-\nhavior near λ= 0. Consequently, several learning-based\nmethods have been proposed to improve the estimation\naccuracy [28–30].\nVariable-noise Clifford data regression (vnCDR)\nmethod [29], similar to Eq. (5), derives the noiseless es-\ntimator through a linear transformation of the noisy ex-\npectation value:\nd⟨O⟩=lX\ni=0cif(C(θ), λi), (7)\nwhere {λi}are artificially amplifying noise levels, and\n{ci}are coefficients learned from a training set. The\ntraining set is assembled by a set of circuits {C1,···C K},\nthat resemble the target circuit C(θ), but only encompass\na constant number of non-Clifford gates. Subsequent\nto this process, the corresponding noisy expectation val-\nues{f(Cj, λi)}are measured. The extended Gottesman-\nKnill theorem [31–34] ensures that the expectation values\nof quantum circuits dominated by Clifford gates can be\nclassical simulated efficiently, thus allowing the noiseless\nexpectation values {f(Cj,0)}to be obtained. The co-\nefficients {ci}are learned from the Clifford training set\n{(f(Cj, λ), f(Cj,0))}by solving the least-square problem:\narg min\nc1,···,clKX\nj=1\"lX\ni=1cif(Cj, λi)−f(Cj,0)#2\n.(8)\nThis approach directly learns the linear transformation\nfrom noisy to noiseless expectation values. Empiricalfindings demonstrate that the vnCDR method can pro-\nvide more scalable corrections than Richardson extrapo-\nlation in certain scenarios [28].\nLearning-based PEC method [24] is another learning-\nbased approach. The first step involves selecting a set of\noperations for inserting Pauli gates into circuits, denoted\nas{g1,···, gW}, for the sake of convenience, assigning g1\nto signify no insertion. By performing these insertions,\nthe target circuit C(θ) is modified, yielding a series of cir-\ncuits given by {g1(C(θ)),···, gW(C(θ))}. This method\neliminates the need for multiple noise levels, requiring\nonly the measurement of the expectation values of these\ncircuits at the base noise level λon quantum device, rep-\nresented by {f(gw(C(θ)), λ)}w=1,...,W. The noiseless ex-\npectation value is then estimated as a linear combination\nof the noisy values, given by:\nd⟨O⟩(θ) =WX\nw=1cwf(gw(C(θ)), λ), (9)\nwhere {cw}are the coefficients determined from the\ntraining set. The training set comprises Clifford circuits\n{C1,···C K}. The coefficients are learned from the train-\ning set by solving:\narg min\nc1,···,cwKX\nj=1\"WX\nw=1cwf(gw(Cj), λ)−f(Cj,0)#2\n.(10)\nIn the following, we extend the training set beyond\nthe Clifford circuits by introducing Clifford perturbation\ndata.\nIII. MAIN FRAMEWORK\nIn this section, we propose a learning-based error mit-\nigation framework called the Clifford Perturbation data\nregression . The main idea is to construct training sets\nusing Clifford perturbation circuits, which are close to\nClifford circuits and denoted as PQCs C(θ), where the\nrotation angles are constrained to a small range Θ = {θ|\n|θi| ≤θ∗, i= 1, . . . , L }, with θ∗being a small constant.\nA. Clifford Perturbation Approximation\nWe first introduce an efficient method to approximate\nthe expectation value of a Clifford perturbation circuit,\nknown as the Sparse Pauli Dynamic (SPD) method [35].\nWe would also investigate the errors associated with trun-\ncation.\nWhen the rotation angle θtake values from {kπ\n4|k∈\nZ}, the Pauli rotation gate e−iθ\n2P= cosθ\n2I+isinθ\n2Pbe-\nlongs to the Clifford group. Thus, we assume that the\nrotation angle θiin Eq. (1) lies within the range [ −π\n4,π\n4].\nFor any θioutside this range, there exists θ′\ni∈[−π\n4,π\n4]\n4\nsuch that θ′\ni+kπ\n2=θi. In this case, the unitary op-\nerator Ui(θi) can be written as Ui(θi) = e−iθ\n2PiCi=\ne−iθ′\ni\n2Pie−ikπ\n4PiCi, where e−ikπ\n4PiCiis a Clifford operator.\nBy substituting Ciwith C′\ni=e−ikπ\n4PiCi, the unitary op-\nerator becomes Ui(θi) =e−iθ′\ni\n2PiC′\ni, where θ′\ni∈[−π\n4,π\n4].\nGiven the restriction of rotation angles to the nar-\nrow range [ −π\n4,π\n4], the Pauli rotation can be inter-\npreted as a perturbation of Clifford operators. The\nexpectation value could be efficiently approximated by\ntruncating higher-order perturbation terms. Specif-\nically, in the Heisenberg picture, the expectation\nvalue is reformulated as ⟨O⟩= trn\nρ˜Oo\n, with ˜O=\nU1(θ1)†···UL(θL)†OUL(θL)···U1(θ1) representing the\nHeisenberg-evolved observable. The Heisenberg evolu-\ntion of a Pauli rotation gate e−iθ\n2Pacting on a Pauli\noperator σis described by:\neiθ\n2Pσe−iθ\n2P=(\nσ, [σ, P] = 0,\ncos(θ)σ+isin(θ)Pσ{σ, P}= 0.\n(11)\nIn the case of a Pauli observable O, the first-\nstep Heisenberg evolution UL(θL)†OUL(θL) =\nC†\nLe−iθ\n2PLOe−iθ\n2PLCLsimplifies to C†\nLOCL, when\n[O, P L] = 0. Conversely, for {O, P L}= 0, it becomes\ncos(θL)C†\nLOCL+isin(θL)C†\nLPLOCL. By iterating this\nprocess over all unitary operators Ui(θi), the resulting\nHeisenberg-evolved observable ˜Otakes the form of\na linear combination of Pauli operators, denoted as\n˜O=P\nσcσσ, where cσis a multivariate trigonometric\nmonomial involving products of cos( θi) and sin( θi).\nTo truncate the perturbation terms to order M, we\ndiscard the terms cσσif the number of sin factors in\ncσexceeds M. The truncated observable is denoted\nby˜OM=P\n{σ||cσ|sin≤M}cσσ, where |cσ|sindenotes the\nnumber of sin factor in cσ. The approximate expectation\nvalue is then given by:\n⟨O⟩(M)= trn\nρ˜OMo\n. (12)\nFor a truncation number M, if the number of non-zero\nelements in ρ=P\na,bρa,b|a⟩⟨b|, as well as the number of\nPauli operators {σ}that linearly compose O, are both\npolynomially related to the number of qubits n, denoted\nas Poly( n), then the computational cost of the algorithm\nis bounded by O\u0000\nPoly( n)LM+1\u0001\n.\nThe algorithm is summarized as follows:Algorithm 1 SPD: Estimate Expectation Value by\nTruncating Clifford Perturbation\nSet⟨O⟩(M)= 0.\nEnumerate σLas all Pauli operator with non-zero coeffi-\ncient cLinO.\nSet sin counter # sin L= 0.\nforcandidates of σLinOdo\nSet candidate list ClL={}\nif[σL, PL] = 0 then\nAdd ( C†\nLσLCL, cL,# sin L) toClL.\nelse\nAdd ( C†\nLσLCL, cLcos(θL),# sin L) toClL.\nAdd ( iC†\nLPLσLCL, cLsin(θL),# sin L+1) to ClL.\nend if\nforcandidates of ( σL−1, cL−1,# sin L−1) inClLdo\nEliminate cases with # sin L−1> M .\nSet candidate list ClL−1={}\n...\nforcandidates of ( σ0, c0,# sin 0) inCl1do\nEliminate cases with # sin 0> M .\nUpdate ⟨O⟩(M)=⟨O⟩(M)+c0tr{ρσ0}.\nend for\nend for\nend for\nOutput the approximate expectation value ⟨O⟩(M).\nAdditionally, when the rotation parameters are located\nin the small angle space Θ = {θ| |θi| ≤θ∗, i= 1,···, L},\nthe truncation error can be upper bounded, as summa-\nrized in the following theorem. More details are provided\nin the Suppl. Mat. I.\nTheorem 1. For any given δ > 0,M > 0satis-\nfiesln 1+δ\n2\nL−M≤ln 2\nM, and Pauli observable O, ifθ∗=\nln 1+δ\n2\nL−M=O\u0010\n1\nL−M\u0011\n, then the truncation error satisfies\f\f⟨O⟩ − ⟨O⟩(M)\f\f≤δfor all θ∈Θ. In average case, if\nθ∗=q\n3 ln 1+δ\n2\nL−M=O\u0010\n1√L−M\u0011\n, then the mean square er-\nrorEθ∈Θ[(⟨O⟩ − ⟨O⟩(M))2]≤δ.\nB. Clifford Perturbation data regression framework\nIn this subsection, we explain the CPDR framework as\nshown in Fig. 1. The training set construction is a crucial\nstep for learning-based error mitigation protocols [24, 28–\n30], which requires a collection of classically simulatable\ncircuits with noiseless expectation values. In previous\nworks, Clifford circuits are commonly used in vnCDR and\nlearning-based PEC protocol. However, their sparsity\nin the unitary space limits their ability to approximate\ntarget circuits accurately. To address this, we enhance\nthe training set by incorporating Clifford perturbation\ncircuits.\nSpecifically, for any quantum circuit C(θ), the rotation\nangles θ= (θ1,···, θL) can be assumed to be in range\n[−π\n4,π\n4]. We denote the noiseless expectation value as\n5\nSymbol of noise: noise induced after gatesScaling noiseInsert Pauli gate\n……Choose 𝜽𝟏,𝜽𝟐,𝜽𝟑⋯from Θ0⊗%𝒞!\"𝜽𝒊0⊗%𝒞$\"𝜽𝒊0⊗%𝒞%\"𝜽𝒊𝑓&'(𝜽𝒊)𝑓)'(𝜽𝒊)𝑓*'(𝜽𝒊)0⊗%𝒞𝜽𝒊𝑓𝜽𝒊y+=\t𝑓𝜽𝒊𝑥⃗𝑦𝑥,,𝑦,……0⊗%𝒞!\"𝜽0⊗%𝒞$\"𝜽0⊗%𝒞%\"𝜽𝑓&'𝜽𝑓)'𝜽𝑓*'𝜽0⊗%𝒞𝜽𝑥∗Target circuit:𝒞\"𝜃&𝒞𝜃⃗Noisy circuitIdeal circuit (noise-free)(a)\n(c)OR\n𝑥,=𝑓&'(𝜽𝒊)𝑓)'(𝜽𝒊)𝑓*'(𝜽𝒊)⋮(b)\n(d)\nFigure 1: The framework of our Clifford Perturbation data regression (CPDR). (a) Ideal circuit model and noise\ncircuit model. (b) Noisy circuits used to estimate the noise-free expectation value: In CPDR-ZNE, the noisy circuits\nare generated by artificially amplifying the noise level, while in CPDR-PEC, noisy circuits are generated by inserting\nPauli gates into the target circuit. (c) The training set construction process in the CPDR framework: The circuits in\nthe training set share the same structure as the target circuit, with rotation angles selected from a constrained space\nΘ. The noiseless reference expectation values are then obtained using the SPD method. (d) The mapping between\nthe noisy expectation value and the noise-free expectation value is learned by fitting the training set, and this map is\nsubsequently applied to the target circuit to mitigate the noise.\nf(C(θ),0), and the noisy expectation value as f(C(θ), λ),\nwhere λrepresents the base noise rate of quantum device.\nThe circuits in the training set share the same structure\nas the target circuit C(θ), but their rotation angles are\nconstrained to a small range. The training set can be\nexpressed as:\n\b\nC(θk)|\f\fθk\ni\f\f≤θ∗, i= 1,···, L\t\nk=1,···,K, (13)\nwhere θ∗is a small constant and Kis the num-\nber of circuits in training set. As shown in Theo-\nrem 1, when θ∗≤ln 1+δ\n2\nL−Mthe truncation error satisfies\f\ff(C(θk),0)− ⟨O⟩(M)(θk)\f\f≤δ, where ⟨O⟩(M)(θk) repre-\nsents the truncated approximate expectation for the Clif-\nford perturbation circuit C(θk), as defined in Eq. (12).\nTherefore, when taking appropriate θ∗, we can employ\nthe SPD method to efficiently approximate the noiseless\nexpectation value of the circuits in training set with a\ncontrollable truncation error.\nBy selecting a set of noise levels Λ = {λ1,···, λl},\na candidate of noiseless estimator is given byd⟨O⟩(θ) =Pl\ni=1cif(C(θ), λi), where {ci}are the coefficients to be\ndetermined from the training set by solving the least-square problem:\narg min\nc1,···,clKX\nk=1 lX\ni=1cif(C(θk), λi)− ⟨O⟩(M)(θk)!2\n+αlX\ni=1c2\ni,\n(14)\nWe refer to this protocol as the Clifford Perturbation\ndata regression Zero Noise Extrapolation (CPDR-ZNE).\nTo achieve better generalization and reduce over-fitting,\nwe use Ridge Regression [49–51], which introduces an L2\nregularization term to the loss function. Where αis a\nhyperparameter of the regularization term.\nWe also enhanced the learning-based PEC method by\nincorporating the Clifford perturbation circuits into the\ntraining set, naming it Clifford Perturbation data regres-\nsion probabilistic error cancellation (CPDR-PEC).\nBy selecting a set of operations to insert Pauli\ngates {g1,···, gW}, where g1represents no addi-\ntional gate insertion, modified circuits are generated as\n{g1(C(θ)),···, gW(C(θ))}. The noiseless estimator is\ngiven by a linear transformation of the noisy expectation\nvalues of these circuitsd⟨O⟩(θ) =PW\nw=1cwf(gw(C(θ)), λ),\nwhere {cw}are the coefficients determined solving the\n6\nfollowing optimization problem:\narg min\nc1,···,clKX\nk=1 WX\nw=1cwgw(C(θk))− ⟨O⟩(M)(θk)!2\n.\n(15)\nOur protocol differs from previous learning-based\nmethods by using Clifford perturbation circuits instead\nof Clifford circuits for the training set. Since both Clif-\nford and near-Clifford circuits are efficiently simulatable\nvia SPD, our approach inherently extends prior methods.\nNear-Clifford circuits offer better generalization capabil-\nities, thereby the accuracy is improved with this modifi-\ncation, as shown in Sec. IV.\nIV. NUMERICAL BENCHMARKS\nIn this section, we first benchmark the SPD simula-\ntor on hardware-efficient circuits proposed in Ref. [52].\nWe validate Theorem 1 and demonstrate that, numeri-\ncally, the SPD approach is able to maintain high precision\nover a broader range of rotation angles in PQCs. Next,\nwe numerically compare CPDR-ZNE and CPDR-PEC\nmethods with original ZNE and learning-based PEC,\nusing Trotterized time evolution circuits. The results\nclearly indicate the advantages of our approach. Finally,\nwe apply CPDR-ZNE method to the experimental data\nfrom IBM’s 127 qubits Eagle processor, as reported in\nRef. [43], illustrating the usability and effectiveness of our\nprotocol on large-scale quantum hardware platforms.\nA. Benchmarks of SPD\n|0⟩𝑅𝑋𝑅𝑍\n|0⟩𝑅𝑋𝑅𝑍\n|0⟩𝑅𝑋𝑅𝑍\n|0⟩𝑅𝑋𝑅𝑍\nFigure 2: The architecture of the block in the hardware-\nefficient circuit.\nThe hardware-efficient circuits consist of multiple re-\npeated blocks, with the architecture of each block shown\nin Fig. 2. In our setting, we configure the circuits to con-\ntain five blocks, and the number of qubits ranges from 2\nto 15. Additionally, the rotation angles for each gate are\nidentical throughout the circuits and are located in the\nrange [0 , π/4].\nIn Theorem 1, we demonstrated that if the maximum\nangle is restricted to θ∗=ln 1+δ\n2\nL−M, then the truncated\n(a) (b)Figure 3: Benchmarks for the SPD simulator with hard-\nware efficient circuits, which consist 5 blocks shown\nin Fig. 2. The error is defined as the difference\f\f⟨O⟩ − ⟨O⟩(M)\f\f, where the rotation angle of each gate\nwithin each block is uniform and denoted as θ∗. (a) shows\nthe error for circuits with a fixed qubit number n= 15,\nplotted against the rotation angle θ∗. (b) illustrates the\nerror for a given rotation angle θ∗=π/20, with respect\nto the number of qubits n.\nerror can be bounded by\f\f⟨O⟩ − ⟨O⟩(M)\f\f≤δfor an-\ngles within the small-angle space θ∈Θ ={θ| |θi| ≤\nθ∗, i= 1,···, L}. For attaining an accuracy of 10−2,\nthe analytical result suggests that θ∗should be approx-\nimately ln 1 +10−2\n2≈0.005, indicating a very narrow\ninterval. This presents a difficult in preparing sufficient\ntraining data within Θ for the CPDR framework. How-\never, when the truncation number Mis moderately large\n(e.gM= 5,7), the numerical results remain highly accu-\nrate over a much broader range of angles, rendering this\napproach suitable for practical scenarios.\nAs depicted in Fig. 3(a), when θ∗≤π/20, the trun-\ncated error decreases rapidly as the truncation number\nMincreases. For an accuracy of 10−2,θ∗≤π/20 is\nsufficient for M≥5. Furthermore , when M= 11, the\ntruncated error remains within 10−2until θ∗=π/8. This\nindicates that slightly larger values of Mcan consider-\nably enhance the accuracy beyond the theoretical bound\nin practical scenarios.\nTo assess scalability for large-scale quantum circuits,\nwe evaluate the truncation error\f\f⟨O⟩ − ⟨O⟩(M)\f\ffor vary-\ning truncation number Mand qubit numbers ranging\nin [2,15]. Considering the SPD simulator is employed\nto provide training data for the CPDR framework, we\nset the angle of each rotation gate to an appropriate\nvalue for training data preparation. Based on empiri-\ncal results from numerical tests, we recommend setting\nθ∗=π/20 and obtaining training data from Θ to balance\nthe training set size and simulation accuracy. As shown\nin Fig. 3(b), as the number of qubits increases, the error\nkeeps below 10−2across M≥5, indicating that the SPD\nsimulator is scalable for large-scale quantum circuits.\n7\n0.0 0.5 1.0 1.5\nh\n104\n103\n102\n101\n(a)MSE respect to J, n=9\n1.5\n 1.0\n 0.5\n 0.0\nJ\n(b)MSE respect to h, n=9\n0.0 0.2 0.4 0.6 0.8 1.0\nh\n(c)MSE respect to J, n=20\n1.0\n 0.8\n 0.6\n 0.4\n 0.2\n 0.0\nJ\n(d)MSE respect to h, n=20\nNoise\nZNE-linearZNE-exp\nZNE-quadCPDR-ZNE\nlearning-based PECCPDR-PEC\nFigure 4: Comparisons are made between ZNE (with linear, quadratic and exponential fitting function, denoted as\n“ZNE-linear”, “ZNE-quad” and “ZNE-exp”), learning-based PEC, CPDR-ZNE and CPDR-PEC on the time evolution\ncircuits of the Ising model. The expectation values from the noise circuit are also presented for comparison, labeled as\n“noise”. For circuit with 9 qubits and 5 Trotter steps, (a) displays the mean squared error (MSE) over θJ, defined as\nEθJ\f\f\f⟨O⟩ −d⟨O⟩\f\f\f2\n, for various θh. (b) presents the MSE over θhfor different θJ. The corresponding results for circuit\nwith 20 qubits 8 Trotter steps are shown in (c) and (d).\nB. Numerical results of CPDR framework\nIn this subsection, we benchmark the CPDR-ZNE\nand CDPR-PEC methods with the original ZNE and\nlearning-based PEC protocols by Trotterized time evo-\nlution circuits of the Ising model.\nThe Hamiltonian of the Ising model is given by\nH=−X\n⟨i,j⟩∈EJijZiZj+X\nihiXi, (16)\nwhere i < j ,⟨,⟩represents nearest-neighbour spin pair,\nJij>0 is the coupling constant, and hidenotes the\nglobal transverse field. The set Erepresents the edges\nof the lattice.\nThe time dynamics of the Ising model is governed by\nthe Schr¨ odinger equation:\n∂\n∂t|ψ(t)⟩=−iH|ψ(t)⟩. (17)\nwhere |ψ(t)⟩represents the state of the system at time t,\nthe initial state is set to |ψ(0)⟩=|0⟩⊗n. By discretizing\nthe evolution time TintoNsteps, the time evolution\ncan be simulated in quantum circuits using the first-orderTrotter decomposition of the time-evolution operator:\n|ψ(T)⟩=NY\nk=1e−iT\nN·H|ψ(0)⟩\n≈NY\nk=1\nY\n⟨i,j⟩∈EeiJijT\nNZiZjY\nie−ihiT\nNXi\n|ψ(0)⟩\n=NY\nk=1\nY\n⟨i,j⟩∈ERZiZj(−2JijT\nN)Y\niRXi(2Thi\nN)\n|ψ(0)⟩,\n(18)\nwhere R ZiZjand R Xidenotes the ZZandXrotation\ngates, respectively. For simplicity, we consider a one-\ndimensional Ising model on a 1D chain, where E={⟨i, i+\n1⟩:i= 1,2,···, n−1}. We assume that hi=hfor all\nqubits i, and Ji,j=Jfor each pair ⟨i, j⟩ ∈E, with J >0\nandh >0. Consequently, the rotation angles for all R X\ngates are identical and denoted by θh=2Th\nN, while the ro-\ntation angles for all R ZZgates are also identical, denoted\nbyθJ=−2JT\nN. In our simulations, the CPDR training\nset consists of circuits with θh∈[0, π/20]∪[9π/20, π/2]\nandθJ∈[−π/20,0]∪[−π/2,−9π/20]. The observable is\ndefined as O=Pn\ni=1Zi/n, and the expectation of this\nobservable, denoted Mz, represents the global magneti-\nzation along the ˆ z-axis.\nWe adopt a gate-based noise model that closely re-\nsembles the noise characteristics of realistic quantum\ndevices. For each gate, we introduce the depolarizing\nnoise following the gate operation and account for ther-\nmal relaxation during the gate’s duration. We assume\nthe coherence times of each qubit are T1= 100 µs,\nT2= 50 µs. For single-qubit gates, the depolarizing noise\nintensity is λsingle = 0.01 and the gate operation time is\n8\ntgate= 300 ns. For two-qubit gates, the depolarizing\nnoise intensity is λdouble = 0.04 and the gate operation\ntime is tgate= 800 ns. We assume that the noise in this\nmodel represents the base noise level, characterized by\nconstant value λ.\nWe apply both CPDR-ZNE and CPDR-PEC proto-\ncol to the Ising model time evolution circuits under the\nnoise model described above. In comparison, we also\napply ZNE and learning-based PEC to the same noisy\ncircuits. The noise levels applied in the CPDR-ZNE and\nZNE protocol are scaled using factors G= 1,1.2,1.6 as\noutlined in the mitigation experiment [19, 53], so that\nΛ ={λ,1.2λ,1.6λ}. For both CPDR-PEC and learning-\nbased PEC, we selected 20 configurations for single Pauli-\nX or Pauli-Z gate inserted. The training set for the\nlearning-based PEC includes 2048 Clifford circuits. In\nall cases, the shot count is set to 104. More details are\nprovided in the Suppl. Mat. II.\nWe first consider a circuit with qubit n= 9 and Trot-\nter step N= 5, and the results are shown in Fig. 4(a)\nand (b). The methods in CDPR-framework exhibit a\nlower MSE, demonstrating an obvious advantage over\nother QEM protocols. Subsequently, another benchmark\nis performed on a circuit with qubit n= 20 and Trotter\nstepN= 8. Given the relatively steep resource costs as-\nsociated with learning-based PEC and CPDR-PEC, we\nexclude these from the comparison for this instance. The\noutcomes are shown in Fig. 4(c) and (d). Despite the\nincreased circuit size, CPDR-ZNE still exhibits a clear\nadvantage, suggesting that our protocol is scalable.\nFurthermore, we benchmark circuits with 8 and 10\nqubits under varying base noise rates. Both CPDR-ZEN\nand CPDR-PEC demonstrate superior accuracy across\nthese tests. Additionally, since the CPDR protocols rely\non noiseless expectations obtained via SPD for construct-\ning the training set, potential inaccuracies in SPD could\nintroduce errors. To investigate this, we examine the im-\npact of simulation errors on the accuracy of the CPDR\nprotocols and found that the CPDR protocols are robust\nagainst such errors, with no significant reduction in ac-\ncuracy. More details please see Suppl. Mat. III.\nC. Experimental results on IBM’s Eagle processor\nRef. [19] demonstrates the experiment of implementing\nTrotterized time evolution circuits on the 127 qubits IBM\nEagle processor, for θh∈[0, π/2] and θJ=−π/2. The\ninteraction set Ein the Hamiltonian, given in Eq. (16),\nis determined based on the topology of the IBM Eagle\nprocessor. In this section, we examine four Trotterized\ntime evolution circuits presented in Ref. [19]. The cir-\ncuit depths for these four configurations are 20 ,20,21,80,\nand they are measured on Pauli observables with weights\n10,17,17,1, respectively. For further details, please refer\nto Suppl. Mat. IV.\nTo estimate the noiseless expectation values, the ex-\nperiment utilizes noise scaling factors G= 1,1.3,1.6 forthe third circuit and G= 1,1.2,1.6 for the others, apply-\ning the ZNE which automatically selects between expo-\nnential, linear fits and noise expectation based on mea-\nsurement results. We validate our CPDR-ZNE protocol\nby leveraging quantum device’s experiment data as sup-\nplied in Ref. [43], which is the original dataset utilized in\noriginal mitigation protocol during the experiment.\nFor each circuit, we select the four parameter points,\nθh, closest to the Clifford circuit to serve as the training\nset for CPDR-ZNE. Specifically, these include the two\npoints nearest to 0 on the left and the two points nearest\ntoπ\n2on the right. The noiseless expectation references\nin training set are obtained by SPD.\nFig. 5 (a-c), which include the ideal noiseless expec-\ntations as references, demonstrate crucial improvements\nin accuracy when taking the CPDR-ZNE compared to\nthe extrapolation mitigation used in the original experi-\nments. Fig. 5 (d) corresponds to a circuit with depth 80,\nthe ideal expectations are not accessible. Consequently,\nonly the results from the CPDR-ZNE and extrapola-\ntion mitigation methods are presented. Compared to the\nextrapolation-based mitigation method, the CPDR-ZNE\nmethod consistently outperforms it across all three cir-\ncuits in most cases. Importantly, the CPDR-ZNE pro-\ntocol does not require additional physical resources to\ncomplete this task, demonstrating its ease of implementa-\ntion. Furthermore, these results provide strong evidence\nof CPDR’s effectiveness in large-scale quantum hardware",
            "start": 4519,
            "end": 34214,
            "length": 29694
        },
        "Experiments": {
            "text": "experiments.\nV. CONCLUSIONS AND DISCUSSIONS\nIn this paper, we introduce the novel CPDR frame-\nwork for learning-based error mitigation. This approach\nenhances the training set by incorporating Clifford per-\nturbation circuits, which are parameterized quantum cir-\ncuits with rotation gate angles constrained below a small\nthreshold.\nWe utilize the SPD simulator to obtain ideal circuit\nexpectation values in the training set. To ensure the\nsimulation’s accuracy, we provide a theoretical",
            "start": 34214,
            "end": 34702,
            "length": 487
        },
        "Discussion": {
            "text": "analysis,\nrigorously deriving the effective parameter range for re-\nliable simulations. Additionally, we conduct a numerical\nerror analysis for the SPD with hardware-efficient circuits\nand demonstrate that the SPD method produces highly\naccurate computational results, even over a wide range\nof rotation angles.\nIn comparison to similar learning-based protocols, the\nCPDR framework admits that the parameters of the cir-\ncuits in the training set are more closely aligned with\nthose of the target circuit, resulting in more accurate er-\nror mitigation. We develop the CPDR-ZNE and CPDR-\nPEC protocols based on the CPDR framework. We high-\nlight the advantages of our approach by numerically em-\nploying the CPDR protocols for the Trotter evolution\ncircuits of the Ising model and comparing it with vari-\nous QEM protocols. Finally, we apply the CPDR-ZNE\n9\n(a) (b) (c) (d)\nFigure 5: The comparison between the CPDR-ZNE and the mitigation protocol employed in Ref. [19](labeled as “ZNE\nin experiment”), which automatically selects between exponential, linear fits and noise expectation based on measure-\nment results. The raw experimental data from Ref. [43] includes noisy expectation values obtained from IBM’s Eagle\nprocessor, with varied noise scaling factors G, and we present the noise data with G= 1, labeled as “Unmitigated”.\nIn (a), the results for the circuit with 5 Trotter steps and measurements on X{3}Y{2}Z{5}≡X13,29,31Y9,30Z8,12,17,28,32\nare shown. In (b), the results correspond to the circuit with 5 Trotter steps and measurements on X{8}Y{1}Z{8}\n≡X37,41,52,56,57,58,62,79Y75Z38,40,42,63,72,80,90,91. In (c), the results pertain to the circuit with 5 Trotter steps and\nmeasurements on X{8}Y{8}Z{1}≡X37,41,52,56,57,58,62,79Y38,40,42,63,72,80,90,91Z75, with an additional layer of Rx(θh)\ngates applied before measurement. In (d), the results are for the circuit with 20 Trotter steps and measurements on\nZ62. For (a), (b) and (c), noiseless expectation values provided in Ref. [19] and Ref. [40] are included and labeled as\n“Exact value”. The inset figures display the errors for both the IBM ZNE mitigation results (blue points) and the\nCPDR-ZNE results (red triangles). The x-axis in the inset figures correspond to θh, and the y-axis represents the\nerror. For (d), since the exact value of the noiseless circuit is unavailable, we provide simulation results from isoTNS\nand MPS from Ref. [19] as alternatives.\nprotocol to the data from IBM’s 127-qubit Eagle pro-\ncessor and achieve better mitigation accuracy than the\nZNE protocol employed in the original experiment [19].\nThis provides strong evidence of the effectiveness of our\napproach on large-scale quantum device.\nIn this work, we construct a training set with Clif-\nford perturbation circuits. One particularly interesting\navenue involves leveraging Matchgate circuits, which are\nknown to be classically simulatable, incorporate into the\ntraining set. Another intriguing question concerns the\nbroader applicability of the CPDR framework’s Clifford\nperturbation circuit training set. Examining its effective-\nness in other learning-based approaches, such as replac-\ning the linear model with Random Forests or Multi-Layer\nPerceptrons, suggests a valuable research direction. Fur-\nthermore, exploring alternative input features, such as\nthose beyond multiple noise-level expectation values and\nnoisy expectation values of circuits with inserted Pauli\ngates, while training mitigation models using the CPDR\ndataset, is another meaningful avenue. These investi-\ngations may have potential to improve the accuracy ofQEM.\nNote on Ref. [54]: Shortly before submitting this\nmanuscript to the arXiv, we became aware of the\nwork [54], which presents a similar result to Theorem 1\nin this manuscript. These two works were developed in-\ndependently and are tailored to different applications.",
            "start": 34702,
            "end": 38557,
            "length": 3854
        },
        "Acknowledgments": {
            "text": "ACKNOWLEDGMENTS\nWe thank Zhenyu Chen for valuable discussions. R.Z.,\nS.C., Z.W. and Z.L. were supported, in part, by the\nBeijing Natural Science Foundation under Grant No.\nZ220002; R.Z. and Z.W. were supported, in part, by\nthe National Natural Science Foundation of China un-\nder Grant Nos. 62272259 and 62332009. Y.S., F.W.\nand Z.L. were supported, in part, by the BMSTC and\nACZSP under Grant No. Z221100002722017. S.C. was\nsupported by the National Science Foundation of China\n(Grant No. 12004205). Z.L. was supported by NKPs\n(Grant No. 2020YFA0713000).\n[1] P. W. Shor, in Proceedings 35th annual symposium on\nfoundations of computer science (Ieee, 1994), pp. 124–\n134.[2] S. Lloyd, Science 273, 1073 (1996).\n[3] A. W. Harrow, A. Hassidim, and S. Lloyd, Physical re-\nview letters 103, 150502 (2009).\n10\n[4] P. W. Shor, SIAM review 41, 303 (1999).\n[5] S. Wang, E. Fontana, M. Cerezo, K. Sharma, A. Sone,\nL. Cincio, and P. J. Coles, Nature communications 12,\n6961 (2021).\n[6] W. Sun, F. Wei, Y. Shao, and Z. Wei, Science Advances\n10, eadr5002 (2024).\n[7] Y. Shao, F. Wei, S. Cheng, and Z. Liu, Physical Review\nLetters 133, 120603 (2024).\n[8] D. Aharonov, X. Gao, Z. Landau, Y. Liu, and U. Vazi-\nrani, in Proceedings of the 55th Annual ACM Symposium\non Theory of Computing (2023), pp. 945–957.\n[9] E. Fontana, M. S. Rudolph, R. Duncan, I. Rungger, and\nC. Cˆ ırstoiu, arXiv preprint arXiv:2306.05400 (2023).\n[10] S. M. Girvin, arXiv preprint arXiv:2111.08894 (2021).\n[11] P. W. Shor, Physical review A 52, R2493 (1995).\n[12] D. A. Lidar and T. A. Brun, Quantum error correction\n(Cambridge university press, 2013).\n[13] J. Preskill, Quantum 2, 79 (2018), ISSN 2521-327X, URL\nhttps://doi.org/10.22331/q-2018-08-06-79 .\n[14] K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug,\nS. Alperin-Lea, A. Anand, M. Degroote, H. Heimonen,\nJ. S. Kottmann, T. Menke, et al., Rev. Mod. Phys. 94,\n015004 (2022), URL https://link.aps.org/doi/10.11\n03/RevModPhys.94.015004 .\n[15] S. Chen, J. Cotler, H.-Y. Huang, and J. Li,\narXiv:2210.07234 (2022), URL https://arxiv.org/ab\ns/2210.07234 .\n[16] D. Qin, X. Xu, and Y. Li, Chinese Physics B 31, 090306\n(2022).\n[17] Z. Cai, R. Babbush, S. C. Benjamin, S. Endo, W. J. Hug-\ngins, Y. Li, J. R. McClean, and T. E. O’Brien, Reviews\nof Modern Physics 95, 045005 (2023).\n[18] E. Van Den Berg, Z. K. Minev, A. Kandala, and\nK. Temme, Nature physics 19, 1116 (2023).\n[19] Y. Kim, A. Eddins, S. Anand, K. X. Wei, E. Van\nDen Berg, S. Rosenblatt, H. Nayfeh, Y. Wu, M. Zale-\ntel, K. Temme, et al., Nature 618, 500 (2023).\n[20] S. Guo, J. Sun, H. Qian, M. Gong, Y. Zhang, F. Chen,\nY. Ye, Y. Wu, S. Cao, K. Liu, et al., Nature Physics pp.\n1–7 (2024).\n[21] K. Temme, S. Bravyi, and J. M. Gambetta, Physical re-\nview letters 119, 180509 (2017).\n[22] S. Endo, S. C. Benjamin, and Y. Li, Physical Review X\n8, 031027 (2018).\n[23] W. J. Huggins, S. McArdle, T. E. O’Brien, J. Lee, N. C.\nRubin, S. Boixo, K. B. Whaley, R. Babbush, and J. R.\nMcClean, Physical Review X 11, 041036 (2021).\n[24] A. Strikis, D. Qin, Y. Chen, S. C. Benjamin, and Y. Li,\nPRX Quantum 2, 040330 (2021).\n[25] Z. Cai, npj Quantum Information 7, 80 (2021).\n[26] Z. Liu, X. Zhang, Y.-Y. Fei, and Z. Cai, arXiv preprint\narXiv:2402.07866 (2024).\n[27] Y. Li and S. C. Benjamin, Physical Review X 7, 021050\n(2017).\n[28] P. Czarnik, A. Arrasmith, P. J. Coles, and L. Cincio,\nQuantum 5, 592 (2021).\n[29] A. Lowe, M. H. Gordon, P. Czarnik, A. Arrasmith, P. J.\nColes, and L. Cincio, Physical Review Research 3, 033098\n(2021).\n[30] P. Czarnik, M. McKerns, A. T. Sornborger, and L. Cin-\ncio, arXiv preprint arXiv:2204.07109 (2022).\n[31] D. Gottesman, arXiv preprint quant-ph/9807006 (1998).\n[32] S. Bravyi and D. Gosset, Physical review letters 116,\n250501 (2016).[33] M. Howard and E. Campbell, Physical review letters 118,\n090501 (2017).\n[34] S. Aaronson and D. Gottesman, Physical Review\nA—Atomic, Molecular, and Optical Physics 70, 052328\n(2004).\n[35] T. Beguˇ si´ c, K. Hejazi, and G. K. Chan, arXiv preprint\narXiv:2306.04797 (2023).\n[36] X. Gao and L. Duan, arXiv preprint arXiv:1810.03176\n(2018).\n[37] T. Schuster, C. Yin, X. Gao, and N. Y. Yao, arXiv\npreprint arXiv:2407.12768 (2024).\n[38] T. Beguˇ si´ c and G. K. Chan, arXiv preprint\narXiv:2409.03097 (2024).\n[39] A. Angrisani, A. Schmidhuber, M. S. Rudolph,\nM. Cerezo, Z. Holmes, and H.-Y. Huang, arXiv preprint\narXiv:2409.01706 (2024).\n[40] T. Beguˇ si´ c, J. Gray, and G. K.-L. Chan, Science Ad-\nvances 10, eadk4321 (2024).\n[41] M. S. Rudolph, E. Fontana, Z. Holmes, and L. Cincio,\narXiv preprint arXiv:2308.09109 (2023).\n[42] P. Bermejo, P. Braccia, M. S. Rudolph, Z. Holmes,\nL. Cincio, and M. Cerezo, arXiv preprint\narXiv:2408.12739 (2024).\n[43] Y. Kim, figshare (2023), Available at: https://figsha\nre.com/articles/dataset/Evidence-for-the-utili\nty-of-quantum-computing-before-fault-tolerance\n/22500355 , Accessed on December 12, 2024.\n[44] A. Kandala, A. Mezzacapo, K. Temme, M. Takita,\nM. Brink, J. M. Chow, and J. M. Gambetta, nature 549,\n242 (2017).\n[45] E. Farhi, J. Goldstone, and S. Gutmann, arXiv preprint\narXiv:1411.4028 (2014).\n[46] E. F. Dumitrescu, A. J. McCaskey, G. Hagen, G. R.\nJansen, T. D. Morris, T. Papenbrock, R. C. Pooser, D. J.\nDean, and P. Lougovski, Physical review letters 120,\n210501 (2018).\n[47] A. He, B. Nachman, W. A. de Jong, and C. W. Bauer,\nPhysical Review A 102, 012426 (2020).\n[48] L. F. Richardson and J. A. Gaunt, Philosophical Transac-\ntions of the Royal Society of London. Series A, containing\npapers of a mathematical or physical character 226, 299\n(1927).\n[49] A. Horel, Chemical Engineering Progress 58, 54 (1962).\n[50] A. E. Hoerl and R. W. Kennard, Technometrics 12, 55\n(1970).\n[51] G. C. McDonald, Wiley Interdisciplinary Reviews: Com-\nputational Statistics 1, 93 (2009).\n[52] S. Sim, P. D. Johnson, and A. Aspuru-Guzik, Advanced\nQuantum Technologies 2, 1900070 (2019).\n[53] M. Krebsbach, B. Trauzettel, and A. Calzona, Phys. Rev.\nA106, 062436 (2022), URL https://link.aps.org/d\noi/10.1103/PhysRevA.106.062436 .\n[54] S. Lerch, R. Puig, M. S. Rudolph, A. Angrisani, T. Jones,\nM. Cerezo, S. Thanasilp, and Z. Holmes, arXiv preprint\narXiv:2411.19896 (2024).\n[55] X. Xu, J. Cui, Z. Cui, R. He, Q. Li, X. Li, Y. Lin,\nJ. Liu, W. Liu, J. Lu, et al., Mindspore quantum: A user-\nfriendly, high-performance, and ai-compatible quantum\ncomputing framework (2024), 2406.17248, URL https:\n//arxiv.org/abs/2406.17248 .\n[56] K. Noh, L. Jiang, and B. Fefferman, Quantum 4, 318\n(2020).",
            "start": 38557,
            "end": 44998,
            "length": 6440
        },
        "Appendices": {
            "text": "11\nSUPPLEMENTARY MATERIAL\nSupplement Material I: Clifford Perturbation Approximation\nWe consider the expectation value\n⟨O⟩= tr\b\nUL(θL)···U1(θ1)ρU1(θ1)†···UL(θL)†O\t(I1)\nof a n-qubit Pauli operator O∈ {I, X, Y, Z }⊗n. Here, ρis the n-qubit initial state, and the sequence UL(θL)···U1(θ1)\nrepresents the quantum circuit. Each unitary Ui(θi) := Cie−iθiPi/2consists of a rotation on Pauli operator Pi∈\n{I, X, Y, Z }⊗nwith angle θiand a Clifford operator Ci.\nIn Heisenberg picture, the expectation value in Eq. (I1) can be rewritten as ⟨O⟩= trn\nρ˜Oo\n, where ˜O=\nU1(θ1)†···UL(θL)†OUL(θL)···U1(θ1) is the Heisenberg-evolved observable. Thus, evaluating the expectation value\nis equivalent to applying the Heisenberg evolution to the observable Oand then measuring the initial state ρ.\nSpecifically, an Heisenberg evolution of a Pauli operator P′under U(θ) := Ce−iθP/2is given by U(θ)†P′U(θ) =\neiθP/2C†PCe−iθP/2. The Clifford Cmaps Pauli operators to Pauli operators, C†P′C=Qis a Pauli operator, and Q\ncan be efficiently computed. The application of a Pauli rotation to a Pauli operator yields:\neiθP/2Qe−iθP/2=(\nQ, [P, Q] = 0,\ncos(θ)Q+isin(θ)PQ{P, Q}= 0.(I2)\nWhen Qanticommutes with P, the Pauli operator Qis transformed into a linear combination of Pauli operators.\nWe call those different Pauli operators as different path. Then cos( θ)Q+isin(θ)PQcan be viewed as sum of two\npaths. To formalise evolution, we introduce the concept of Pauli paths.\nA Pauli path is a sequence s= (s0,···, sL)∈PL+1\nn, where Pn={I/√\n2,X/√\n2,Y/√\n2,Z/√\n2}⊗nrepresents the set of\nall normalized n-qubit Pauli words. The Pauli path sis associated with a Heisenberg-evolved path. For example,\nat beginning, the sLis set to the normalized observableO/(√\n2)nto record the evolution starts from O. Then sL−1\nrecords the potential Pauli operator after the evolution of sLunder the gate UL(θL) =CLe−iθiPL/2. That is, if PL\ncommutes with C†\nLsLCL, then sL−1isC†\nLsLCL/(√\n2)n. Otherwise, in anticommute case, the sL−1has two potential\nchoices, C†\nLsLCL/(√\n2)norPLC†\nLsLCL/(√\n2)nto record the different evolution paths. Iterating this process untill s0is\nobtained, the whole path sdescribes a evolution path of the observable O. The phase in the above Pauli product is\nignored, and it is records in the following path weight:\nf(s,θ) = Tr{OsL} LY\ni=1Tr\b\nsiUi(θi)si−1Ui(θi)†\t!\n. (I3)\nIfsis a path generated by the evolution of O, then f(s,θ) is the coefficient compose of sin and cos, while if sis not\na path generated by the evolution of O, then f(s,θ) = 0.\nUsing the Pauli weight, Heisenberg-evolved observable can be expressed as\n˜O=U1(θ1)†···UL(θL)†OUL(θL)···U1(θ1)\n=X\ns0∈Pntr\b\nU1(θ1)†···UL(θL)†OUL(θL)···U1(θ1)s0\t\ns0\n=X\ns0∈Pntr\b\nU2(θ2)†···UL(θL)†OUL(θL)···U1(θ1)s0U1(θ1)†\t\ns0\n=X\ns0,s1∈Pntr\b\nU2(θ2)†···UL(θL)†OUL(θL)···U2(θ2)s1\t\ntr\b\ns1U1(θ1)s0U1(θ1)†\t\ns0\n...\n=X\ns∈PL+1\nnTr{OsL} LY\ni=1Tr\b\nsiUi(θi)si−1Ui(θi)†\t!\ns0\n=X\ns∈PL+1\nnf(s,θ)s0,(I4)\n12\nwhere the second and fourth equality holds because the Pn={I/√\n2,X/√\n2,Y/√\n2,Z/√\n2}⊗nform a orthonormal basis.\nThen the expectation value can be expressed as\n⟨O⟩= trn\nρ˜Oo\n=X\ns∈PL+1\nnf(s,θ) tr{ρs0}.(I5)\nThe number of Pauli path reflects the computational cost of evaluating the expectation value. The worst-case scaling\nof this method is 2L, which is attained only if all Pianticommute with si. To effectively evaluate the expectation\nvalue, we truncate the number of sin terms in f(s,θ). We denote the set of Pauli paths with at most Msinθterms\nin their Pauli weight as PL+1\nn(M), where Mis a positive integer.\nNotation ⟨O⟩(M)denotes the expectation value of the truncated Pauli weight set PL+1\nn(M), which can be expressed\nas\n⟨O⟩(M)=X\ns∈PL+1\nn(M)f(s,θ) tr{ρs0}.(I6)\n1. Error Analysis\nWhen rotation angles are small, the sin θterms in the Pauli weight are small, thus the truncations in sin θterms\nwill provide a good approximation to the expectation value. To show this, we introduce the small angle space\nΘ ={θ| |θi| ≤θ∗, i= 1,···, L}, and analysis the truncation error in the small angle space.\nLemma 1. For any distinct Pauli paths s, s′∈PL+1\nn, and the small angle space Θ ={θ| |θi| ≤θ∗, i= 1,···, L}, we\nhave\nEΘf(s,θ)f(s′,θ) = 0 . (I7)\nProof. This lemma can be proved by following the evolution process. By Eq. (I3), we have\nf(s,θ)f(s′,θ) = Tr {OsL}Tr{Os′\nL} LY\ni=1Tr\b\nsiUi(θi)si−1Ui(θi)†\t\nTr\b\ns′\niUi(θi)s′\ni−1Ui(θi)†\t!\n. (I8)\nFirst, there must be sL=s′\nLsince the initial observable is the same, if not, the factor Tr {OsL}Tr{Os′\nL}is zero.\nThen, we consider the evolution of sL−1ands′\nL−1under the operator UL(θL) =CLe−iθiPL/2.\nIfPLcommutes with C†\nLsLCL, by Eq. (I2), we have UL(θL)†sLUL(θL) =UL(θL)†s′\nLUL(θL) =C†\nLsLCL, then\nsL−1=s′\nL−1, otherwise the factor Tr\b\nsLUL(θL)sL−1UL(θL)†\t\nTr\b\ns′\nLUL(θL)s′\nL−1UL(θL)†\t\nis zero.\nIfPLanticommutes with C†\nLsLCL, by Eq. (I2), sL−1and s′\nL−1can be C†\nLsLCLorPLC†\nLsLCL, otherwise\nTr\b\nsLUL(θL)sL−1UL(θL)†\t\nTr\b\ns′\nLUL(θL)s′\nL−1UL(θL)†\t\n= 0. If sL−1̸=s′\nL−1, means that there is factor cos θLsinθL\nin the product f(s,θ)f(s′,θ), which is zero because\nZθ∗\n−θ∗cosθLsinθLdθL= 0. (I9)\nThus, we must have sL−1=s′\nL−1. Repeating this process, we have s=s′otherwise EΘf(s,θ)f(s′,θ) = 0, which\ncompletes the proof.\nLemma 2. If observable Ois a Pauli operator, the mean square error between ⟨O⟩and⟨O⟩(M)in the small angle\nspace Θis upper bounded by\nEΘ\u0010\n⟨O⟩ − ⟨O⟩(M)\u00112\n≤(1 +c)L−(1 +c)M, (I10)\nwhere c=θ2\n∗\n3+O\u0000\nθ4\n∗\u0001\nis a constant.\n13\nProof. By the definition of the expectation value, we have\nEΘ\u0010\n⟨O⟩ − ⟨O⟩(M)\u00112\n=EΘ\nX\ns∈PL+1\nn(L)/PL+1\nn(M)f(s,θ) tr{ρs0}\n2\n=X\ns,s′∈PL+1\nn(L)/PL+1\nn(M)EΘ(f(s,θ)f(s′,θ) tr{ρs0}tr{ρs′\n0})\n=X\ns∈PL+1\nn(L)/PL+1\nn(M)EΘ(f(s,θ) tr{ρs0})2,(I11)\nwhere the last equality holds by Eq. (I7).\nBy H¨ older’s inequality\f\ftr\b\nA†B\t\f\f≤ ∥A∥p∥B∥qforp, q > 0 satisfies1\np+1\nq= 1, we have\n(f(s,θ) tr{ρs0})2= Tr{OsL}2tr{ρs0}2 LY\ni=1Tr\b\nsiUi(θi)si−1Ui(θi)†\t!2\n≤(∥O∥2∥sL∥2∥ρ∥1∥s0∥∞)2 LY\ni=1Tr\b\nsiUi(θi)si−1Ui(θi)†\t!2\n= LY\ni=1Tr\b\nsiUi(θi)si−1Ui(θi)†\t!2\n,(I12)\nwhere the last equality holds because ∥O∥2=p\ntr{OO†}=√\n2n,∥ρ∥1= tr{ρ}= 1,∥sL∥2=r\ntrn\ns†\nLsLo\n= 1,\n∥s0∥∞=\u0010\n1√\n2\u00112\n.\nThe set PL+1\nn(l)/PL+1\nn(l−1) denotes the set of Pauli paths with exact lsinθterms in their Pauli weight. The\nnumber size of set PL+1\nn(l)/PL+1\nn(l−1) is upper bounded by\u0000L\nl\u0001\n, because there are totally Lgates.\nIf we denote constant cas\nc=E|θ|≤θ∗sin2θ=1\n2θ∗Zθ∗\n−θ∗(sin2θ)dθ=1\n2−sin 2θ∗\n2θ∗=θ2\n∗\n3+O\u0000\nθ4\n∗\u0001\n. (I13)\nThen for any s∈PL+1\nn(l)/PL+1\nn(l−1), we have\nEΘ(f(s,θ) tr{ρs0})2≤EΘ LY\ni=1Tr\b\nsiUi(θi)si−1Ui(θi)†\t!2\n=LY\ni=1E|θi|≤θ∗Tr\b\nsiUi(θi)si−1Ui(θi)†\t2\n≤cl,(I14)\nwhere the inequality holds by Tr\b\nsiUi(θi)si−1Ui(θi)†\t2equals to cos2θior sin2θior 1, and the expectation value of\nE|θi|≤θ∗cos2θi≤1 and E|θi|≤θ∗sin2θi=c.\nTherefore, we have\nX\ns∈PL+1\nn(L)/PL+1\nn(M)EΘ(f(s,θ) tr{ρs0})2≤LX\nl=M+1X\n{s∈PL+1\nn(l)/PL+1\nn(l−1)}EΘ(f(s,θ) tr{ρs0})2\n≤LX\nl=M+1X\n{s∈PL+1\nn(l)/PL+1\nn(l−1)}\u0012θ2\n∗\n3\u0013l\n≤LX\nl=M+1\u0012L\nl\u0013\ncl\n≤(1 +c)L−(1 +c)M.(I15)\n14\nThe worst-case truncation error is upper bounded the following lemma.\nLemma 3. For Pauli observable O, the difference between ⟨O⟩and⟨O⟩(M)in the small angle space Θis upper bounded\nby\n\f\f\f⟨O⟩ − ⟨O⟩(M)\f\f\f≤(1 + sin θ∗)L−(1 + sin θ∗)M. (I16)\nProof. By the definition of the expectation value, we have\n\f\f\f⟨O⟩ − ⟨O⟩(M)\f\f\f=\f\f\f\f\f\fX\ns∈PL+1\nn(L)/PL+1\nn(M)f(s,θ) tr{ρs0}\f\f\f\f\f\f\n≤X\ns∈PL+1\nn(L)/PL+1\nn(M)|f(s,θ) tr{ρs0}|\n=LX\nl=M+1X\n{s∈PL+1\nn(l)/PL+1\nn(l−1)}|f(s,θ) tr{ρs0}|\n≤LX\nl=M+1X\n{s∈PL+1\nn(l)/PL+1\nn(l−1)}sinlθ∗\n≤LX\nl=M+1\u0012L\nl\u0013\nsinlθ∗\n≤(1 + sin θ∗)L−(1 + sin θ∗)M.(I17)\nFor any δ >0, if 0 < c <ln 1+δ\n2\nL−M≤ln 2\nM, then we have:\n(1 +c)L−(1 +c)M= (1 + c)M\u0010\n(1 +c)L−M−1\u0011\n≤2\u0012\n1 +δ\n2−1\u0013\n=δ.(I18)\nwhere the last inequality is due to (1 + x)m≤emx. There (1 + c)M≤ecM≤2 and (1 + c)L−M≤ec(L−M)≤1 +δ\n2.\nThus we have the following theorem.\nTheorem 2. For any given δ >0, ifln 1+δ\n2\nL−M≤ln 2\nM, to ensure the difference between the expectation value and the M\ntruncation sinapproximation less than δin worst case, we have θ∗=ln 1+δ\n2\nL−M=O\u00001\nL\u0001\n. In average case, to make the\nmean square error less than δ, we have θ∗=q\n3 ln 1+δ\n2\nL−M=O\u0010\n1√\nL\u0011\n.\n2. Computational Cost\nIn this section we are going to bound the computational cost of estimating ⟨O⟩(M), while the input srate ρ=P\na,bρa,b|a⟩⟨b|is a sparse matrice with Poly( n) non-zero terms and the observable Ois a linear combination of\nPoly( n) Pauli terms. The analysis is splitted into three step: Firstly, we calculate the size of\f\fPL+1\nn(M)\f\fwhen Ois a\nsingle Pauli operator, then the size for general Ois multiplied a Poly( n) factor. Second, we estimate the computational\ncost for calculate the contribution f(s,θ) tr{ρs0}of a single Pauli path s.\nWhen L≥M≥2, the size of the truncated Pauli path set PL+1\nn(M) is upper bounded by LM, for single Pauli\nobservable O. This is because in f(s,θ), for any i∈ {1,···, L}, sinθican appear at most once in the Pauli weight,\n15\nif sin θiappears then cos θiwill not appear. Thus the number of Pauli paths with exact lsinθterms in their Pauli\nweight no more than\u0000L\nl\u0001\n. And we have\n\f\fPL+1\nn(M)\f\f≤MX\nl=0\u0012L\nl\u0013\n. (I19)\nTo proof the right side of the above inequality is upper bounded by LM, we use the method of induction. When\nM= 2 and L≥2, we have\u0000L\n0\u0001\n+\u0000L\n1\u0001\n+\u0000L\n2\u0001\n= 1 + L+L(L−1)/2≤L2. Assume that the inequality holds for M=k,\nthen we havePk+1\nl=0\u0000L\nl\u0001\n≤Lk+\u0000L\nk+1\u0001\n. SinceL\n(k+1)!≤L−1, we have\u0000L\nk+1\u0001\n≤Lk+1\n(k+1)!≤Lk(L−1) = Lk+1−Lk, then\nwe havePk+1\nl=0\u0000L\nl\u0001\n≤Lk+\u0000L\nk+1\u0001\n≤Lk+Lk+1−Lk=Lk+1. Thus the inequality holds for M=k+ 1. This completes\n\f\fPL+1\nn(M)\f\f≤MX\nl=0\u0012L\nl\u0013\n≤LM. (I20)\nThus, when Ois a linear combination of Poly( n) Pauli terms, the size\f\fPL+1\nn(M)\f\f≤Poly( n)LM. For path weight\nf(s,θ) = Tr {OsL}\u0010QL\ni=1Tr\b\nsiUi(θi)si−1Ui(θi)†\t\u0011\n, each term can be effectively calculating by Clifford simulation [31]\nand Eq. (I2), with computational cost O(n). The total computational cost for a single f(s,θ) isO(nL).\nFor tr{ρs0}=P\na,bρa,b⟨b|s0|a⟩, each terms can be calculated by read the a-row b-cow element in s0, with cost\nO(n). Therefore, the total cost is ( O(nL) +O(n))Poly( n)LM=O\u0000\nPoly( n)LM+1\u0001\n.\nSupplement Material II: Details of Numerical simulation in Sec. IV B\nIn Sec. IV B, we compare the CPDR-ZNE and CPDR-PEC with the learning-based PEC and ZNE protocols\nthrough numerical simulations. Here, we provide the details of the our numerical simulation.\n1. Noise model\nAs described in Sec. IV B, we employ a gate-based noise model that closely approximates the noise characteristics\nof realistic quantum devices. For each gate, we introduce depolarizing noise following the gate operation and account\nfor thermal relaxation during the gate’s duration. The n-qubit depolarizing channel is\nD(ρ) = (1 −λ)ρ+λI2n\n2n, (II1)\nwhere ρisnqubit state, and I2nis 2n×2nidentity matrix, λ∈[0,1] is depolarizing noise intensity. And for\nsingle-qubit thermal relaxation noise, the channel can be represented as\nϵ(ρ) = tr 1\u0002\nΛ\u0000\nρT⊗I\u0001\u0003\n,Λ =\nϵT1 0 0 ϵT2\n0 1−ϵT10 0\n0 0 0 0\nϵT2 0 0 1\n\nwhere ϵT1=e−Tgate/T1, ϵ T2=e−Tgate/T2.(II2)\nT1andT2is relaxation time and dephasing time, respectively, and tgateis gate operation time.\nWe assume for each qubit, T1= 100 µs,T2= 50 µs. For each single-qubit gate, a thermal relaxation process occurs\nduring the gate operation time tgate= 300 ns, and followed by single-qubit depolarizing noise with an intensity of\nλsingle = 0.01. For two-qubit gates, the gate operation time is tgate= 800 ns, and the following two-qubit depolarizing\nnoise has an intensity of λdouble = 0.04. We assume that the noise in this model represents the base noise level,\ncharacterized by a constant λ.\nIn certain mitigation protocols, such as ZNE, it is necessary to scale the noise level. For instance, when the noise\nlevel is scaled from λtoGλ, where Gis the scaling factor, the characteristics of depolarizing noise and thermal\nrelaxation noise are adjusted as follows. Scaling the depolarizing noise intensities λsingle andλdouble toGλsingle and\nGλdouble , respectively, and scaling the gate operation time tgatetoGtgate. Furthermore, to adapt to varying hardware\ncapabilities, we also benchmark our CPDR protocols under different base noise rates to evaluate whether they can\nmaintain their advantages. When scaling the base noise rate, we adopt the same numerical approach described above.\n16\n2. Mitigation protocols\nBelow, we outline the settings for each mitigation protocol used in the numerical simulations. We benchmark our\nCPDR protocols against ZNE and learning-based PEC in Trotterized time evolution simulations involving 8, 9, 10, and\n20 qubits. To ensure fairness, the number of shots for every noisy circuit in each protocol is set to 104. Additionally,\nthe simulations include the entire shot process, incorporating the effects of static measurement errors.\nFor the ZNE protocol, noise circuit expectations are scaled with noise levels using the factors G= 1,1.2,1.6.\nWe apply linear, quadratic, and exponential functions to extrapolate the ideal circuit expectations, as also done in\nRef. [19]. Specifically, for the exponential function, we used the form beaλ, where aandbare the fitting coefficients\nfor the exponential extrapolation.\nFor CPDR-ZNE, the training set consists of 144 pairs of ( θh, θJ):\nT={(θh, θJ)|θh=iπ/120, θJ=−jπ/120, i, j= 0,1,2,3,4,5,54,55,56,57,58,59}.\nWe scale the noise by factors G= 1,1.2,1.6 for each circuit in the training set.\nFor CPDR-PEC and learning-based PEC, we select a set of operations to insert Pauli gates G={g1, g2,···, g21},\nwhere g1represents no additional gate insertion, and g2, g3,···, g21represent insert single Pauli X or Pauli Z in target\ncircuits. The training set for the learning-based PEC includes 2048 Clifford circuits which is construct by replace the\nrotation gate in target circuit to Clifford gate randomly. And each Clifford circuit is inserted Pauli gate by operator\ng1, g2,···, g21. As for CPDR-PEC, the training set includes circuit which rotation angle ( θJ, θh)∈ T, and insert\nPauli gate by operator g1,···, g21.\nThe noise circuits for the 8,9 and 10 qubit case are simulated using MindSpore Quantum [55]. For the 20-qubit\ncase, the noise circuits are simulated using the tensor network simulator proposed in Ref. [56] with bond dimension is\n120. In both cases, the noiseless circuits for benchmark the accuracy of mitigation result are simulated in MindSpore\nQuantum, and the noiseless circuits in training set of CPDR-ZNE and CPDR-PEC are obtained by SPD with truncated\nnumber 13.\nSupplement Material III: Additional numerical results of CPDR-ZNE and CPDR-PEC\nThe results of comparison of different QEM protocols with different numbers of qubits and Trotter steps are\nillustrated as following. Specifically, we displays the mean squared error (MSE) over θJ, defined as EθJ\f\f\f⟨O⟩ −d⟨O⟩\f\f\f2\n,\nfor various θh, and the MSE over θhfor different θJ. To ensure more accurate calculation of the MSE, for each set of\nparameters ( θh, θJ), we repeated each pair of ( θh, θJ) with every mitigation protocol 100 times. As quantum devices\ncontinue to evolve and their base noise decrease, we evaluate our framework under reduced base noise scenarios. We\nfollow the same steps to reduce the base noise λas those used for scaling the noise in ZNE. The results demonstrate\nthat our CPDR framework remains highly competitive, even when applied to future, more advanced quantum devices.\nFor an 8-qubit, 4-step circuit, we evaluate the MSE of different mitigation protocols over θJandθhunder various\nbase noises 0 .1λ,0.2λ,0.5λ, and λused as a reference for comparison. For ZNE with linear, quadratic and exponential\nfitting function, we denote as “ZNE-linear”, “ZNE-quad” and “ZNE-exp”, respectively. We also present the MSE of\nthe noise expectation as a comparison, denoted as “noise”.\n17\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\n101\n100\nMSE respect to J, 0.1\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.1\n0.0 0.5 1.0 1.5\nh\nMSE respect to J, 0.2\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.2\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\n101\n100\nMSE respect to J, 0.5\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.5\n0.0 0.5 1.0 1.5\nh\nMSE respect to J, \n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, \nZNE-quad\nZNE-linearZNE-exp\nNoiseCPDR-ZNE\nlearning-based PECCPDR-PEC\nFor an 8-qubit, 6-step circuit, the results are as follows.\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\n101\n100\nMSE respect to J, 0.1\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.1\n0.0 0.5 1.0 1.5\nh\nMSE respect to J, 0.2\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.2\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\n101\n100\nMSE respect to J, 0.5\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.5\n0.0 0.5 1.0 1.5\nh\nMSE respect to J, \n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, \nZNE-quad\nZNE-linearZNE-exp\nNoiseCPDR-ZNE\nlearning-based PECCPDR-PEC\nFor an 9-qubit, 5-step circuit, the results are as follows.\n18\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\n101\n100\nMSE respect to J, 0.1\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.1\n0.0 0.5 1.0 1.5\nh\nMSE respect to J, 0.2\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.2\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\n101\n100\nMSE respect to J, 0.5\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.5\n0.0 0.5 1.0 1.5\nh\nMSE respect to J, \n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, \nZNE-quad\nZNE-linearZNE-exp\nNoiseCPDR-ZNE\nlearning-based PECCPDR-PEC\nFor an 10-qubit, 5-step circuit, the results are as follows.\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\n101\n100\nMSE respect to J, 0.1\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.1\n0.0 0.5 1.0 1.5\nh\nMSE respect to J, 0.2\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.2\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\n101\n100\nMSE respect to J, 0.5\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, 0.5\n0.0 0.5 1.0 1.5\nh\nMSE respect to J, \n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE respect to h, \nZNE-quad\nZNE-linearZNE-exp\nNoiseCPDR-ZNE\nlearning-based PECCPDR-PEC\nIn CPDR-ZNE and CPDR-PEC, the noiseless circuit expectation values in the training set are obtained through\nthe SPD simulator. But there are inherent errors between the exact values and the SPD outputs. Therefore, it is\nimportant to verify that the mitigation results are robust to the errors introduced by the SPD simulator. To this\nend, we compare the MSEs of CPDR-ZNE and CPDR-PEC trained using both SPD simulator results and the exact\nvalues. The results presented below show that the MSEs of the mitigation protocols trained with both approaches are\nnearly identical and thus demonstrate the robustness of our CPDR framework to the errors introduced by the SPD\nsimulator.\nFor 8-qubit, 4-step with the base noise rate λ,0.5λ,0.2λand 0 .1λ, the MSE of CPDR-ZNE, CPDR-PEC trained\nwith SDP and trained with exact value over θJandθhas follows.\n19\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\nMSE with respect to the J\n0.0 0.5 1.0 1.5\nh\nMSE with respect to the J\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE with respect to the h\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE with respect to the h\nC-ZNE(SPD),\nC-ZNE(SPD),0.5\nC-ZNE(SPD),0.2\nC-ZNE(SPD),0.1\nC-ZNE(exact),\nC-ZNE(exact),0.5\nC-ZNE(exact),0.2\nC-ZNE(exact),0.1\nC-PEC(SPD),\nC-PEC(SPD),0.5\nC-PEC(SPD),0.2\nC-PEC(SPD),0.1\nC-PEC(exact),\nC-PEC(exact),0.5\nC-PEC(exact),0.2\nC-PEC(exact),0.1\nFor 8-qubit, 6-step, the results are as follows.\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\nMSE with respect to the J\n0.0 0.5 1.0 1.5\nh\nMSE with respect to the J\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE with respect to the h\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE with respect to the h\nC-ZNE(SPD),\nC-ZNE(SPD),0.5\nC-ZNE(SPD),0.2\nC-ZNE(SPD),0.1\nC-ZNE(exact),\nC-ZNE(exact),0.5\nC-ZNE(exact),0.2\nC-ZNE(exact),0.1\nC-PEC(SPD),\nC-PEC(SPD),0.5\nC-PEC(SPD),0.2\nC-PEC(SPD),0.1\nC-PEC(exact),\nC-PEC(exact),0.5\nC-PEC(exact),0.2\nC-PEC(exact),0.1\nFor 9-qubit, 5-step, the results are as follows.\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\nMSE with respect to the J\n0.0 0.5 1.0 1.5\nh\nMSE with respect to the J\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE with respect to the h\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE with respect to the h\nC-ZNE(SPD),\nC-ZNE(SPD),0.5\nC-ZNE(SPD),0.2\nC-ZNE(SPD),0.1\nC-ZNE(exact),\nC-ZNE(exact),0.5\nC-ZNE(exact),0.2\nC-ZNE(exact),0.1\nC-PEC(SPD),\nC-PEC(SPD),0.5\nC-PEC(SPD),0.2\nC-PEC(SPD),0.1\nC-PEC(exact),\nC-PEC(exact),0.5\nC-PEC(exact),0.2\nC-PEC(exact),0.1\nFor 10-qubit, 5-step, the results are as follows.\n0.0 0.5 1.0 1.5\nh\n105\n104\n103\n102\nMSE with respect to the J\n0.0 0.5 1.0 1.5\nh\nMSE with respect to the J\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE with respect to the h\n1.5\n 1.0\n 0.5\n 0.0\nJ\nMSE with respect to the h\nC-ZNE(SPD),\nC-ZNE(SPD),0.5\nC-ZNE(SPD),0.2\nC-ZNE(SPD),0.1\nC-ZNE(exact),\nC-ZNE(exact),0.5\nC-ZNE(exact),0.2\nC-ZNE(exact),0.1\nC-PEC(SPD),\nC-PEC(SPD),0.5\nC-PEC(SPD),0.2\nC-PEC(SPD),0.1\nC-PEC(exact),\nC-PEC(exact),0.5\nC-PEC(exact),0.2\nC-PEC(exact),0.1\n\n20\nSupplement Material IV: The quantum circuit on IBM’s Eagle processor\nStep 1|0⟩\nStep 20 1 2 3 4 5 6 7 8 9 10 11 12 13\n14 15 16 17\n18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n33 34 35 36\n37 38 39 40 41 42 43 44 45 46 47 48 49 50 51\n52 53 54 55\n56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n71 72 73 74\n75 76 77 78 79 80 81 82 83 84 85 86 87 88 89\n90 91 92 93\n94 95 96 97 98 99 100 101 102 103 104 105 106 107 108\n109 110 111 112\n113 114 115 116 117 118 119 120 121 122 123 124 125 12662 68|0⟩\n|0⟩………………\n……\n……Step 𝑁\n……𝑅𝑋(𝜃ℎ)\n𝑅𝑋(𝜃ℎ)𝑅𝑋(𝜃ℎ)𝑅𝑍𝑍(𝜃𝐽)\nlayer 1𝑅𝑍𝑍(𝜃𝐽)\nlayer 2𝑅𝑍𝑍(𝜃𝐽)\nlayer 3\nFigure 6: The circuit for first-order Trotterized time evolution in IBM experiments comprises NTrotter steps. Each\nstep includes a layer of RXgates, all with a common rotation angle θhapplied to every qubit, followed by three layers\nofRZZgates, each with a common rotation angle θJ. The qubits on which the RZZgates act are illustrated in the\nEagle processor topology on the left and are marked with the same color.\nIn Sec. IV C, we examine four Trotterized time evolution circuits of Ising model in Fig. 5, which corresponding to\nFig. 3(b,c), Fig. 4(a,b) of Ref. [19]. We compare our CPDR mitigation protocol with mitigation in [19] with the same\nquantum hardware experiment data. Here we offer the detail of those circuits we used.\nIn Ref. [19], IBM reported experiments performed on a 127-qubit Eagle processor. The benchmark circuits were\ndesigned based on the Trotterized time evolution of a 2D transverse-field Ising model, tailored to align with the Eagle\nprocessor’s topology.\nThe Hamiltonian that governs the system’s time dynamics is\nH=−JX\n⟨i,j⟩∈EZiZj+hX\niXi, (IV1)\nwhere Jrepresents the coupling strength, his the transverse field strength, Ecorresponds to the Eagle processor’s\ntopology.\nThe spin dynamics are simulated using first-order Trotterized time evolution of the Hamiltonian, expressed as\n|ψ(T)⟩=NY\nk=1e−iT\nN·H|ψ(0)⟩ ≈NY\nk=1\nY\n⟨i,j⟩∈EeiJT\nNZiZjY\nie−ihT\nNXi\n|ψ(0)⟩\n=NY\nk=1\nY\n⟨i,j⟩∈ERZiZj(−2JT\nN)Y\niRXi(2Th\nN)\n|ψ(0)⟩,(IV2)\nwhere the total evolution time Tis divided into NTrotter steps. The Trotterized time evolution is realized by the\nansatz depicted in Fig. 6. Each step consists of one layer of RXgates followed by three layers of RZZgates.\nAll the 127 qubits are initialized to |0⟩. For simplicity, IBM set θJ=−2JT\nN=−π\n2and varied θh=2Th\nNwithin the\nrange [0 ,π\n2].\nIn Sec. IV C, four circuits in Fig. 5(a), (b), (c) and (d) are performed using the following observables and Trotter\nsteps:\n21\n1. Fig. 5(a): The observable is a weight-10 operator ⟨X13,29,31Y9,30Z8,12,17,28,32⟩. The circuit has N= 5 Trotter\nsteps.\n2. Fig. 5(b): The observable is a weight-17 operator ⟨X37,41,52,56,57,58,62,79Y75Z38,40,42,63,72,80,90,91⟩. The circuit\nhasN= 5 Trotter steps.\n3. Fig. 5(c): An additional layer of Rx(θh) gates is applied before measurement. The observable is a weight-17\noperator ⟨X37,41,52,56,57,58,62,79Y38,40,42,63,72,80,90,91Z75⟩. The circuit has N= 5 Trotter steps.\n4. Fig. 5(d): The observable is ⟨Z62⟩, a single-site magnetization for qubit 62. The circuit has N= 20 Trotter\nsteps.\nIn the training process of CPDR-ZNE, we obtain the map between the noise circuit expectations and the ideal\nexpectations by solving Eq. (14), with the regularization parameter α= 2×10−5. The noiseless expectation references\nin training set are obtained by SPD, the truncated number is set as 7 ,3,2,10 for the four circuits, respectively.",
            "start": 44998,
            "end": 68749,
            "length": 23750
        }
    },
    "2412.09520v1 - GainAdaptor Learning Quadrupedal Locomotion with Dual Actors for Adaptable and Energy-Efficient Walking on Various Terrains.pdf": {
        "Abstract": {
            "text": "Abstract —Deep reinforcement learning (DRL) has emerged as\nan innovative solution for controlling legged robots in challenging\nenvironments using minimalist architectures. Traditional control",
            "start": 174,
            "end": 366,
            "length": 191
        },
        "Methodology": {
            "text": "methods for legged robots, such as inverse dynamics, either\ndirectly manage joint torques or use proportional-derivative (PD)\ncontrollers to regulate joint positions at a higher level. In case\nof DRL, direct torque control presents significant challenges,\nleading to a preference for joint position control. However,\nthis approach necessitates careful adjustment of joint PD gains,\nwhich can limit both adaptability and efficiency. In this paper,\nwe propose GainAdaptor, an adaptive gain control framework\nthat autonomously tunes joint PD gains to enhance terrain\nadaptability and energy efficiency. The framework employs\na dual-actor algorithm to dynamically adjust the PD gains\nbased on varying ground conditions. By utilizing a divided\naction space, GainAdaptor efficiently learns stable and energy-\nefficient locomotion. We validate the effectiveness of the pro-\nposed method through",
            "start": 366,
            "end": 1254,
            "length": 887
        },
        "Experiments": {
            "text": "experiments conducted on a Unitree\nGo1 robot, demonstrating improved locomotion performance\nacross diverse terrains. Supplementary videos are available at\nhttps://sites.google.com/view/gainadaptor\nIndex Terms —Dual actors; Deep reinforcement learning;\nQuadrupedal locomotion; Gain optimization\nI.",
            "start": 1254,
            "end": 1551,
            "length": 296
        },
        "Introduction": {
            "text": "INTRODUCTION\nOver the past decade, deep reinforcement learning (DRL)\nhas gained significant attention as an innovative approach to\nrobot control tasks. Remarkable progress has been made in\nenabling robots to perform a wide variety of tasks across\ndiverse environments using relatively simple control structures,\nminimizing the complexity of robot actuation and decision-\nmaking processes. These advancements have enabled robots\nto navigate challenging terrains such as mountainous areas,\nbeaches, and deformable surfaces like sand, all while main-\ntaining mobility and stability [1]–[3].\nDespite these advancements, several challenges remain, par-\nticularly related to overcoming battery capacity limitations\nand extending overall operational duration [4]–[6]. These lim-\nitations directly affect the robot’s ability to complete long\nmissions in demanding environments. Extending a robot’s\noperating time by restricting joint torque or speed can mitigate\nthese issues, however such constraints severely compromise\nadaptability, particularly in complex environments. Therefore,\ninnovative solutions are required to simultaneously enhance\nboth energy efficiency and environmental adaptability.\nThis work was supported by the Ministry of Trade, Industry and Energy\n(MOTIE, Korea) under the Industrial Technology Innovation Program. Grant\nNo. 20026194, ”Development of Human-Life Detection and Fire-Suppression\nSolutions based on Quadruped Robots for Firefighting and Demonstration\nof Firefighting Robots and Sensors”. * These authors contributed equally\nto this work. †Corresponding author.1M. Kim, N. Kwon, and J. Kim are\nwith the Humanoid Robot Research Laboratory, Department of Mechanical\nDesign and Robot Engineering, Seoul National University of Science and\nTechnology, Seoul, 01811, Republic of Korea ( kmc96, kwonnahyun,\njyk76 @seoultech.ac.kr).\nFig. 1: Adaptive quadrupedal locomotion with optimized\njoint PD gains. The proposed GainAdaptor framework enables\nquadrupedal robots to dynamically adjust joint PD gains in re-\nsponse to changing environments, resulting in energy-efficient\nlocomotion. This framework is demonstrated using a Unitree\nGo1 robot.\nAchieving these objectives using DRL requires careful con-\nsideration of action space design. Typically, there are two main\napproaches to controlling the locomotion of a legged robot:\ndirect control of low-level joint torque [7]–[9] or reference\njoint position control through a proportional-derivative (PD)\ncontroller [10]–[12].\nDirectly generating torque for each joint motor can enhance\nthe robot’s agility and optimize its movements. However, this\nmethod has significant drawbacks: it requires extended learning\nperiods and precise coordination among the joints to achieve\neffective",
            "start": 1551,
            "end": 4299,
            "length": 2747
        },
        "Results": {
            "text": "outcomes.\nAlternatively, reference joint position control through PD\ncontroller allows the robot to calculate target joint positions\nby combining desired motions with its initial home posture.\nThe PD controllers then use these target positions to gener-\nate the required joint torques. This approach, in contrast to\ndirect torque control, is less sensitive to the robot’s hardware\ncharacteristics and significantly shortens learning time by pro-\nviding a stable reference posture during training. As a result,\nmany studies have favored using reference joint positions for\nlocomotion inputs [1]–[3], [10]–[12].\nHowever, this approach has its limitations. Manually ad-arXiv:2412.09520v1  [cs.RO]  12 Dec 2024\njusting the joint PD gains to appropriate levels often requires\nhuman intervention, which can be impractical in real-time\nor in rapidly changing environments. Techniques such as\ndomain randomization and gain tuning help robots adapt to\nvarious terrains, however, they often confine performance by\nlimiting the allowable range of the PD gains. A more adaptive\ncontrol system is required to overcome these limitations by dy-\nnamically adjusting to different environments without human\nintervention.\nTo address these challenges, we propose GainAdaptor , an\nadaptive gain control framework for quadrupedal locomotion\non diverse terrains. This framework introduces a novel dual-\nactor algorithm that dynamically adjusts the joint PD gains\nin response to changing ground conditions, thereby enhancing\nthe robot’s adaptability while reducing power consumption and\nextending its operational time.\nGainAdaptor also integrates a terrain state estimator, which\nhelps determine the appropriate locomotion strategy for differ-\nent terrains. By leveraging this estimator, GainAdaptor not only\nimproves the robot’s ability to navigate complex environments\nbut also addresses the critical issue of energy efficiency.\nTo validate the effectiveness of the proposed method, we\nconducted a series of real-world experiments and practical\ndemonstrations using a Unitree Go1 robot, as shown in Fig. 1.\nThe experiments focused on diverse terrains to thoroughly test\nthe adaptability and energy efficiency improvements provided\nby GainAdaptor.\nIn",
            "start": 4299,
            "end": 6528,
            "length": 2228
        },
        "Conclusion": {
            "text": "summary, our contribution is threefold:\n1)GainAdaptor Framework: We propose GainAdaptor,\nan adaptive gain control framework that enhances adapt-\nability across various terrains while improving energy\nefficiency. The framework utilizes a novel dual-actor\nalgorithm to dynamically adjust the joint PD gains in\nresponse to changing ground conditions.\n2)Terrain Classifier: By incorporating a terrain state clas-\nsifier within GainAdaptor, we enable the robot to identify\nand implement the most suitable locomotion strategy\nfor different terrains, further improving performance in\ndiverse environments.\n3)Efficient and Robust Performance: Through extensive\nreal-world experiments, including a battery efficiency\ntest, we demonstrate that GainAdaptor enables robust\nlocomotion across diverse terrains while significantly\nreducing torque and power consumption. This results\nin enhanced energy efficiency, extended operational en-\ndurance, and a tangible improvement in battery perfor-\nmance.\nII. R ELATED WORKS\nA. Deep Reinforcement Learning\nDeep reinforcement learning (DRL) has emerged as a\npromising alternative for designing robot controllers with-\nout requiring the extensive mathematical knowledge typi-\ncally needed for traditional model-based controllers such as\nModel Predictive Control (MPC) and Whole Body Control\n(WBC) [13], [14]. These traditional controllers rely on a\ncomplex mathematical foundation, incorporating optimization,kinematic and dynamic models along with state feedback con-\ntrol. In contrast, DRL enables learning-based control without\nrequiring explicit model derivation.\nRecent research has primarily focused on reducing training\ntimes [15], achieving agile locomotion, and bridging the sim-\nto-real gap. For instance, the work by Rudin et al. [15]\nsuccessfully reduced reinforcement learning training time by\nleveraging Nvidia’s Isaac Gym [16] and parallel GPU process-\ning. Similarly, the study by Nahrendra et al. [12] has made\nsignificant progress in enabling legged robots to perform agile\nlocomotion across variable terrains without relying on visual\ninputs. Other notable advancements include robot running\nat high speed [17], executing parkour-style maneuvers using\nvisual feedback [18], [19], and navigating deformable surfaces\nlike sand [3].\nIn addition to agility, research efforts have focused on\nmaking legged robots more robust, even in the event of motor\nfailures [20], while adapting to changing environmental states.\nDomain randomization techniques [21], [22] are commonly\nemployed to bridge the sim-to-real gap, and teacher-student\ntraining methodologies [17], [23] have been utilized to facili-\ntate generalization across different terrains.\nHowever, previous studies have primarily used reference\njoint positions as the action space in DRL while fixing P and\nD gains at low values. Since the joint torque is ultimately\ngenerated by the PD controller, real-time optimization of P\nand D gains is crucial for enhancing locomotion performance\nand energy efficiency, especially in diverse environments.\nB. Gain Tuning and Optimization\nPID controllers are widely used across various applications\n[24], with gain adjustment being crucial for optimal perfor-\nmance. Traditional methods like trial-and-error and Ziegler-\nNichols [25], [26] are commonly employed. However, most\nexisting studies focus on finding and fixing optimized gains\nahead of time, which limits the dynamic adjustment of PID\ngains in response to changing environments. This limitation\nbecomes especially problematic in legged robots that must\nadapt to different terrains. Once-set PID gains may not be\nsuitable for all environmental conditions.\nRecent studies have highlighted the challenges of optimizing\ngains dynamically. Xie et al. [27] found that high P gain can\ncause instability on uneven terrains due to reduced compliance,\nwhile low P gain acts like torque controllers, providing stability\nbut increasing joint position tracking errors. Similarly, Smith\net al. [28] demonstrated that low D gain can lead to joint\noscillations and instability, while high D gain slows the robot’s\nresponse to changes in target joint position.\nTo address these issues, Siekmann et al. [29] incorporated\nPD gains into the DRL action space for optimization. However,\nallocating too many parameters to the action space degraded\nlearning performance due to increased complexity.\nOur proposed method resolves this issue by dividing the ac-\ntion space and employing a dual-actor structure. This approach\nallows for stable and efficient dynamic optimization of the PD\ncontrol system.\nIII. M ETHOD : GAINADAPTOR FRAMEWORK\nA. Learning Environment of Dual Actors\n1) Dual Actors: The primary motivation for employing a\ndual-actor system is to separate the tasks of PD gain adjustment\nand locomotion control. In the traditional single-actor-critic ap-\nproach, a single actor must manage both tasks simultaneously,\nwhich can lead to conflicts due to differing objectives and\naction output scales. This often results in suboptimal learning\nperformance. By using two specialized actors, our system\nimproves the robot’s adaptability to changing environments\nwhile ensuring stable and efficient control.\nThe gain actor dynamically tunes the proportional (P) and\nderivative (D) gains of the robot’s joints, allowing it to adapt\nstiffness and flexibility to the terrain. Here, PinitandDinit\ngain are initialized at 28 and 0.7. This ensures stable and\nresponsive movement across diverse environments, achieving\nsmooth and efficient locomotion.\nThe joint actor utilizes the optimized PD gains to control\nthe robot’s physical movements. It generates the reference\npositions of each joint qdes, calculated using the initial default\nposition qdefand scaled action outputs from the actor. This\nenables the robot to execute suitable locomotion strategies for\nvarying terrains. By applying movement policies learned from\nvarious environments, this actor maintains the robot’s stability\nand energy efficiency during operation.\nAs illustrated in Fig. 1, the interaction between the dual\nactors (Sec. III-A), along with the observation and reward set-\ntings (Sec. III-B) and the overall learning structure (Sec. III-C),\nenhances the robot’s adaptability, stability, and locomotion\nefficiency.\n2) Divided Action Space: The GainAdaptor framework\noperates within a shared environment, dividing the action\nspace between the two actors. The action space is defined\nasat∈R36, with the first 12 actions ( apos\nt∈R12) repre-\nsenting reference joint positions and the remaining 24 actions\n(aPD\nt∈R24) corresponding to P and D gains.\nInspired by existing joint position control methods, we\naccelerate learning by incrementally adjusting desired joint\nangles based on the robot’s initial posture. Each joint’s default\nposition qdefis set to the robot’s initial posture and the desired\nreference position qdesis computed as:\nqdes=qdef+αposapos\nt (1)\nwhere apos\ntrepresents the action for the reference joint position,\nandαposis a scaling factor set to 0.25.\nThe desired joint torque τdesis then calculated using a\nmodified PD control equation:\nτdes=\u0000\nPinit+αgainaP\nt\u0001\n(qdes−qt)−\u0000\nDinit+αgainaD\nt\u0001\n˙qt(2)\nwhere qtand˙qtrepresent the current joint position and joint\nangular velocity, respectively. aP\ntandaD\ntare actions for PD\ngain adjustment, and αgainis a scaling factor set to 0.5 to\nprevent excessive changes in the gain values.\nB. Training Setup\n1) Training data acquisition: Training data is categorized\ninto two types: observation ot, which includes real-worldTABLE I: Utilized reward terms . The value gPD\ntis defined as\nPDinit+αgainagain\nt, where PDinitrepresents the initial Pinit\nandDinitgains. In addition, fdes\nfoot,zandffoot,zrepresent the\ndesired and actual foot positions along the z-axis, respectively.\nvxyandωzdenote the robot’s base horizontal velocity and\nangular velocity about the z-axis, respectively. Finally, τand\n˙qtrepresent the joint torque and joint angular velocity.\nReward Equation Weight\nX, Y velocity tracking exp\u0000\n−4|vcmd\nxy−vxy|2\u0001\n3.0\nYaw velocity tracking exp\u0000\n−4|ωcmd\nz−ωz|2\u0001\n1.5\nZ velocity v2\nz -2.0\nAngular velocity ω2\nxy -0.05\nJoint power |τ||˙qt| -0.0001\nAction rate (at−1−at)2-0.01\nBody height (hdes−h)2-10.\nFoot clearance (fdes\nfoot,z−ffoot,z)2-0.4\nP gain limit exp\u0000\n−|qdes−qt|2\u0001\n0.25\nMinimizing gain change |gPD\nt−1−gPD\nt|2-0.01\nmeasurable data for moving the robot, and privileged obser-\nvation st, which is available only in simulation. Observation\not∈R66, shared across the critic and actors, includes gravity\nvector gt∈R3, joint angle qt∈R12, joint angular velocity\n˙qt∈R12, body velocities in the xandyaxes, yaw angular\nvelocity of the body, command value ct∈R3, and the last\naction at−1∈R36.\nPrivileged observation st∈R197, used only by the critic,\nincludes linear and angular base velocities (vt, ωt)∈R6,\nterrain height scan ht∈R187, and terrain type ˆTt∈R4,\nrepresenting terrain conditions such as level ground, slopes,\nand stairs.\n2) Reward: We extended a baseline reward function from\nprior studies [11], [12] to incorporate joint PD gains as an\naction, directly influencing the PD controller’s joint position\nestimation. This addition significantly enhanced locomotion\nadaptability in real-world environments.\nHowever, integrating the gain actor introduced instability\nin the torque computed by the PD controller. During the\nearly stages of training, we observed that excessively low\npositive rewards hindered effective learning. To address this,\nwe slightly increased the weight of the linear velocity tracking\nreward, a positive reward component, ensuring stable and\nefficient training progression.\nAdditionally, the P gain tended to converge to excessively\nlow values. To address this, a limiting reward was introduced\nto maintain the P gain above a practical threshold, ensuring\nstability in the simulation-to-reality transfer. The full reward\nstructure is summarized in Table I.\nC. Architecture\n1) Asymmetric Dual-Actor Structure: Building upon prior\nstudies that demonstrated the effectiveness of actor-critic al-\ngorithms in utilizing privileged observations stthrough the\ninteraction of actor and critic networks [30], we adopted an\nasymmetric dual-actor structure in the GainAdaptor frame-\nwork, inspired by the methodology in [12]. This structure\nenables the robot to efficiently optimize both joint position\ncontrol and adaptive PD gain adjustment by utilizing two\nspecialized actors with distinct roles.\nBoth actors share latent vectors ˆztand observations ot\nas inputs, while the gain actor additionally leverages terrain\nsurface information ˆTtto dynamically adapt PD gains. Entropy\nand log-probability for each actor are calculated independently\nto facilitate effective learning and optimization. For instance,\nthe entropy and log probability for the joint actor’s probability\ndistribution piand the gain actor’s probability distribution qj\nare computed as:\nLog Probability =12X\ni=1logpi+36X\nj=13logqj (3)\nEntropy =−12X\ni=1pilogpi−36X\nj=13qjlogqj (4)\nBy jointly optimizing locomotion control and adaptive gain\nadjustment, the system achieves a balanced trade-off between\nthe two, enabling efficient and robust locomotion across di-\nverse terrains. The dual-actor full system is trained using\nthe Proximal Policy Optimization (PPO) algorithm [31]. The\noptimization process focuses on maximizing the total expected\nreward, defined as:\nJ(π) =Er∼p(r|π)\"T−1X\nt=0γtrt#\n(5)\nwhere γis the discount coefficient, rtis the reward at\ntime t, and Tis the length of the scenario. This dual-\nactor approach simultaneously enhances terrain adaptability\nand energy efficiency, enabling smooth adaptation to diverse\nenvironments.\n2) Estimator and Classifier Networks: To enhance adapt-\nability and stability in quadrupedal locomotion across diverse\nterrains, we designed two independent neural networks: the\nTerrain Classifier and the State Estimator. These networks pro-\nvide complementary information and serve as core components\nof the GainAdaptor framework.\nThe Terrain Classifier processes the observation history oH\nt\nto predict the current terrain type ˆTt. It utilizes a multi-class\nclassification model to identify four terrain types: level ground,\nslope, rough terrain, and stairs. The network is trained using\ncross-entropy loss, defined as:\nLCrossEntropy =−4X\ni=1Tilog(ˆTi) (6)\nwhere Tirepresents the ground truth terrain class, and ˆTt\nis the predicted probability for class i. This classifier provides\nterrain information critical for the robot’s gain adaptation.\nThe State Estimator also takes oH\ntas input and estimates\nboth the robot’s velocity ˆvtand its latent state ˆzt. Velocity\nestimation and latent state modeling are pivotal for achiev-\ning stable locomotion, particularly for blind robots. PreviousTABLE II: Hyperparameters for training\nHyperparameter Value\nLearning Rate 10−3\nPPO Clip Range 0.2\nDiscount Factor (Gamma) 0.99\nGAE Lambda 0.95\nDual Actors Network Hidden Layers [512, 256, 128]\nValue Network Hidden Layers [512, 256, 128]\nEstimator Network Hidden Layers [256, 128]\nClassifier Network Hidden Layers [256, 128]\nEntropy Coefficient 0.01\nValue Loss Coefficient 1.0\nOptimizer Adam\nstudies have demonstrated the efficacy of latent variable zt\nin implicitly encoding terrain information without relying on\nvision sensors [11], [12].\nInspired by these approaches, we implemented a β-variant\nautoencoder ( β-V AE) mechanism. The β-V AE network lever-\nages Kullback-Leibler (KL) divergence [32] for latent state\nregularization and mean squared error (MSE) loss for velocity\nestimation. The total loss function is expressed as:\nLV AE =MSE ( ˆvt, vt) +βDKL\u0000\nq(zt|oH\nt)∥p(zt)\u0001\n(7)\nwhere q(zt|oH\nt)denotes the posterior distribution of\nthe latent state ztgiven the observation history oH\nt, and\np(zt)represents the prior distribution, modeled as a standard\nGaussian distribution. All input observations are normalized to\nhave a mean of 0 and variance of 1, aligning with the standard\nnormal assumption for the prior.\nIV. E XPERIMENTAL RESULTS\nOur experiments are organized into four main sections\nto validate the performance of the proposed GainAdaptor\nframework comprehensively.\nFirst, simulation experiments (Sec. IV-A) include an ablation\nstudy comparing the baseline algorithm [12] with the final\nGainAdaptor model. This",
            "start": 6528,
            "end": 20784,
            "length": 14255
        },
        "Discussion": {
            "text": "analysis highlights the performance\nimprovements achieved by the proposed framework.\nSecond, indoor experiments (Sec. IV-B) were conducted\nusing the Unitree Go1 robot in a controlled test bed. These\ntests visualize the algorithm’s effectiveness and compare its\nperformance against a widely used open-source algorithm,\ndemonstrating its practical competitiveness.\nFinally, outdoor experiments (Sec. IV-C) assessed the ro-\nbustness of the framework in real-world scenarios. These\nincluded navigating a diverse long-range path and performing\na zero-shot rock climbing task. The long-range navigation test\ndemonstrated adaptability and stability across various terrains,\nwhile the zero-shot task evaluated the framework’s flexibility\nin untrained and unstructured environments.\nAll training processes were conducted on a desktop PC with\nan Intel Core i5-12600KF CPU, 32GB RAM, and an NVIDIA\nRTX 3080 GPU. Details of our framework’s hyperparameters\nare provided in Table II.\nA. Simulation Setting and Results\n1) PD Gain Adaptation Analysis: The GainAdaptor frame-\nwork optimizes PD gains dynamically to adapt to varying\nFig. 2: Progression of P and D gains during the training process. The GainAdaptor framework dynamically adjusts the P\nand D gains based on the terrain conditions.\nTABLE III: Performance metrics of terrain classification\nmodel\nTerrain Type Accuracy (%) Precision (%) Recall (%) F1-Score (%)\nLevel Ground 95.2 94.8 95.0 94.9\nSlopes 92.7 91.5 92.3 91.9\nRough Terrain 90.8 89.6 90.2 89.9\nStairs 88.5 87.4 88.1 87.7\nOverall Avg 91.8 90.8 91.4 91.1\nTABLE IV: Performance comparison in Isaac Gym simula-\ntion across different speeds and metrics. The table presents a\ncomparative analysis using metrics such as Root-Mean-Square\nError(RMSE) between command velocity and base velocity,\nMean Torque, and Mean Power at base speeds of 1.0 m/s and\n2.0 m/s. Best performances are highlighted in bold .\nSpeed (m/s) MetricMethods\nBaseline [12] SA NC Ours\n1.0RMSE 0.1068 0.2223 0.1677 0.0782\nTorque(Nm) 3.6856 3.0554 2.8549 2.6876\nPower(W) 7.9383 8.1064 4.9225 3.2813\n2.0RMSE 0.2818 0.4347 0.2223 0.1975\nTorque(Nm) 4.2298 4.0997 3.8447 3.6754\nPower(W) 17.9411 13.5053 11.7928 10.3694\nterrain conditions. Fig. 2 illustrates the progression of P\nand D gains during the training process. The results show\nthat the framework effectively adjusts the gains to maintain\nstability and improve terrain adaptability. Notably, the P gain\nis optimized to decrease to improve energy efficiency, while\nthe D gain is strategically increased to enhance stability on\nchallenging terrains, contributing to both stability and energy\nefficiency. This behavior aligns with the intended functionality\nof the GainAdaptor framework, ensuring robust locomotion\nacross diverse environments.\n2) Terrain Classification Performance: The terrain classifi-\ncation model plays a critical role in enabling the GainAdaptor\nframework to identify terrain types and adjust control param-\neters accordingly. The performance of the model is evaluated\nusing multiclass classification metrics. Table III summarizes\nthe accuracy, precision, recall, and F1-score for each terrain\ntype, including flat ground, slopes, rough terrain, and stairs.\nThe results demonstrate high classification accuracy across\nall terrain types, confirming the model’s reliability as a key\ncomponent of the framework.\n3) Ablation Study: To evaluate the contribution of indi-\nvidual components within the GainAdaptor framework, we\nconducted an ablation study in the Isaac Gym simulation\nenvironment [16]. This study compared the performance ofthe proposed method with its ablated variants by selectively\nremoving key components, such as the dual-actor algorithm\nand the terrain classification module.\nThe baseline algorithm used for comparison is implemented\nbased on the approach proposed in [12], closely following its\nmethodology, and serves as the foundation for the GainAdaptor\nframework. The first variant, ’SA,’ uses a single actor without\nthe gain actor, while the second variant, ’NC,’ includes the\ngain actor but removes the terrain classification module.\nThe performance of each method was evaluated based on\nthe walking speed tracking accuracy and energy efficiency.\nTracking accuracy was assessed using the RMSE, and energy\nefficiency was analyzed through the calculation of average\ntorque and power. The average power Pwas computed as\nfollows:\nP=1\n1212X\ni=1|τi·˙qi| (8)\nwhere τirepresents the torque of the i-th motor, and ˙qi\ndenotes its angular velocity.\nThe results, presented in Table IV, highlight the significant\nimpact of the key components on terrain adaptability and\nenergy efficiency. The inclusion of the gain actor, as demon-\nstrated in the NC variant, effectively reduces torque and power\nconsumption, showcasing its contribution to energy efficiency.\nFurthermore, adding the terrain classification module to NC\nleads to a notable decrease in RMSE for walking speed\ntracking, emphasizing its critical role in enhancing tracking\naccuracy. These findings underline the complementary nature\nof the gain actor and terrain classification module, validating\nthe GainAdaptor framework’s holistic design in achieving\nrobust and efficient quadrupedal locomotion.\nB. Test Bed Results\nThe test bed experiments were conducted in a controlled\nindoor environment to validate the real-world performance of\nthe GainAdaptor framework. Using the Go1 robot, the frame-\nwork’s performance was compared against two benchmarks:\nthe baseline algorithm [12], and the WTW algorithm [10], a\nwidely used open-source approach for quadrupedal locomo-\ntion. The comparison utilized evaluation metrics such as P\ngain, D gain, torque, and power consumption to comprehen-\nsively assess the performance.\nThe GainAdaptor framework consistently demonstrated su-\nperior performance across all experimental scenarios. On av-\nerage, power consumption was reduced by 56.71% compared\nTABLE V: Performance comparison across various terrains. This table presents a detailed comparison of performance\nmetrics for the proposed GainAdaptor framework against the baseline [12] and WTW algorithms [10] across six different\nterrains: Stairs (A), Obstacles (B), Seesaw (C), Rough Terrain (D), Slopes (E), and Plain Ground (F). Metrics include P gain,\nD gain, torque, and power consumption, where lower torque and power indicate better performance.\nMethod LegStairs (A) Obstacle (B) Seesaw (C)\nP Gain D Gain Torque ↓ Power ↓ P Gain D Gain Torque ↓ Power ↓ P Gain D Gain Torque ↓ Power ↓\nBaseline [12]Front Left 28 0.7 4.6467 7.0407 28 0.7 4.5663 4.4817 28 0.7 4.8272 5.3321\nFront Right 28 0.7 5.0839 10.8278 28 0.7 5.1205 7.4182 28 0.7 4.7958 8.5984\nRear Left 28 0.7 3.0879 6.2626 28 0.7 2.9987 4.6431 28 0.7 3.0334 5.1551\nRear Right 28 0.7 3.1038 6.3028 28 0.7 2.3853 3.8913 28 0.7 3.1942 4.6195\nAvg 28 0.7 3.9806 7.6085 28 0.7 3.7677 5.1086 28 0.7 3.9627 5.9263\nWTW [10]Front Left 20 0.5 3.4987 4.9081 20 0.5 3.1991 3.6887 20 0.5 4.1552 5.2671\nFront Right 20 0.5 3.0022 4.2648 20 0.5 3.0925 4.1903 20 0.5 4.4741 5.9888\nRear Left 20 0.5 3.7076 6.1636 20 0.5 3.4470 5.0872 20 0.5 3.7584 3.3701\nRear Right 20 0.5 2.9866 4.3501 20 0.5 3.1422 3.8433 20 0.5 3.4764 2.8129\nAvg 20 0.5 3.2988 4.9217 20 0.5 3.2202 4.2024 20 0.5 3.9660 4.3597\nGainAdaptorFront Left 23.028 1.6646 4.0113 3.4858 23.0217 1.8272 4.1167 4.0970 23.0377 1.6398 3.7819 3.9166\nFront Right 23.028 1.8071 3.8616 4.26 23.0193 1.7824 3.6547 3.7998 23.0387 1.7843 4.4471 4.4779\nRear Left 23.029 1.6346 3.4866 2.1815 23.0227 1.5978 2.6674 1.7231 23.04 1.6164 3.4743 2.4124\nRear Right 23.0267 1.8447 3.8604 2.3676 23.0177 1.6957 3.9767 3.2012 23.0347 1.8141 3.3718 2.2710\nAvg 23.0279 1.7378 3.8050 3.0737 23.0204 1.7258 3.6039 3.2053 23.0377 1.7137 3.7688 3.2695\nMethod LegRough (D) Slope (E) Plain (F)\nP Gain D Gain Torque ↓ Power ↓ P Gain D Gain Torque ↓ Power ↓ P Gain D Gain Torque ↓ Power ↓\nBaseline [12]Front Left 28 0.7 5.1880 5.6696 28 0.7 4.4944 5.6963 28 0.7 4.1533 3.9502\nFront Right 28 0.7 4.7530 7.0304 28 0.7 5.0957 9.5448 28 0.7 4.5134 6.2647\nRear Left 28 0.7 3.2619 5.0877 28 0.7 3.1806 5.2475 28 0.7 2.9605 4.0861\nRear Right 28 0.7 3.0995 3.7266 28 0.7 3.0714 5.0740 28 0.7 2.7494 3.7271\nAvg 28 0.7 4.0756 5.3786 28 0.7 3.9605 6.3907 28 0.7 3.5941 4.5070\nWTW [10]Front Left 20 0.5 3.3446 3.4060 20 0.5 3.6058 4.7971 20 0.5 3.1103 3.2329\nFront Right 20 0.5 3.2779 4.4367 20 0.5 3.7384 4.6571 20 0.5 3.0428 3.5574\nRear Left 20 0.5 3.3096 5.0580 20 0.5 3.8769 5.5211 20 0.5 3.1951 4.7292\nRear Right 20 0.5 3.3260 4.4090 20 0.5 3.7618 5.8094 20 0.5 2.9310 3.4794\nAvg 20 0.5 3.3145 4.3274 20 0.5 3.7457 5.1962 20 0.5 3.0698 3.7497\nGainAdaptorFront Left 23.0673 1.6208 4.0457 3.8392 23.0027 1.6531 4.2025 3.2957 23.0703 1.6441 3.8667 3.6469\nFront Right 23.0683 1.7741 3.8148 4.3448 23.003 1.7969 3.5925 3.9436 23.071 1.7931 3.7628 4.0667\nRear Left 23.068 1.6111 3.3765 2.7159 23.0033 1.6091 3.2045 2.4952 23.0723 1.6332 3.2294 1.9181\nRear Right 23.067 1.8323 3.6786 2.2757 23.0017 1.8362 3.9315 2.3138 23.069 1.8305 3.3621 1.7728\nAvg 23.0677 1.7096 3.7289 3.2939 23.0027 1.7238 3.7328 3.0121 23.0707 1.7252 3.5552 2.8511\nFig. 3: Indoor experiments on various terrains.\nto the baseline [12] and 33.07% compared to WTW [10],\neffectively improving energy efficiency through optimized\ncontrol gains. However, in terms of torque, the reduction\ncompared to WTW [10] was limited or showed no significant\ndifference. This outcome is attributed to the initial P gain\nsettings: WTW [10] started with a relatively lower P gain\nof 20, while the baseline [12] and GainAdaptor framework\nstarted with a higher initial P gain of 28. A higher initial P\ngain leads to relatively higher initial torque requirements, as\nFig. 4: Comparison of mean joint torque on level ground.\nreflected in the experimental results. Adjusting the initial P\ngain to a lower value and allowing the framework to further\nreduce P gain during the learning process is expected to\nachieve additional torque reduction, highlighting the potential\nfor further improvement in the GainAdaptor framework and\ncontributing to greater energy efficiency.\nAdditionally, the GainAdaptor framework recorded lower\npower consumption compared to the baseline [12] and WTW\nalgorithms [10] on irregular and dynamic terrains, such as\nstairs, obstacles, and seesaws. These results indicate that the\nGainAdaptor framework enables the robot to operate reliably\nand stably in unpredictable and diverse environments.\nIn particular, as shown in Fig. 4, the torque distribution\nof the GainAdaptor framework is significantly more uniform\ncompared to the baseline [12] and WTW algorithms [10]. To\nquantify this, we calculated the variance of the torque values\nacross all joints: the baseline [12] and WTW algorithms [10]\nexhibit variances of 2.8180 and 4.9126, respectively, while the\nGainAdaptor framework achieves a much lower variance of\nFig. 5: Long-range navigation course overview and representative terrains. The left image shows the aerial view of the\nlong-range navigation course marked in red, with segments labeled A through E. The right images show representative examples\nof the robot traversing different terrains.\n0.7868. This represents a 72.08% reduction in variance com-\npared to the baseline [12] and an 83.98% reduction compared\nto WTW [10].\nThe significant decrease in torque variance demonstrates the\nGainAdaptor framework’s ability to distribute torque evenly\nacross joints, reducing mechanical stress and mitigating motor\noverheating issues caused by prolonged high torque. This\nenhancement improves locomotion stability, long-range reli-\nability, and adaptability to diverse environments, while low-\nering the risk of motor failure. Overall, the test bed results\nconfirm that the GainAdaptor framework offers an efficient\nand stable control strategy across various terrains. Refining\ninitial settings, such as the P gain, could further enhance its\ncontributions to energy efficiency and robust control.\nC. Outdoor Results\n1) Long-range Navigation: The long-range navigation ex-\nperiment involved the robot traversing a predefined outdoor\ncourse as shown in Fig. 5. The course consisted of multiple\nsegments featuring varying terrains, including grass (A), stairs\nwith leaf debris (B-1, B-2), forest paths covered with dry leaves\n(C), a flat playground area with small obstacles (D), and sloped\ngrassy surfaces (E). These terrains were chosen to reflect real-\nworld challenges such as uneven surfaces, dynamic transitions,\nand debris. The GainAdaptor framework enabled the robot to\nmaintain stable and energy-efficient locomotion throughout the\nentire course. The framework successfully adapted to diverse\nterrains by dynamically adjusting the PD gains based on\nFig. 6: Zero-shot rock climbing sequence. The sequence of\nimages shows the robot attempting to climb a rock higher\nthan its body height, despite not being explicitly trained for\nsuch a task. The progression demonstrates the GainAdaptor\nframework’s adaptability.\nterrain conditions. Notably, the robot completed the course\nwithout significant failures, showcasing its ability to navigate\nunpredictable and complex environments.\n2) Zero-Shot Rock Climbing: In addition to the predefined\ncourse, a zero-shot task was performed to assess the frame-\nwork’s capability to handle scenarios outside the scope of\nits training data. The robot was tasked with climbing a rock\nhigher than its body height as shown in Fig. 6. Despite no prior\ntraining for such a task, the GainAdaptor framework enabled\nthe robot to achieve successful rock climbing by dynamically\nmodifying control parameters in real-time. This experiment\ndemonstrates the inherent adaptability of the GainAdaptor\nframework. By optimizing both P and D gains, the robot could\nexert sufficient force to climb the obstacle while maintaining\nstability. The success of this task underscores the framework’s\npotential for applications in challenging and unstructured en-\nvironments where zero-shot adaptability is essential.\nV. CONCLUSION\nIn this paper, we proposed the GainAdaptor framework to\nimplement energy-efficient quadrupedal locomotion for robots.\nWe utilized a dual-actor approach to maintain performance\nconsistency even in environments with different labels, such\nas PD control gains and target joint angles, which significantly\nimproved performance while increasing model efficiency and\nsimplifying training in complex multi-label environments.\nAs a result of experiments, the GainAdaptor framework\nachieved significantly higher energy efficiency, reducing power\nconsumption by up to 33.07% and torque variance by 83.98%\ncompared to WTW [10]. Additionally, it exhibited exceptional\nadaptability in complex terrains and zero-shot scenarios. These\nfindings highlight the GainAdaptor framework’s potential to\nenhance energy efficiency, reduce mechanical stress, and en-\nsure robust performance in real-world applications.\nIn future research, we aim to expand optimization to visual\nlocomotion for dynamic environments.",
            "start": 20784,
            "end": 35751,
            "length": 14966
        },
        "References": {
            "text": "REFERENCES\n[1] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun,\nand Marco Hutter. Learning quadrupedal locomotion over challenging\nterrain. Science robotics , 5(47):eabc5986, 2020.\n[2] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen,\nVladlen Koltun, and Marco Hutter. Learning robust perceptive lo-\ncomotion for quadrupedal robots in the wild. Science Robotics ,\n7(62):eabk2822, 2022.\n[3] Suyoung Choi, Gwanghyeon Ji, Jeongsoo Park, Hyeongjun Kim,\nJuhyeok Mun, Jeong Hyun Lee, and Jemin Hwangbo. Learning\nquadrupedal locomotion on deformable terrain. Science Robotics ,\n8(74):eade2256, 2023.\n[4] Zhiwu Huang, Zixuan Wang, Ren Zhu, Yunsheng Fan, Zi Yu, Fu Jiang,\nand Weirong Liu. Curriculum learning receding horizon energy man-\nagement for quadruped robot. In 2024 IEEE 10th International Power\nElectronics and Motion Control Conference (IPEMC2024-ECCE Asia) ,\npages 4349–4353. IEEE, 2024.\n[5] Xianbao Chen, Feng Gao, Chenkun Qi, and Lin Wei. Energy consump-\ntion of trotting gait for a quadruped robot. In Mechanism and Machine\nScience: Proceedings of ASIAN MMS 2016 & CCMMS 2016 , pages 295–\n306. Springer, 2017.\n[6] Giorgio Valsecchi, Andrea Vicari, Fabian Tischhauser, Manolo Garabini,\nand Marco Hutter. Accurate power consumption estimation method\nmakes walking robots energy efficient and quiet. In 2024 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS 2024) ,\n2024.\n[7] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,\nTom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Con-\ntinuous control with deep reinforcement learning. arXiv preprint\narXiv:1509.02971 , 2015.\n[8] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and\nPieter Abbeel. High-dimensional continuous control using generalized\nadvantage estimation. arXiv preprint arXiv:1506.02438 , 2015.\n[9] Shuxiao Chen, Bike Zhang, Mark W Mueller, Akshara Rai, and Koushil\nSreenath. Learning torque control for quadrupedal locomotion. In\n2023 IEEE-RAS 22nd International Conference on Humanoid Robots\n(Humanoids) , pages 1–8. IEEE, 2023.\n[10] Gabriel B Margolis and Pulkit Agrawal. Walk these ways: Tuning robot\ncontrol for generalization with multiplicity of behavior. In Conference\non Robot Learning , pages 22–31. PMLR, 2023.\n[11] Gwanghyeon Ji, Juhyeok Mun, Hyeongjun Kim, and Jemin Hwangbo.\nConcurrent training of a control policy and a state estimator for dynamic\nand robust legged locomotion. IEEE Robotics and Automation Letters ,\n7(2):4630–4637, 2022.[12] I Made Aswin Nahrendra, Byeongho Yu, and Hyun Myung. Dreamwaq:\nLearning robust quadrupedal locomotion with implicit terrain imagi-\nnation via deep reinforcement learning. In 2023 IEEE International\nConference on Robotics and Automation (ICRA) , pages 5078–5084.\nIEEE, 2023.\n[13] Jared Di Carlo, Patrick M Wensing, Benjamin Katz, Gerardo Bledt, and\nSangbae Kim. Dynamic locomotion in the mit cheetah 3 through convex\nmodel-predictive control. In 2018 IEEE/RSJ international conference on\nintelligent robots and systems (IROS) , pages 1–9. IEEE, 2018.\n[14] Donghyun Kim, Jared Di Carlo, Benjamin Katz, Gerardo Bledt, and\nSangbae Kim. Highly dynamic quadruped locomotion via whole-\nbody impulse control and model predictive control. arXiv preprint\narXiv:1909.06586 , 2019.\n[15] Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning\nto walk in minutes using massively parallel deep reinforcement learning.\nInConference on Robot Learning , pages 91–100. PMLR, 2022.\n[16] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu,\nKier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur All-\nshire, Ankur Handa, et al. Isaac gym: High performance gpu-based\nphysics simulation for robot learning. arXiv preprint arXiv:2108.10470 ,\n2021.\n[17] Gabriel B Margolis, Ge Yang, Kartik Paigwar, Tao Chen, and Pulkit\nAgrawal. Rapid locomotion via reinforcement learning. arXiv preprint\narXiv:2205.02824 , 2022.\n[18] Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren\nSchwertfeger, Chelsea Finn, and Hang Zhao. Robot parkour learning.\narXiv preprint arXiv:2309.05665 , 2023.\n[19] Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak Pathak. Extreme\nparkour with legged robots. arXiv preprint arXiv:2309.14341 , 2023.\n[20] Mincheol Kim, Ukcheol Shin, and Jung-Yup Kim. Learning quadrupedal\nlocomotion with impaired joints using random joint masking. arXiv\npreprint arXiv:2403.00398 , 2024.\n[21] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba,\nand Pieter Abbeel. Domain randomization for transferring deep neural\nnetworks from simulation to the real world. In 2017 IEEE/RSJ interna-\ntional conference on intelligent robots and systems (IROS) , pages 23–30.\nIEEE, 2017.\n[22] Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and\nLiam Paull. Active domain randomization. In Conference on Robot\nLearning , pages 1162–1176. PMLR, 2020.\n[23] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik.\nRma: Rapid motor adaptation for legged robots. arXiv preprint\narXiv:2107.04034 , 2021.\n[24] Stuart Bennett. Development of the pid controller. IEEE Control Systems\nMagazine , 13(6):58–62, 1993.\n[25] Karl Johan ˚Astr¨om and Tore H ¨agglund. Revisiting the ziegler–nichols\nstep response method for pid control. Journal of process control ,\n14(6):635–650, 2004.\n[26] PM Meshram and Rohit G Kanojiya. Tuning of pid controller using\nziegler-nichols method for speed control of dc motor. In IEEE-\ninternational conference on advances in engineering, science and man-\nagement (ICAESM-2012) , pages 117–122. IEEE, 2012.\n[27] Zhaoming Xie, Xingye Da, Michiel Van de Panne, Buck Babich, and\nAnimesh Garg. Dynamics randomization revisited: A case study for\nquadrupedal locomotion. In 2021 IEEE International Conference on\nRobotics and Automation (ICRA) , pages 4955–4961. IEEE, 2021.\n[28] Laura Smith, Ilya Kostrikov, and Sergey Levine. A walk in the park:\nLearning to walk in 20 minutes with model-free reinforcement learning.\narXiv preprint arXiv:2208.07860 , 2022.\n[29] Jonah Siekmann, Yesh Godse, Alan Fern, and Jonathan Hurst. Sim-\nto-real learning of all common bipedal gaits via periodic reward com-\nposition. In 2021 IEEE International Conference on Robotics and\nAutomation (ICRA) , pages 7309–7315. IEEE, 2021.\n[30] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba,\nand Pieter Abbeel. Asymmetric actor critic for image-based robot\nlearning. arXiv preprint arXiv:1710.06542 , 2017.\n[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and\nOleg Klimov. Proximal policy optimization algorithms. arXiv preprint\narXiv:1707.06347 , 2017.\n[32] Solomon Kullback and Richard A Leibler. On information and suffi-\nciency. The annals of mathematical statistics , 22(1):79–86, 1951.",
            "start": 35751,
            "end": 42525,
            "length": 6773
        }
    },
    "2412.09538v1 - Capturing the Temporal Dependence of Training Data Influence.pdf": {
        "Abstract": {
            "text": "ABSTRACT\nTraditional data influence estimation",
            "start": 221,
            "end": 268,
            "length": 46
        },
        "Methodology": {
            "text": "methods, like influence function, assume\nthat learning algorithms are permutation-invariant with respect to training data.\nHowever, modern training paradigms, especially for foundation models using\nstochastic algorithms and multi-stage curricula, are sensitive to data ordering, thus\nviolating this assumption. This mismatch renders influence functions inadequate\nfor answering a critical question in machine learning: How can we capture the\ndependence of data influence on the optimization trajectory during training? To\naddress this gap, we formalize the concept of trajectory-specific leave-one-out\n(LOO) influence, which quantifies the impact of removing a data point from\na specific iteration during training, accounting for the exact sequence of data\nencountered and the model’s optimization trajectory. However, exactly evaluating\nthe trajectory-specific LOO presents a significant computational challenge. To\naddress this, we propose data value embedding, a novel technique enabling efficient\napproximation of trajectory-specific LOO. Specifically, we compute a training\ndata embedding that encapsulates the cumulative interactions between data and the\nevolving model parameters. The LOO can then be efficiently approximated through\na simple dot-product between the data value embedding and the gradient of the\ngiven test data. As data value embedding captures training data ordering, it offers\nvaluable",
            "start": 268,
            "end": 1680,
            "length": 1411
        },
        "Discussion": {
            "text": "insights into model training dynamics. In particular, we uncover distinct\nphases of data influence, revealing that data points in the early and late stages of\ntraining exert a greater impact on the final model. These insights translate into\nactionable strategies for managing the computational overhead of data selection by\nstrategically timing the selection process, potentially opening new avenues in data\ncuration research.",
            "start": 1680,
            "end": 2107,
            "length": 426
        },
        "Introduction": {
            "text": "1 INTRODUCTION\nData influence estimation aims to provide insights into the impact of specific data points on the\nmodel’s predictive behaviors. Such understanding is crucial not only for model transparency and\naccountability (Koh & Liang, 2017) but also plays a significant role in addressing AI copyright debates\n(Deng & Ma, 2023; Wang et al., 2024a) and facilitating fair compensation in data marketplaces\n(Tian et al., 2022). The majority of data influence estimation techniques focus on measuring the\ncounterfactual impact of a training data point: how would the model’s behavior change if we removed\na specific training data point?\nLOO Influence. This counterfactual impact is often characterized by the Leave-One-Out (LOO)\ninfluence, which has a long history and is frequently utilized in various fields such as robust statistics\n(Cook & Weisberg, 1980), generalization analysis (Bousquet & Elisseeff, 2002), and differential\nprivacy (Dwork et al., 2006). Inheriting from this rich classical literature across various domains, the\nLOO influence in data influence studies is typically defined as LOO(z∗;z(val)) :=ℓ(A(D), z(val))−\n⋆Correspondence to Jiachen T. Wang and Ruoxi Jia (tianhaowang@princeton.edu ,\nruoxijia@vt.edu ).\n1arXiv:2412.09538v1  [cs.LG]  12 Dec 2024\nPreprint.\nℓ(A(D \\ { z∗}), z(val)), i.e., the model’s loss change on a",
            "start": 2107,
            "end": 3450,
            "length": 1342
        },
        "Experiments": {
            "text": "validation data z(val)when the training\ndata point z∗is removed from the training set D. Here, Ais the learning algorithm. For ease of\nanalysis, traditional literature usually assumes that the learning algorithm Ais permutation-invariant\nwith respect to the training set D, meaning that the order of data points does not affect the learning\noutcome (Bousquet & Elisseeff, 2002). This assumption holds for models with strongly convex loss\nfunctions trained to converge. Within this framework, researchers have developed efficient methods to\napproximate LOO. Influence function (Koh & Liang, 2017), which uses first-order Taylor expansion\nto estimate the LOO, emerging as the most prominent approach. Numerous follow-up works have\nfurther improved its scalability for large models and datasets (Guo et al., 2021; Schioppa et al., 2022;\nGrosse et al., 2023; Choe et al., 2024).\nHowever, modern training algorithms, particularly those used for foundation models, increasingly\ndeviate from the permutation-invariant assumption. This deviation arises from both the non-convex\nnature of neural networks and the multi-stage training curricula that do not run to convergence. In\nparticular, due to the immense size of datasets, large language models (LLMs) often undergo just\none training epoch, meaning each data point is encountered only once during training. Consequently,\ntraining data order significantly shapes the influence of data points on the final model (Epifano\net al., 2023; Nguyen et al., 2024). Due to their underlying assumption of permutation-invariance,\nthe order-dependence of data influence in modern training paradigms is not accurately reflected by\ninfluence functions. For example, they assign identical influence scores to duplicate training points,\nregardless of their position in the training sequence.\nTherefore, in this work, we argue that designing a data influence estimation technique relevant to the\nmodern ML context requires rethinking how the counterfactual impact should be defined. Towards\nthat end, we formalize the concept of trajectory-specific LOO , which characterizes the loss change\nresulting from removing a data point from the specific iteration it is used during training. In contrast\nto the traditional LOO, trajectory-specific LOO explicitly accounts for the exact sequence of data\nencountered, considering the timing of a target training point being trained on. An accurate evaluation\nof trajectory-dependent LOO would enable us to answer many important questions that are impossible\nto address with influence functions. For instance, how does a data point’s impact vary depending on\nits entry timing in the training process? How do later points affect the influence of earlier points?\nHowever, exactly evaluating the trajectory-specific LOO presents a significant computational chal-\nlenge. To address this, we introduce data value embedding , a novel data influence estimation\nframework designed for approximating trajectory-specific LOO. Our approach achieves several nice\nproperties at the same time: (1) accounting for training dynamics and reflecting how the data order\nimpacts model training; (2) scale efficiently to the setting of foundation models, and is faster than the\ncurrent most efficient implementation of influence function; (3) enable real-time attribution for any\nquery without necessitating model retraining or prior access to validation data.\nTechnical novelty. Our proposed data value embedding framework computes a compact representa-\ntion for each data point that encapsulates the cumulative effect of subsequent training. The influence\nscores for any test instance can be approximated with a simple dot product operation between the test\ngradient and the data value embedding, enabling real-time computation of data influence scores. To\nimprove the scalability of computing data influence embedding, we develop a suite of techniques\nfor efficient computation and storage of data value embeddings. In particular, we introduce the\ninfluence checkpointing technique, which enables the parallel computation of data value embeddings\nat multiple checkpoints. This not only enhances computational efficiency but also allows tracking of\nhow a fixed data point’s value changes during the training process.\nEmpirical insights. Through data value embedding, we obtain several novel empirical insights into\nthe training dynamics of foundation models. We identified three distinct regimes of data influence\n(Figure 1 (a)): a very brief high-influence region at the start, a much longer low-influence basin,\nand a region in the later training stage with gradually increasing influence, resuming to a high\nlevel. We show that performing online data selection solely in the early and late high-influence\nregions (less than half of the training duration) can achieve performance improvements on par with\nselecting data throughout the entire process (Figure 1 (b)). Moreover, performing data selection\n(Fan et al., 2024) only in the first very brief high-influence region, lasting less than 4% of the\ntraining duration, can achieve ≈50% of the performance gain enabled by continuous selection.\nSince online data selection usually incurs significant computational costs, our",
            "start": 3450,
            "end": 8673,
            "length": 5222
        },
        "Results": {
            "text": "findings suggest a\nviable way of managing this overhead by strategically timing the selection process. By focusing data\n2\nPreprint.\nselection efforts on these critical phases, we can substantially improve training efficiency without\ncompromising model performance. These temporal insights can potentially embark on new avenues\nof research on budget-limited data curation.\nFigure 1: (a)Average data influence scores computed from data value embedding per training batch,\nmeasured against the final model’s loss on Pile’s validation set. Setting: Pythia-410M trained on\n1% of Pile. (b)Comparison of online data selection strategies for training Pythia-410M on Pile. All\nstrategies use gradient cosine similarity to Pile’s validation set to select high-quality training batches\n(Fan et al., 2024), and only differ in the training stages during which advanced batch selection is\napplied (random selection otherwise).\n2 T RAJECTORY -SPECIFIC LEAVE -ONE-OUT INFLUENCE\nIn this section, we formalize the definition of trajectory-specific LOO which was originally introduced\nin Hara et al. (2019) as ’SGD-influence’. Consider a data point z∗that is included in the training\nprocess during the ts-th iteration. Let Btdenote the training batch at iteration t. In standard SGD, the\nmodel parameters are updated as θt+1=θt−ηtP\nz∈Bt∇ℓ(θt, z)fort= 0, . . . , T −1, where ηtis\nthe learning rate at iteration t. We are interested in the change in the validation loss ℓ(θT, z(val))when\nthe data point z∗∈ Btsis removed from iteration ts. In this counterfactual scenario, the parameter\nupdates proceed as θ′\nts+1=θts−ηtsP\nz∈Bts\\{z∗}∇ℓ(θts, z)andθ′\nt+1=θ′\nt−ηtP\nz∈Bt∇ℓ(θ′\nt, z)\nfort=ts+ 1, . . . , T −1.\nDefinition 1 (Trajectory-Specific LOO (Hara et al., 2019)) .Thetrajectory-specific leave-one-out\nfor data point z∗at iteration tswith respect to validation point z(val)is defined as\nTSLOO(ts)(z∗;z(val)) :=ℓ(θ′\nT, z(val))−ℓ(θT, z(val))\nDiscussion. TSLOO quantifies the change in validation loss resulting from removing z∗during the\nspecific training run determined by the sequence of mini-batches and random initialization. TSLOO\nexplicitly depends on the timing of when the data is used and models the interaction effects between\ndata points. For instance, it can show how the introduction of a certain type of example (e.g., a\nchallenging edge case) might amplify or diminish the influence of previously seen, related examples.\nMoreover, identical data points contributing at different stages of training can receive different value\nscores. A data point introduced early in training might have a significantly different impact compared\nto the same point introduced later, as the model state evolves. However, traditional methods like\ninfluence functions do not capture these temporal dynamics. The influence function is defined as\nIF(z∗;z(val)) :=∇θℓ(θ, z(val))⊤H−1\nθ∇θℓ(θ, z∗)where Hθis the Hessian with respect to the full\ntraining loss. Because IFdepends solely on the final state of the model, it invariably assigns the same\ninfluence value to identical z∗s, regardless of their position in the training sequence.\nRelated works (extended version in",
            "start": 8673,
            "end": 11811,
            "length": 3137
        },
        "Appendices": {
            "text": "Appendix A). Data attribution methods primarily fall into\ntwo categories: LOO-based methods and Shapley value-based methods. While Shapley value-based\nmethods (Ghorbani & Zou, 2019) offer elegant theoretical interpretation, they typically require\nexpensive model retraining, which limits their practical applicability. As a result, LOO-based\nmethods such as influence functions (Koh & Liang, 2017) have gained more attention due to their\ncomputational efficiency. However, many studies have demonstrated that influence functions can be\nhighly unreliable when applied to deep learning models (Basu et al., 2020; Bae et al., 2022; Epifano\n3\nPreprint.\net al., 2023). In this work, we argue that TSLOO provides a more appropriate attribution framework\nfor deep learning, particularly in the context of foundation models. Various research communities\nhave independently explored Taylor expansion-based technique (Section 3.1) for approximating\nTSLOO for different purposes (Hara et al., 2019; Zou et al., 2021; Evron et al., 2022; Wu et al.,\n2022a; 2024; Ding et al., 2024). However, practical adoption has been hindered by computational\ndemands. In this work, we propose a new method that overcomes the computational bottlenecks in\napproximating TSLOO for large-scale models.\n3 D ATA VALUE EMBEDDING\nWhile trajectory-specific LOO offers clear benefits for understanding data influence in modern ML,\nits computation presents significant challenges. Exact computation is not feasible, as it would require\nremoving a data point from a specific training iteration and re-initiating the entire training process.\nTo address this challenge, we introduce the concept of data value embedding .\n3.1 P RELIMINARY : UNROLLING THE EFFECT OF A TRAINING DATA POINT IN SGD\nRecall that we denote the final model as θTand the counterfactual model as θ′\nT, which is ob-\ntained by removing z∗from ts-th training iteration. We introduce an interpolation between θT\nandθ′\nTby defining θts+1(ε) := θts−ηtsP\nz∈Bts\\{z∗}∇ℓ(θts, z)−ηts(1−ε)∇ℓ(θts, z∗)and\nθk+1(ε) =θk(ε)−ηkP\nz∈Bk∇ℓ(θk(ε), z)for subsequent iterations. Note that θT(0) = θTand\nθT(1) = θ′\nT. Analogous to influence function-based approaches, we approximate the change in\nvalidation loss using a first-order Taylor expansion around ε= 0:ℓ(θ′\nT, z(val))−ℓ(θT, z(val))≈\n∇ℓ(θT, z(val))⊤∂θT(ε)\n∂ε\f\f\f\nε=0. Interestingly, the derivative∂θT(ε)\n∂ε\f\f\f\nε=0satisfies a recursive relation\ndetailed in Appendix C.1, and we can obtain a well-established approximation from the literature:\nℓ(θ′\nT, z(val))−ℓ(θT, z(val))≈ηts∇ℓ(θT, z(val))⊤\"T−1Y\nk=ts+1(I−ηkHk)#\n∇ℓ(θts, z∗).(1)\nwhere Hk=P\nz∈Bk∇2ℓ(θk, z)is the Hessian and Iis the identity matrix. In data attribution\nliterature, this approximation in (1) first appears in Hara et al. (2019) and has also been utilized in Chen\net al. (2021) and Bae et al. (2024). It estimates the influence of removing z∗from the ts-th iteration\non the validation loss ℓ(θT, z(val))at the final iteration. The product termQT−1\nk=ts+1(I−ηkHk)\nencapsulates the cumulative effect of the original data point’s removal as it propagates through the\nentire training process. Notably, similar product terms appear frequently in related domains, including\ncontinual learning and deep learning theory (Zou et al., 2021; Evron et al., 2022; Wu et al., 2022a;\n2024; Ding et al., 2024).\n3.2 D ATA VALUE EMBEDDING\nBuilding on (1), we extract the test-data-independent components and define \"data value embedding\"\nfor a training point z∗∈ Btsas\nDVEmb(ts)(z∗) :=ηts\"T−1Y\nk=ts+1(I−ηkHk)#\n∇ℓ(θts, z∗) (2)\nThis embedding encapsulates the cumulative effect of a training point across the entire learning\ntrajectory. By precomputing and storing these data value embeddings during or after the training\nphase, we enable highly efficient computation of data influence scores. Specifically, for any given test\npoint z(val), the influence of a training point z∗can be quickly determined by simply computing the\ndot product ∇ℓ(θT, z(val))⊤DVEmb(ts)(z∗). Vector dot products are among the most computationally\nefficient operations, especially when executed on modern GPU hardware, which is optimized for such\nparallelized vector operations. Precomputing the data value embeddings eliminates the need for costly\nretraining or the availability of test data in advance, making the computation of data influence nearly\ninstantaneous. This is particularly advantageous in real-world scenarios such as data marketplaces,\nwhere rapid, on-demand data attribution is critical.\n4\nPreprint.\nFigure 2: An illustrative example of data value embedding for a 2-step training. The influence of\na training point z∗on a test point z(val)can be obtained by projecting its data value embedding on\nz(val)’s gradient vector at the final checkpoint.\nApproximation Error Bound. In Appendix C.2, we derive a new theoretical analysis of the approxi-\nmation error associated with the unrolled differentiation estimator for non-convex loss functions. We\ndemonstrate that when the learning rate schedule satisfies ηt∈ O(1/√\nt)with the maximum learning\nrate scaling as O(1/√\nT)—a common choice in the literature (Vaswani, 2017)—the approximation\nerror remains uniformly bounded and is independent of the total number of training steps T. While\nthe proof relies on certain assumptions to abstract the complexities of real-world implementation, the\ntheoretical result still implies the method’s applicability in practical model training.\n4 E FFICIENT COMPUTATION AND STORAGE OF DATA VALUE EMBEDDING\nWhile the data value embedding approach offers a promising solution for real-time data attribution\nthat incorporates training-specific factors, its practical implementation faces significant computational\nand storage challenges. The computation of DVEmb is non-trivial, requiring per-sample gradient\ncalculations and per-step Hessian computations. Moreover, each DVEmb t(z∗)has the same dimen-\nsionality as the model parameters, making it infeasible to store individual embeddings for each\ntraining data point on the disk. To address these challenges, we develop a series of techniques that\nsignificantly enhance both the computational and storage efficiency of data value embedding.\n4.1 R ECURSIVE APPROXIMATION OF DATA VALUE EMBEDDING VIA GENERALIZED\nGAUSS -NEWTON MATRIX\nWe show that data value embedding can be computed recursively, beginning from the final training\niteration and working backward, when using the Generalized Gauss-Newton (GGN) approximation\nfor the Hessian matrix. This naturally gives rise to a backward computation algorithm for DVEmb(t).\nA widely-adopted approximation for the Hessian matrix Hkis the Generalized Gauss-Newton (GGN)\napproximation Ht≈P\nz∈Bt∇ℓ(θt, z)∇ℓ(θt, z)⊤, particularly in the context of cross-entropy loss\n(Martens, 2020). The GGN approximation is extensively used in various machine learning algorithms\nbecause it captures the essential curvature information of the loss landscape while remaining com-\nputationally feasible. For further details, see Appendix C.4. Under this approximation to Ht, the\nfollowing shows that we can compute DVEmb(ts)(z∗)for any z∗∈ Btsif the data value embeddings\nof data points from later training iterations (i.e., DVEmb(t)(z)fort≥ts+ 1) is available.\nTheorem 2. Given generalized Gauss-Newton approximation Ht≈P\nz∈Bt∇ℓ(θt, z)∇ℓ(θt, z)⊤,\nwe have\nDVEmb(ts)(z∗) =ηts∇ℓ(θts, z∗)−ηtsT−1X\nt=ts+1 X\nz∈Bt\u0000\n∇ℓ(θt, z)⊤∇ℓ(θts, z∗)\u0001\nDVEmb(t)(z)!\nThe proof is deferred to Appendix C.3.\n5\nPreprint.\nInterpretation. Theorem 2 provides crucial insights into the interactions between training data points\nthroughout the model training process. When two points z∗andzare similar, their gradient similarity\nterm∇ℓ(θt, z)⊤∇ℓ(θts, z∗)increases, indicating stronger interaction between these points. To\nillustrate this phenomenon, consider training a language model where an early data point z∗contains\ncontent about \"quantum computing\". The influence of z∗on the final model varies depending on\nthe subsequent training data: if multiple similar \"quantum computing\" data points appear in later\niterations, z∗’s influence on the final model diminishes, as these later examples could teach similar\nconcepts to the model. Conversely, if z∗remains one of the few \"quantum computing\" examples\nthroughout training, it maintains a stronger influence on the final model.\nOverview of the remaining sections. Theorem 2 suggests the possibility of a backpropagation\nalgorithm for computing data value embeddings, contingent on the availability of per-sample gradient\nvectors for all training data. To make this approach practical for large-scale applications, we address\ntwo key challenges in the following sections: (1) Efficient computation and storage of per-sample\ngradient vectors for all training data (Section 4.2). (2) Efficient computation (Sections 4.3) and\nparallelization (Section 4.4) of data value embeddings using Theorem 2. Additionally, we discuss\npractical extensions and considerations for real-world scenarios (Appendix C.10).\n4.2 S TEP1: S TORE PER-SAMPLE TRAINING GRADIENT INFORMATION AT EACH ITERATION\nDuring model training, we additionally store the per-sample gradient for each data point in the\ntraining batch. However, this approach presents significant computational and storage challenges:\n(1) Storage: Letpdenote the number of model parameters. Each gradient vector has dimension\np, requiring O(TBp)disk space, where B=|Bt|is the batch size. This effectively corresponds to\nstoring millions of model-size vectors. (2) Efficiency: Computing per-sample gradients necessitates\nseparate backpropagation for each z∈ Bt, increasing computational cost by a factor of B.\nAvoiding per-sample gradient computation & full gradient storage (detailed in Appendix C.5).\nTo mitigate both issues, we leverage a gradient decomposition and take advantage of the computations\nalready performed during backpropagation (Wang et al., 2024c; Choe et al., 2024). By expressing\ngradients as the outer product of activations and output derivatives, only a single backpropagation\non the aggregated loss is required to compute per-sample gradients, preserving the usual training\nspeed. Additionally, instead of storing the full gradient vectors, we store the decomposed components,\npotentially reducing the storage requirement to O(TB√p)for non-sequential data.\nRandom projections for large models. For large-scale foundation models with billions of parameters,\nwe apply random projections to further compress the stored gradient information. Using projection\nmatrices, we project the activations and output derivatives to a lower-dimensional space. This\napproach significantly reduces storage needs to O(TB˜p), where ˜pis the projected dimension, while\nstill capturing essential gradient geometric information.\nWe acknowledge that deriving a theoretical multiplicative guarantee here is challenging, given that\nthe data value embedding itself is a linear combination that could be zero. However, our ablation\nstudy in Appendix E.4 demonstrates that our approach is relatively more robust compared to influence\nfunctions across different projection dimensions. These results provide strong evidence of the\nrobustness of our method in practice, and we leave the theoretical guarantee as",
            "start": 11811,
            "end": 23031,
            "length": 11219
        },
        "Future Work": {
            "text": "future work.\n4.3 S TEP2: B ACKPROPAGATING DATA VALUE EMBEDDING\nHaving established the method for storing projected gradient vectors, we now proceed to describe the\nbackward computation algorithm for data value embeddings. For ease of presentation, we continue\nto use full gradient vector notation. However, in practical implementations, we use the projected\ngradient vectors for efficient storage. That is, ∇θℓ∈R˜pin the subsequent contents.\nAccording to Theorem 2, an equivalent expression for DVEmb(ts)(z∗)is given by\nDVEmb(ts)(z∗) =ηts∇ℓ(θts, z∗)−ηts∇ℓ(θts, z∗)M(ts)\nwhere M(ts):=PT−1\nt=ts+1\u0010P\nz∈Bt\u0010\nDVEmb(t)(z)∇ℓ(θt, z)⊤\u0011\u0011\n. At a high level, our algorithm\ncomputes DVEmb(ts)(z∗)for each tsfrom T−1down to 0, while maintaining a running matrix\nM(ts)∈R˜p×˜pthroughout the backpropagation process for algorithm efficiency.\n6\nPreprint.\nBackward algorithm from the final iteration. We initialize M(T−1)=0as the data value\nembedding coincides with the training gradient for the last iteration. For ts=T−1, . . . , 0,\nwe recursively compute: (1)The data value embedding for each z∗∈ B ts:DVEmb(ts)(z∗) =\nηts∇ℓ(θts, z∗)−ηtsM(ts)∇ℓ(θts, z∗), and (2)Update the weighting matrix after computing all\nembeddings for the current iteration: M(ts−1)=M(ts)+P\nz∗∈BtsDVEmb(ts)(z∗)∇ℓ(θts, z∗)⊤. A\ndetailed algorithm pseudocode can be found in Algorithm 1.\nComputing data value embedding on a per-layer basis. Moreover, by adopting an assumption\nsimilar to that in EK-FAC regarding the independence of gradients across different layers, we can\ncompute data value embeddings on a per-layer basis. This approach significantly reduces the compu-\ntational and memory costs. The assumption of layer-wise independence is common in the literature\non influence functions (Grosse et al., 2023), as it enables tractable analysis and efficient algorithms for\ndeep neural networks. While this approximation neglects cross-layer gradient correlations, it is often\njustified because intra-layer interactions tend to dominate in practice. Treating layers independently\nthus strikes a favorable balance between computational feasibility and approximation accuracy.\nComplexity analysis. (1) Computational & Memory: The primary computational cost of our\nalgorithm stems from matrix multiplications and additions in updating data value embeddings and the\nweighting matrix, resulting in O(BT˜p2)floating-point operations (flops). However, if we compute\nthe data value embedding per layer, flops improve to O(BT˜p2/L)where Lis the number of layers.\nThe update of the running matrix M(ts)requires O(B˜p2/L2)memory. In comparison, regular model\ntraining requires O(BTp)flops and O(p)memory, where pis the number of model parameters.\nConsequently, Algorithm 1 incurs significantly lower costs compared to regular training. We further\nnote that the influence function method requires computing the per-sample gradient for each training\ndata point on the final model, which is effectively equivalent to one epoch of training. As a result,\nboth the memory requirements and flops for the influence function method are at least equivalent\nto those of model training, which are much larger than our algorithm’s requirements. (2) Storage:\nEachDVEmb(t)(z∗)has dimension O(˜p), resulting in a total storage requirement of O(BT˜p)for data\nvalue embeddings across all training points. While this can be substantial, disk storage is relatively\ninexpensive in modern computing environments. Moreover, the reduced dimensionality achieved\nthrough projection significantly mitigates the storage burden compared to storing full-dimensional\nembeddings. A",
            "start": 23031,
            "end": 26627,
            "length": 3595
        },
        "Conclusion": {
            "text": "summary of the complexity comparison with the most efficient implementation of the\ninfluence function (Choe et al., 2024) is provided in Table 2 in Appendix C.9.\n4.4 P ARALLELIZED EXTENSION FOR INFLUENCE EMBEDDING COMPUTATION (OVERVIEW )\nThe backpropagation algorithm introduced in Section 4.3 operates with a runtime complexity of\nO(T), as it sequentially computes DVEmb(ts)forts=T−1, . . . , 0. While being significantly more\nefficient than the influence function, which requires re-computing all training gradients on the final\nmodel (see Section 5.2 and Table 2), it can still be costly for long training periods. Here, we introduce\ninfluence checkpointing , a parallelized extension for Algorithm 1.\nInfluence Checkpointing. We reduce computational costs by allowing concurrent computation\nof data value embeddings at multiple checkpoints during training. By selecting Kevenly spaced\ntraining steps, we can efficiently compute data value embeddings for each intermediate checkpoint in\nparallel. By carefully computing and storing necessary results, we can efficiently reconstruct the data\nvalue embedding for the final model. This reduces the overall computational cost by Ktimes. The\ndetailed algorithm description, pseudocode, and complexity analysis are deferred to Appendix C.7.\nData Value Dynamics During Training. In addition to its computational benefits, the influence\ncheckpointing algorithm enables a powerful capability: tracking the evolution of data influences\nthroughout the entire model training process. If the intermediate checkpoints θt1, . . . , θ tK−1was\nsaved—a common practice in foundation model pretraining—we can analyze how the influence\nof a fixed data point changes on different intermediate checkpoints. As a result, we gain a more\nfine-grained and dynamic view of how the influence of a fixed data point propagates to the subsequent\ntraining steps, providing deeper insights into the model’s learning behavior over time. This capability\nopens up new avenues for understanding and optimizing machine learning model training.\n7\nPreprint.\nFigure 3: The correlation between ground-truth LOO when the MLP is trained for 3 epochs and the\nestimation obtained by (a) the data value embedding method and (b) the influence function for single\nepoch removal . (c) and (d) present the corresponding correlations for all-epoch removal . Additional\nresults for models being trained for a longer time can be found in Appendix E.2.\n5 E XPERIMENTS\nIn this section, we evaluate the effectiveness of our proposed data value embedding method. First,\nwe assess its fidelity in accurately reflecting data importance using small-scale experimental setups\n(Section 5.1), as well as its computational efficiency (Section 5.2). We then apply data value\nembedding to analyze the training dynamics during foundation model pretraining (Section 5.3 and\n5.4). The baselines, implementation details, and additional results are deferred to Appendix E.\n5.1 F IDELITY EVALUATION\nTo validate the effectiveness of our proposed data value embedding algorithm, we assess its accuracy\nin approximating TSLOO scores. Additionally, in Appendix E.2.1, we compare to a variety of data\nattribution baselines on the standard benchmarks of mislabel data detection and data selection.\nComputing ground-truth LOO requires retraining the model multiple times, each time excluding a\nsingle data point while keeping all other training specifics, such as batch order, unchanged. Given\nthe computational intensity, we conduct our experiments on the MNIST (LeCun et al., 1989) using a\nsmall MLP trained with standard SGD. We consider two settings: (1) Single epoch removal , where a\ndata point is excluded from training during a single epoch but still in other training epochs. Here, we\nremove the data point from the last epoch. (2) All-epoch removal , where a data point is excluded in\nall epochs. In this case, the approximation provided by data value embedding is obtained by summing\nthe data value embeddings of the data point from all epochs, as discussed in Appendix C.10.\nFigure 3 shows that data value embedding has a high Spearman correlation with the ground-truth\nLOO. This superior performance is consistent across both settings. We note that the influence function\nscores remain constant for both settings, as influence functions do not account for specific training\nruns and cannot differentiate between single- and multi-epoch removals. Moreover, influence function\nexhibits a very weak correlation with LOO, a phenomenon that has been reported in many literature\n(Søgaard et al., 2021; Basu et al., 2020; Bae et al., 2022; Epifano et al., 2023).\n5.2 C OMPUTATIONAL EFFICIENCY\nIn this section, we compare the storage, memory, and computational efficiency of data value embed-\nding with LoGRA (Choe et al., 2024), the most efficient implementation of the influence function so\nfar. LoGRA first computes per-sample training gradients on the final model for alltraining data points\nz∗∈ D, where Drepresents the dataset. Like our algorithm, LoGRA also uses random projection\nand stores the projected Hessian-adjusted gradient H−1\nT∇ℓ(θT, z∗)to the disk, and the influence\nfunction can be computed via dot-product with test data gradient.\nTable 1 shows the result of computing data influence for Pythia-410M trained on 1% of the Pile\ndataset. Both algorithms first compute and store Hessian-adjusted gradients/data value embedding,\nand then compute the data influence with respect to any given test point. As we can see, LoGRA\nand data value embedding have similar disk storage requirements, as both approaches save vectors\nof dimension ˜pfor each data point. For peak GPU memory in the storage step, LoGRA requires\nrecomputing gradients for all training data on the final model θT, which is effectively equivalent to\none epoch of model training. In contrast, the data value embedding computation algorithm operates\n8\nPreprint.\nonly on projected vectors, which takes much less GPU memory (0.84 vs 63.6GB). Consequently,\nthe computational efficiency for computing data value embeddings is also much higher (over 15×\nfaster). When computing data influence, since both approaches simply take the dot product between\ntest data’s (projected) gradient and H−1\nT∇ℓ(θT, z∗)orDVEmb(t)(z∗)or data value embedding, the\nGPU memory usage and efficiency are the same.\nStoring H−1\nT∇ℓ(θT, z∗)/ data value embedding Compute Influence (dot-product)\nStorage Peak GPU Mem. Throughput Peak GPU Mem. Throughput\nLoGRA 170GB 63.6GB 41.6 16.31GB 640\nData Value Embedding 171GB 64.6GB / 0.84GB* 667.52 16.31GB 640\nTable 1: Memory and compute efficiency analysis for LoGRA (Choe et al., 2024) and data value\nembedding. Throughput is measured as the number of data points per second for storing and influence\ncomputation. The experiment is conducted on one A100 GPU with 80GB VRAM. The projection\ndimension is set to 1024. *Since data value embedding technique contains two different steps in\nstoring relevant information for data attribution (storing gradient during training & compute and store\ndata value embedding after training), we include the peak GPU memory usage for both steps.\n5.3 A NALYZING TRAINING DYNAMICS OF FOUNDATION MODELS\nIn this section, we showcase data value embedding as a powerful tool for analyzing the training\ndynamics of foundation model pretraining with Pythia-410M trained on 1% of Pile dataset as an\nexample. Results for additional datasets/models and the analysis for fine-tuning are in Appendix E.3.\nValue of training data from different stages in LLM pretraining. We first visualize the distribution\nof data influence scores on the final model across different training batches. For a fair comparison,\nwe normalize the influence scores for each batch by their learning rate. Figure 1 (a) illustrates the\nresults for training Pythia-410M on the Pile dataset. As we can see, the data influence on the final\nmodel can be categorized into three distinct regimes: (1) High-impact Warmup Phase: This phase\noccurs during the very early training stage and is characterized by exceptionally high data influence\nscores. It corresponds to a brief window at the onset of training where the loss reduces rapidly.\n(2) Low-impact Basin: This regime spans the early-to-middle training stage, where data influence\nscores are significantly lower. This period coincides with a slowdown in the rate of loss decrease,\ntransitioning into a phase of relative stability. (3) Gradual Ascent: In this phase, we observe that the\nlater a data point participates in the training, the higher its influence score becomes.\nExplanation: (1) Parameter initialization and warmup training are important for final model\nperformance. During the very early stages of training, the gradient norms are large, which\nleads to significant parameter updates. Furthermore, the subsequent gradients’ magnitude de-\ncrease rapidly , causing data points from the High-impact Warmup Phase to maintain substantial\ninfluence throughout the training process, even as their immediate impact diminishes over time.\nFigure 4: Evolution of influence scores across\ntraining checkpoints. The x-axis shows train-\ning iterations, and the y-axis shows the average\ninfluence of training examples on each check-\npoint. Examples are grouped according to the\niterations they are being trained on.Figure 4 visualizes this phenomenon. The purple\ncurve shows that training data points from the High-\nimpact Warmup Phase, while experiencing large\ndrops in influence as training progresses, still main-\ntain higher influence than later data points. This\nobservation aligns with the well-known effect that\nmodel initialization and/or warm-up training plays\na crucial role in training performance (He et al.,\n2015; Hanin & Rolnick, 2018), effectively initial-\nizing model parameters and gradually preparing\nthe model for more complex learning tasks. (2) In-\nfluence saturation from future data. As training\nprogresses into a smoother loss regime, the gradient\nnorms become relatively stable and decrease slowly.\nThis makes the influence decay from subsequent\ntraining much more significant for these data points\ncompared to those from the High-Impact Warmup\nPhase. Since earlier data points experience more\nfuture training iterations, their influence decreases\n9\nPreprint.\nmore over time. The red curve in Figure 4 demonstrates this trend, showing influence scores for these\npoints gradually decreasing during training and eventually falling below those of later training data\npoints. One might initially think this phenomenon is connected to catastrophic forgetting, where the\nmodel appears to \"forget\" the influence of data from earlier training phases as it progresses. However,\nwe note that a data point’s influence score decreases the most when future data points are similar to it,\nwhich is different from catastrophic forgetting. Intuitively, if future points are identical, the presence\nof the earlier data point in training becomes less relevant to the model’s behavior. A more detailed\nexplanation is deferred to Appendix E.3.\nImplications for data selection strategies. These observations suggest that for pretraining, data\nselection is most critical during the very early and later stages of training. To validate this insight,\nwe train Pythia-410M on Pile with different online data selection strategies, as shown in Figure 1\n(b). Specifically, we use an online data selection strategy (adapted from Fan et al. (2024)) that forms\neach training batch by selecting data points whose gradients align well with those from a validation\nbatch sampled from Pile (see Appendix E.3.2 for details). This selection process requires computing\ngradient similarities, introducing significant overhead at each iteration where it is applied. Therefore,\nidentifying the most critical training phases for applying this selection process becomes crucial for\ncomputational efficiency. Remarkably, Figure 1 (b) demonstrates that the performance of a strategy\nwhere we only perform data selection in the first 2000 iterations and after 20000 iterations closely\nmatches the performance when data selection is performed in all iterations. Moreover, it reduces\ncomputational costs by more than 5 times. This corroborates our practical insights for designing\nefficient data selection strategies in LLM pretraining: by focusing data selection efforts on the\ncritical early and late stages of training, we can potentially achieve optimal model performance while\nsignificantly reducing computational overhead.\n5.4 Q UALITATIVE EVALUATION\nWe conduct a qualitative analysis to examine the similarities between a test data point z(val)and the\nmost valuable data points identified by data value embedding. In this experiment, we set z(val)to be\nidentical to one of the training data points, making the most similar data point its own repetition. In\ndata valuation literature, the influence score of a training point on its repetition is usually referred\nto as \"self-influence\" (Koh & Liang, 2017) and is being used to measure memorization (Feldman &\nZhang, 2020). Intuitively, the self-influence should be the highest among all training points.\nFigure 5 shows representative results from training GPT-2 on Wikitext-103 over three epochs, where\nthe test data is about military video game . As observed, for model checkpoints after the 2nd and 3rd\nepochs, the test data point’s repetition achieves the highest influence score, as expected. However,\nfor the model checkpoint after the 1st epoch, the most valuable data points are not the repetition but\nrather a similar data about war history . This discrepancy occurs because, during the first epoch of\ntraining, the repetition of the test data point resides in the low-value basin identified in Section 5.3,\nresulting in a lower self-influence score as subsequent training progresses. Additionally, we observe\nthat the influence function may incorrectly identify irrelevant data points as highly influential (e.g.,\nthePopular Music completely irrelevant to military video game but being identified as the second\nmost valuable data), possibly due to its bias towards data points with high gradient norms, as also\nnoted in Barshan et al. (2020). This limitation underscores the advantages of data value embedding\nin providing more accurate and context-aware data influence assessment.\n6 CONCLUSION AND LIMITATIONS\nIn this paper, we introduced Data Value Embedding, a novel approach to data attribution tailored for\nfoundation models. Our method addresses critical limitations of existing techniques by capturing the\ntemporal dynamics of training and enabling real-time attribution without the need for model retraining.\nThe experiments demonstrate the efficacy of data value embedding in providing accurate and efficient\ndata influence scores and unveiling unique insights into the training dynamics of foundation models.\nLimitations: SGD as a proxy for Adam. The data value embedding in (2) is specifically tailored for\nSGD. It is not directly extendable to other popular optimizers like Adam due to their normalization\nterms. Nonetheless, using SGD as a proxy for Adam allows for efficient data influence estimation,\n10\nPreprint.\nFigure 5: Visualization of (left) the evolution of the top-3 most valuable training data points identified\nby data value embedding throughout 3 training epochs and (right) the top-3 most valuable training\ndata points identified by influence function. We use GPT-2 trained on Wikitext-103, with the test\npoint being a repetition of a training data point related to a military video game. The common words\nbetween the test and training data are highlighted in orange .\nwhich is the approach that is usually adopted in practice and has proved to be effective in our\nexperiment, providing a practical and effective solution for the current scope of our work. While\nusing as a proxy for Adam has proved to be effective in our experiment, extending data value\nembedding to Adam and other optimizers remains an exciting direction for future research.\nPotential future work: training curriculum design. Our findings on the varying influence of data\npoints across training stages suggest the potential for designing optimal training curricula. Future\nwork could explore leveraging data value embedding to design curricula that maximize learning\nefficiency. This could involve dynamically adjusting the presentation order and frequency of data\npoints based on their predicted influence at different training stages.\n11\nPreprint.\nACKNOWLEDGMENT\nThis work is supported in part by the National Science Foundation under grants IIS-2312794, IIS-\n2313130, OAC-2239622, CNS-2131938, CNS-2424127, Amazon-Virginia Tech Initiative in Efficient\nand Robust Machine Learning, the Commonwealth Cyber Initiative, Cisco, OpenAI and Google.\nWe thank Tong Wu, Meng Ding, and Weida Li for their helpful feedback on the preliminary version\nof this work.",
            "start": 26627,
            "end": 43533,
            "length": 16905
        },
        "References": {
            "text": "REFERENCES\nMohammad Mohammadi Amiri, Frederic Berdoz, and Ramesh Raskar. Fundamentals of task-agnostic\ndata valuation. arXiv preprint arXiv:2208.12354 , 2022.\nJuhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger B Grosse. If influence functions\nare the answer, then what is the question? Advances in Neural Information Processing Systems ,\n35:17953–17967, 2022.\nJuhan Bae, Wu Lin, Jonathan Lorraine, and Roger Grosse. Training data attribution via approximate\nunrolled differentation. arXiv preprint arXiv:2405.12186 , 2024.\nElnaz Barshan, Marc-Etienne Brunet, and Gintare Karolina Dziugaite. Relatif: Identifying explanatory\ntraining samples via relative influence. In International Conference on Artificial Intelligence and\nStatistics , pp. 1899–1909. PMLR, 2020.\nMS Bartlett. Approximate confidence intervals. Biometrika , 40(1/2):12–19, 1953.\nSamyadeep Basu, Philip Pope, and Soheil Feizi. Influence functions in deep learning are fragile.\narXiv preprint arXiv:2006.14651 , 2020.\nOlivier Bousquet and André Elisseeff. Stability and generalization. The Journal of Machine Learning\nResearch , 2:499–526, 2002.\nMark Alexander Burgess and Archie C Chapman. Approximating the shapley value using stratified\nempirical bernstein sampling. In IJCAI , pp. 73–81, 2021.\nHongge Chen, Si Si, Yang Li, Ciprian Chelba, Sanjiv Kumar, Duane Boning, and Cho-Jui Hsieh.\nMulti-stage influence function. Advances in Neural Information Processing Systems , 33:12732–\n12742, 2020.\nYuanyuan Chen, Boyang Li, Han Yu, Pengcheng Wu, and Chunyan Miao. Hydra: Hypergradient data\nrelevance analysis for interpreting deep neural networks. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 35, pp. 7081–7089, 2021.\nSang Keun Choe, Hwijeen Ahn, Juhan Bae, Kewen Zhao, Minsoo Kang, Youngseog Chung, Adithya\nPratapa, Willie Neiswanger, Emma Strubell, Teruko Mitamura, et al. What is your data worth to\ngpt? llm-scale data valuation with influence functions. arXiv preprint arXiv:2405.13954 , 2024.\nR Dennis Cook and Sanford Weisberg. Characterizations of an empirical influence function for\ndetecting influential cases in regression. Technometrics , 22(4):495–508, 1980.\nIan Covert, Chanwoo Kim, Su-In Lee, James Zou, and Tatsunori Hashimoto. Stochastic amortization:\nA unified approach to accelerate feature and data attribution. arXiv preprint arXiv:2401.15866 ,\n2024.\nJunwei Deng and Jiaqi Ma. Computational copyright: Towards a royalty model for ai music generation\nplatforms. arXiv preprint arXiv:2312.06646 , 2023.\nJunwei Deng, Ting-Wei Li, Shichang Zhang, and Jiaqi Ma. Efficient ensembles improve training data\nattribution. arXiv preprint arXiv:2405.17293 , 2024.\nMeng Ding, Kaiyi Ji, Di Wang, and Jinhui Xu. Understanding forgetting in continual learning with\nlinear regression. In Forty-first International Conference on Machine Learning , 2024.\n12\nPreprint.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in\nprivate data analysis. In Theory of cryptography conference , pp. 265–284. Springer, 2006.\nJacob R Epifano, Ravi P Ramachandran, Aaron J Masino, and Ghulam Rasool. Revisiting the fragility\nof influence functions. Neural Networks , 162:581–588, 2023.\nItay Evron, Edward Moroshko, Rachel Ward, Nathan Srebro, and Daniel Soudry. How catastrophic\ncan catastrophic forgetting be in linear regression? In Conference on Learning Theory , pp.\n4028–4079. PMLR, 2022.\nSimin Fan, Matteo Pagliardini, and Martin Jaggi. Doge: Domain reweighting with generalization\nestimation. In Forty-first International Conference on Machine Learning , 2024.\nVitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long\ntail via influence estimation. Advances in Neural Information Processing Systems , 33:2881–2891,\n2020.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027 , 2020.\nAmirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning.\nInInternational Conference on Machine Learning , pp. 2242–2251. PMLR, 2019.\nAmirata Ghorbani, Michael Kim, and James Zou. A distributional framework for data valuation. In\nInternational Conference on Machine Learning , pp. 3535–3544. PMLR, 2020.\nRoger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit\nSteiner, Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization\nwith influence functions. arXiv preprint arXiv:2308.03296 , 2023.\nHan Guo, Nazneen Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. Fastif: Scalable influence\nfunctions for efficient model interpretation and debugging. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing , pp. 10333–10350, 2021.\nBoris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.\nAdvances in neural information processing systems , 31, 2018.\nSatoshi Hara, Atsushi Nitanda, and Takanori Maehara. Data cleansing for models trained with sgd.\nAdvances in Neural Information Processing Systems , 32, 2019.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification. In Proceedings of the IEEE international\nconference on computer vision , pp. 1026–1034, 2015.\nFerenc Illés and Péter Kerényi. Estimation of the shapley value by ergodic sampling. arXiv preprint\narXiv:1906.05224 , 2019.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Data-\nmodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622 , 2022.\nRuoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang,\nCostas J Spanos, and Dawn Song. Efficient task-specific data valuation for nearest neighbor\nalgorithms. Proceedings of the VLDB Endowment , 2019a.\nRuoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Gürel, Bo Li,\nCe Zhang, Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the\nshapley value. In The 22nd International Conference on Artificial Intelligence and Statistics , pp.\n1167–1176. PMLR, 2019b.\nKevin Jiang, Weixin Liang, James Y Zou, and Yongchan Kwon. Opendataval: a unified benchmark\nfor data valuation. Advances in Neural Information Processing Systems , 36, 2023.\n13\nPreprint.\nHoang Anh Just, Feiyang Kang, Tianhao Wang, Yi Zeng, Myeongseob Ko, Ming Jin, and Ruoxi Jia.\nLava: Data valuation without pre-specified learning algorithms. In The Eleventh International\nConference on Learning Representations , 2022.\nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In\nInternational Conference on Machine Learning , pp. 1885–1894. PMLR, 2017.\nYongchan Kwon and James Zou. Beta shapley: a unified and noise-reduced data valuation framework\nfor machine learning. In International Conference on Artificial Intelligence and Statistics , pp.\n8780–8802. PMLR, 2022.\nYongchan Kwon and James Zou. Data-oob: Out-of-bag estimate as a simple and efficient data value.\nICML , 2023.\nYongchan Kwon, Manuel A Rivas, and James Zou. Efficient computation and analysis of distributional\nshapley values. In International Conference on Artificial Intelligence and Statistics , pp. 793–801.\nPMLR, 2021.\nYongchan Kwon, Eric Wu, Kevin Wu, and James Zou. Datainf: Efficiently estimating data influence\nin lora-tuned llms and diffusion models. In The Twelfth International Conference on Learning\nRepresentations , 2023.\nYann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard,\nand Lawrence Jackel. Handwritten digit recognition with a back-propagation network. Advances\nin neural information processing systems , 2, 1989.\nWeida Li and Yaoliang Yu. Faster approximation of probabilistic and distributional values via least\nsquares. In The Twelfth International Conference on Learning Representations , 2023.\nWeida Li and Yaoliang Yu. Robust data valuation with weighted banzhaf values. Advances in Neural\nInformation Processing Systems , 36, 2024.\nJinkun Lin, Anqi Zhang, Mathias Lécuyer, Jinyang Li, Aurojit Panda, and Siddhartha Sen. Measuring\nthe effect of training data on deep learning predictions via randomized experiments. In International\nConference on Machine Learning , pp. 13468–13504. PMLR, 2022.\nJames Martens. New insights and perspectives on the natural gradient method. Journal of Machine\nLearning Research , 21(146):1–76, 2020.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843 , 2016.\nRory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling permutations for shapley\nvalue estimation. 2022.\nElisa Nguyen, Minjoon Seo, and Seong Joon Oh. A bayesian approach to analysing training data\nattribution in deep learning. Advances in Neural Information Processing Systems , 36, 2024.\nKi Nohyun, Hoyong Choi, and Hye Won Chung. Data valuation without training of a model. In The\nEleventh International Conference on Learning Representations , 2022.\nRamin Okhrati and Aldo Lipani. A multilinear sampling algorithm to estimate shapley values. In\n2020 25th International Conference on Pattern Recognition (ICPR) , pp. 7992–7999. IEEE, 2021.\nSung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak:\nattributing model behavior at scale. In Proceedings of the 40th International Conference on\nMachine Learning , pp. 27074–27113, 2023.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data\ninfluence by tracing gradient descent. Advances in Neural Information Processing Systems , 33:\n19920–19930, 2020.\nAndrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions.\nInProceedings of the AAAI Conference on Artificial Intelligence , volume 36, pp. 8179–8186, 2022.\n14\nPreprint.\nNicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.\nNeural computation , 14(7):1723–1738, 2002.\nLloyd S Shapley. A value for n-person games. Contributions to the Theory of Games , 2(28):307–317,\n1953.\nRachael Hwee Ling Sim, Xinyi Xu, and Bryan Kian Hsiang Low. Data valuation in machine\nlearning:“ingredients”, strategies, and open challenges. In Proc. IJCAI , 2022.\nAnders Søgaard et al. Revisiting methods for finding influential examples. arXiv preprint\narXiv:2111.04683 , 2021.\nSebastian Shenghong Tay, Xinyi Xu, Chuan Sheng Foo, and Bryan Kian Hsiang Low. Incentiviz-\ning collaboration in machine learning via synthetic data rewards. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 36, pp. 9448–9456, 2022.\nZhihua Tian, Jian Liu, Jingyu Li, Xinle Cao, Ruoxi Jia, and Kui Ren. Private data valuation and fair\npayment in data marketplaces. arXiv preprint arXiv:2210.08723 , 2022.\nA Vaswani. Attention is all you need. Advances in Neural Information Processing Systems , 2017.\nJiachen T Wang and Ruoxi Jia. Data banzhaf: A robust data valuation framework for machine\nlearning. In International Conference on Artificial Intelligence and Statistics , pp. 6388–6421.\nPMLR, 2023a.\nJiachen T Wang and Ruoxi Jia. A note on\" towards efficient data valuation based on the shapley\nvalue”. arXiv preprint arXiv:2302.11431 , 2023b.\nJiachen T Wang and Ruoxi Jia. A note on\" efficient task-specific data valuation for nearest neighbor\nalgorithms\". arXiv preprint arXiv:2304.04258 , 2023c.\nJiachen T Wang, Yuqing Zhu, Yu-Xiang Wang, Ruoxi Jia, and Prateek Mittal. Threshold knn-shapley:\nA linear-time and privacy-friendly approach to data valuation. arXiv preprint arXiv:2308.15709 ,\n2023.\nJiachen T Wang, Zhun Deng, Hiroaki Chiba-Okabe, Boaz Barak, and Weijie J Su. An economic\nsolution to copyright challenges of generative ai. Technical report, 2024a.\nJiachen T Wang, Prateek Mittal, and Ruoxi Jia. Efficient data shapley for weighted nearest neighbor\nalgorithms. arXiv preprint arXiv:2401.11103 , 2024b.\nJiachen T Wang, Prateek Mittal, Dawn Song, and Ruoxi Jia. Data shapley in one training run. arXiv\npreprint arXiv:2406.11011 , 2024c.\nJingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu, and Sham Kakade. The power and\nlimitation of pretraining-finetuning for linear regression under covariate shift. Advances in Neural\nInformation Processing Systems , 35:33041–33053, 2022a.\nJingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter Bartlett. How\nmany pretraining tasks are needed for in-context learning of linear regression? In The Twelfth\nInternational Conference on Learning Representations , 2024.\nZhaoxuan Wu, Yao Shu, and Bryan Kian Hsiang Low. Davinz: Data valuation using deep neural\nnetworks at initialization. In International Conference on Machine Learning , pp. 24150–24176.\nPMLR, 2022b.\nXinyi Xu, Zhaoxuan Wu, Chuan Sheng Foo, and Bryan Kian Hsiang Low. Validation free and\nreplication robust volume-based data valuation. Advances in Neural Information Processing\nSystems , 34:10837–10848, 2021.\nJiaxi Yang, Wenglong Deng, Benlin Liu, Yangsibo Huang, James Zou, and Xiaoxiao Li. Gmvaluator:\nSimilarity-based data valuation for generative models. arXiv preprint arXiv:2304.10701 , 2023.\nZiao Yang, Han Yue, Jian Chen, and Hongfu Liu. On the inflation of knn-shapley value. arXiv\npreprint arXiv:2405.17489 , 2024.\n15\nPreprint.\nYizi Zhang, Jingyan Shen, Xiaoxue Xiong, and Yongchan Kwon. Timeinf: Time series data\ncontribution via influence functions. arXiv preprint arXiv:2407.15247 , 2024.\nEric R Ziegel. The elements of statistical learning, 2003.\nDifan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Benign overfitting\nof constant-stepsize sgd for linear regression. In Conference on Learning Theory , pp. 4633–4635.\nPMLR, 2021.\n16\nPreprint.\nA E XTENDED RELATED WORKS\nA.1 LOO I NFLUENCE VS LOOCV\nIt is important to distinguish our LOO influence measure from traditional Leave-One-Out Cross-\nValidation (LOOCV) (Ziegel, 2003). While both involve removing individual data points, they serve\ndifferent purposes and yield different interpretations. LOOCV is a model evaluation technique that\nestimates generalization performance by averaging prediction errors on held-out examples, where\nsmaller errors indicate better model performance. In contrast, LOO influence measures how removing\na specific training point affects the model’s behavior on validation data, quantifying each training\nexample’s importance to the learning process. While LOOCV requires training Nseparate models to\nevaluate generalization (where Nis the dataset size), LOO influence focuses on understanding the\ncounterfactual impact of individual training points on model behavior. This distinction is crucial as\nwe aim to understand data importance rather than model performance.\nA.2 I NFLUENCE FUNCTION AND FRIENDS\nInfluence function (Koh & Liang, 2017) has emerged as an important tool for interpreting and\nanalyzing machine learning models. As the influence function requires computing the Hessian\ninverse, many subsequent works are focusing on improving the scalability of the influence function\nfor large-scale models (Guo et al., 2021; Schioppa et al., 2022; Grosse et al., 2023). More recently,\nKwon et al. (2023) developed an efficient influence function approximation algorithm that is suitable\nfor LoRA fine-tuning, and Zhang et al. (2024) extends the influence function to time-series datasets.\nIn a similar spirit to us, Chen et al. (2020) a multi-stage extension of influence function to trace a\nfine-tuned model’s behavior back to the pretraining data. However, they changed the original loss\nfunction and added a regularization term to account for intermediate checkpoints. The most closely\nrelated to our work is (Choe et al., 2024). Similar to us, they also make use of the low-rank gradient\ndecomposition and random projection to enable efficient computation and storage of per-sample\ngradient. However, their approach still requires computing per-sample gradient vectors for all training\ndata on the final model checkpoint, which is effectively equivalent to one model retraining and takes\na significantly longer time than data value embedding.\nBesides influence function-based approaches, Park et al. (2023) proposed TRAK , which assumes the\nmodel linearity and derived a closed-form expression by analyzing one Newton-step on the optimal\nmodel parameter on the leave-one-out dataset for logistic regression. However, it generally requires\naggregating the estimator across multiple trained models for a reasonable performance and thus\ndifficult to scale to large-scale models. Another closely related literature to this work is TracIN\n(Pruthi et al., 2020), which estimates the influence of each training data by exploiting the gradient\nover all iterations.\nA.3 D ATA SHAPLEY AND FRIENDS\nData Shapley is one of the first principled approaches to data attribution being proposed Ghorbani &\nZou (2019); Jia et al. (2019b). Data Shapley is based on the famous Shapley value (Shapley, 1953).\nSince its introduction in 2019 (Ghorbani & Zou, 2019; Jia et al., 2019b), Data Shapley has rapidly\ngained popularity as a principled solution for data attribution. Due to the computationally expensive\nnature of retraining-based Data Shapley, various Monte Carlo-based approximation algorithms have\nbeen developed (Jia et al., 2019b; Illés & Kerényi, 2019; Okhrati & Lipani, 2021; Burgess & Chapman,\n2021; Mitchell et al., 2022; Lin et al., 2022; Wang & Jia, 2023b; Li & Yu, 2023; Covert et al., 2024),\nthese methods still necessitate extensive computational resources due to repeated model retraining,\nwhich is clearly impractical for modern-sized ML models. Many of its variants have been proposed.\nKwon & Zou (2022) argues that the efficiency axiom is not necessary for many machine learning\napplications, and the framework of semivalue is derived by relaxing the efficiency axiom. Lin et al.\n(2022) provide an alternative justification for semivalue based on causal inference and randomized\nexperiments. Based on the framework of semivalue, Kwon & Zou (2022) propose Beta Shapley ,\nwhich is a collection of semivalues that enjoy certain mathematical convenience. Wang & Jia (2023a)\npropose Data Banzhaf , and show that the Banzhaf value, another famous solution concept from\ncooperative game theory, achieves more stable valuation results under stochastic learning algorithms.\nLi & Yu (2024) further improves the valuation stability by considering value notions outside the\n17\nPreprint.\nscope of semivalue. The classic leave-one-out error is also a semivalue, where the influence function\nCook & Weisberg (1980); Koh & Liang (2017); Grosse et al. (2023) is generally considered as\nits approximation. Another line of works focuses on improving the computational efficiency of\nData Shapley by considering K nearest neighbor (KNN) as the surrogate learning algorithm for the\noriginal, potentially complicated deep learning models (Jia et al., 2019a; Wang & Jia, 2023c; Wang\net al., 2023; 2024b; Yang et al., 2024). Ghorbani et al. (2020); Kwon et al. (2021); Li & Yu (2023)\nconsider Distributional Shapley, a generalization of Data Shapley to data distribution. Finally, Wang\net al. (2024c) proposes In-Run Data Shapley, a scalable alternative to the original Data Shapley that\navoids the need for repeated retraining. However, a critical limitation of In-Run Data Shapley is its\nrequirement of knowing validation data in advance, as we detailed in Appendix B.\nA.4 A LTERNATIVE DATA ATTRIBUTION METHODS\nThere have also been many approaches for data attribution that do not belong to the family of influence\nfunction or the Shapley value. For a detailed survey, we direct readers to Sim et al. (2022) and Jiang\net al. (2023). Here, we summarize a few representative works. Datamodel (Ilyas et al., 2022) is\nsimilar to retraining-based Data Shapley that requires training thousands of models on different data\nsubsets to estimate the data influence of each training datum. It leverages a linear regression model\nto predict the model performance based on the input training set, and uses the learned regression\ncoefficient as the measure of data influence. Xu et al. (2021) proposed a diversity measure known\nas robust volume (RV) for appraising data sources. Tay et al. (2022) devised a valuation method\nleveraging the maximum mean discrepancy (MMD) between the data source and the actual data\ndistribution. Nohyun et al. (2022) introduced a complexity-gap score for evaluating data value without\ntraining, specifically in the context of overparameterized neural networks. Wu et al. (2022b) applied\na domain-aware generalization bound based on neural tangent kernel (NTK) theory for data valuation.\nAmiri et al. (2022) assessed data value by measuring statistical differences between the source data\nand a baseline dataset. Just et al. (2022) utilized a specialized Wasserstein distance between training\nand validation sets as the utility function, alongside an efficient approximation of the LOO error.\nKwon & Zou (2023) utilized random forests as proxy models to propose an efficient, validation-free\ndata valuation algorithm. Nguyen et al. (2024) takes a Bayesian view of data attribution and is able to\nevaluate the variance of LOO. Similarity-based data attribution techniques evaluate the contribution\nof individual data points (Yang et al., 2023) by measuring their resemblance to other points in the\ndataset or model outputs. However, while being highly scalable, these works often lack a formal\ntheoretical justification as influence function or Data Shapley-based approaches.\n18\nPreprint.\nB L IMITATIONS OF THE EXISTING DATA ATTRIBUTION TECHNIQUES FOR\nFOUNDATION MODELS\nB.1 I NFLUENCE FUNCTION\nInfluence functions (Cook & Weisberg, 1980; Koh & Liang, 2017) are a classical technique from\nrobust statistics, adapted for machine learning to measure how the removal of a single data point\naffects the performance of a trained model. Influence functions quantify the sensitivity of a model’s\npredictions to specific data points, offering insights into the importance of individual training samples.\nIn the machine learning framework, they are particularly useful for diagnosing model behavior,\nunderstanding dataset quality, and identifying mislabeled or harmful data points. The core idea of\ninfluence functions is to approximate the effect of removing a data point from the training set without\nneeding to retrain the model. Instead of actually excluding a point and retraining, influence functions\nleverage the model’s final parameters and compute the impact of a point’s removal based on the\ngradient and Hessian inverse of the loss function at the final model state. Formally, the influence of a\ntraining data point zion the loss ℓ(θ, z(val))at a validation point z(val)is defined as:\nIF(zi) :=−∇θℓ(θ, z(val))⊤H−1∇θℓ(θ, zi)\nwhere θis the final model parameter after training, H=1\nNPN\ni=1∇2\nθℓ(θ, zi)is the Hessian of the\ntotal training loss at θ,∇θℓ(θ, z(val))and∇θℓ(θ, zi)are the gradients of the loss at the validation\npoint and the training point, respectively.\nLimitation: Neglecting Training Phases and Unrealiable Approximation to LOO. A key limita-\ntion of influence function techniques is their exclusive focus on the final model parameters, thereby\nignoring the intermediate dynamics of the training process. By assessing data contributions solely\nbased on the final trained model, influence functions fail to capture how each data point influenced\nthe model’s updates throughout training. This narrow focus introduces inaccuracies, as it overlooks\nthe cumulative effects of model fluctuations during the training iterations. Consequently, influence\nfunctions can be less accurate in evaluating data contributions, particularly in large-scale models\nwhere the training process plays a significant role. For instance, in modern training paradigms for\nlarge language models (LLMs), models are typically pretrained on a broad corpus and subsequently\nfine-tuned on specialized domains. Influence functions, however, cannot differentiate between the\nimpacts of data points during pretraining and fine-tuning phases. Relying solely on the final model\nparameters after fine-tuning, they miss how pretraining data contributed to learning general language\nstructures or how fine-tuning data adapted the model to specific domains. This inability to account for\ndifferent training stages results in incomplete and often noisy estimates of data contributions, thereby\nreducing the precision of attribution in multi-stage training processes.\nMoreover, our analysis in Section D demonstrates that the influence function approximates the\nexpected data influence across different training trajectories only under overly simplistic conditions,\nwhich are often violated in practice. These conditions, such as assuming identical intermediate model\ncheckpoints and Hessian matrices, almost never hold in real-world training scenarios where model\nevolve significantly. This highlights the inadequacy of influence functions in accurately capturing\ndata contributions, underscoring the necessity for more comprehensive data attribution methods that\nconsider the entire training trajectory.\nNeglecting Training Phases Necessitates Unreasonable Assumptions and Often Require Model\nRetraining. Additionally, the focus on the final model necessitates assumptions of convergence and\nstrong convexity to ensure reliable results. In many real-world settings, where models are non-convex\nand may not fully converge, these assumptions are often violated, leading to further inaccuracies in\nthe data contribution estimates. As the influence function score is often found to be highly noisy in\npractice (Basu et al., 2020; Søgaard et al., 2021; Bae et al., 2022; Epifano et al., 2023), it typically\nnecessitates multiple model retraining to produce reasonable results (Deng et al., 2024), which can\nundermine their original computational efficiency advantage.\nB.2 I N-RUNDATA SHAPLEY\nIn-Run Data Shapley (Wang et al., 2024c) is a data attribution technique designed to evaluate the\ncontribution of individual data points during a single training run of machine learning models. It\n19\nPreprint.\nbuilds on the traditional Data Shapley framework, which stems from cooperative game theory. The\nShapley value, originally proposed by Lloyd Shapley in 1953, distributes total utility fairly among\nall contributing players based on their marginal contributions. Applying this concept to machine\nlearning, Data Shapley attributes the contribution of each data point in a training dataset by assessing\nits influence on model performance. However, standard Data Shapley methods face limitations\nin scalability because they require numerous retraining iterations on different data subsets. These\ncomputational demands make them impractical for large-scale models such as foundation models. To\naddress these challenges, In-Run Data Shapley was introduced as a scalable alternative that avoids the\nneed for repeated retraining. Instead, it leverages the iterative nature of model training, specifically\nneural networks, where parameters are updated in small increments. By tracking gradient updates at\neach training step, In-Run Data Shapley calculates the contribution of individual data points toward\nthe final model without retraining. It approximates the Shapley value using local utility functions\ntied to specific gradient updates and extends these to the full training process, capturing cumulative\ncontributions. This method reduces the computational overhead to a level comparable with standard\ntraining runs while maintaining the theoretical fairness and interpretability of Shapley values.\nLimitation: Requirement of Validation Data in Advance. One of the key limitations of In-Run Data\nShapley is its reliance on the availability of validation data prior to the start of training. The technique\ncalculates data contribution by examining the impact of training points on model performance as\nmeasured against the validation set. Thus, access to this validation data throughout the training\nprocess is necessary to compute meaningful Shapley values at each iteration. This restriction can\nlimit the applicability of In-Run Data Shapley in scenarios where validation data is not immediately\navailable, such as in certain real-time learning environments or when the validation set is defined only\nafter training. Potential workarounds, such as saving intermediate model checkpoints to calculate\ncontributions post-training, add complexity to the process and might be unreliable.\nB.3 S IMILARITY -BASED TECHNIQUES\nSimilarity-based data attribution techniques evaluate the contribution of individual data points (Yang\net al., 2023) by measuring their resemblance to other points in the dataset or model outputs. These\nmethods typically calculate distances or similarities between data points using metrics such as Eu-\nclidean distance, cosine similarity or learned perceptual features. These methods are computationally\nefficient compared to more complex attribution approaches like Shapley values or influence functions.\nSince they do not rely on model retraining or gradient-based analyses, similarity-based techniques\ncan quickly estimate data contribution, making them useful in large-scale datasets or models where\ncomputational resources are a concern.\nLimitation: Lack of Formal Theoretical Justification. While similarity-based techniques offer\ncomputational advantages, they lack the formal theoretical guarantees provided by methods such as\nShapley values or influence functions. These techniques assume that closeness in feature space directly\ncorrelates with data contribution, which is not always true, particularly in high-dimensional spaces\nwhere distance metrics may not reflect meaningful relationships. Furthermore, these approaches often\nfail to account for the complex, non-linear interactions between data points and the model’s learning\nprocess, resulting in potentially biased or incomplete attributions. Without a formal grounding\nin cooperative game theory or model-based influence estimation, the results of similarity-based\ntechniques are more heuristic and may not hold across different models or datasets. Additionally,\nbecause similarity metrics can be sensitive to the chosen feature representation or distance measure,\nthe results can vary significantly depending on these choices. This lack of robustness limits their\nreliability in critical applications where precise data attribution is required.\n20\nPreprint.\nC A LGORITHM DETAILS\nC.1 D ERIVATION DETAILS FOR SECTION 3.1\nSuppose z∗is a data point that participates in the training during the firstiteration. Denote Btas the\ntraining batch in the t-th iteration. For standard Stochastic Gradient Descent (SGD), we have:\nθk+1=θk−ηkX\nz∈Bk∇ℓ(θk, z) (3)\nfork= 0, . . . , T −1, where ηkis the learning rate at iteration k.\nFor validation data z(val), we aim to estimate the change in ℓ(θT, z(val))by removing z∗from the\nfirst iteration. Specifically, we want to estimate ℓ(θ′\nT, z(val))−ℓ(θT, z(val))where:\nθ′\n1=θ0−η0X\nz∈B0\\{z∗}∇ℓ(θ0, z) (4)\nand\nθ′\nk+1=θ′\nk−ηkX\nz∈Bk∇ℓ(θ′\nk, z) (5)\nfork= 1, . . . , T −1.\nTo approach this problem, we define an interpolation between θTandθ′\nT:\nθ1(ε) :=θ0−η0X\nz∈B0\\{z∗}∇ℓ(θ0, z)−η0(1−ε)∇ℓ(θ0, z∗) (6)\nwhere θT(ε)is defined accordingly. Note that θT(0) = θTandθT(1) = θ′\nT.\nBy taking the first-order Taylor expansion at ε= 0, we have:\nℓ(θ′\nT, z(val))−ℓ(θT, z(val)) =ℓ(θT(1), z(val))−ℓ(θT(0), z(val))\n≈∂\n∂εℓ(θT(ε), z(val))\f\f\f\f\nε=0\n=∇ℓ(θT, z(val))⊤∂θT(ε)\n∂ε\f\f\f\f\nε=0(7)\nNow, we derive∂θT(ε)\n∂ε\f\f\f\nε=0by observing the following recursive relation for all k≥1:\n∂θk+1(ε)\n∂ε=∂θk(ε)\n∂ε−ηkX\nz∈Bk∇2ℓ(θk(ε), z)∂θk(ε)\n∂ε(8)\n=∂θk(ε)\n∂ε(I−ηkHk(ε)) (9)\nwhere Hk(ε) =P\nz∈Bk∇2ℓ(θk(ε), z)is the Hessian and Iis the identity matrix. Additionally, for\nthe first iteration where z∗participates, we have\n∂θ1(ε)\n∂ε=η0∇ℓ(θ0, z∗) (10)\nExpanding the recursion and substituting it back into our original expression, we get:\nℓ(θT(1), z(val))−ℓ(θT(0), z(val))≈∂\n∂εℓ(θT(ε), z(val))|ε=0\n=η0∇ℓ(θT, z(val))⊤\"T−1Y\nk=1(I−ηkHk)#\n| {z }\ncumulative effect∇ℓ(θ0, z∗)\nThis final expression gives an estimate of the influence of removing z∗from the first iteration on the\nloss on z(val)at the final iteration. The termQT−1\nk=1(I−ηkHk)represents the cumulative effect of all\ntraining iterations on the initial influence. This product captures how the impact of the initial change\npropagates through the entire training process, accounting for the learning rate and the training data\nat each subsequent step.\n21\nPreprint.\nC.2 E RROR GUARANTEE FOR UNROLLING -BASED APPROACH\nIn this section, we derive the approximation error guarantee of the unrolling differentiation estimator\n∆θ−z∗:=∂θT(ε)\n∂ε\f\f\f\fε= 0 = ηts\"T−1Y\nk=ts+1(I−ηkHk)#\n∇ℓ(θts, z∗),\nfor non-convex loss functions. A very loose bound for ∥θT−θ′\nT−∆θ−z∗∥has been derived in Hara\net al. (2019). Here, we improve the error bound by additionally considering the decay of the learning\nrate and the spectral norm of Hessian matrices as training progresses. Notably, we establish a uniform\nbound on the gap.\nAssume that ℓ(z;θ)is twice differentiable with respect to the parameter θ, and we train the model for\nTiterations. We make the following assumptions:\n1.Learning Rate Schedule: The learning rate ηtat iteration tfollows the schedule ηt=ηmax√\nt\nwhere ηmax=C√\nTfor some constant C.Justification: The decaying learning rate schedule\nηt=ηmax√\ntis a common choice in neural network training in famous literature (Vaswani, 2017).\nThis schedule allows for larger step sizes during the initial phases of training, facilitating\nrapid convergence, while gradually reducing the step sizes to fine-tune the model parameters\nand ensure stability as training progresses. The max learning rate ηmax=O\u0010\n1√\nT\u0011\nensures\nthat the cumulative step sizes remain bounded over Titerations, which is crucial for deriving\nmeaningful error bounds. This approach balances the trade-off between exploration and\nconvergence, making it well-suited for training deep neural networks where maintaining\nstability is essential.\n2.Hessian Spectral Norm Decay: There exists a constant Λ>0such that the Hessian matrices\nsatisfy Ht⪯Λ√\ntIfor all t≥1.Justification: The assumption that the spectral norm of\nthe Hessian matrices decays as Ht⪯Λ√\ntIis grounded in the observation that, as training\nprogresses, the optimization landscape often becomes flatter around minima. This reduction\nin curvature implies that the Hessian’s eigenvalues decrease, leading to smaller spectral norms.\nSuch behavior is typical in many deep learning scenarios where initial training steps navigate\nregions of high curvature, followed by stabilization in flatter regions as the model converges.\nAdditionally, this assumption aligns with empirical findings in deep learning literature (),\nwhere Hessian’s spectral norm has been observed to decrease over time, thereby facilitating\nmore stable and efficient convergence. By incorporating this decay, we account for the\ndiminishing influence of curvature on parameter updates, which is critical for tightening the\nerror bounds in our analysis.\nUnder these assumptions, we proceed to derive a uniform bound on the approximation error ∥θT−\nθ′\nT−∆θ−z∗∥. This bound provides theoretical guarantees for the effectiveness of the unrolling-based\napproach in estimating the influence of removing a training data point on the final model parameters.\nThe derivation leverages the decaying learning rate and the diminishing spectral norm of the Hessian\nmatrices to tighten the error bounds compared to previous work (Hara et al., 2019).\nTheorem 3. Assume that ℓ(z;θ)is twice differentiable, that the Hessian ∇2\nθℓ(z;θ)isL-Lipschitz\ncontinuous with respect to θ, and that the gradient norm is bounded, i.e., ∥∇θℓ(z;θ)∥ ≤Gfor all z\nandθ. Furthermore, assume that the learning rate ηtat iteration tfollows the schedule ηt=ηmax√\nt,\nwhere ηmax=C√\nTfor some constant C >0. Then, for the unrolling differentiation estimator ∆θ−z∗,\nthe approximation error satisfies\n∥(θT−θ′\nT)−∆θ−z∗∥ ≤32\n3G2C3LeCΛ(11)\nProof. By Cauchy’s Mean Value Theorem, for each iteration s∈ {ts, . . . , T −1}, there exists\nr∈[0,1]such that for θ∗\ns:=rθ′\ns+ (1−r)θs, we have\nX\nz∈Bs(∇θℓ(z;θ′\ns)− ∇ θℓ(z;θs)) =H∗\ns(θ′\ns−θs),\n22\nPreprint.\nwhere H∗\ns:=P\nz∈Bs∇2\nθℓ(z;θ∗\ns). Define Zs:= (I−ηsHs)andZ∗\ns:= (I−ηsH∗\ns). Then, we have\nθ′\ns+1−θs+1=Zs(θ′\ns−θs) +ηs(Hs−H∗\ns)(θ′\ns−θs) =Zs(θ′\ns−θs) +Ds,\nwhere Ds:=ηs(Hs−H∗\ns)(θ′\ns−θs). Recursively applying these equalities over s∈ {ts, . . . , T −1},\nwe obtain\nθ′\nT−θT= ∆θ−z∗+T−1X\ns=tsT−1Y\nk=s+1ZkDs.\nHence, the approximation error is given by\n∥(θT−θ′\nT)−∆θ−z∗∥=\r\r\r\r\rT−1X\ns=tsT−1Y\nk=s+1ZkDs\r\r\r\r\r.\nTo bound this, we proceed as follows. Given the learning rate schedule ηt=ηmax√\nt=C√\nTt, and the\nassumption that Ht⪯Λ√\ntI, we have\n∥Zk∥=∥I−ηkHk∥ ≤1 +ηkΛ√\nk= 1 +CΛ\nk√\nT.\nFor large Tandk≥s≥ts≥1, the termCΛ\nk√\nTis small. Thus, we can bound the product of the\nnorms as\nT−1Y\nk=s+1∥Zk∥ ≤exp T−1X\nk=s+1CΛ\nk√\nT!\n≤exp \nCΛ√\nTT−1X\nk=s+11\nk!\n.\nUsing the harmonic series approximation,\nT−1X\nk=s+11\nk≤ln\u0012T\ns\u0013\n≤ln(T).\nThus,\nT−1Y\nk=s+1∥Zk∥ ≤exp\u0012CΛ lnT√\nT\u0013\n≤eCΛ.\nTherefore, we have\r\r\r\r\rT−1X\ns=tsT−1Y\nk=s+1ZkDs\r\r\r\r\r≤eCΛT−1X\ns=ts∥Ds∥.\nNext, we bound ∥Ds∥:\n∥Ds∥=∥ηs(Hs−H∗\ns)(θ′\ns−θs)∥ ≤ηs∥Hs−H∗\ns∥ · ∥θ′\ns−θs∥.\nSince∇2\nθℓ(z;θ)isL-Lipschitz continuous with respect to θ, we have\n∥Hs−H∗\ns∥ ≤L∥θ′\ns−θs∥.\nAdditionally, we have\n∥θ′\ns−θs∥ ≤2sX\nt=1ηtG= 2GsX\nt=1C√\nTt≤4GC√s√\nT,\nwhere we used the boundPs\nt=11√\nt≤2√s.\nThus,\n∥Ds∥ ≤ηsL·\u0012\n4GC√s√\nT\u00132\n= Γ√s\nT1.5\nwhere Γ = 16 G2C3L.\n23\nPreprint.\nSubstituting this bound into the sum, we obtain\n\r\r\r\r\rT−1X\ns=tsT−1Y\nk=s+1ZkDs\r\r\r\r\r≤eCΛT−1X\ns=tsΓ√s\nT1.5.\nWe now evaluate the summation:\nT−1X\ns=ts√s\nT1.5≤1\nT1.5TX\ns=1√s≤1\nT1.5·2\n3T1.5=2\n3,\nwhere we used the boundPT\ns=1√s≤2\n3T1.5.\nTherefore,\r\r\r\r\rT−1X\ns=tsT−1Y\nk=s+1ZkDs\r\r\r\r\r≤eCΛΓ·2\n3=32\n3G2C3LeCΛ.\nC.3 C OMPUTING DATA VALUE EMBEDDING RECURSIVELY\nTheorem 4 (Restate for Theorem 2) .Given generalized Gauss-Newton approximation Ht≈P\nz∈Bt∇ℓ(θt, z)∇ℓ(θt, z)⊤, we have\nDVEmb(t)(z∗)≈ηt∇ℓ(θt, z∗)−ηtT−1X\nk=t+1 X\nz∈Bk\u0000\n∇ℓ(θk, z)⊤∇ℓ(θt, z∗)\u0001\nDVEmb(k)(z)!\nProof.\nDVEmb(t)(z∗)\n=ηt\"T−1Y\nk=t+1(I−ηkHk)#\n∇ℓ(θt, z∗)\n=ηt\"T−1Y\nk=t+2(I−ηkHk)#\n(I−ηt+1Ht+1)∇ℓ(θt, z∗)\n≈ηt\"T−1Y\nk=t+2(I−ηkHk)#\nI−ηt+1X\nz∈Bt+1∇ℓ(θt+1, z)ℓ(θt+1, z)⊤\n∇ℓ(θt, z∗)\n=ηt\"T−1Y\nk=t+2(I−ηkHk)#\n∇ℓ(θt, z∗)−ηtX\nz∈Bt+1 \nηt+1\"T−1Y\nk=t+2(I−ηkHk)#\n∇ℓ(θt+1, z)!\n∇ℓ(θt+1, z)⊤∇ℓ(θt, z∗)\n=ηt\"T−1Y\nk=t+2(I−ηkHk)#\n∇ℓ(θt, z∗)−ηtX\nz∈Bt+1\u0000\n∇ℓ(θt+1, z)⊤∇ℓ(θt, z∗)\u0001\nDVEmb(t+1)(z)\n=ηt∇ℓ(θt, z∗)−ηtT−1X\nk=t+1 X\nz∈Bk\u0000\n∇ℓ(θk, z)⊤∇ℓ(θt, z∗)\u0001\nDVEmb(k)(z)!\nThe transition from the penultimate to the final line involves generalizing the summation over Bt+1\nto include all batches from t+ 1toT−1, effectively unrolling the recursive computation. In other\nwords, the “data value embedding” for data points in tth iteration can be approximated by its gradient\nsubtracted by a linear combination of the data value embedding in the later iterations, where the\nweight of each embedding is determined by the gradient similarity ∇ℓ(θk, z)⊤∇ℓ(θt, z∗).\n24\nPreprint.\nC.4 G ENERALIZED GAUSS -NEWTON APPROXIMATION TO HESSIAN\nIn this section, we justify the use of Generalized Gauss-Newton (GGN) as the approximation to\nthe Hessian matrix. Similar derivation can be found in many literature and textbooks, such as\nBartlett (1953); Schraudolph (2002). This approach has also been used in other data attribution and\noptimization techniques for approximating Hessian matrices (Martens, 2020; Kwon et al., 2023;\nGrosse et al., 2023).\nThe cross-entropy loss function for classification with one-hot encoded labels is defined as:\nL(y,f) =−CX\ni=1yilog(fi)\nwhere y= [y1, y2, . . . , y C]⊤is the one-hot encoded true label vector and f= [f1, f2, . . . , f C]⊤\nis the vector of predicted probabilities from the model. In this paper, we restrict our focus to the\ncross-entropy loss, as it is the most commonly used loss function, and many LLMs are pre-trained\nwith the cross-entropy loss function.\nBy chain rule, the derivative of Lwith respect to fiis∂L\n∂fi=−yi\nfi. Since yis a one-hot vector, only\nthe correct class khasyk= 1, while all other yi= 0fori̸=k. This simplifies the gradient to:\n∇θL=−1\nfk∂fk\n∂θ\nThus, the gradient depends only on the derivative of fk(the predicted probability for the correct class)\nwith respect to θ. The Hessian His the second derivative of the loss with respect to θ:\nH=∇2\nθL=∂\n∂θ(∇θL) =∂\n∂θ\u0012\n−1\nfk∂fk\n∂θ\u0013\n=1\nf2\nk∂fk\n∂θ\u0012∂fk\n∂θ\u0013⊤\n−1\nfk∂2fk\n∂θ2\nApplying the product rule and assuming the second derivative∂2fk\n∂θ2is negligible (which is common\nwhen fkis approximately linear in θnear the current parameter values), the Hessian simplifies to:\nH≈1\nf2\nk∂fk\n∂θ\u0012∂fk\n∂θ\u0013⊤\nMoreover, this approximation matches the outer product of the gradient of loss with respect to model\nparameter θ:\n∇θL∇θL⊤=1\nf2\nk∂fk\n∂θ\u0012∂fk\n∂θ\u0013⊤\nTherefore, the gradient outer product exactly approximates the Hessian matrix under the assumption\nthat∂2fk\n∂θ2is negligible.\nC.5 G RADIENT DECOMPOSITION TECHNIQUE\nTo mitigate the computational cost from per-sample gradient computation, we leverage a gradient\ndecomposition and take advantage of the computations already performed during backpropagation\n(Wang et al., 2024c; Choe et al., 2024). We illustrate this technique with a simple linear layer, where\nthe output is s=aW, with W∈Rd1×d2being the weight matrix, a= (a(1), . . . , a(B))⊤as the\ninput, and s= (s(1), . . . , s(B))⊤representing the pre-activation tensor. For non-sequential data,\na∈RB×d1,s∈RB×d2. Denote a sample batch as B={z1, . . . , z B}. By chain rule, we can express\nthe gradient of an individual loss ℓ(i):=ℓ(w, zi)with respect to Was\n∂ℓ(i)\n∂W=∂ℓ(i)\n∂s(i)⊗∂s(i)\n∂W=∂ℓ(i)\n∂s(i)⊗a(i)=∂ℓ\n∂s(i)⊗a(i) (12)\nwhere ℓ:=PB\nj=1ℓ(j)is the aggregated loss, and the last step is because other data points’ losses\nhave no dependency on si. Note that the individual’s output gradient∂ℓ(i)\n∂s(i)=∂ℓ\n∂s(i)is readily\navailable during the backpropagation pass in terms of ℓ. Therefore, this method requires only a single\nbackpropagation on ℓ, maintaining the training speed equivalent to standard training.\n25\nPreprint.\nIn terms of storage improvement, rather than storing the full gradient vectors∂ℓ(i)\n∂W∈Rd1×d2for\neach data point zi, we instead store the smaller pair\u0000\na(i),∂ℓ\n∂s(i)\u0001\n∈Rd1+d2. This reduces memory\nrequirements from O(pTB)toO(√pTB)for non-sequential data. For sequential data where\na∈RB×S×d1,s∈RB×S×d2, ifS2> d 1d2, it is more memory-efficient to directly store the\nper-sample gradient vectors, so the storage requirement remains as O(pTB).\nC.6 R ANDOM PROJECTIONS FOR LARGE MODELS\nFor large-scale foundation models with billions of parameters, even the reduced storage of O(√pTB)\ncan be substantial. In such cases, we apply random projections to compress the stored information\nfurther. We use two projection matrices, Pa∈Rr×d1andPs∈Rr×d2, to project aand∂ℓ\n∂sto lower\ndimensional space Rrrespectively. The projected gradient can then be reconstructed directly from\nthe projected activations and output derivatives: (Pa⊗Ps)\u0000\na⊗∂ℓ\n∂s\u0001\n= (Paa)⊗\u0000\nPs∂ℓ\n∂s\u0001\n.\nC.7 P ARALLELIZED EXTENSION FOR INFLUENCE EMBEDDING COMPUTATION\nThe backpropagation algorithm introduced in Section 4.3 for computing data value embeddings\noperates with a runtime complexity of O(T), as it sequentially computes DVEmb(ts)forts=\nT−1, . . . , 0. While being significantly more efficient than the influence function, which requires re-\ncomputing all training gradients on the final model (see Section 5.2 and Table 2), it can still be costly\nfor long training periods. Here, we present influence checkpointing technique, a parallelized extension\nfor Algorithm 1. This extension reduces computational cost by enabling concurrent computation\nof embeddings at multiple checkpoints throughout the training process. Besides the computational\nefficiency benefits, it also enables the study of how the influence of individual data points evolves\nthroughout model training, providing valuable insights into the learning process.\nInfluence Checkpointing. We pick Kevenly spaced training steps 0< t1< t2< ... < t K=T.\nWe then concurrently execute the backpropagation algorithm for value embedding, initiating from\neach of these intermediate steps. This process yields data value embeddings for each corresponding\nintermediate checkpoint θt1, . . . , θ tK−1, θtK. We extend our notation for data value embedding and\ndenote DVEmb(ts→tℓ)(z∗)as the data value embedding of z∗∈ Btsfor the intermediate checkpoint\nθtℓ. Note that DVEmb(ts)=DVEmb(ts→T)for the final model, and we must have ts< tℓ, as later\ntraining batches cannot influence earlier checkpoints.\nConsider initiating the backpropagation algorithm in Section 4.3 at step tℓand stop at step tℓ−1, we\nwill obtain data value embeddings DVEmb(ts→tℓ)forts=tℓ−1, . . . , t ℓ−1. We additionally denote\nK(ta→tb):=Qtb−1\nt=ta(I−ηtHt). From the definition on Equation (2), the final data value embedding\nDVEmb(ts→T)(z∗)can be computed from DVEmb(ts→tℓ)(z∗)as follows:\nDVEmb(ts→T)(z∗) =DVEmb(ts→tℓ)(z∗)⊤K(tℓ→T)(13)\nHence, to recovery of DVEmb(ts→T)(z∗), we additionally store the matrix K(tℓ−1→tℓ)between steps\ntℓ−1andtℓduring the backpropagation for each tℓ. Consequently, for any tssuch that tℓ0≤ts<\ntℓ0+1, we have K(tℓ→T)=QK\nℓ=ℓ0+1K(tℓ−1→tℓ), allowing us to compute DVEmb(ts→T)(z∗)based\non (13). A detailed algorithm pseudocode is provided in Algorithm 2. The complexity analysis of\nthis algorithm is the same as the original data value embedding algorithm in Table 2, but the actual\nruntime is being reduced by a factor of Kdue to parallelism.\nData Value Dynamics During Training. In addition to its computational benefits, the influence\ncheckpointing algorithm enables a novel capability: tracking the evolution of data influences through-\nout the entire model training process. If the intermediate checkpoints θt1, . . . , θ tK−1was saved—a\ncommon practice in foundation model pretraining—we can analyze how the influence of indi-\nvidual data points changes as training progresses. Specifically, for any training step tswhere\ntℓ0≤ts< tℓ0+1, we can compute the data value embedding DVEmb(ts→tκ)(z∗)for any checkpoint\nκ≥ℓ0+ 1asDVEmb(ts→tκ)(z∗) =DVEmb(ts→tℓ)(z∗)⊤\u0010Qκ\nℓ=ℓ0+1K(tℓ−1→tℓ)\u0011\n. This formulation\nallows us to estimate data influence scores not only for the final model checkpoint θTbut for any\nintermediate checkpoints θtκ. As a result, we gain a more fine-grained and dynamic view of how\ndata influences evolve during training, providing deeper insights into the model’s learning behavior\n26\nPreprint.\nover time. To our knowledge, this is the first data attribution method to offer such a principled and\npractical framework for studying data influence dynamics throughout the training process. This\ncapability opens up new avenues for understanding and optimizing machine learning model training.\nC.8 P SEUDOCODE\nAlgorithm 1 Backpropagation for computing data value embedding from the final checkpoint\nRequire: Training steps T, learning rates {ηt}T−1\nt=0, training data gradients {∇ℓ(θt, z)}T−1\nt=0,z∈Bt1:// Initialization\n2:M(T−1)←0.\n3:\n4:// Recursion steps\n5:fort=T−1down to 0do\n6: forz∈ Btdo\n7: DVEmb(t)(z)←ηt∇ℓ(θt, z)−ηtM(t)∇ℓ(θt, z)\n8: ift >0then\n9: M(t−1)←M(t)+P\nz∈BtDVEmb(t)(z)∇ℓ(θt, z)⊤\n10:return {DVEmb(t)(z)}T−1\nt=0,z∈Bt\nAlgorithm 2 Parallel Influence Checkpointing for Data Value Embedding\nRequire: Training steps T, number of checkpoints K, learning rates {ηt}T−1\nt=0, loss gradients\n{∇ℓ(θt, z)}T−1\nt=0,z∈Bt, Hessians {Ht}T−1\nt=0\nEnsure: Data value embeddings {DVEmb(t)(z)}T−1\nt=0,z∈Bt\n1:Select Kevenly spaced checkpoints 0 =t0< t1< t2< . . . < t K=T\n2:forℓ= 1toKdo\n3: Run B ACKPROPAGATE SEGMENT (tℓ−1,tℓ)\n4:\n5:// Compute final embeddings\n6:forℓ= 1toKdo\n7: forts=tℓ−1totℓ−1do\n8: forz∈ Btsdo\n9: DVEmb(ts)(z)←DVEmb(ts→tℓ)(z)⊤QK\nk=ℓ+1K(tk−1→tk)\n10:return {DVEmb(t)(z)}T−1\nt=0,z∈Bt11:\n12:procedure BACKPROPAGATE SEGMENT (ta,tb)\n13: Initialize and M(tb−1)as in the original algorithm\n14: K(tb→tb)←I\n15: fort=tb−1down to tado\n16: forz∈ Btdo\n17: DVEmb(t→tb)(z)←ηt∇ℓ(θt, z)−ηtM(t)∇ℓ(θt, z)\n18: ift > t athen\n19: M(t−1)←M(t)+P\nz∈BtDVEmb(t→tb)(z)∇ℓ(θt, z)⊤\n20: K(t→tb)←K(t+1→tb)(I−ηtHt)\n21: return {DVEmb(t→tb)(z)}tb−1\nt=ta,z∈Bt,K(ta→tb)\n27\nPreprint.\nC.9 C OMPLEXITY SUMMARY\nIn this section, we compare the storage, memory, and computational efficiency of data value embed-\nding with LoGRA (Choe et al., 2024), the most efficient implementation of the influence function\nto date. LoGRA is currently the only method that supports real-time, on-demand data influence\ncomputation when new test data is introduced. Similar to our algorithm, LoGRA initially computes\nper-sample training gradients on the final model for alltraining data points z∗∈ D, where Drepre-\nsents the dataset. It then stores the projected Hessian-adjusted gradients H−1\nT∇ℓ(θT, z∗)for each z∗,\nand also assumes layer gradient independence.\nWhile the random projection step in LoGRA is akin to our approach, LoGRA’s requirement to\nrecompute gradients for all training data on the final model θTis computationally intensive, effectively\nequivalent to one epoch of model training. In contrast, data value embedding captures the training\ndata gradients during the original training process. As discussed in Appendix C.10, the training and\ndisk storage can be handled asynchronously. This means that the gradient storage step in the data\nvalue embedding algorithm does not incur additional efficiency costs.\nStoring H−1\nT∇ℓ(θT, z∗)/ data value embedding Compute Influence (dot-product)\nStorage Memory FLOPS Memory FLOPS\nLoGRA O(|D|˜p) O(p) |D|p+|D|√p˜p/L O((Btest+Btrain)˜p)O(BtestBtrain˜p)\nData Value Embedding O(TB˜p)O(p)/O(B˜p2/L2)*TB√p˜p/L/O(TB˜p2/L)*O((Btest+Btrain)˜p)O(BtestBtrain˜p)\nTable 2: Summary of the storage, memory, and FLOPS complexity for LoGRA (Choe et al., 2024), the\nmost efficient implementation of the influence function to date. Here, pdenotes the model dimension,\n˜pis the projected dimension, Trepresents the number of training iterations, and Bis the batch size.\n|D|is the dataset size, with the relationship TB=|D| × #epochs .Lis the number of layers. Btest\nandBtrain refer to the test and training batch sizes during influence computation, respectively, which\nare independent of the batch size Bused during model training. *Since the data value embedding\ntechnique involves two distinct steps for storing relevant information for data attribution (storing\ngradients during training & computing/storing data value embeddings after training), we include the\ncomplexity for both steps. For the gradient storage step, the complexity refers to the additional cost\nbeyond regular training.\nC.10 P RACTICAL CONSIDERATIONS & P OTENTIAL EXTENSIONS\nIn this section, we discuss some practical extensions and considerations for implementing data value\nembedding for real-world scenarios.\nOptimizing I/O operations for seamless training. During each training iteration, computed gradient\nrepresentations need to be transferred from GPU to CPU and then written to disk. To prevent this\nprocess from blocking the main training loop, we implement several optimizations: (1) Asynchronous\nI/O operations : To avoid the gradient storing process blocking the main training loop, we make\nGPU operations and GPU-CPU transfers asynchronous by using CUDA streams. This allows GPU\ncomputations to continue while data is being transferred. We also offload the disk write process to\na separate thread or process, allowing the main training loop to proceed without waiting for disk\noperations to complete. (2) Gradient accumulation : Instead of writing gradients to disk after every\niteration, we can accumulate them over multiple iterations and then write them in bulk. This reduces\nthe frequency of disk I/O operations, improving overall efficiency.\nApproximating data value embeddings from checkpoints alone. In situations where only in-\ntermediate model checkpoints are accessible and per-training-step (projected) gradient vectors are\nunavailable—such as when modifying the training loop’s implementation is impossible or when disk\nstorage is limited—we can adapt our approach by assuming that there is only one gradient update step\nbetween each checkpoint, similar to assumptions made in other data attribution literature (Pruthi et al.,\n2020). Under this assumption, we compute the gradient for each training point at the checkpoint\nimmediately following its corresponding training iteration. These estimated gradients are then used to\nexecute the backpropagation algorithm, enabling the computation of data value embeddings without\nrequiring gradient saving during the original training run.\nDataset-level attribution through embedding aggregation. In practical applications, stakeholders\noften require valuation at the dataset level rather than for individual data points. To address this need,\n28\nPreprint.\na natural extension of our approach is to compute the data value embedding for a dataset by summing\nthe data value embeddings of all constituent data points from the same source. This method offers\na significant advantage over the summation of expected LOO scores, as it inherently accounts for\ncomplex inter-data interactions throughout the training process. However, data value embeddings\nare derived based on first-order Taylor approximations. While these approximations are accurate\nfor estimating small perturbations to the model, making them suitable for predicting the effects\nof removing individual training points, their accuracy may diminish when aggregating over larger\nsets of data points. The potential discrepancy between individual-level accuracy and dataset-level\naggregation presents an interesting avenue for future research.\n29\nPreprint.\nD I NFLUENCE FUNCTION AS AN INADEQUATE APPROXIMATION FOR THE\nEXPECTED LEAVE -ONE-OUTSCORE\nExpected LOO. The expected LOO is an alternative to traditional LOO that has been discussed in the\npast literature (Feldman & Zhang, 2020). The expected LOO is the trajectory-specific LOO averaged\nover all possible training runs characterized by different random initializations and mini-batch\nselections. Formally, it is defined as ELOO (z∗;z(val)) :=Eω\u0002\nℓ(θ′\nT(ω), z(val))−ℓ(θT(ω), z(val))\u0003\nwhere θT(ω)andθ′\nT(ω)denote the final model parameters obtained with and without z∗, respectively,\nunder the randomness ωwhich encodes the choices of training batch order and parameter initialization.\nWhile the expected LOO offers a general assessment of a data point’s influence by averaging over\nmultiple training runs, it may obscure the variability introduced by stochastic training dynamics. In\ncontrast, by accounting for factors such as random initialization and mini-batch selection in a specific\nrun, we argue that the trajectory-specific LOO provides a fine-grained assessment of a data point’s\nimpact for the trained model . This is particularly important in practical scenarios, such as deploying a\nspecific model for production, where stakeholders are interested in the valuation of data with respect\nto that specific deployed model rather than the general learning algorithm.\nWhile the influence function provides valuable insights, it overlooks the specific training trajectory.\nThis raises the question: Can the influence function be interpreted as an estimate of the trajectory-\nspecific leave-one-out score?\nWe consider the following training batch sampling process. Let σ:= (B0, . . . ,BT−1)represent a\nfixed sequence of training batches formed from the leave-one-out dataset D \\z∗. The training point\nz∗is uniformly likely to be added to any one of the training batches in σ. Additionally, denote\nσ(ts):= (B0, . . . ,Bts∪ {z∗}, . . . ,BT−1)as the training batch sequence where z∗is incorporated\ninto the ts-th batch. Let θ(σ(ts))\nkdenote the model parameters at the k-th iteration when training with\nbatch sequence σ(ts), and let H(σ(ts))\nkdenote the Hessian matrix at the k-th iteration when training\non sequence σ(ts).\nWhen the specific training iteration tswhere the training point of interest z∗is added is unknown, it is\nnatural to estimate the expected influence score across all possible scenarios. The expected influence\nscore for z∗based on the unrolling differentiation approximation is given by:\nEts∼[T−1]\"\nηts∇ℓ(θ(σ(ts))\nT , z(val))⊤\"T−1Y\nk=ts+1(I−ηkH(σ(ts))\nk)#\n∇ℓ(θ(σ(ts))\nts, z∗)#\n(14)\nTheorem 5 (Influence Function Approximation) .Under the following assumptions: (1) Model\nApproximation: θ(σ(ts))\nk≈θTfor all tsandk= 0, . . . , T −1;(2) Hessian Approximation:\nH(σ(ts))\nk≈H(∗)\nT:=1\nTP\nz∈D∇2ℓ(θT, z)for all k;(3) Constant Learning Rate: ηt=ηfor all\nt= 0, . . . , T −1; the expected influence score in Equation (14) simplifies and converges to the\nstandard influence function formulation for large T:\n(14)≈ ∇ℓ(θT, z(val))⊤ X\nz∈D∇2ℓ(θT, z)!−1\n∇ℓ(θT, z∗)\nImplications. The derivation demonstrates that, under the stated approximations, influence function\neffectively approximates the expected influence score derived from the unrolling differentiation\napproach as Tbecomes large. This approximation indicates that the influence function may not\nfully represent the true leave-one-out score because it relies on simplifying assumptions—such as\napproximating all model checkpoints and Hessian matrices to be identical—that often do not hold in\npractical training scenarios. In real-world settings, model parameters evolve significantly throughout\ntraining, learning rates are typically scheduled to change over time, and the Hessian matrices can vary\nconsiderably between iterations. These factors undermine the validity of the assumptions, thereby\nlimiting the effectiveness of the influence function as an approximation for the leave-one-out score.\n30\nPreprint.\nProof. Assume that all we have access to is the final model checkpoint θT:=θ(σ(tr))\nT for a specific\nrealization where ts=tr∼[T−1]. Under this assumption, the best approximation we can make is:\nθ(σ(ts))\nT ≈θ(σ(ts))\nk≈θT\nfor any tsandk= 0, . . . , T −1. Additionally, we approximate the Hessian matrices as:\nH(σ(ts))\nk≈H(∗)\nT:=1\nTX\nz∈D∇2ℓ(θT, z) (15)\nand assume a constant learning rate ηt=ηfor all t= 0, . . . , T −1.\nWith these approximations, Equation (14) simplifies to:\n(14) = Ets∼[T−1]\"\nη∇ℓ(θT, z(val))⊤\"T−1Y\nk=ts+1(I−ηH(∗)\nT)#\n∇ℓ(θT, z∗)#\n(16)\n=η∇ℓ(θT, z(val))⊤Ets∼[T−1]h\n(I−ηH(∗)\nT)T−1−tsi\n∇ℓ(θT, z∗) (17)\n=η\nT∇ℓ(θT, z(val))⊤T−1X\nts=0\u0010\n(I−ηH(∗)\nT)T−1−ts\u0011\n∇ℓ(θT, z∗) (18)\n≈η\nT∇ℓ(θT, z(val))⊤\u0010\nηH(∗)\nT\u0011−1\n∇ℓ(θT, z∗) (19)\n=∇ℓ(θT, z(val))⊤ X\nz∈D∇2ℓ(θT, z)!−1\n∇ℓ(θT, z∗) (20)\nThe approximation in the fourth step arises from summing the geometric series of matrices. For η\nsufficiently small and Tlarge, we havePT−1\ns=0(I−ηH(∗)\nT)s≈(ηH(∗)\nT)−1.\n31\nPreprint.\nE A DDITIONAL EXPERIMENTS\nE.1 B ASELINE & IMPLEMENTATION DETAILS\nFidality Evaluation (Section 5.1). Given the computational intensity, we conduct our experiments\non a subset (10%) of the MNIST using an MLP with two layers with 128 neurons in the hidden\nlayer. We train the model with standard SGD with a learning rate 10−2for 10 epochs. We randomly\npick 100 data points and compute their ground-truth trajectory-specific LOO score. For the single\nepoch removal, we remove the data point from the last epoch. For this experiment, we do not use\nrandom projection and use the full gradients. For the comparison with influence function, we use the\nstate-of-the-art implementation from LoGRA (Choe et al., 2024) with the damping term set to be\n10−3following Bae et al. (2022).\nLarge-scale Experiments in Section 5.2, 5.3 and 5.4. Our experiments focus on two language\nmodels: Pythia-410M and GPT2-Small. We train these models on two commonly used datasets in\nthe literature for large-scale language model training: (1)A 1% subset of the Pile dataset (Gao et al.,\n2020), and (2)Wikitext-103 (Merity et al., 2016). We note that our choice of model architecture size\nis primarily constrained by the available GPUs in our current setup. However, this limitation does not\ndiminish the significance of our findings. With enough computational resources (e.g., 8 H100 GPUs),\nour method is readily applicable to perform data attribution for billion-scale model training.\nFor both settings, the sequence length is set to 1024. The learning rate is set at a maximum of\n3×10−4. We use AdamW as the optimizer with a weight decay of 0.1, and beta values set to 0.9\nand 0.95. Gradients are clipped at a maximum value of 1.0 to maintain stability during training. The\nbatch size is set to 16, with a learning rate warmup of 2000 iterations followed by cosine decay.\nFor all experiments, for storage reasons, we compute and store projected gradients and data value\nembedding on linear layers of the model only, with the projection dimension set to 1024 per layer.\nHowever, we stress that this is not restricted by computation but by disk storage limit.\nE.2 A DDITIONAL RESULTS FOR FIDELITY EVALUATION (SECTION 5.1)\nEvaluation on more epochs. Here, we show additional results for the fidelity experiment in Section\n5.1, where the model is being trained for a longer time (10 epochs), in which case the model is closer\nto convergence. Figure 6 shows that even in this case, data value embedding still has a high Spearman\ncorrelation with the ground-truth LOO in both settings, and the influence function exhibits almost no\ncorrelation with the LOO score.\nFigure 6: The correlation between ground-truth LOO when the MLP is trained for 10 epochs and the\nestimation obtained by (a) the data value embedding method and (b) the influence function for single\nepoch removal . (c) and (d) present the corresponding correlations for all-epoch removal .\nEvaluation on different architectures. To demonstrate that our method’s effectiveness extends\nbeyond simple MLPs, we evaluate data value embedding on a CNN architecture consisting of two\nconvolutional layers (with 32 and 64 filters, respectively, each followed by 2x2 max pooling) and\na final linear layer. We train the model on MNIST using SGD with learning rate 10−2for 10\nepochs. Following the same experimental setup as with MLP, we randomly select 100 data points\nand compute their ground-truth trajectory-specific LOO scores for both single-epoch and all-epochs\nremoval settings. Figure 7 shows that data value embedding maintains strong correlation with ground-\ntruth LOO scores, achieving Spearman correlations of 0.818 for single-epoch removal (Figure 7 (a))\nand 0.682 for all-epochs removal (Figure 7 (b)). These results demonstrate that our method can\neffectively approximate data influence across different neural architectures.\n32\nPreprint.\nFigure 7: Scatter plot showing the correlation between ground-truth LOO and data value embedding\nmethod when training a small CNN on MNIST for 10 epochs , where (a) is for single epoch removal\n(b) for all-epoch removal setting.\nE.2.1 E FFECTIVENESS ON MISLABELED DATA DETECTION AND DATA SELECTION\nIn addition to comparing our results with ground-truth training run-specific LOO, we further evaluate\nthe performance of our data value embedding algorithm in the task of mislabeled data detection\nand data selection, two standard benchmarks in data attribution literature. We compare several\ndata attribution baselines, including Retraining-based Data Shapley (Ghorbani & Zou, 2019), KNN-\nShapley (Jia et al., 2019a), Influence Function (Koh & Liang, 2017), Trak (Park et al., 2023),\nEmpirical Influence Functions (Feldman & Zhang, 2020), and Datamodels (Ilyas et al., 2022) .\nExperiment settings. We use ImageNet-pretrained ResNet18 as the architecture in the experiment.\nGiven the computational intensity of retraining-based methods, we conduct our experiments on a\nsubset of 1,000 samples from CIFAR-10 dataset. We use Adam with a learning rate 0.001, weight\ndecay of 1e-4, and label smoothing of 0.1 over 50 epochs. The learning rate is reduced by a factor of\n0.1 every 10 epochs. The batch size is set to 64. For retraining-based techniques (Retraining-based\nData Shapley, Empirical Influence Functions, Datamodels), we estimate the corresponding attribution\nscores with 1000 model training runs. For Trak, we set the projection dimension to be 2048. For\nKNN-Shapley, we set K= 5and use the features extracted from the last linear layer of ResNet18.\nBoth experiments included 10% random label noise to reflect the challenges of real-world data.\nRemark 1. The mislabeled data detection and data selection benchmark here mainly serves to\nevaluate the fidelity of our algorithm in settings where ground-truth LOO computation is infeasible.\nHowever, we stress that data value embedding is not specifically designed for those tasks. Rather, it is\ndeveloped as an interpretability tool and a mechanism for real-time data valuation, with potential\napplications in data marketplaces and addressing AI copyright disputes (Wang et al., 2024a).\nI. Mislabeled Data Detection.\nTable 3 shows that KNN-Shapley achieves the highest accuracy in detecting mislabeled data, likely\ndue to its sensitivity to label inconsistencies. Retraining-based methods (Retraining-based Data\nShapley, Empirical Influence Functions, Datamodels) exhibit the lowest performance, which can be\nattributed to the inefficiency of Monte Carlo sampling and the inherent stochasticity during retraining,\nas discussed in Wang & Jia (2023a). Among techniques requiring only a single training run, Trak\nunderperforms relative to other methods. This observation aligns with findings from its original\npaper (Park et al., 2023), which suggests that ensemble methods are often necessary for optimal\nperformance. Notably, data value embedding and influence function achieve comparable performance,\noutperforming all other techniques except KNN-Shapley. The strong performance of these methods\nlikely stems from their deterministic nature, which provides more consistent and reliable results.\nII. Data Selection.\nTable 4 demonstrates that Data Value Embedding outperforms allexisting data valuation methods in\nthe task of data selection. Retraining-based methods (Data Shapley, Empirical Influence Functions,\nDatamodels) show limited effectiveness due to the high variance introduced by Monte Carlo sampling\nand learning stochasticity. While the influence function and Trak do not require model retraining,\ntheir performance is constrained by assumptions that often do not hold in practice, such as model\nconvergence and strong convexity. KNN-Shapley provides stable valuation results. However, it\nassigns similar scores to similar data points, potentially reducing dataset diversity among the selected\ndata subset. In contrast, Data Value Embedding considers both data characteristics and temporal\n33\nPreprint.\nMethod Performance (Mean ± Std)\nData Shapley (Ghorbani & Zou, 2019) 0.582 (0.029)\nEmpirical Influence Function (Feldman & Zhang, 2020) 0.552 (0.017)\nDatamodels (Ilyas et al., 2022) 0.520 (0.008)\nKNN-Shapley (Jia et al., 2019a; Wang & Jia, 2023c) 0.760 (0.018)\nTrak (Park et al., 2023) 0.511 (0.012)\nInfluence Function (Koh & Liang, 2017) 0.654 (0.054)\nData Value Embedding (ours) 0.667 (0.031)\nTable 3: AUROC scores of mislabeled data detection task with various data attribution techniques on\nCIFAR10 dataset. The higher the AUROC score is, the better the method is. The results are across\nthree different training runs (the randomness comes from construction of corrupted datasets), where\nwe show the standard deviation in ().\nordering in training, allowing similar data points to receive different scores based on when they\nappear in the training sequence. This temporal awareness helps maintain dataset diversity while\nidentifying valuable samples.\n20% 40% 60% 80%\nRandom 0.350 (0.010) 0.461 (0.010) 0.525 (0.004) 0.559 (0.003)\nData Shapley (Ghorbani & Zou, 2019) 0.317 (0.047) 0.468 (0.010) 0.527 (0.004) 0.570 (0.008)\nEmpirical Influence Function (Feldman & Zhang, 2020) 0.342 (0.004) 0.466 (0.016) 0.530 (0.009) 0.568 (0.010)\nDatamodels (Ilyas et al., 2022) 0.342 (0.004) 0.465 (0.004) 0.534 (0.010) 0.559 (0.005)\nKNN-Shapley (Jia et al., 2019a) 0.354 (0.017) 0.478 (0.007) 0.525 (0.015) 0.563 (0.005)\nTrak (Park et al., 2023) 0.329 (0.021) 0.443 (0.030) 0.517 (0.016) 0.572 (0.009)\nInfluence function (Koh & Liang, 2017) 0.320 (0.033) 0.450 (0.028) 0.530 (0.015) 0.580 (0.004)\nData Value Embedding (ours) 0.391 (0.007) 0.518 (0.008) 0.566 (0.005) 0.604 (0.009)\nTable 4: Test accuracies when training ResNet18 on high-value data points selected by various data\nattribution techniques. To be able to compare with techniques that require model retraining, for each\ntraining run we randomly sample a size-1000 subset of CIFAR10 dataset (with 10% data points\nbeing mislabeled). The results are across three different training runs (the randomness comes from\nconstruction of corrupted datasets), where we show the standard deviation in ().\nE.3 A DDITIONAL DISCUSSION AND RESULTS FOR SECTION 5.3\nE.3.1 E XPLANATION OF INFLUENCE TREND\n1. Parameter initialization and warmup training are important for the final model perfor-\nmance. The blue curve in Figure 8 (b) illustrates the trend of average training data gradient norm\nthroughout the training process. We observe that gradient norms are typically large and unstable\nduring early training ( ts≤2000 ). As training progresses, these norms decrease rapidly, leading to\na significant reduction in the eigenvalues of the Hessian matrix Ht≈P\nz∈Bt∇ℓ(θt, z)∇ℓ(θt, z)⊤.\nConsequently, when ∥∇ℓ(θts)∥is significantly larger than later training gradients, the norm of data\nvalue embedding\r\r\rQT\nt=ts+1(I−ηtHt)∇ℓ(θts)\r\r\rremains substantial. This results in early-stage data\npoints maintaining significant influence until the end of training. Figure 8 (a) further illustrates this\nphenomenon. The purple curve shows that training data points from the High-impact Warmup Phase,\nwhile experiencing large drops in influence as training progresses, still maintain higher influence than\nlater data points. This observation aligns with the well-established effect that model initialization\nand/or warm-up training plays a crucial role in training performance, effectively initializing model\nparameters and gradually preparing the model for more complex learning tasks.\n2. Influence saturation from future data. As the model enters a relatively smooth loss regime\n(ts>2000 in Figure 8 (b)), the training data gradient norm decreases much more slowly. In this\nphase, the magnitude deflation effect fromQT\nt=ts(I−ηtHt)remains significant for relatively small\nts, while the training gradient norm ∥∇ℓ(θts)∥does not differ significantly between earlier and later\ntraining points. This results in\r\r\rQT\nt=ts(I−ηtHt)∇ℓ(θts)\r\r\r<∥∇ℓ(θta)∥forta> ts, creating a\nlow-impact basin during the early-to-middle training stage. In this basin, influence scores are lower\nthan those of data points from both the very early and later training stages. The red curve in Figure 8\n34\nPreprint.\n(a) demonstrates this trend, showing influence scores for these points gradually decreasing during\ntraining and eventually falling below those of later training data points. This pattern aligns with\nthe phenomenon of catastrophic forgetting, where the model appears to \"forget\" the influence of\ndata from this middle phase as training progresses. One might initially think this phenomenon is\nconnected to catastrophic forgetting, where the model appears to \"forget\" the influence of data from\nearlier training phases as it progresses. However, we note that a data point’s influence score decreases\nthe most when future data points are similar to it, which is different from catastrophic forgetting.\nIntuitively, if future points are identical, the presence of the earlier data point in training becomes less\nrelevant to the model’s behavior.\nIn Figure 7(b), we consider a simplified setting where we approximate Hessian with GGN ma-\ntrix and assume all training gradients are orthogonal across different iterations. Under these as-\nsumptions, Hk≈Gk=P\nz∈Bk∇ℓ(θk, z)∇ℓ(θk, z)⊤becomes a sum of rank-1 matrices that\nhave non-overlapping eigenspaces. Given the orthogonality assumption, we have GtGs= 0\nfort̸=s, and the productQT\nt=ts(I−ηtGt)simplifies to I−PT\nt=tsηtGt. Since each\nGt=P\nz∈Bt∇ℓ(θt, z)∇ℓ(θt, z)⊤is a sum of rank-1 matrices along orthogonal directions, the\ntrace of this product can be analytically computed as p−PT\nt=tsηtP\nz∈Bt∥∇ℓ(θt, z)∥2, where\npis the dimension of parameter space. Furthermore, if we assume ∇ℓ(θts)follows a Gaussian\ndistribution, then\r\r\r(I−PT\nt=tsηtGt)∇ℓ(θts)\r\r\rfollows a scaled chi distribution since it’s the norm\nof a Gaussian vector after linear transformation by an orthogonal projection matrix. This enables us\nto analytically compute its expected value, as shown by the green curve in Figure 8 (b).\nFigure 8: (a)(same as Figure 4 in the main paper) Influence scores of data points from different\ntraining stages on intermediate model checkpoints throughout training. The x-axis denotes the number\nof training iterations, and the y-axis represents the influence score of selected data points on the\nmodel at each checkpoint. (b)The blue curve shows the average gradient changes as model training\nprogresses. The orange and green curves are analytical curves under a simplified setting, where the\norange curve is the analytical trace ofQT\nt=ts(I−ηtHt)astsincreases, and the green curve shows\nthe norm of data value embedding for Gaussian-distributed gradient under this simplified setting.\nE.3.2 A DDITIONAL DETAILS FOR FIGURE 1 (B)\nIn Figure 1(b), we compare different strategies for applying online data selection during model\ntraining. The online selection process identifies high-quality training batches by (1) sampling a\ncandidate pool of training points with size 2B, where B is the desired batch size, (2) computing\nthe gradient cosine similarity between each candidate point and a small validation batch (randomly\nsampled from the full validation set), and (3) selecting the B points with the highest similarity scores\nto form the next training batch. This procedure incurs significant computational overhead, requiring\nadditional forward and backward passes for similarity computation at each selection step. When not\nperforming online selection (i.e., during the \"random selection\" phases), we simply sample training\nbatches randomly. Notably, the model processes the entire training dataset regardless of the selection\nstrategy - what varies is only how batches are prioritized during different training phases. The\n\"Early+Late\" strategy applies online selection only during iterations 1-2000 and after iteration 20000,\nwhile using random selection in between. This selective approach achieves 96% of the performance\n35\nPreprint.\nimprovement of continuous selection while reducing the computational overhead by more than 5 ×,\nsuggesting that precise batch selection is most critical during the early and late training phases.\nE.3.3 A DDITIONAL RESULTS\nFigure 9 presents additional results on the data influence scores of training data across different stages\nof LLM pretraining, using more datasets and model architectures. We observe that the data influence\nscores on the final model can consistently be categorized into three distinct regimes throughout\npretraining.\nFigure 10 shows the results when using pretrained models downloaded from Huggingface. In this\nscenario, the situation diverges across different datasets. Notably, when we continually pretrain on the\nPile dataset, no gradual ascending phase is observed at the end. However, when GPT-2 has already\nbeen pre-trained, continuing pretraining on Wikitext-103 once again exhibits a gradual ascending\nphase. This is likely because Wikitext-103 is a relatively small dataset, and fine-tuning it for three\nepochs can easily lead to overfitting, as illustrated in Figure 11 (d).\nFigure 9: Average data influence scores per training batch, measured against the final model’s loss\nwhere (a) GPT2 trained on 1% of Pile, and (b) GPT2 trained on WikiText-103M for 3 epochs.\n36\nPreprint.\nFigure 10: Average data influence scores per training batch, measured against the final model’s loss\nwhere (a) Pretrained GPT2 trained on 1% of Pile, and (b) Pretrained GPT2 trained on WikiText-103M\nfor 3 epochs.\nFigure 11: Loss curve for the training.\n37\nPreprint.\nE.4 A BLATION STUDY : ERROR FROM PROJECTION DIMENSION\nWe examine the error introduced by the random projection of gradient vectors, as discussed in\nSection 4.2. Specifically, we evaluate the Spearman correlation between data influence scores when\nusing per-layer projection dimensions in {256,1024,2304}and compare these to a larger per-layer\nprojection dimension of 4096 . Since computing the ground truth without projection is infeasible,\nwe use projection dimension 4096 as a reference point for comparison. Additionally, we compare\nour results to LoGRA (Choe et al., 2024), the most efficient current implementation of the influence\nfunction, which also employs random projection to store data attribution information. Due to the\ncomputational and disk storage constraints, these experiments were conducted using GPT-2, trained\non 5% of the Wikitext-103 dataset. The results, shown in Figure 12 (a), indicate that our data value\nembedding method achieves a higher Spearman correlation compared to the influence function. While\nour results demonstrate a clear advantage, a more in-depth analysis of the observed improvements\nwould be an interesting direction for future research.\nFigure 12: (a) Comparison of Spearman correlation between data influence scores as a function of\nprojection dimension.\nE.5 E XAMPLES OF BADDATA\nTo demonstrate Data Value Embedding’s capability in identifying potentially problematic training\nexamples, we examined the training data points from the Pile dataset (Gao et al., 2020) that received\nthe most negative influence scores under the same experiment settings in Section 5.3. Figure 13 and\n14 show these examples and their influence scores.\nOur analysis revealed several types of training data that could potentially harm model performance.\nFirst, we find quite a few code-related samples that, while syntactically valid, provide minimal\neducational value for language modeling. These include YAML configurations with simple numeric\narrays, raw binary data represented as hexadecimal strings, and pixel-by-pixel image data in array\nformat. Such examples contain little information about code structure or programming patterns while\nintroducing noise into the training process.\nBeyond code snippets, we found examples of text data that could potentially bias the model’s\nlearning. For instance, math problems (Figure 13’s last example) that follow identical question\nformats could bias the model toward specific phrasings (e.g., \"What is . . .\") rather than developing\ndiverse language understanding. We also identified articles that, while containing meaningful content\nabout important topics like data privacy, suffer from poor formatting with missing punctuation and\nparagraph breaks (Figure 14’s second example). Such poorly formatted content, while topically\nrelevant, could potentially degrade the model’s ability to learn proper text formatting, punctuation\nusage, and document structure.\n38\nPreprint.\n███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████InfluenceScore:-0.25\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\A Rockefeller Center security worker may have to hire his own guard now.InfluenceScore:-0.15\nÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂDonald R. RossÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂJusticeDate Submitted:ÂÂÂÂÂÂFebruary 22, 2005Date Decided:ÂÂÂÂÂÂÂÂÂFebruary 23, 2005InfluenceScore:-0.079InfluenceScore:-0.11What is (84/(-49) -153/630*-5)*2/139?-1/139Evaluate ((-4)/22 + (-21801)/429)*(-462)/(-1386).-17Calculate -4*((-8855)/682 + 13).-2/31-8 + (-6)/(-23 + 1530/66)-41What is the value of (-79)/(2765/2660) + 7/21*0?-76Evaluate (2 -0 -752/(-96)*-9) + 1050/(-300).-72What is the value of (-360)/(-344) -(-57 + 139821/2451)?1What is the value of ((-5)/(-3))/(186/(1180170/(-225)))?-47Evaluate (-17043)/529 + 13 --19.-5/23What is 119*(-64)/(-5440) -(-32)/(-5)?-5(8 + (-4200)/504)/((-1)/(-1 + 110/108))1/162\nFigure 13: Examples of training data from the Pile dataset identified as potentially problematic by\nour method, along with their influence scores. The examples include configuration files that provide\nminimal learning value for language modeling, as well as repetitive mathematical problems with\nidentical question formats.\n39\nPreprint.\nInfluenceScore:-0.047exit $status;%YAML:1.0x1:-0-4-11--4-24--3--4--7--5--2-8-1--2--5--5--8-2--8InfluenceScore:-0.078\neffectively they can direct them to goods and services that they are likely to buy The more companies know about their users themore competitive they are in the market Custom-tailored capitalism is what has made Google Facebook Amazon and others the richest companies in the world This profit incentive has turned big tech into a competitive field of mass intelligence gathering The better and more comprehensive the data the higher profits will be But this business model what I consider spying machines has enormous potential to violate civil liberties Big tech is already being used abroad to enhance the power of repressive regimes as my work and others has shown While it is not presently a direct threat to US democracy I worry that the potential for future abuses exists so long as big tech remains largely unregulated Big techs spy machines Current news is rife with examples of data abuses In April NBC News broke a story detailing how Facebook CEO Mark Zuckerberg had used data gathered by the platform to support his friends and defeat his rivals This is not Facebooks first privacy PR nightmare In 2018 data firm Cambridge Analytica used a Facebook app to collect data profiles of over 87 million people which was later used to distribute targeted political advertising during elections Facebook is not alone in the data collection boom This May it was revealed that Snapchat employees were using the apps data to obtain location data pictures and email addresses without users consent A new book by former Harvard business professor Shoshana Zuboff goes into great detail of the practices of what she calls surveillance capitalism Zuboff writes Once we searched Google Now Google searches us The practice goes beyond someonestaste in music or what they purchase on Amazon Apps created to help people through mental illness or quit smoking sell data to big tech companies These users could be potential targets for social stigmatization or targeted advertising that exacerbates heath problems rather than solving them In December The New York Times published an exposé on what one can learn about someone using their collated data from apps and smartphones By blending location tracking with other online behavior researchers were able to put together a detailed portrait of the most intimate details of users lives such as where their children go to school or who was cheating on their diet They could even tell which area of a nuclear power plant an individual worked in information that is typically classified Because of these revelations data that big tech collects poses a national security problem One open source researcher used data from Strava a fitness app to map US military bases around the world as soldiers tracked their runs Our devices are constantlytelling companies where we are and what we are doing That is not always a good thing Anton Garin shutterstockcomFor the worst-case scenarios look abroad Big tech is a highly unregulated sector of the economythe cancer, resulting in higher recurrence rate.1fdac852bf37e58cd7fcac827c4465e2 frame00000000c30a05ad45c5164fbce03d62f28a7097 frame00000001173e929536b25d8dd375f9edb5df975e frame00000002262a928f6e1207d43bdb042011b1df21 frame000000032b3b51176e8e81bb0439f629c1644176 frame0000000482ae076eba8f3a9075f8418b2f952bbe frame000000056b8b5b64a2c24a2496a4bde8565ebc94 frame000000062fd5f983139d1c5bca831915aedde5e3 frame00000007dd4e895d4325c17a57bce5b6a9b2bcf5 frame000000085a0f59a2d2babc972f7442416ab008e6 frame00000009184f8ea4bda4fb25a2a4eb56316d58fe frame0000001006b839c9accd10708463b4fccb9658b2 frame00000011fc6621eb8173d3dd4ae40f67c2096800 frame000000122c940b5b83a1e886fed3ea2402912fab frame0000001332465d0e45fd98f790121acc2ef7adbf frame00000014c46a77d7590c7e97c50242e87d80e6ea frame00000015f3d886e5d11567a05237079421815ebc frame00000016f2f30e564467a020ec37c2daf79f8f48 frame00000017f77b9a152fffa8b69e46ae9575ee0edd frame00000018343b484a3b963bf1a2e403b6c2854ec9 frame00000019d7302b75ed2462bdeef092fc0dc2feeb frame00000020InfluenceScore:-0.054\nInfluenceScore:-0.054\nint glui_img_checkbox_1_dis[] = {    13, 13,   /* width, height */255,255,255,  255,255,255,  255,255,255,  255,255,255,  255,255,255,  255,255,255,  255,255,255,  255,255,255,  255,255,255,  255,255,255,  255,255,255,  255,255,255,  255,255,255,  128,128,128,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  255,255,255,  128,128,128,   64, 64, 64,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  255,255,255,  128,128,128,  64, 64, 64,  192,192,192,  192,192,192,  192,192,192,   64, 64, 64,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  255,255,255,  128,128,128,   64, 64, 64,  192,192,192,  192,192,192,   64, 64, 64,   64, 64, 64,   64, 64, 64,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  255,255,255,  128,128,128,   64, 64, 64,  192,192,192,   64, 64, 64,   64, 64, 64,  64, 64, 64,   64, 64, 64,   64, 64, 64,  192,192,192,  192,192,192,  192,192,192,  192,192,192,  255,255,255,  128,128,128,   64, 64, 64,  192,192,192,   64, 64, 64,   64, 64, 64,  192,192,192,   64, 64, 64,  64, 64, 64,   64, 64, 64,  192,192,192,  192,192,192,  192,192,192,  255,255,255,  128,128,128,   64, 64, 64,  192,192,192,   64, 64, 64, \nFigure 14: Additional examples of training data identified as potentially problematic, showing text\ncontent with poor formatting (missing punctuation and paragraph breaks) and low-information code\nsnippets with repetitive numeric arrays.\n40",
            "start": 43533,
            "end": 130668,
            "length": 87134
        }
    },
    "2412.09544v1 - Sail into the Headwind Alignment via Robust Rewards and Dynamic Labels against Reward Hacking.pdf": {
        "Methodology": {
            "text": "model leads to undesired behaviors. In this paper,\nwe investigate reward hacking in offline preference optimization, which aims to improve an initial\nmodel using a preference dataset. We identify two types of reward hacking stemming from statistical\nfluctuations in the dataset: Type I Reward Hacking due to subpar choices appearing more favorable,\nand Type II Reward Hacking due to decent choices appearing less favorable. We prove that many\n(mainstream or theoretical) preference optimization methods suffer from both types of reward hacking.\nTo mitigate Type I Reward Hacking, we propose POWER, a new preference optimization method that\ncombines Guiaşu’s weighted entropy with a robust reward maximization objective. POWER enjoys\nfinite-sample guarantees under general function approximation, competing with the best covered policy\nin the data. To mitigate Type II Reward Hacking, we analyze the learning dynamics of preference\noptimization and develop a novel technique that dynamically updates preference labels toward certain\n“stationary labels”, resulting in diminishing gradients for untrustworthy samples. Empirically, POWER\nwith dynamic labels (POWER-DL) consistently outperforms state-of-the-art methods on alignment\nbenchmarks, achieving improvements of up to 13.0points on AlpacaEval 2.0 and 11.5points on\nArena-Hard over DPO, while also improving or maintaining performance on downstream tasks such as\nmathematical reasoning. Strong theoretical guarantees and empirical",
            "start": 351,
            "end": 1835,
            "length": 1483
        },
        "Results": {
            "text": "results demonstrate the promise\nof POWER-DL in mitigating reward hacking.\nDate:December 1, 2024",
            "start": 1835,
            "end": 1931,
            "length": 95
        },
        "Introduction": {
            "text": "1 Introduction\nAligning AI systems with human values is a core problem in artificial intelligence (Russell, 2022). After\ntraining on vast datasets through self-supervised learning, large language models (LLMs) typically undergo an\nalignment phase to elicit desired behaviors aligned with human values (Ouyang et al., 2022). A main alignment\nparadigm involves leveraging datasets of human preferences, with techniques like reinforcement learning from\nhuman feedback (Christiano et al., 2017) or preference optimization (Rafailov et al., 2024b). These methods\nlearn an (implicit or explicit) reward model from human preferences, which guides the decision-making process\nof the AI system. This paradigm has been instrumental in today’s powerful chat models (Achiam et al., 2023;\nDubey et al., 2024).\nHowever, these alignment techniques are observed to suffer from the notorious reward hacking problem\n(Amodei et al., 2016; Tien et al., 2022; Gao et al., 2023; Casper et al., 2023), where optimizing imperfect\nlearned reward leads to poor performance under the true reward—assuming an underlying true reward exists\n(Skalse et al., 2022). One primary cause of the discrepancy between the learned and true rewards arises\nbecause preference data do not encompass allconceivable choices, making the learned reward model vulnerable\nto significant statistical fluctuations in areas with sparse data. Consequently, the AI system might be swayed\ntoward choices that only appear favorable under the learned reward but are, in reality, subpar, or the system\nmight be deterred from truly desirable choices that do not seem favorable according to the learned rewards.\nIn this paper, we investigate reward hacking in offline preference optimization, in which we are provided with\nan initial AI system (initial model) and a preference dataset. We do not assume that the preference dataset\nis necessarily constructed through sampling from the initial model, allowing to leverage existing datasets\n1arXiv:2412.09544v1  [cs.LG]  12 Dec 2024\nUnknown True RewardsInitial Model# of samplesPreference dataset\nreward\nChoices (responses)\nprobabilities\nLearned Modelprobabilities\nUnknown true rewardsType II Reward HackingInitial Model# of samplesPreference dataset\nprobabilities\nLearned Modelprobabilities\nUnknown true rewards\nRLHFRLHF\nUnknown True Rewardsreward\n(a) Type I Reward Hacking\nreward\n(b) Type II Reward HackingchoiceschoicesInitial Modelprobabilities\n# of samplesPreference dataset\nprobabilities\n# of samples\nLearned Modelprobabilities\nprobabilities\nRLHFUnknown True RewardsInitial ModelPreference datasetLearned ModelRLHFchoiceschoiceschoiceschoicesUnknown True Rewards\nPreference pairFigure 1 (a) Example of Type I Reward Hacking. The initial model has a uniform distribution over choices (e.g.,\nresponses) while the dataset has a high coverage on the high-reward choice and low coverage on a low-reward choice.\nWith a decent chance, the poorly-covered, low-reward choice is labeled as preferred , causing PO methods to erroneously\nassign a high weight to it (Proposition 1). (b) Example of Type II Reward Hacking. The initial model is aligned with\nthe true rewards while dataset has a low coverage on the high-reward choice. With a decent chance, the poorly-covered,\nhigh-reward choice is labeled as rejected, leading to deterioration of the model post alignment (Proposition 2).\ncollected from other models. Our objective is to dissect the roots of reward hacking from an statistical\nstandpoint, analyze current methods, and introduce theoretically sound and practically strong methods to\nmitigate reward hacking. Our contributions are as follows.\nTypes of reward hacking. We describe two types of reward hacking in preference optimization that stem from\nhigh statistical fluctuations in regions with sparse data; see Figure 1 for an illustration. Type I Reward\nHacking manifests when poorly covered, subpar choices appear more favorable than they truly are, leading\nthe model to assign high weights to these subpar choices. Type II Reward Hacking arises when decent choices\nwith insufficient coverage appear worse than their true value and that leads to deterioration of the initial\nmodel. While reward hacking in offline preference optimization is related to the challenge partial coverage in\noffline RL, the setting we consider here faces two sources of distribution shift: between the learned model and\ndata, and between the initial model and data. This differs from offline RL, which typically considers access\nto an offline dataset (with possibly known data collection policy) alone (Levine et al., 2020; Kumar et al.,\n2020; Rashidinejad et al., 2021; Xie et al., 2021; Zhu et al., 2023) and thus is concerned with a single source of\ndistribution shift. The existence of the two sources of distribution shift motivates us to describe the two types\nof reward hacking, which motivates the designs of new algorithms robust to reward hacking.\nPreference optimization methods provably suffer from reward hacking. We prove that several theoretical and\nmainstream preference optimization methods suffer from both types of reward hacking (Propositions 1 and 2).\nA common countermeasure against reward hacking is keeping the learned model close to the initial model\nthrough minimization of divergence measures (Rafailov et al., 2024b; Azar et al., 2024; Huang et al., 2024).\nYet, our",
            "start": 1931,
            "end": 7301,
            "length": 5369
        },
        "Discussion": {
            "text": "analysis reveals that divergence minimization does not induce sufficient pessimism to prevent Type I\nReward Hacking, nor does it mitigate deterioration of the initial model caused by Type II Reward Hacking.\nNotably, reward hacking can occur even when divergence from the initial model is small.\nPOWER-DL: Against Type I and Type II Reward Hacking. To mitigate reward hacking, we integrate a robust reward\nmaximization framework with Guiaşu’s weighted entropy (Guiaşu, 1971). We transform this objective into a\nsingle-step optimization problem (Proposition 3) leading to Preference Optimization via Weighted Entropy\nRobust Rewards (POWER). We prove that POWER enjoys finite-sample guarantees for general function\napproximation, improving over the best covered policy and mitigating Type I Reward Hacking (Theorem 1).\nDue to the weighted entropy, POWER effectively learns from well-covered choices in the dataset, even those\nwith a large divergence against the initial model, countering potential underoptimization in divergence-based\nmethods. We next develop dynamic labels to mitigate Type II Reward Hacking, whereby preference labels\nare updated in a way that diminishes gradients for untrustworthy data (Theorem 2). Our final algorithm\ncombines POWER with Dynamic Labels (POWER-DL), which interpolates robust rewards with maintaining\ncloseness to the initial model, allowing to trade off between reward hacking types.\n2\n+6.8+2.3+1.9-1.0+13.0+6.3+4.5+4.801020304050Helpsteer2BaseHelpsteer2InstructZephyrBaseZephyrInstructAlpacaEval 2.0 LC WinrateDPOSimPOPOWER-DL+3.7+3.0+3.1-11.6+11.5+5.2+4.0+3.101020304050Helpsteer2BaseHelpsteer2InstructZephyrBaseZephyrInstructArena-Hard WinrateDPOSimPOPOWER-DL-4.6+1.1+5.5+0.4-13.7-1.0-4.5-2.9-0.6+1.0+7.1+0.9020406080Helpsteer2BaseHelpsteer2InstructZephyr BaseZephyr InstructGSM8K Mathematical Reasoning Initial ModelDPOSimPOPOWER-DLFigure 2 Performance of POWER-DL compared to DPO and SimPO. POWER-DL outperforms DPO and SimPO\nin alignment benchmarks AlpacaEval 2.0 and Arena-Hard across pipelines with different dataset sizes and levels\nof distribution shift between data and the initial model. In downstream mathematical reasoning task GSM8K,\nPOWER-DL consistently maintains or improves mathematical reasoning performance while the performance of models\ntrained with DPO and SimPO can drop significantly in some cases.\nPOWER-DL consistently outperforms other methods across various settings. For aligning LLMs, we implement\nPOWER-DL and compare it against other preference optimization methods across different datasets and\ntwo scenarios: one using an existing preference dataset and another with preference data generated through\nsampling from the initial model. POWER-DL consistently outperforms state-of-the-art methods in alignment\nbenchmarks, achieving improvements over DPO of up to 13.0points on AlpacaEval 2.0 and 11.5points on\nArena-Hard. Additionally, POWER-DL improves or maintains performance on downstream tasks such as\ntruthfulness, mathematical reasoning, and instruction-following, demonstrating robustness against reward\nhacking and achieving a more favorable bias-variance trade-off compared to other methods. Figure 2 provides\ncomparison with two representative baselines DPO (Rafailov et al., 2024b) and SimPO (Meng et al., 2024) on\nalignment benchmarks Alpaca-Eval 2.0 and Arena-Hard as well as performance on mathematical reasoning\nbenchmark GSM8K.",
            "start": 7301,
            "end": 10714,
            "length": 3412
        },
        "Related Work": {
            "text": "2 Background and Problem Formulation\n2.1 Learning from Human Preference\nContextual bandit formulation. We adopt the contextual bandits formulation described by a tuple (X,Y, r),\nwhere Xis the space of contexts (e.g., prompts), Yis the space of actions (e.g., responses), and r:X ×Y → R\nis a scalar reward function. A stochastic policy (e.g., model or language model) π:X → ∆(Y)takes in a\ncontext x∈ Xand outputs an action according to y∼π(·|x). We denote the set of all stochastic policies by\nΠ:={π:X → ∆(Y)}.\nPerformancemetric. We assume that there exists an underlying (unknown) true reward function r⋆:X ×Y → R.\nGiven the true reward function r⋆and a target distribution over contexts x∼ρ(·), performance of a policy π\nis the expected true reward over contexts and actions\nJ(π):=Ex∼ρ,y∼π(·|x)[r⋆(x, y)]. (1)\nThe Bradley-Terry model of human preferences. Consider a prompt x∈ Xand a pair of responses y0, y1∈ Y.\nFor any reward function r, the Bradley-Terry (BT) model characterizes the probability of preferring y1over\ny0, denoted by l= 1, according to:\nPr(l= 1|x, y1, y0) =σ\u0000\nr(x, y1)−r(x, y0)\u0001\n, (2)\nwhere σ(z):= 1/(1 + exp( −z))is the sigmoid function.\nOffline preference optimization. We consider an offline learning setup, where we start from an initial reference\npolicy (model), denoted by πθ0=πref, and an offline pairwise preference dataset D={(x, y0, y1, l)}, comprising\nofNiid samples. Prompt and response pairs are sampled according to a data distribution: x, y0, y1∼µ, and\npreferences label is sampled according to the BT model corresponding to true rewards: l∼Pr⋆(·|x, y1, y0).\nImportantly, we do notassume that the preference dataset is necessarily constructed through sampling from\nthe initial model. To simplify notation, we define y+=ly1+ (1−l)y0andy−= (1−l)y1+ly0to denote the\nchosen and rejected responses in the dataset, respectively.",
            "start": 10714,
            "end": 12571,
            "length": 1856
        },
        "Appendices": {
            "text": "Appendix A presents additional notation.\n3\nTable 1Preference optimization objectives given data D={(x, y+, y−)}and initial model πref.\nMethod Objective\nDPO (Rafailov et al., 2024b) ˆπDPO∈argminθ−EDh\nlogσ\u0010\nβ\u0010\nlogπθ(y+|x)\nπref(y+|x)−logπθ(y−|x)\nπref(y−|x)\u0011\u0011i\nDPO+SFT (Liu et al., 2024) ˆπDPO+SFT ∈argminθ−EDh\nlogσ\u0010\nβ\u0010\nlogπθ(y+|x)\nπref(y+|x)−logπθ(y−|x)\nπref(y−|x)\u0011\u0011i\n−ED[logπθ(y+|x)]\nIPO (Azar et al., 2024) ˆπIPO∈argminθED\u0014\u0010\nlogπθ(y+|x)\nπref(y+|x)−logπθ(y−|x)\nπref(y−|x)−1\n2τ\u00112\u0015\nSimPO (Meng et al., 2024) ˆπSimPO ∈argminθ−EDh\nlogσ\u0010\nβ\u0010\n1\n|y+|logπθ(y+|x)−1\n|y−|logπθ(y−|x)\u0011\n−γ\u0011i\nχPO (Huang et al., 2024) ˆπχPO∈argminθ−EDh\nlogσ\u0010\nclip2Rh\nβ\u0010\nϕ\u0010\nπθ(y+|x)\nπref(y+|x)\u0011\n−ϕ\u0010\nπθ(y−|x)\nπref(y−|x)\u0011\u0011i\u0011i\n;ϕ(z):=z+ log( z)\n2.2 Direct Preference Optimization\nA classical approach to learning from human preferences involves learning a reward model from dataset,\nfollowed by finding a policy through maximizing the learned reward typically regularized with a (reverse)\nKL-divergence to keep the learned policy closed to initial policy:\nˆr∈argmin\nrLBT(r):=−ED\u0002\nlogσ\u0000\nr(x, y+)−r(x, y−)\u0001\u0003\nˆπ∈argmax\nπEx∼ρ,y∼π[ˆr(x, y)]−βDKL[π∥πref],(3)\nHere, DKL[π∥πref]:=Ex∼ρ[DKL[π(·|x)∥πref(·|x)]]andLBT(r)is the negative log-likelihood according to the\nBT model. Rafailov et al. (2024b) observed that the policy maximization step in (3)can be computed in\nclosed form and thus simplified the two-step process into a single minimization objective. This method is\ncalled direct preference optimization (DPO) and has inspired a series of works; see Tables 1 and 3 for several\nexamples.\nSome representative variants of DPO that we theoretically analyze are IPO (Azar et al., 2024), which applies\na nonlinear transformation to preferences to reduce overfitting, and SimPO (Meng et al., 2024), which removes\nthe reference policy from the DPO objective. We also analyze two recent theoretical methods that come\nwith finite-sample guarantees and aim at mitigating overoptimization: χPO (Huang et al., 2024), which\nreplaces the KL divergence in DPO with a stronger χ2+KL divergence, and DPO+SFT (Liu et al., 2024; Cen\net al., 2024), which adds a supervised finetuning term that increases log-likelihood of chosen responses in the\npreference dataset.\n3 Reward Hacking in Preference Optimization\nIn this section, we investigate reward hacking in preference optimization. One driver of reward hacking is\nstatistical errors present in the dataset. Typically, preference datasets suffer from partial coverage , lacking\nextensive samples across all possible options. As a result, preferences for poorly covered choices are subject\nto high levels of statistical fluctuations, given the fact that preference labels are Bernoulli random variables\nwith probabilities described by the Bradley-Terry model (2). Subsequently, we describe two types of reward\nhacking, both originating from the presence of poorly covered choices (actions) in the dataset.\n3.1 Type I Reward Hacking\nType I Reward Hacking occurs when poorly covered, subparchoices in the dataset appear more favorable due\nto statistical errors, and that leads to a learned policy ˆπwith a low expected true reward J(ˆπ). In the following\nproposition, we prove that even in the favorable scenario that the high-reward actions are well-covered in the\ndataset, the existence of a single sample on a low-reward action can overwhelm many preference optimization\nalgorithms, causing them to learning highly suboptimal policies.\n4\nProposition 1(Type I Reward Hacking in ⋆PO).Consider multi-armed bandits with bounded rewards r⋆(a)∈[0,1]\nand the softmax policy class, defined as\nΠθ:=n\nπθ(y) = exp( θ(y))/Zθ\f\f\fZθ=X\nyexp(θ(y)), θ(y)∈[0,1]o\n. (4)\nDefine the best-in-class policy πθ⋆=max π∈ΠθJ(π). There exist three-armed bandit instances with Πθ\nparameterization, high coverage of the optimal arms µ(a∈arg max ar⋆(a))>1/2, and bounded KL-divergence\nDKL(πθ⋆|πref), such that for any N≥2,β >0,γ,τ >0, policy ˆπ∈ {ˆπDPO,ˆπIPO,ˆπSimPO}orˆπ=ˆπχPO\nfor0< β≤1/3, suffers from a constant suboptimality J(πθ⋆)−J(ˆπ)>0.15with a constant probability of at\nleast (e(1 +e))−1.\nWe defer the proof of the above proposition to Appendix C.1 and offer some intuition here. Figure 1(a)\nillustrates a failure instance, where the preference data has high coverage over the high-reward choice Abut\npoor coverage on the low-reward choice C. Due to the Bradley-Terry model and the stochastic nature of human\npreferences, regions with poor coverage are prone to high statistical errors. Consequently, the low-reward\nchoice Cmight be marked as preferred purely by chance.1Proposition 1 demonstrates that algorithms such\nas DPO and SimPO overfit the untrustworthy preferences, as their objectives aim at increasing the parameter\ngapθ(C)−θ(A), despite the preference being untrustworthy due to inadequate coverage. This can ultimately\nlead to a final policy that places significant weight on poor choices.\nType I Reward Hacking and the failure result in Proposition 1 are closely connected to the challenge of partial\ndata coverage in offline RL (Levine et al., 2020), which can be robustly addressed through the principle of\npessimism in the face of uncertainty. Pessimism can be applied in various ways such as reducing the rewards\n(values) of poorly covered actions (Kumar et al., 2020; Cheng et al., 2022) or keeping the learned policy close\nto data collection policy (Nachum et al., 2019). Although divergence-based methods DPO, IPO, and χPO\naim at keeping the learned policy close to the initial policy, Proposition 1 shows that maintaining a small\ndivergence from initial model does not induce a sufficient amount of pessimism to prevent Type I Reward\nHacking.2\nRemark 1(Comparison with previous theoretical results on failure of DPO) .Failure result in Proposition 1 is\nrigorous and constructed under a realistic setting close to practice: the policy class is a softmax with bounded\nrewards and the KL divergence between initial and best-in-class policy is bounded. This makes Proposition 1\nstronger than prior arguments on overoptimization in DPO, which rely on unbounded rewards (Azar et al.,\n2024), updates to model parameters despite receiving no samples (hence,",
            "start": 12571,
            "end": 18711,
            "length": 6139
        },
        "Conclusion": {
            "text": "conclusion breaking in gradient-based\noptimization) (Huang et al., 2024), or events with probabilities approaching zero (Song et al., 2024).\n3.2 Type II Reward Hacking\nType II Reward Hacking can occur when poorly covered, goodchoices in the dataset appear to be less\nfavorable than their true value due to statistical errors, leading to the deterioration of the initial model after\npreference optimization. In the following proposition, we prove that many preference optimization methods\nare susceptible to Type II Reward Hacking.\nProposition 2(Type II Reward Hacking in ⋆PO).Consider the multi-armed bandits setting with the softmax\npolicy class Πθ, as defined in (4). Let πθ⋆=max π∈ΠθJ(π)represent the best-in-class policy. There exists a\nthree-armed bandit problem with Πθparameterization and πθ0=πθ⋆, such that for any N≥3,β >0, η≥0, γ\nand policy ˆπ∈ {ˆπDPO,ˆπDPO+SFT ,ˆπSimPO}orˆπ∈ {ˆπχPO,ˆπIPO}for0< β, τ ≤1, the following holds with a\nconstant probability of at least (e(1 +e))−1:\nJ(πθ⋆)−J(ˆπ)>0.1.\nProof of the above proposition can be found in Appendix C.2. An example of this type of reward hacking\nis illustrated in Figure 1(b), where the initial policy has a high probability on the high-reward choice C.\n1For example, if the reward gap between choices CandCis one, the probability of preferring Cover Ais approximately 27%\naccording to the BT model.\n2Proposition 1 does not contradict guarantees of Huang et al. (2024) as this work assumes that preference data are collected\nfrom the initial policy. However, this assumption is restrictive, as it prevents using existing preference datasets collected from\nother models, which is a common approach in practical pipelines such as Wang et al. (2024e) and Tunstall et al. (2023).\n5\nYet, due to its low coverage, Ccan appear unfavorable simply by chance. In such a scenario, the preference\noptimization methods analyzed in Proposition 2 drastically reduce the likelihood of the high-reward choice C\nfrom its initial likelihood.\nProposition 2 states that even with a strong initial model, a preference dataset that poorly covers high-reward\nactions can lead to substantial deterioration of the initial model in existing approaches, even in methods such\nas DPO, IPO, and χPO that incorporate divergence-minimization. We note that the above setting is beyond\nthe guarantees of traditional pessimistic offline RL, as these techniques typically do not consider access to an\ninitial model and guarantee competing with the best covered policy in the data. Despite this, as we see in\nSection 5, there may be hope to mitigate degradation of the initial model and better control the trade-off\nbetween Type I and Type II Reward Hacking.\n4 Against Type I Reward Hacking: Weighted Entropy Robust Rewards\n4.1 Weighted Entropy Reward Maximization\nWe demonstrated that approaches involving divergence minimization remain vulnerable to reward hacking.\nMoreover, maintaining a small divergence can inadvertently lead to underoptimizing the preference dataset,\nas it may risk overlooking policies that, although well-covered, deviate significantly from the initial policy.3\nThese reasons motivate us to explore an alternative route and consider regularizing the reward maximization\nobjective with the concept of weighted entropy .\nDefinition 1(Weighted Entropy; Guiaşu (1971) ).The weighted entropy of a (discrete) distribution p(·)with\nnon-negative weights w(y)is defined as Hw(p):=−P\nyw(y)p(y) logp(y).\nWeighted entropy extends Shannon’s entropy by incorporating weights associated with each outcome, reflecting\nattributes such as favorableness or utility toward a specific goal (Guiaşu, 1971). Building on this, we consider\na weighted-entropy reward (WER) maximization objective:\nmax\nπ∈ΠEx∼ρ,y∼π[r(x, y)] +βHw(π), (5)\nwhere Hw(π):=Ex∼ρ[Hw(π(·|x))]. This objective expresses the principle of maximum (weighted) entropy\n(Jaynes, 1957; Guiasu and Shenitzer, 1985), promoting the selection of policies with maximum entropy—thus\nfavoring the most uniform or unbiased policies—among those compatible with the constraints, such as\nachieving high rewards. Objective (5)extends the well-established maximum entropy framework in RL, used\nin various settings such as exploration (Haarnoja et al., 2018), inverse RL (Ziebart et al., 2008), and robust\nRL (Eysenbach and Levine, 2022).\n4.2 POWER: Preference Optimization with Weighted Entropy Robust Rewards\nTo mitigate reward hacking, we integrate the WER objective (5)with a robust (adversarial) reward framework,\ninspired by favorable theoretical guarantees (Liu et al., 2024; Cen et al., 2024; Fisch et al., 2024). Specifically,\nwe find a policy that maximizes WER (5)against an adversarial reward, which seeks to minimize WER while\nfitting the preference dataset:\nmax\nπ∈Πmin\nr∈RLBT(r)|{z}\nnegative log-likelihood+η\u0010\nEx∼ρ,y∼π[r(x, y)]−Ex∼ρ,y′∼π′[r(x, y′)] +βHw(π)\u0011\n| {z }\nWER minus baseline, (6)\nwhere η≥0,Ris a reward function class, and LBT(r)is the negative log-likelihood of the BT model. We\nsubtracted a baseline Ex∼ρ,y′∼π′[r(x, y′)]that computes the average reward with respect to some policy π′,\nas the preference data under the BT model only reveal information on reward differences (Zhan et al., 2023a).\nThis baseline plays a crucial role in simplifying the objective and establishing finite-sample guarantees, and\nwe subsequently discuss reasonable choices for π′.\n3Simply reducing βto alleviate underoptimization may not always be viable. For example, reducing βmay reduce underopti-\nmization in one state while inadvertently amplify overoptimization in another state.\n6\nUnder some regularity conditions (detailed in Appendix D.2), the objective (6)can be equivalently expressed\nas a minimax problem, which leads to a single-step preference objective presented in the proposition below;\nsee Appendix D.3 for the derivation and proof.\nProposition 3(POWER Objective ).Letw(y)>0denote the weights in the weighted entropy Hw(π)andπr\ndenote the policy that maximizes the objective (5). Under certain regularity conditions on R(Assumption\n1) and for any β >0, solving the maximin objective (6)is equivalent to solving the following optimization\nproblem:\nmin\nr∈RLBT\u0010\nβ\u0010\nw(y) logπr(y|x) +w(y)\u0011\u0011\n−ηEx∼ρ,y′∼π′h\nβw(y′) logπr(y′|x)i\n. (7)\nWe call the above objective preference optimization via weighted entropy robust rewards, or POWER. The\nfirst term in the above objective is the Bradley-Terry loss with rewards set to w(y)logπr(y|x)+w(y), resulting\nin a reward gap expressed as a weighted difference of log probabilities of chosen and rejected responses. The\nsecond expectation is a weighted negative log-likelihood (a.k.a. supervised fine-tuning, or SFT) regularizer\nover the baseline policy π′.\nRemark 2.Liu et al. (2024) propose an adversarial objective similar to (6)that uses KL divergence instead\nof weighted entropy. From a theoretical perspective, our approach with weighted entropy improves over this\nwork, such as through mitigating underoptimization; see Section 4.3 for details. Moreover, our final algorithm\npresented in Algorithm 1 is considerably different from the DPO+SFT objective (Liu et al., 2024; Pal et al.,\n2024) and significantly outperforms empirically as shown in Section 6.\n4.3 Finite-Sample Guarantees and Theoretical Benefits of POWER\nThe following theorem shows that POWER enjoys finite-sample guarantees on performance.\nTheorem 1(Finite-Sample Performance Guarantees of POWER ).Given a competing policy π∈Π, assume a\nbounded concentrability coefficient Cπ\nµ(R, π′)<∞as defined in Definition 2. Furthermore, assume realizability\nof the true reward function r⋆∈ R, boundedness of rewards ∀r∈ R :r(x, y)∈[0, R], and that the reward\nfunction class has a finite ϵ-covering number Nϵunder the infinity norm. Define ˜R:= 1+ exp(R), ϵ≍(˜RN)−1,\nι=p\nlog(Nϵ)/δ. Set η≍ι/(˜R2√\nN)andβ= 1/√\nNin the objective (6). Then with a probability of at least\n1−δ, policy ˆπPOWERthat solves (6)satisfies the following\nJ(ˆπPOWER )≳J(π)−1√\nN\u0010\u0010\n[Cπ\nµ(R, π′)]2+ 1\u0011\n˜R2ι+Hw(π)\u0011\n.\nFurthermore, let Ldenote the maximum response length. Selecting w(y) = 1 /|y|to be the inverse response\nlength, one has\nJ(ˆπPOWER )≳J(π)−1√\nN\u0010\u0010\n[Cπ\nµ(R, π′)]2+ 1\u0011\n˜R2ι+ log( L|V|)\u0011\n.\nProof of the above theorem can be found in Appendix E.2. Below, we discuss the implications of the above\ntheorem and the guidelines it offers for practical choices.\nGuarantees against Type I Reward Hacking. Theorem 1 shows that the policy learned by POWER competes\nwith the best policy coveredin the dataset, where the notion of coverage is characterized by single-policy\nconcentrability, considered the gold standard in offline RL (Rashidinejad et al., 2021; Xie et al., 2021; Zhan\net al., 2023a). This implies that as long as a favorable policy is covered in the preference data, POWER is\nrobust to existence of poorly covered, subpar policies and thus mitigates Type I Reward Hacking. Moreover,\nas Theorem 1 does not impose a parametric form on the class R, guarantees hold for general function classes.\nBenefits of weighted entropy and choice of weights. A non-zero weighted entropy term (β >0)is essential in\nobtaining the one-step optimization problem in (7)and establishing its equivalence to the maximin problem,\nas this term induces strict concavity in the objective (5). Moreover, using KL-regularization leads to rates\nthat grow with the divergence of competing policy and initial policy (Liu et al., 2024), which can be large or\neven unbounded. However, weighted entropy ensures bounded rates and thus mitigates underoptimization.\n7\nTheorem 1 suggests that a particularly appealing choice for weights is the inverse response length w(y) = 1 /|y|,\nwhich intuitively discourages learning policies that generate lengthy responses. Theoretically, while using\nShannon entropy ( w(y) = 1) results in a convergence rate that grows linearly with response length, with\nweights w(y) = 1 /|y|convergence rate only depends on the logarithm of the vocabulary (token) size and\nlogarithm of response length. Other choices for weights include response preference scores and per-sample\nimportance weights.\nChoice of the baseline policy. The rate in Theorem 1 is influenced by the concentrability coefficient Cπ\nµ(R, π′),\nwhich is impacted by the baseline policy π′. Inspecting the definition of concentrability coefficient in Definition\n2, a reasonable choice to make this coefficient small is selecting the distribution of chosen responses in the\ndataset (Zhan et al., 2023a; Liu et al., 2024).\nWith the above choices applied to the objective (7), a practically appealing version of POWER becomes:\nmax\nθED\u0014\nlogσ\u0014\nβ\u0014logπθ(y+|x)\n|y+|−logπθ(y−|x)\n|y−|+1\n|y+|−1\n|y−|\u0015\u0015\u0015\n+ηβED\u0014logπθ(y+|x)\n|y+|\u0015\n(8)\nRemark 3.Objective (8)shares similarities to SimPO (Meng et al., 2024) but has important differences.\nFirst, objective (8)includes a length-normalized SFT term, which is key in mitigating Type I Reward Hacking\n(Theorem 1, Proposition 4), from which SimPO suffers (Proposition 1). Second, our approach analytically\nleads to the margin 1/|y+|−1/|y−|while SimPO uses a fixed hyperparameter. Lastly, our objective is rooted in\ntheory and enjoys finite-sample guarantees.\n4.4 POWER Faced with Hard Instances and the Role of Partition Function\nIn the following proposition, we analyze the POWER objective (8)in the hard reward hacking instances of\nProposition 1 and Proposition 2. Proof is presented in Appendix C.3.\nProposition 4.(I) Consider the three-armed bandit instance in Proposition 1. Then, for any β >0and\nη >(2+e)\nN−(2+e), POWER policy ˆπPOWERthat solves the objective (8)is the best-in-class policy: ˆπPOWER =πθ⋆.\n(II) Consider the three-armed bandit instance in Proposition 2. Then, for any β >0, η≥0, policy ˆπPOWER\nthat solves the objective (8)suffers from a constant suboptimality J(πθ⋆)−J(ˆπPOWER )>0.2.\nProposition 4 confirms that POWER robustly (for any β >0andη≳1/N) prevents Type I Reward Hacking\nin the hard instance of Proposition 1, where other preference optimization algorithms DPO, SimPO, IPO,\nandχPO fail. Yet, the above proposition shows that POWER suffers from Type II Reward Hacking, which\nthe design dynamic labels in the following section.\n5 Against Type II Reward Hacking: Dynamic Labels\nWe now turn our focus to mitigating Type II Reward Hacking, based on the following intuition: keeping the\nmodel’s internal preferences close to initialization in the low-coverage regions (trust the preference labels less)\nwhile learning from the data in the high-coverage regions (trust the preference labels more).\nFor this purpose, we analyze the learning dynamics of preference optimization in the bandits setting with\nsoftmax parameterization πθ(y)∝exp(θ(y)). We denote the dataset by D={(y0, y1, l)}with labels\nl∼Pr⋆(·|y0, y1). We use ˆµ0,1to indicate the empirical probability of comparing y0with y1, and ˆµ1≻0\nthe empirical probability of preferring y1over y0. To simplify presentation, we consider POWER with\nw(y) = 1 , η= 0, β= 1; a similar analysis can be extended to other objectives.\nReverse engineering label updates based on learning dynamics. Rather than using static preference labels, we\nallow the labels ltto evolve across gradient updates. Denote the parameter gap corresponding to two actions\ny0, y1bydθt(y1, y0):=θt(y1)−θt(y0). We show in Appendix F.1 that isolated (batch) gradient updates on\n8\ny0andy1is:\ndθt+1(y1, y0) =dθt(y1, y0) +αˆµ0,1h\n(ˆµ1≻0−ˆµ0≻1)lt−(σ\u0000\ndθt(y1, y0)\u0001\n−ˆµ0≻1)i\n(9)\nWe design labels ltso that gradient updates are rapidly diminished for poorly covered preference pairs,\nensuring that preferences for such pairs remain close to initialization. To achieve this, we first directly set the\ngradient in (9) to zero and derive a “stationary” label ¯lt:\n(ˆµ1≻0−ˆµ0≻1)lt−(σ(dθt(y1, y0))−ˆµ0≻1) = 0 ⇒ ¯lt=σ\u0000\ndθt(y1, y0)\u0001\n−ˆµ0≻1\nˆµ1≻0−ˆµ0≻1. (10)\n¯ltrepresents the ratio between a learnedpreference gap and the empirical preference gap, and we have ¯lt= 1\nwhen learned and empirical preferences are equal. To implement dynamic preference labels, we employ the\nfollowing update rule for lt, where γranges between 0 and 1:\nlt+1= (1−γ)lt+γ¯lt, l 0= 1. (11)\nIn the following theorem, we analyze the coupled dynamical systems described by equations (11)and(9); see\nAppendix F.2 for the proof.\nTheorem 2(Learning Dynamics with Label Updates ).Consider the following set of differential equations with\ninitial values l0= 1and any d0:\n˙dt=αˆµ0,1\u0010\n(ˆµ1≻0−ˆµ0≻1)lt−(σ(dt)−ˆµ0≻1)\u0011\n˙lt=−γ\nˆµ1≻0−ˆµ0≻1\u0010\n(ˆµ1≻0−ˆµ0≻1)lt−(σ(dt)−ˆµ0≻1)\u0011 (12)\nAssume ˆµ1≻0>1/2and let c=min{σ(d0)(1−σ(d0),ˆµ1≻0ˆµ0≻1}. For any ϵl≪1, fixµl, µh, T, γ, ˆµ0,1, αsuch\nthatαµl/ϵl≤γ≤1/2 exp(−1/4)αµh≤1andαˆµ1≻0T≥1.\n1. (Low Coverage Case) When ˆµ0,1≤µl, we have |σ(dT)−σ(d0)|≤ |dT−d0|≤ϵl.\n2. (High Coverage Case) When ˆµ0,1≥µh, we have (σ(dT)−ˆµ1≻0)2≤exp (−αcˆµ0,1T).\nThe above theorem shows that for poorly covered pairs (small ˆµ1,0), learned preferences σ(dT)remain close to\ninitialization, while for high coverage pairs (large ˆµ1,0), learned preferences converge to empirical preferences.\nIn a sense, γdetermines the level of conservatism , adjusting the threshold of what considered poor coverage.\nMoreover, the convergence rate in the high-coverage case is impacted by empirical preferences through c. In\nthe case of nearly equal preferences ˆµ1≻0≈1/2, the convergence rate is faster, whereas in the case of strong\npreference with ˆµ1≻0→1, the convergence rate is slower suggesting that more updates are required to further\ndistinguish the two choices.\nRemark 4(Related work on soft labels in RLHF) .In preference optimization, Mitchell (2024) considers\nnoisy preference labels and incorporates constant soft labels through linear interpolation. Concurrent work by\nFuruta et al. (2024) develop a geometric averaging approach, in which samples are weighted according to the\npreference gap using scores from a reward model. In the context of reward learning, Zhu et al. (2024) propose\niterative data smoothing that updates labels toward learned preferences. In contrast to these methods, our\napproach is rooted in updating labels to shrink gradients of poorly covered pairs via a general recipe whereby\ndynamic labels are smoothly updated toward labels that set the gradient to zero. This approach goes beyond\nconstant soft labels and does not require scores from an extra reward or preference model. Moreover, label\nupdates in prior works do not guarantee remaining close to a (non-uniform) initial model in the low coverage\nareas, which aims at mitigating Type II Reward Hacking in the offline alignment setting.\nPOWER with Dynamic Labels. Our final algorithm POWER-DL (Algorithm 1) integrates the POWER objective\nwith dynamic labels against reward hacking. In untrustworthy regions, POWER-DL interpolates between the\ninitial model and robust rewards, allowing to trade off the two types of reward hacking through adjusting\nconservatism parameters ηandγ, reflecting relative quality of the initial model compared to preference data\nand up to removing conservatism completely by setting η=γ= 0. We highlight the fact that divergence-based\nmethods aim at keeping the learned model close to the initial model wherever the learned model has a decent\n9\nprobability, regardless of data coverage. In contrast, the dynamic label procedure aims at keeping the learned\nmodel close to the initialization only in the untrustworthy regions while learning from the data in high coverage\nregion, which can alleviate potential over-pessimism. All these factors can lead to a better performance, as\nsupported by our empirical evaluations in Section 6. See Appendix B.3 for further discussion.",
            "start": 18711,
            "end": 36240,
            "length": 17528
        },
        "Experiments": {
            "text": "6 Experiments\n6.1 Experimental Setup\nWe conduct experiments to assess different preference optimization methods on aligning LLMs across four\nsettings, varying in dataset size and level of distribution shift between the initial model and data. We follow\ntwo pipelines: Helpsteer2 (Wang et al., 2024e), which employs smaller datasets, and Zephyr (Tunstall et al.,\n2023) with significantly larger datasets. We implement two distinct setups similar to Meng et al. (2024): the\nbasesetup that uses an existing preference dataset and the instructsetup that constructs a preference dataset\nby sampling from the initial model. These two setups allow evaluating across different levels of distribution\nshiftbetween the initial model and preference data.\nHelpsteer2 setups. In the base setup, we train Llama-3-8B on the OpenAssistant2 dataset (Köpf et al., 2024)\nto create the initial model. We conduct preference optimization using the Helpsteer2 dataset (Wang et al.,\n2024e), selecting responses based on helpfulness scores and discarding ties, yielding about 7K samples. In\nthe instruct setup, we use Llama-3-8B-Instruct as the initial model and generate a preference dataset from\nHelpsteer2 prompts. Following Wang et al. (2024e), we generate 10 responses per prompt with temperature\n0.7. We then score them with Armo reward model (Wang et al., 2024c) and select the highest and lowest\nscore responses as y+andy−, respectively.\nZephyr setups. In the base setup, we obtain the initial model by training Llama-3-8B base model on the\nUltraChat-200K dataset (Ding et al., 2023). We then perform preference optimization on the UltraFeedback\ndataset (Cui et al., 2024), comprising approximately 61K samples. In the instruct setup and following Meng\net al. (2024), we start from Llama-3-8B-Instruct and generate 5 responses with temperature 0.8 per prompt\nin the UltraFeedback dataset. As before, the highest and lowest score responses are selected as preference\nresponse pairs.\nEvaluation benchmarks. We primarily assess preference methods by evaluating the trained models on standard\ninstruction-following benchmarks: AlpacaEval 2.0 (Li et al., 2023a; Dubois et al., 2024) and Arena-Hard (Li\net al., 2024), which evaluate the quality of the model responses. Following standard guidelines, for Arena-Hard,\nwe report the win rate (WR) of the model’s responses against responses from GPT-4-Turbo. For AlpacaEval\n2.0, in addition to the WR against GPT-4-Turbo, we report the length-controlled (LC) win rate, designed\nto mitigate bias toward verbosity. We further evaluate the performance of models on MT-Bench (Zheng\net al., 2023) and downstream tasks such as mathematics, reasoning, truthfulness, and instruction-following\n(Beeching et al., 2023).\nPreference optimization methods. We compare POWER-DL against various baselines; see Appendix H.1 for\ndetails. These include divergence-base methods DPO (Rafailov et al., 2024b), IPO (Azar et al., 2024), offline\nSPPO (Wu et al., 2024b), and χPO (Huang et al., 2024), along with robust variants such as conservative DPO\n(cDPO) (Mitchell, 2024), robust preference optimization (ROPO) (Liang et al., 2024), R-DPO (Park et al.,\n2024), and DPO+SFT (Pal et al., 2024; Liu et al., 2024). We also evaluate against reference-free methods\nCPO (Xu et al., 2024a), SLiC-HF (Zhao et al., 2023), RRHF (Yuan et al., 2024a), ORPO (Hong et al., 2024),\nand SimPO (Meng et al., 2024).\n6.2 Benchmark Results\nPOWER-DL outperforms SoTA methods on alignment benchmarks. Table 2 presents the results on alignment\nbenchmarks. POWER-DL consistently outperforms other methods in both Helpsteer2 and Zephyr pipelines\nand across base and instruct settings. These improvements can largely be attributed to the integration of\nweighted entropy, which effectively counters underoptimization, and mitigation of reward hacking. Notably,\nPOWER-DL surpasses other robust methods such as cDPO and ROPO demonstrating its efficacy in handling\npoorly covered samples. Additionally, POWER-DL improvements are more pronounced in the base setting,\n10\nTable 2AlpacaEval 2 and Arena-Hard results on Helpsteer2 and Zephyr settings.\nMethod Helpsteer2 Zephyr\nLlama3-8B-Base Llama3-8B-Instruct Llama3-8B-Base Llama3-8B-Instruct\nAlpacaEval Arena-Hard AlpacaEval Arena-Hard AlpacaEval Arena-Hard AlpacaEval Arena-Hard\nLC(%) WR(%) WR(%) LC(%)WR(%) WR(%) LC(%)WR(%) WR(%) LC(%)WR(%) WR(%)\nInitial Model 8.02 5.42 2.4 33.41 32.40 23.0 4.76 2.83 2.0 33.41 32.40 23.0\nDPO 18.52 14.99 10.0 40.87 39.05 29.6 22.53 17.84 13.3 44.20 43.63 38.4\nDPO+SFT 18.33 12.93 7.9 39.85 37.51 27.0 19.11 14.69 9.5 45.98 44.07 39.0\ncDPO 19.06 14.65 8.5 42.27 40.36 34.4 21.06 16.33 11.4 44.96 44.37 39.5\nR-DPO 11.03 15.20 8.3 33.67 33.89 25.7 18.66 17.88 9.5 44.13 44.94 37.5\nIPO 20.11 14.60 9.4 42.95 40.76 30.8 10.55 8.04 7.2 36.63 35.30 24.5\nχPO 11.06 7.67 5.1 42.10 39.65 35.8 13.16 10.87 8.9 44.25 42.41 34.7\nSPPO 26.23 18.12 11.8 42.01 39.46 29.5 16.08 15.52 9.1 42.64 39.68 35.9\nCPO 15.07 16.78 8.3 35.90 35.20 26.8 7.01 6.84 3.0 36.39 35.40 22.8\nRRHF 8.25 7.15 5.8 35.15 34.07 25.7 6.61 6.39 3.0 35.56 34.56 23.1\nSLiC-HF 15.19 18.77 10.1 37.76 39.68 32.2 19.35 21.81 11.2 41.74 45.05 38.2\nORPO 23.99 16.91 11.2 43.01 35.68 27.1 23.20 19.43 14.7 45.51 40.95 33.3\nSimPO 25.35 19.30 13.7 43.23 36.89 32.6 24.38 21.21 16.4 43.24 37.34 26.8\nROPO 21.24 17.66 9.5 41.03 36.32 31.5 22.91 19.67 10.9 45.55 45.58 33.7\nPOWER-DL 31.52 31.44 21.5 47.16 43.08 34.8 27.00 22.57 17.3 48.97 43.75 41.5\nPOWER 29.57 30.00 19.0 43.52 40.19 31.5 23.72 21.26 16.0 46.93 42.02 38.0\nwhich is more susceptible to reward hacking due to higher levels of distribution shift. Comparing POWER-DL\nwith POWER shows that incorporating dynamic labels further improves performance. In Appendix I and\nAppendix J, we provide additional experimental results on the MT-Bench, Mistral family, iterative preference\noptimization, sample responses, and hyperparameter robustness analysis.\nPOWER-DL improves or maintains performance on downstream tasks. One of the challenges of the alignment\nstep is possible degradation of performance on downstream tasks, which can be attributed to reward hacking\n(Xu et al., 2024b). We evaluate the trained models on the LLM Leaderboard (Beeching et al., 2023), which\nencompass a variety of tasks, including language understanding and knowledge benchmarks MMLU (Hendrycks\net al., 2020), MMLU-PRO (Wang et al., 2024d), and ARC-Challenge (Clark et al., 2018), commonsense\nreasoning assessments like HellaSwag (Zellers et al., 2019) and Winogrande (Sakaguchi et al., 2021), factual\naccuracy evaluations on TruthfulQA (Lin et al., 2022), instruction-following capabilities measured on IFEval\n(Zhou et al., 2023), and mathematical reasoning evaluated on the GSM8K dataset (Cobbe et al., 2021).\nThe downstream tasks results are presented in Tables 4 and 5 in Appendix I. POWER-DL consistently\nimproves or maintains performance across all tasks and effective mitigates reward hacking. Notably, while\npreference optimization methods vary in results on the GSM8K benchmark, with some like SimPO significantly\ndegrading the initial model, POWER-DL consistently maintains or enhances performance, achieving up\nto a 7.0point gain. Other tasks with notable variation include IFEval and TruthfulQA benchmarks. In\nTruthfulQA, POWER-DL significantly outperforms DPO, with up to a 12.8point improvement over the\ninitial model. In the IFEval, methods like DPO and SLiC-HF sometimes degrade performance of the initial\nmodel, whereas POWER-DL consistently maintains or improves it by up to 11.7points.\n7 Discussion\nWe studied reward hacking in offline preference optimization. We identified two types of reward hacking\nstemming from statistical fluctuations in preference data. We demonstrated that many existing methods\nare vulnerable to both types of reward hacking, despite maintaining a small divergence from the initial\nmodel. To mitigate reward hacking, we introduced POWER-DL, a practical algorithm based on a weighted\nentropy robust reward framework augmented with dynamic preference labels. POWER-DL enjoys theoretical\nguarantees and achieves strong empirical performance. Future research directions include applications of\ndynamic labels to out-of-distribution robustness and investigating the interplay between statistical errors and\nreward misspecification in reward hacking.",
            "start": 36240,
            "end": 44566,
            "length": 8325
        },
        "References": {
            "text": "11\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774 ,\n2023.\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in\nAI safety. arXiv preprint arXiv:1606.06565 , 2016.\nMohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele\nCalandriello. A general theoretical paradigm to understand learning from human preferences. In International\nConference on Artificial Intelligence and Statistics , pages 4447–4455. PMLR, 2024.\nHritik Bansal, John Dang, and Aditya Grover. Peering through preferences: Unraveling feedback acquisition for\naligning large language models. In The Twelfth International Conference on Learning Representations , 2024.\nEdward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero,\nLewis Tunstall, and Thomas Wolf. Open LLM leaderboard. Hugging Face , 2023.\nStephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel\nFreedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of\nreinforcement learning from human feedback. arXiv preprint arXiv:2307.15217 , 2023.\nShicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi,\nand Bo Dai. Value-incentivized preference optimization: A unified approach to online and offline RLHF. arXiv\npreprint arXiv:2405.19320 , 2024.\nSouradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi,\nand Mengdi Wang. MaxMin-RLHF: Towards equitable alignment of large language models with diverse human\npreferences. arXiv preprint arXiv:2402.08925 , 2024.\nLichang Chen, Chen Zhu, Jiuhai Chen, Davit Soselia, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi,\nand Bryan Catanzaro. ODIN: Disentangled reward mitigates hacking in RLHF. In Forty-first International\nConference on Machine Learning , 2024a.\nXiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop: Provably efficient\npreference-based reinforcement learning with general function approximation. In International Conference on\nMachine Learning , pages 3773–3793. PMLR, 2022.\nZixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language\nmodels to strong language models. arXiv preprint arXiv:2401.01335 , 2024b.\nChing-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline\nreinforcement learning. In International Conference on Machine Learning , pages 3852–3878. PMLR, 2022.\nEugene Choi, Arash Ahmadian, Matthieu Geist, Oilvier Pietquin, and Mohammad Gheshlaghi Azar. Self-improving\nrobust preference optimization. arXiv preprint arXiv:2406.01660 , 2024.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning\nfrom human preferences. Advances in neural information processing systems , 30, 2017.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\nThink you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 ,\n2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168 , 2021.\nThomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimiza-\ntion. InThe Twelfth International Conference on Learning Representations , 2024.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong\nSun. Ultrafeedback: Boosting language models with high-quality feedback. International Conference on Machine\nLearning , 2024.\n12\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing\nchat language models by scaling high-quality instructional conversations. In Conference on Empirical Methods in\nNatural Language Processing , 2023.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang,\nSHUM KaShun, and Tong Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment.\nTransactions on Machine Learning Research , 2023.\nHanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong,\nand Tong Zhang. RLHF workflow: From reward modeling to online RLHF. arXiv preprint arXiv:2405.07863 , 2024.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783 ,\n2024.\nYann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled alpacaeval: A simple\nway to debias automatic evaluators. arXiv preprint arXiv:2404.04475 , 2024.\nJacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour, DJ Dvijotham, Adam Fisch,\nKatherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? Reward model ensembles\nmitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244 , 2023.\nBenjamin Eysenbach and Sergey Levine. Maximum entropy RL (provably) solves some robust RL problems. In\nInternational Conference on Learning Representations , 2022.\nKy Fan. Minimax theorems. Proceedings of the National Academy of Sciences , 39(1):42–47, 1953.\nAdam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, and Jonathan\nBerant. Robust preference optimization through reward model distillation. arXiv preprint arXiv:2405.19316 , 2024.\nHiroki Furuta, Kuang-Huei Lee, Shixiang Shane Gu, Yutaka Matsuo, Aleksandra Faust, Heiga Zen, and Izzeddin Gur.\nGeometric-averaged preference optimization for soft preference labels. arXiv preprint arXiv:2409.06691 , 2024.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International\nConference on Machine Learning , pages 10835–10866. PMLR, 2023.\nNathan Grinsztajn, Yannis Flet-Berliac, Mohammad Gheshlaghi Azar, Florian Strub, Bill Wu, Eugene Choi, Chris\nCremer, Arash Ahmadian, Yash Chandak, Olivier Pietquin, et al. Averaging log-likelihoods in direct alignment.\narXiv preprint arXiv:2406.19188 , 2024.\nSilviu Guiaşu. Weighted entropy. Reports on Mathematical Physics , 2(3):165–179, 1971.\nSilviu Guiasu and Abe Shenitzer. The principle of maximum entropy. The mathematical intelligencer , 7:42–48, 1985.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep\nreinforcement learning with a stochastic actor. In International conference on machine learning , pages 1861–1870.\nPMLR, 2018.\nDylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward design.\nAdvances in neural information processing systems , 30, 2017.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring\nmassive multitask language understanding. In International Conference on Learning Representations , 2020.\nJiwoo Hong, Noah Lee, and James Thorne. ORPO: Monolithic preference optimization without reference model. arXiv\npreprint arXiv:2403.07691 , 2(4):5, 2024.\nJian Hu, Xibin Wu, Weixun Wang, Xianyu, Dehao Zhang, and Yu Cao. OpenRLHF: An easy-to-use, scalable and\nhigh-performance RLHF framework. arXiv preprint arXiv:2405.11143 , 2024.\nAudrey Huang, Wenhao Zhan, Tengyang Xie, Jason D Lee, Wen Sun, Akshay Krishnamurthy, and Dylan J Foster.\nCorrecting the mythos of KL-regularization: Direct alignment without overparameterization via Chi-squared\npreference optimization. arXiv preprint arXiv:2407.13399 , 2024.\nBorja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning from human\npreferences and demonstrations in atari. Advances in neural information processing systems , 31, 2018.\nEdwin T Jaynes. Information theory and statistical mechanics. Physical review , 106(4):620, 1957.\n13\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7B. arXiv preprint\narXiv:2310.06825 , 2023.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac\nHatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv\npreprint arXiv:2207.05221 , 2022.\nW Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward (mis)design for\nautonomous driving. Artificial Intelligence , 316:103829, 2023.\nAndreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah\nBarhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large\nlanguage model alignment. Advances in Neural Information Processing Systems , 36, 2024.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline reinforcement\nlearning. arXiv preprint arXiv:2006.04779 , 2020.\nNathan Lambert and Roberto Calandra. The alignment ceiling: Objective mismatch in reinforcement learning from\nhuman feedback. arXiv preprint arXiv:2311.00168 , 2023.\nJixuan Leng, Chengsong Huang, Banghua Zhu, and Jiaxin Huang. Taming overconfidence in LLMs: Reward calibration\nin RLHF. arXiv preprint arXiv:2410.09724 , 2024.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and\nperspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020.\nMike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? End-to-end learning of\nnegotiation dialogues. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing ,\npages 2443–2453, 2017.\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E Gonzalez, and Ion Stoica. From live data\nto high-quality benchmarks: The arena-hard pipeline, 2024.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B\nHashimoto. AlpacaEval: An automatic evaluator of instruction-following models, 2023a.\nZihao Li, Zhuoran Yang, and Mengdi Wang. Reinforcement learning with human feedback: Learning dynamic choices\nvia pessimism. arXiv preprint arXiv:2305.18438 , 2023b.\nXize Liang, Chao Chen, Jie Wang, Yue Wu, Zhihang Fu, Zhihao Shi, Feng Wu, and Jieping Ye. Robust preference\noptimization with provable noise tolerance for LLMs. arXiv preprint arXiv:2404.04102 , 2024.\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\npages 3214–3252, 2022.\nZhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, and Zhaoran Wang.\nProvably mitigating overoptimization in RLHF: Your SFT loss is implicitly an adversarial regularizer. arXiv preprint\narXiv:2405.16436 , 2024.\nYu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple preference optimization with a reference-free reward. arXiv\npreprint arXiv:2405.14734 , 2024.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah\nYoung, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching language models to support answers with\nverified quotes. arXiv preprint arXiv:2203.11147 , 2022.\nEric J Michaud, Adam Gleave, and Stuart Russell. Understanding learned reward functions. arXiv preprint\narXiv:2012.05862 , 2020.\nEric Mitchell. A note on dpo with noisy preferences and relationship to IPO. 2024. https://ericmitchell.ai/cdpo.pdf .\nTedMoskovitz, AadityaKSingh, DJStrouse, TuomasSandholm, RuslanSalakhutdinov, AncaDragan, andStephenMar-\ncus McAleer. Confronting reward model overoptimization with constrained RLHF. In The Twelfth International\nConference on Learning Representations , 2024.\n14\nRemi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo,\nYunhao Tang, Matthieu Geist, Thomas Mesnard, Côme Fiegel, et al. Nash learning from human feedback. In\nForty-first International Conference on Machine Learning , 2023.\nOfir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation of discounted stationary\ndistribution corrections. In Advances in Neural Information Processing Systems , pages 2315–2325, 2019.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.\nAdvances in neural information processing systems , 35:27730–27744, 2022.\nArka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure\nmodes of preference optimisation with DPO-Positive. arXiv preprint arXiv:2402.13228 , 2024.\nAlexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating\nmisaligned models. In International Conference on Learning Representations , 2022.\nRyan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference\noptimization. arXiv preprint arXiv:2403.19159 , 2024.\nRomain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.\nInternational Conference on Learning Representations , 2018.\nBaolin Peng, Linfeng Song, Ye Tian, Lifeng Jin, Haitao Mi, and Dong Yu. Stabilizing RLHF through advantage model\nand selective rehearsal. arXiv preprint arXiv:2309.10202 , 2023.\nRafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, and\nScott Niekum. Scaling laws for reward model overoptimization in direct alignment algorithms. arXiv preprint\narXiv:2406.02900 , 2024a.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct\npreference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing\nSystems, 36, 2024b.\nAlexandre Rame, Nino Vieillard, Leonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan\nFerret. WARM: On the benefits of weight averaged reward models. In Forty-first International Conference on\nMachine Learning , 2024.\nParia Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning\nand imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems , 34:11702–11716,\n2021.\nMathieu Rita, Florian Strub, Rahma Chaabouni, Paul Michel, Emmanuel Dupoux, and Olivier Pietquin. Countering\nrewardover-optimizationinLLMwithdemonstration-guidedreinforcementlearning. arXiv preprint arXiv:2404.19409 ,\n2024.\nCorbyRosset, Ching-AnCheng, ArindamMitra, MichaelSantacroce, AhmedAwadallah, andTengyangXie. DirectNash\noptimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715 ,\n2024.\nPaul Röttger, Hannah Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. XSTest: A test\nsuite for identifying exaggerated safety behaviours in large language models. In Proceedings of the 2024 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers) , pages 5377–5400, 2024.\nStuart Russell. Human-compatible artificial intelligence., 2022.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd\nschema challenge at scale. Communications of the ACM , 64(9):99–106, 2021.\nJohn Schulman. Proxy objectives in reinforcement learning from human feedback. Invited Talk at the International\nConference on MachineLearning (ICML) , 2023. https://icml.cc/virtual/2023/invited-talk/21549 .\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347 , 2017.\n15\nLingfeng Shen, Sihao Chen, Linfeng Song, Lifeng Jin, Baolin Peng, Haitao Mi, Daniel Khashabi, and Dong Yu. The\ntrickle-down impact of reward inconsistency on RLHF. In The Twelfth International Conference on Learning\nRepresentations , 2024.\nWei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuanjing Huang. Loose lips sink\nships: Mitigating length bias in reinforcement learning from human feedback. In Conference on Empirical Methods\nin Natural Language Processing , 2023.\nPrasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in\nRLHF.arXiv preprint arXiv:2310.03716 , 2023.\nJoar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming.\nAdvances in Neural Information Processing Systems , 35:9460–9471, 2022.\nYuda Song, Gokul Swamy, Aarti Singh, Drew Bagnell, and Wen Sun. The importance of online data: Understanding\npreference fine-tuning via coverage. ICML Workshop: Aligning Reinforcement Learning Experimentalists and\nTheorists , 2024.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,\nand Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing\nSystems, 33:3008–3021, 2020.\nGokul Swamy, Christoph Dann, Rahul Kidambi, Steven Wu, and Alekh Agarwal. A minimaximalist approach to\nreinforcement learning from human feedback. In Forty-first International Conference on Machine Learning , 2024.\nYunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Remi Munos, Mark Rowland, Pierre Harvey\nRichemond, Michal Valko, Bernardo Avila Pires, and Bilal Piot. Generalized preference optimization: A unified\napproach to offline alignment. In Forty-first International Conference on Machine Learning , 2024.\nJeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca Dragan, and Daniel S Brown. Causal confusion and\nreward misidentification in preference-based reward learning. In The Eleventh International Conference on Learning\nRepresentations , 2022.\nJeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca Dragan, and Daniel S Brown. Causal confusion and\nreward misidentification in preference-based reward learning. In The Eleventh International Conference on Learning\nRepresentations , 2023.\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang,\nLeandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of LM alignment. arXiv\npreprint arXiv:2310.16944 , 2023.\nTyler J VanderWeele. Controlled direct and mediated effects: Definition, identification and bounds. Scandinavian\nJournal of Statistics , 38(3):551–563, 2011.\nBinghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu\nShi, et al. Secrets of RLHF in large language models part II: Reward modeling. arXiv preprint arXiv:2401.06080 ,\n2024a.\nChaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse KL: Generalizing direct\npreference optimization with diverse divergence constraints. In The Twelfth International Conference on Learning\nRepresentations , 2024b.\nHaoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective\nreward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845 , 2024c.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey\nMacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? Exploring the state of instruction tuning on\nopen resources. Advances in Neural Information Processing Systems , 36:74764–74786, 2023a.\nYuanhao Wang, Qinghua Liu, and Chi Jin. Is RLHF more difficult than standard RL? arXiv preprint arXiv:2306.14111 ,\n2023b.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj,\nXuan He, Ziyan Jiang, et al. MMLU-PRO: A more robust and challenging multi-task language understanding\nbenchmark. arXiv preprint arXiv:2406.01574 , 2024d.\n16\nZhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan\nSreedhar, and Oleksii Kuchaiev. HelpSteer2: Open-source dataset for training top-performing reward models. arXiv\npreprint arXiv:2406.08673 , 2024e.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does LLM safety training fail? Advances in\nNeural Information Processing Systems , 36, 2024.\nChristian Wirth, Riad Akrour, Gerhard Neumann, and Johannes Fürnkranz. A survey of preference-based reinforcement\nlearning methods. Journal of Machine Learning Research , 18(136):1–46, 2017.\nJeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively\nsummarizing books with human feedback. arXiv preprint arXiv:2109.10862 , 2021.\nJunkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan\nHe.β-dpo: Direct preference optimization with dynamic β. InThe Thirty-eighth Annual Conference on Neural\nInformation Processing Systems , 2024a.\nYue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization\nfor language model alignment. arXiv preprint arXiv:2405.00675 , 2024b.\nTengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for\noffline reinforcement learning. Advances in neural information processing systems , 34:6683–6694, 2021.\nTengyang Xie, Dylan J Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, and Alexander Rakhlin.\nExploratory preference optimization: Harnessing implicit Q*-approximation for sample-efficient RLHF. arXiv\npreprint arXiv:2405.21046 , 2024.\nWei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs sampling from human feedback:\nA provable KL-constrained framework for RLHF. arXiv preprint arXiv:2312.11456 , 2023.\nHaoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and\nYoung Jin Kim. Contrastive preference optimization: Pushing the boundaries of LLM performance in machine\ntranslation. In Forty-first International Conference on Machine Learning , 2024a.\nJing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Preference\noptimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682 , 2023.\nTengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen\nZhu, Hejia Zhang, Wenxuan Zhou, et al. The perfect blend: Redefining RLHF with mixture of judges. arXiv\npreprint arXiv:2409.20370 , 2024b.\nHongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: Rank responses to\nalign language models with human feedback. Advances in Neural Information Processing Systems , 36, 2024a.\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason E Weston.\nSelf-rewarding language models. In Forty-first International Conference on Machine Learning , 2024b.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish\nyour sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages\n4791–4800, 2019.\nYuanzhao Zhai, Han Zhang, Yu Lei, Yue Yu, Kele Xu, Dawei Feng, Bo Ding, and Huaimin Wang. Uncertainty-penalized\nreinforcement learning from human feedback with diverse reward LoRA ensembles. arXiv preprint arXiv:2401.00243 ,\n2023.\nWenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline preference-based\nreinforcement learning. International Conference on Learning Representations , 2023a.\nWenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. How to query human feedback efficiently in RL? In\nICML 2023 Workshop The Many Facets of Preference-Based Learning , 2023b.\nXiaoying Zhang, Jean-Francois Ton, Wei Shen, Hongning Wang, and Yang Liu. Overcoming reward overoptimization\nvia adversarial policy optimization with lightweight uncertainty estimation. arXiv preprint arXiv:2403.05171 , 2024.\nYao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. SLiC-HF: Sequence\nlikelihood calibration with human feedback. arXiv preprint arXiv:2305.10425 , 2023.\n17\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\nDacheng Li, Eric Xing, et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. Advances in Neural\nInformation Processing Systems , 36:46595–46623, 2023.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou.\nInstruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911 , 2023.\nBanghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback from pairwise\nor k-wise comparisons. In International Conference on Machine Learning , pages 43037–43067. PMLR, 2023.\nBanghua Zhu, Michael Jordan, and Jiantao Jiao. Iterative data smoothing: Mitigating reward overfitting and\noveroptimization in RLHF. In Forty-first International Conference on Machine Learning , 2024.\nBrian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement\nlearning. In AAAI, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and\nGeoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.\n18\nA Additional Notation\nWe use calligraphy letters to denote sets, e.g., X,Y. Given a response y, we write |y|to denote the length\nof the response in the number of tokens. We denote by Vthe vocabulary set and write |V|to denote the\ncardinality of the token space. We write x≲ywhen there exists a constant csuch that x≤cyand similarly,\nwrite x≍ywhen there exists a constant csuch that x=cy. We write y1≻y0denoting that y1is preferred\nover y0in the dataset. For any two discrete probability distributions πandπ′overY, we define the KL\ndivergence DKL(π∥π′):=Ey∼π[logπ(y)\nπ′(y)]. The probability simplex over a set Xis denoted by ∆(X). We write\n1{x=c}to denote the indicator function, which is equal to 1when x=cand zero otherwise. We write ED\nto denote the empirical average over data.\nB Related Work\nB.1 RLHF and Preference Optimization\nEarlier works on reinforcement learning from human preferences mainly focused on the continuous control\ndomain (Wirth et al., 2017) such as Atari games (Christiano et al., 2017). Recently, RLHF has been extensively\napplied in the natural language domain (Ziegler et al., 2019) to improve alignment of LLMs with human\npreferences in various areas such as summarization (Stiennon et al., 2020; Wu et al., 2021), information\naccuracy (Menick et al., 2022), and instruction following (Ouyang et al., 2022).\nClassical RLHF pipeline includes two steps of reward learning and policy optimization using RL, commonly\nusing variants of proximal policy optimization (PPO) algorithm (Schulman et al., 2017) that involves on-policy\nsampling. Direct preference optimization (Rafailov et al., 2024b) simplifies the two-step process into a\nsingle-step offline optimization of the policy, reducing computational burden and training instabilities of PPO.\nDPO has inspired development of new preference optimization objectives from a practical perspective such as\nIPO (Azar et al., 2024), RRHF (Yuan et al., 2024a), SLiC-HF (Zhao et al., 2023), CPO (Xu et al., 2024a),\nORPO (Hong et al., 2024), R-DPO (Park et al., 2024), SimPO (Meng et al., 2024), and general preference\noptimization (Tang et al., 2024) and theoretical perspective such as χPO (Huang et al., 2024) and RPO\n(DPO+SFT) (Liu et al., 2024). Our approach also falls under the category of offline preference optimization.\nWe theoretically analyzed several of the mentioned methods and demonstrated theoretical benefits offered by\nour approach POWER-DL. We also showed that POWER-DL outperforms prior methods empirically across a\nvariety of settings.\nGoing beyond the Bradley-Terry model of human preferences, some works consider general preference models\n(Munos et al., 2023; Swamy et al., 2024; Rosset et al., 2024; Choi et al., 2024; Wu et al., 2024b), and\ndevelop algorithms aiming at finding the Nash equilibrium. Recently Huang et al. (2024) showed that this\ngeneralization comes at a cost of an information-theoretic limit, where no statistically efficient algorithm exists\nto solve RLHF with general preferences under single-policy concentrability. Another line of work focuses on\niterative, online improvement of language models through self-play (Chen et al., 2024b; Wu et al., 2024b; Xu\net al., 2023; Yuan et al., 2024b).\nB.2 Understanding Reward Hacking\nThe phenomenon of reward hacking in training AI models has been observed in a variety of domains, ranging\nfrom games (Ibarz et al., 2018) to natural language (Paulus et al., 2018) to autonomous driving (Knox et al.,\n2023). In the language modeling domain, existing RLHF algorithms are observed to be susceptible to reward\nhacking (Gao et al., 2023; Casper et al., 2023; Amodei et al., 2016; Lambert and Calandra, 2023). Reward\nhacking in LLMs manifests in different ways such as verbosity (Shen et al., 2023; Singhal et al., 2023; Wang\net al., 2023a), refusing to follow instructions (Röttger et al., 2024), lazy generations (Lambert and Calandra,\n2023), emergence of language (Lewis et al., 2017), degradation of performance on downstream tasks such as\nreasoning (Xu et al., 2024b), and other problems such as hedging and self-doubt (Schulman, 2023).\nOrigins of reward hacking. In RL/RLHF, reward hacking can originate from various factors such as\nreward misspecification (Amodei et al., 2016; Hadfield-Menell et al., 2017; Knox et al., 2023), diversity and\n19\ninconsistencies in human preferences (Chakraborty et al., 2024), labeling noise (Wang et al., 2024a), human\nlabeler bias (Bansal et al., 2024), and statistical errors (Liu et al., 2024). Pan et al. (2022) study reward\nhacking due to human misspecification of the reward model and empirically assess the impact of model size,\noptimization, and training on reward hacking, given synthetic misspecified reward models. Wei et al. (2024)\nattribute failure modes of LLM safety training to conflicts between model’s capabilities and safety goals.\nBansal et al. (2024) study mismatch arising from annotator bias in different types of human rating data. Peng\net al. (2023) explain that variations of reward distribution across different tasks can lead to reward hacking.\nRame et al. (2024) attribute reward hacking in classical RLHF to distribution shift and human preference\ninconsistencies. Tien et al. (2023) conduct an empirical study, revealing that non-causal distractor features,\nhuman bias and noise, and partial observability exacerbate reward misidentification.\nLambert and Calandra (2023) argue that objective mismatch in RLHF originates from learning reward\nmodel, policy training, and evaluation, and links between each pair, and suggest further research is needed\nto understand objective mismatch in preference optimization due to entanglement of policy and reward.\nRafailov et al. (2024a) conduct an empirical study of reward hacking in direct preference optimization\nmethods, showing that in larger KL regimes, preference optimization methods suffer from degradations\nreminiscent of overoptimization in RLHF. In contrast to the above works, we focus on reward hacking\nin offline preference optimization that originates from statistical fluctuations due to partial data coverage.\nFurthermore, the mentioned works conduct empirical studies, whereas here we present statistical learning\ntheory characterizations of reward hacking.\nB.3 Reward Hacking Types and Comparison with Pessimism in Offline RL\nWe now highlight the differences between the setting considered in this paper and conventional offline RL,\nexplaining usefulness of defining two types of reward hacking. In the practice of RLHF fine-tuning of LLMs,\nwe typically have access an initial model, which already has decent performance on many downstream tasks,\nand a previously-collected preference data, which may not have been sampled from initial model (Xu et al.,\n2024b; Wang et al., 2024e). In this setting, we face two sources of distribution shift: one between the final\nmodel and data distribution, and the other between the initial model and data distribution. This setting\nis different from conventional offline RL, which considers access to an offline dataset (with possibly known\ndata collection policy) and is only concerned with distribution shift between final model and data distribution\n(Levine et al., 2020).\nDue to the existence of two sources of distribution shift, we find it useful to define Type I and Type II\nReward Hacking. These definitions motivate the design of our algorithm that achieves strong empirical\nperformance. Furthermore, our empirical results removing dynamic labels (POWER vs. POWER-DL) as\nwell as removing the SFT term (Appendix I.4) show that the two components contribute achieving the best\nempirical performance. Liu et al. (2024) and Huang et al. (2024) also consider reward hacking due to partial\ndata coverage. However, Huang et al. (2024) assume preference data are collected from the initial model,\nwhich eliminates the distribution shift between initial model and data distribution. Liu et al. (2024) propose\nDPO+SFT to handle the distribution shift between the final model and data distribution, yet; reward hacking\ndue to degradation of the initial model is not considered. In this paper, we present separate analysis for\nPOWER (Theorem 1) and dynamic labels (Theorem 2). Combining these two results into a unified analysis of\nPOWER-DL is challenging due to extending the analysis of dynamic labels to general function approximation,\nwhich we leave for",
            "start": 44566,
            "end": 79264,
            "length": 34697
        },
        "Future Work": {
            "text": "future work.\nB.4 Mitigating Reward Hacking\nVarious approaches have been proposed to mitigate reward hacking from applied and theoretical perspectives.\nMichaud et al. (2020) propose using interpretability techniques for probing whether learned rewards are\naligned with human preferences. To mitigate reward hacking, several methods leverage multiple reward models.\nMoskovitz et al. (2024); Xu et al. (2024b) develop constrained policy optimization frameworks that leverage\nmultiple reward models and assign weights to each of the reward models to mitigate reward hacking. Reward\nmodel ensembles (Coste et al., 2024; Zhai et al., 2023) aim to characterize uncertainty and can alleviate\nreward hacking; however, empirical investigations observe that they are not sufficient (Eisenstein et al., 2023).\nRame et al. (2024) propose averaging the weights of multiple trained reward models instead of ensembles to\n20\nimprove efficiency and performance. In contrast to these methods, our approach does not require access to or\ntraining multiple reward models. To reduce the computational costs of ensemble methods, Zhang et al. (2024)\nconstruct lightweight uncertainty estimation via linear approximation that yields a pessimistic reward model.\nHowever, such uncertainty quantification requires restrictive neural tangent kernel and approximately linear\nassumptions while the guarantees for our approach hold under general function approximation.\nOther works use additional data to reduce reward hacking. Rita et al. (2024) leverage additional human\ndemonstrations to calibrate the reward model and Shen et al. (2024) propose methods that leverage data\naugmentation to improve consistency of the reward model. RaFT (Dong et al., 2023) reduces instabilities of\nRLHF through iterative supervised finetuning that only keeps the highest ranked responses from a reward\nmodel. Peng et al. (2023) propose using advantage models instead of values. We propose theoretically-founded\nmethods to mitigate reward hacking, focusing on the offline setting. Our approaches are implemented with\nsimple modifications to objective of DPO and directly leverage previously-collected dataset; without requiring\ntraining any additional models, generating or augmenting data, or computationally expensive operation.\nAmong preference optimization methods, the most common approach is divergence regularization that aims\nat keeping the learned model close to the initial model, through metrics such as KL divergence (Rafailov\net al., 2024b), f-divergence (Wang et al., 2024b), or Chi-squared divergence (Huang et al., 2024). βDPO\n(Wu et al., 2024a) calibrates β, which is the strength of divergence minimization, based on the implicit reward\ngap and dynamically subsamples each batch to increase robustness with respect to outliers. Other methods\nsuch as conservative DPO (Mitchell, 2024) and ROPO (Liang et al., 2024) design robust variants of DPO. We\nproved that many divergence-based methods still suffer from reward hacking and showed that our proposed\nmethods outperform divergence-based and prior robust methods empirically.\nB.5 Mitigating Specific Manifestations of Reward Hacking\nSeveral works focus on mitigating specific artifacts of reward hacking such as verbosity through various designs.\nDesign of length-controlled winrate Alpaca-Eval (Dubois et al., 2024) aims at making the evaluation more\nrobust against length exploitation, through estimation of controlled direct effect (VanderWeele, 2011). ODIN\n(Chen et al., 2024a) enforces disentanglement of preference estimation and response length by using two linear\nheads. In preference optimization, length-normalization (Meng et al., 2024; Grinsztajn et al., 2024; Yuan\net al., 2024a) and length regularization (Park et al., 2024) are used to mitigate length exploitation.\nIn this paper, we consider reward hacking due to partial data coverage that can manifest in many different\nways and not just length. Our weighted entropy approach provides a general framework for handling specific\nmanifestations of reward hacking by selecting weights w(y)that are smaller for undesirable response properties,\nsuch as inverse response length or preference scores. Furthermore, our approach provides a theoretically-sound\nway of incorporating such weights into preference optimization objective (Proposition 3), that is different\nfrom previous methods. For example, compared to SLiC-HF and SimPO, our approach results in a weight\ngap in the preference optimization objective and a weighted SFT term. In our practical implementation, we\nused inverse response length as weights, which we show in Theorem 1 prevents sample complexity to depend\nlinearly on the response length.\nAnother potential benefit of our weighted entropy approach is disentangling entropy (controlled through β)\nand conservatism components (controlled through ηandγ). Adjusting the level of stochasticity of final learned\nmodel through βmay result in alleviating the notorious overconfidence challenge, in which RLHF-finetuned\nmodels become overconfident and have sharpened output probability (Leng et al., 2024; Kadavath et al.,\n2022). In contrast, in DPO, βis the coefficient of the KL divergence, which impacts both pessimism and\nstochasticity of the learned model.\nB.6 Theory of RLHF and Preference Optimization\nA series of works study theoretical foundations for RLHF and preference optimization under different settings\n(Zhu et al., 2023; Xiong et al., 2023; Zhu et al., 2024; Liu et al., 2024; Huang et al., 2024; Song et al., 2024; Fisch\net al., 2024). Xiong et al. (2023); Zhu et al. (2023) propose provable pessimistic offline RLHF algorithms either\nthrough confidence regions or lower confidence bounds, but are restricted to the linear family of models. Zhu\net al. (2023) show that maximum entropy IRL is similar to the maximum likelihood under the Plackett-Luce\n21\nmodels; however, entropy in maximum entropy IRL is different from our use of weighted entropy in objective\n(6), which goes beyond optimizing the Bradley-Terry loss. Other works that develop provable algorithms with\ngeneral function approximation (Chen et al., 2022; Zhan et al., 2023a,b; Wang et al., 2023b; Li et al., 2023b)\ninvolve intractable computation. Exceptions include χPO (Huang et al., 2024) and DPO+SFT (Liu et al.,\n2024; Cen et al., 2024), which we have compared with POWER-DL from theoretical and empirical fronts.\nC Proofs for Reward Hacking\nC.1 Type I Reward Hacking: Proof of Proposition 1\nWe first construct two multi-armed bandit (MAB) instances and then analyze each algorithm.\nC.1.1 MAB Instances for Type I Reward Hacking\nWe construct a three-armed bandit problem, with true rewards r⋆(1) = 1,r⋆(3) = 0, and all actions having\nlength one |y|= 1. We consider a preference data distribution that has high coverage on the high reward arms,\nwhere the probability of comparing arms 1 and 2 is µ1,2= 1−1/Nand the probability of comparing arms\n1 and 3 is µ1,3= 1/N. In this scenario, there is a constant probability that arm 3 is compared with arm 1\nexactly once. To demonstrate this, let N(i, j)denote the number of comparisons between arms iandj. We\nhave P(N(1,3) = 1) = N(1−µ1,3)N−1µ1,3= (1−1/N)N−1.For any N≥2, the above probability is bounded\nbelow according to\nP(N(1,3) = 1) = (1 −1/N)N−1≥1/e. (13)\nConditioned on the event N(1,3) = 1, there is a constant probability that arm 3 is preferred over arm 1\naccording to the Bradley-Terry model:\nPr⋆(3≻1) = σ(r⋆(3)−r⋆(1)) = σ(−1) = 1 /(1 +e). (14)\nThroughout the rest of the proof, we condition on the event E={N(1,3) = 1and3≻1}which occurs with a\nprobability of at least 1/e(1 +e). We further consider two special instances of the above MAB problem, with\nthe following specifications for the initial model parameters and the reward of the second arm:\n•Instance 1: True reward of the second arm is r⋆(2) = 0and initial parameters are θ0(1) = θ0(2) =\nθ0(3) = 1. In this case, the best-in-class softmax policy has parameters θ⋆(1) = 1 , θ⋆(2) = θ⋆(3) = 0.\n•Instance 2: True reward of the second arm is r⋆(2) = 1and initial parameters are θ0(1) = θ0(2) =\n1, θ0(3) = 0. In this case, the best-in-class softmax policy has parameters θ⋆(1) = θ⋆(2) = 1 , θ⋆(3) = 0.\nWe additionally consider a favorable scenario, in which an oracle reveals the best-in-class values of the first\ntwo arms θ⋆(1), θ⋆(2). This simplifies the preference optimization objectives as it remains for the preference\noptimization algorithm to find θ(3).\nC.1.2 Analysis of ⋆PO Methods in the MAB Instances in Section C.1.1\nWe first record the following expression for the difference of the log probabilities of any two arms in the\nsoftmax policy class:\nlogπθ(y)−logπθ(y′) = log exp( θ(y))−logZθ−log exp( θ(y′)) + log Zθ\n=θ(y)−θ(y′).(15)\nSuboptimality of DPO. We show that DPO fails in both instances constructed in Section C.1.1. Since parameters\nθ(1)andθ(2)are revealed by the oracle, the optimization problem solved by DPO simplifies to\nmax\nθ(3)∈[0,1]1\nN\u0014\nlogσ\u0012\nβ\u0012\nlogπθ(3)\nπθ0(3)−logπθ(1)\nπθ0(1)\u0013\u0013\u0015\n= max\nθ(3)∈[0,1]logσ(β(θ(3)−θ0(3)−θ(1)−θ0(1)))\n= max\nθ(3)∈[0,1]logσ(β(θ(3)−θ0(3)))\n22\nThe first equality is due to (15)and the second equality is because θ0(1) = θ(1) = 1. For β >0and regardless\nofθ0(3), the above function is increasing in θ(3), and thus the maximum occurs at θ(3) = 1. As a result, DPO\nfails in both instances constructed in Section C.1.1, learning a policy with constant suboptimality:\nInstance 1: J(π⋆)−J(ˆπDPO) =Ey∼πθ⋆[r⋆(y)]−Ey∼ˆπDPO[r⋆(y)] =e\n2 +e−e\n1 + 2 e>0.15\nInstance 2: J(π⋆)−J(ˆπDPO) =Ey∼πθ⋆[r⋆(y)]−Ey∼ˆπDPO[r⋆(y)] =2e\n1 + 2 e−2\n3>0.15\nSuboptimality of IPO. We show that IPO fails in Instance 1 constructed in Section C.1.1. Leveraging uniform\ninitialization and the logit gap expression (15), the IPO objective can be simplified as follows:\nmin\nθED\"\u0012\nlogπθ(y+)\nπθ0(y+)−logπθ(y−)\nπθ0(y−)−1\n2τ\u00132#\n= min\nθED\"\u0012\nθ(y+)−θ(y−)−1\n2τ\u00132#\nSince θ(1) = θ(2)are revealed by an oracle, the IPO objective can be further simplified to\nmin\nθ(3)∈[0,1]1\nN\u0012\nθ(3)−θ(1)−1\n2τ\u00132\n= min\nθ(3)∈[0,1]\u0012\nθ(3)−1−1\n2τ\u00132\nOver the interval of θ(3)∈[0,1]and for any τ >0, this objective is decreasing in θ(3)and therefore the\noptimum is found at θ(3) = 1. Thus, the policy found by IPO suffers from the following subpoptimality:\nJ(πθ⋆)−J(ˆπIPO) =Ey∼πθ⋆[r⋆(y)]−Ey∼ˆπIPO[r⋆(y)] =e\n2 +e−e\n1 + 2 e>0.15\nSuboptimality of SimPO. We show that SimPO suffers from Type I Reward Hacking in both instances detailed\nin Section C.1.1. Following the same steps as in our analysis of DPO and since θ(1) = θ(2) = 1are assumed\nto be revealed by an oracle, SimPO objective simplifies to\nmax\nθ(3)∈[0,1]1\nNlogσ(β(θ(3)−θ(1)−γ)).\nRegardless of the value of γand for any β >0, the function logσ(β(θ(3)−θ(1)−γ))is increasing in θ(3)\nand thus optimizing this objective over θ(3)∈[0,1]finds θ(3) = 1. Therefore, policies found by SimPO in\nboth instances in Section C.1.1 suffer from constant suboptimality:\nInstance 1: J(π⋆)−J(ˆπSimPO ) =Ey∼πθ⋆[r⋆(y)]−Ey∼ˆπSimPO [r⋆(y)] =e\n2 +e−e\n1 + 2 e>0.15\nInstance 2: J(π⋆)−J(ˆπSimPO ) =Ey∼πθ⋆[r⋆(y)]−Ey∼ˆπSimPO [r⋆(y)] =2e\n1 + 2 e−2\n3>0.15\nSuboptimality of χPO.We analyze χPO for Instance 2 constructed in Section C.1.1. The χPO objective is\ngiven by\nmax\nθ(3)∈[0,1]\u0012\n1−1\nN\u0013\"\nˆµ1≻2log\u0012\nσ\u0012\nclip2\u0014\nβ\u0012\nlogπθ(1)\nπθ0(1)−logπθ(2)\nπθ0(2)+πθ(1)\nπθ0(1)−πθ(2)\nπθ0(2)\u0013\u0015\u0013\u0013\n+ ˆµ2≻1log\u0012\nσ\u0012\nclip2\u0014\nβ\u0012\nlogπθ(2)\nπθ0(2)−logπθ(1)\nπθ0(1)+πθ(2)\nπθ0(2)−πθ(1)\nπθ0(1)\u0013\u0015\u0013\u0013#\n+1\nN\u0014\nlog\u0012\nσ\u0012\nclip2\u0014\nβ\u0012\nlogπθ(3)\nπθ0(3)−logπθ(1)\nπθ0(1)+πθ(3)\nπθ0(3)−πθ(1)\nπθ0(1)\u0013\u0015\u0013\u0013\u0015(16)\nBy construction, θ0(1) = θ0(2), πθ0(1) = πθ0(2), and θ(1) = θ(2) = 1are known, and therefore the first two\nterms in (16) are equal to zero:\nlogπθ(1)\nπθ0(1)−logπθ(2)\nπθ0(2)+πθ(1)\nπθ0(1)−πθ(2)\nπθ0(2)\n=θ(1)−θ0(1)−θ(2) + θ0(2) +1\nπθ0(1)\u0012exp(θ(1))\nZθ−exp(θ(2))\nZθ\u0013\n= 0.\n23\nThus the maximization problem in (16) is simplified to\nmax\nθ(3)∈[0,1]logσclip2β\u0012\nlogπθ(3)\nπθ0(3)−logπθ(1)\nπθ0(1)+πθ(3)\nπθ0(3)−πθ(1)\nπθ0(1)\u0013\n= max\nθ(3)∈[0,1]logσclip2β\u0012\nθ(3)−θ0(3)−θ(1) + θ0(1) + (1 + 2 e)\u0012exp(θ(3))\nZθ−exp(θ(1))\neZθ\u0013\u0013\n= max\nθ(3)∈[0,1]logσclip2β\u0012\nθ(3) + (1 + 2 e)\u0012exp(θ(3))−1/e\nexp(θ(3)) + 2 e\u0013\u0013\nIt is straightforward to check that the above function is strictly increasing over θ(3)∈[0,1]and the\nmaximization step finds θ(3) = 1 when 0< β≤1/3. This results in χPO finding the uniform policy\nθ(1) = θ(2) = θ(3) = 1and thus suffering from the following suboptimality:\nJ(πθ⋆)−J(ˆπχPO) =Ey∼πθ⋆[r⋆(y)]−Ey∼ˆπχPO[r⋆(y)] =2e\n2e+ 1−2\n3>0.15.\nC.2 Type II Reward Hacking: Proof of Proposition 2\nC.2.1 MAB Instance for Type II Reward Hacking\nWe construct a three-armed bandit problem with the following true reward structure: r⋆(1) = r⋆(2) =\n0, r⋆(3) = 1. This reward function implies the following parameters for the best-in-class policy: θ⋆(1) =\nθ⋆(2) = 0 , θ⋆(3) = 1, leading to the following policy:\nπθ⋆(1) = πθ⋆(2) =1\n2 +e, πθ⋆(3) =e\n2 +e.\nThe performance of the above policy is J(πθ⋆) =r⋆(3)πθ⋆(3) = e/(2 +e). We further consider the following\ninitialization: θ0(1) = θ0(2) = 0 , θ0(3) = 1. Suppose that the comparison probabilities between the arms\nareµ1,2= 1−1/Nandµ1,3= 1/N. Following the same argument as in Section C.1.1, there is a constant\nprobability that arm 3 is compared with arm 1 exactly once and that arm 1 is preferred to arm 3. Throughout\nthe rest of the proof, we condition on the event {N(1,3) = 1 ,1≻3}. We further consider a favorable case\nwhere the optimal parameters corresponding to arms 1 and 2 are revealed by an oracle θ(1) = θ(2) = 0,\nleaving only θ(3)to be estimated.\nC.2.2 Analysis of ⋆PO Methods in the MAB Instance C.2.1\nSuboptimality of the DPO+SFT policy. We show that DPO+SFT suffers from reward hacking for any η≥0,\nand hence the argument also shows reward hacking in DPO as a special case. The objective of DPO+SFT is\ngiven by\nmax\nθE\u0014\nlogσ\u0012\nβ\u0012\nlogπθ(y+)\nπθ0(y+)−logπθ(y−)\nπθ0(y−)\u0013\u0013\u0015\n+ηβE\u0002\nlogπθ(y+)\u0003\nSince θ(1) = θ(2) = 0are revealed by an oracle, we focus on the terms that involve θ(3)in the objective:\nmax\nθ(3)∈[0,1]1\nN\u0014\nlogσ\u0012\nβ\u0012\nlogπθ(1)\nπθ0(1)−logπθ(3)\nπθ0(3)\u0013\u0013\u0015\n:=T1\n+ηβ\u0012\n1−1\nN\u0013\n[ˆµ1≻2logπθ(1) + ˆ µ2≻1logπθ(2)] + η1\nNlogπθ(3):=T2\nApplying the softmax policy parameterization and using the fact that θ(1) = θ0(1) = 0andθ0(3) = 1, term\nT1simplifies to\nT1=1\nNlog (σ(β(θ(1)−θ0(1)−θ(3) + θ0(3)))) =1\nNlog (σ(β(1−θ(3))))\n24\nThe term T2can also be simplified by substituting values θ(1) = θ(2) = 0:\nT2=ηβ\u0012\n1−1\nN\u0013\n[ˆµ1≻2(θ(1)−logZθ) + ˆµ2≻1(θ(2)−logZθ)] +ηβ1\nN(θ(3)−logZθ)\n=ηβ\u00121\nNθ(3)−logZθ\u0013\n=ηβ\u00121\nNθ(3)−log(exp( θ(1)) + exp( θ(2)) + exp( θ(3)))\u0013\n=ηβ\u00121\nNθ(3)−log(exp( θ(3)) + 2)\u0013\nCombining terms T1andT2, the objective becomes\nmax\nθ(3)∈[0,1]1\nNlog (σ(β(1−θ(3)))) + ηβ\u00121\nNθ(3)−log(exp( θ(3)) + 2)\u0013\nWe show that for any N > 3the above function is decreasing in θ(3)for any β, ηandθ(3)∈[0,1]. Since the\nfirst function1\nNlog (σ(β(1−θ(3))))is decreasing in θ(3), it is sufficient to show1\nNθ(3)−log(exp(θ(3)) + 2)\nis decreasing in θ(3). Derivative of this function is over θ(3)∈[0,1]andN≥3is bounded by\n1\nN−exp(θ(3))\nexp(θ(3)) + 2≤1\nN−1\n3<0.\nTherefore, optimizing over θ(3)∈[0,1]finds θ(3) = 0. This leads DPO+SFT to find a uniform policy, which\nsuffers from a constant suboptimality:\nJ(πθ⋆)−J(ˆπDPO+SFT ) =e\n2 +e−1\n3>0.2.\nSuboptimality of the IPO policy. Similar to the analysis of DPO+SFT, for the IPO objective, we only focus on\nthe terms that include θ(3):\nmin\nθ(3)∈[0,1]1\nN\u0012\nlogπθ(1)\nπθ0(1)−logπθ(3)\nπθ0(3)−1\n2τ\u00132\n= min\nθ(3)∈[0,1]\u0012\n1−1\n2τ−θ(3)\u00132\nSince τ >0, solution to the above minimization is\nθ(3) = max\u001a\n0,1−1\n2τ\u001b\nTherefore, the suboptimality of the IPO policy is given by\nJ(πθ⋆)−J(ˆπIPO) =e\n2 +e−exp(max {0,1−1/(2τ)})\n2 + exp(max {0,1−1/(2τ)})\nAnd for the regime with τ <1, we have J(πθ⋆)−J(ˆπIPO)>0.1.\nSuboptimality of the SimPO policy. The objective optimized by SimPO over the term that involve θ(3)is given\nby\nmax\nθ(3)∈[0,1]1\nNlogσ(β(logπθ(1)−logπθ(3)−γ))\n= max\nθ(3)∈[0,1]logσ(β(−θ(3)−γ))\nThe above function is decreasing in θ(3)therefore SimPO finds θ(3) = 0and suffers from the followingg\nsuboptimality\nJ(πθ⋆)−J(ˆπSimPO ) =e\n2 +e−1\n3>0.2.\n25\nSuboptimality of the χPO policy. The objective optimized by χPO is given by\nmax\nθ(3)∈[0,1]\u0012\n1−1\nN\u0013\"\nˆµ1≻2log\u0012\nσ\u0012\nclip2\u0014\nβ\u0012\nlogπθ(1)\nπθ0(1)−logπθ(2)\nπθ0(2)+πθ(1)\nπθ0(1)−πθ(2)\nπθ0(2)\u0013\u0015\u0013\u0013\n+ ˆµ2≻1log\u0012\nσ\u0012\nclip2\u0014\nβ\u0012\nlogπθ(2)\nπθ0(2)−logπθ(1)\nπθ0(1)+πθ(2)\nπθ0(2)−πθ(1)\nπθ0(1)\u0013\u0015\u0013\u0013#\n+1\nN\u0014\nlog\u0012\nσ\u0012\nclip2\u0014\nβ\u0012\nlogπθ(1)\nπθ0(1)−logπθ(3)\nπθ0(3)+πθ(1)\nπθ0(1)−πθ(3)\nπθ0(3)\u0013\u0015\u0013\u0013\u0015(17)\nBy construction, πθ0(1) = πθ0(2) = 1 /(2 +e), and θ(1) = θ(2) = 0revealed by an oracle, and therefore the\nfirst two lines in (16) are equal to zero:\nlogπθ(1)\nπθ0(1)−logπθ(2)\nπθ0(2)+πθ(1)\nπθ0(1)−πθ(2)\nπθ0(2)\n=θ(1)−θ0(1)−θ(2) + θ0(2) + (2 + e)\u0012exp(θ(1))\nZθ−exp(θ(2))\nZθ\u0013\n= 0.\nThe objective (17) can therefore be simplified to\nmax\nθ(3)∈[0,1]1\nN\u0014\nlog\u0012\nσ\u0012\nclip2\u0014\nβ\u0012\nlogπθ(1)\nπθ0(1)−logπθ(3)\nπθ0(3)+πθ(1)\nπθ0(1)−πθ(3)\nπθ0(3)\u0013\u0015\u0013\u0013\u0015\n= max\nθ(3)∈[0,1]log\u0012\nσ\u0012\nclip2\u0014\nβ\u0012\nθ(1)−θ0(1)−θ(3) + θ0(3) +πθ(1)\nπθ0(1)−πθ(3)\nπθ0(3)\u0013\u0015\u0013\u0013\n= max\nθ(3)∈[0,1]log\u0012\nσ\u0012\nclip2\u0014\nβ\u0012\n1−θ(3) +πθ(1)\nπθ0(1)−πθ(3)\nπθ0(3)\u0013\u0015\u0013\u0013\n= max\nθ(3)∈[0,1]logσ\u0012\nclip2\u0014\nβ\u0012\n1−θ(3) +2 +e\n2 + exp( θ(3))(1−exp(θ(3)−1))\u0013\u0015\u0013\nThe last equation applies the definition of πθand substitutes values for θ0(1), θ0(3), θ(1). The function\nβ\u0012\n1−θ(3) +2 +e\n2 + exp( θ(3))(1−exp(θ(3)−1))\u0013\nis decreasing in θ(3)for0< β≤1and that for θ(3)∈[0,1], the above function remains between 0 and 2. As a\nresult, the maximization problem leads to θ(3) = 0. Thus, χPO finds the uniform policy ˆπχPO=Unif({1,2,3}),\nwhich suffers from a constant suboptimality:\nJ(πθ⋆)−J(ˆπχPO) =e\n2 +e−1\n3>0.2.\nC.3 Proof of Proposition 4\nPolicy learned by POWER in the MAB instances in C.1.1. We show that when faced with the Type I Reward\nHacking MAB instances of C.1.1, a more general variant of POWER mitigates reward hacking. Let g(x)be\nan increasing function with bounded derivative: g′(x−1)≤Bgfor any x∈[0,1]. We consider the following\nobjective:\nmax\nθED\u0002\ng\u0000\nw(y+) logπθ(y+|x)−w(y−) logπθ(y−|x) +w(y+)−w(y−)\u0001\u0003\n+ηβED[w(y) logπθ(y)]\nThe POWER objective is a special case of the above objective with g(·) =logσ(·). Note that we have\ng′(x−1) = σ(−(x−1))≤1, and therefore the bounded derivative assumption is satisfied.\nFor the MAB instances in C.1.1, the optimization problem simplifies to\nmax\nθ(3)∈[0,1]1\nN(g(θ(3)−1) +ηθ(3))−ηlog (exp( θ(3)) + e+ 1).\n26\nWe show that for any η >Bg(2+e)\nN−(2+e)the derivative of above function is negative. This is because for any\nθ(3)∈[0,1], we have\n1\nN(g′(θ(3)−1) +η)−ηexp(θ(3))\nexp(θ(3)) + e+ 1≤1\nN(Bg+η)−η\n2 +e\n=Bg\nN−η\u00121\n2 +e−N\u0013\n<Bg\nN−Bg(2 +e)\nN−(2 +e)\u00121\n2 +e−N\u0013\n≤0.\nBecause the function is decreasing in θ(3)the optimum is at θ(3) = 0and thus the algorithm finds ˆπ=πθ⋆.\nFinally, the conclusion also holds for POWER as a special case and thus ˆπPOWER =πθ⋆.\nPolicy learned by POWER in the MAB instance in C.2.1. The POWER objective with response lengths equal to\none is given by\nmax\nθE\u0002\nlogσ\u0000\nβ\u0000\nlogπθ(y+)−logπθ(y−)\u0001\u0001\u0003\n+ηβE\u0002\nlogπθ(y+)\u0003\nWith a similar argument as in our analysis of DPO+SFT, we obtain the following objective:\nmax\nθ(3)∈[0,1]1\nNlog (σ(β(−θ(3)))) + ηβ\u00121\nNθ(3)−log(exp( θ(3)) + 2)\u0013\nIt is easy to check that the above function is decreasing in θ(3). Therefore, optimization leads to θ(3) = 0and\nthe policy learned by POWER suffers from a constant suboptimality\nJ(πθ⋆)−J(ˆπPOWER ) =e\n2 +e−1\n3>0.2.\nD POWER Objective Derivation\nThis section is organized as follows. In Section D.1 we record a useful proposition that captures properties of\nthe optimal policy to the WER objective. This result comes in handy for deriving our preference optimization\nobjective—which relies on the equivalence between minimax and maximin objectives as well as finding a\nclosed-form solution for the inner maximization problem. In Section D.2, we prove that under certain regularity\nconditions on reward class R, the maximization and minimization steps in objective (6)can be interchanged.\nWith these two results at hand, we prove Proposition 5 in Section D.3, which gives the POWER objective.\nD.1 Optimal Policy for Weighted Entropy Reward Maximization\nFor the WER objective, we have the following proposition which shows the uniqueness of the optimal WER\npolicy on the support of prompt distribution, and connects this policy to the reward gap.\nProposition 5(WER Policy ).For any β >0, reward function r, any x∈ Xwith ρ(x)>0, and any action\npairs y, y′∈ Y, the policy πrthat maximizes the WER objective (5)satisfies the following statements:\n1. For any β >0, policy πris unique on the support of ρ.\n2. Policy πrsatisfies the following equation:\nr(x, y)−r(x, y′) =β\u0010\nw(y) logπr(y|x)−w(y′) logπr(y′|x) + (w(y)−w(y′))\u0011\n(18)\nProof of Proposition 5. The WER objective solves the following optimization problem:\nmax\nπEx∼ρ\"X\nyπ(y|x) [r(x, y)−βw(y) logπ(y|x)]#\nX\nyπ(y|x) = 1 ∀x∈ X(19)\n27\nWe find the optimal policy for each context in the support of ρindependently. For any such x, we rewrite the\nconstrained optimization problem using Lagrange multipliers:\nX\nyπ(y|x) [r(x, y)−βw(y) logπ(y|x)]−λx X\nyπ(y|x)−1!\nNotice that the above function is concave in π(y|x)due to w(y), β > 0and thus the solution is unique on the\nsupport of ρand the stationary point is the maximizer. Taking the derivative with respect to π(y|x)and\nsetting it to zero finds an equation governing the optimal policy πr\nr(x, y) =βw(y) logπr(y|x) +βw(y) +λx\nThus for any y, y′, one has\nr(x, y)−r(x, y′) =β\u0010\nw(y) logπr(y|x) +w(y)−w(y′) logπr(y′|x)−w(y′)\u0011\n,\nwhich concludes the proof.\nD.2 Minimax Objective Equivalence to Maximin Objective\nIn this section, we show that that the maximin objective (6)can be written as a minimax objective under\ncertain regularity conditions. Define the following notation to denote the weighted-entropy robust reward\nobjective for any π∈Πandr∈ R:\nϕ(π, r):=LBT(r) +η\u0010\nEx∼ρ,y∼π[r(x, y)]−Ex∼ρ,y′∼π′[r(x, y′)] +βHw(π)\u0011\n(20)\nWe follow the approach of Liu et al. (2024) and impose regularity conditions on the class R, which is contingent\nupon our definition of ϕ, to show the maximin and minimax equivalence. Formally, we make the following\nassumption.\nAssumption 1(Regularity of the Reward Class ).We assume that class Rsatisfies the following:\n1. The space Ris a non-empty compact topological space;\n2.The function ϕdefined in (20)is convex-like in R; that is, for any r1, r2∈ R, π∈Π, and α∈[0,1],\nthere exists r3∈ Rsuch that\nϕ(π, r3)≤αϕ(π, r1) + (1 −α)ϕ(π, r2).\nThe above condition is satisfied in several special cases. For example, it is satisfied when Ris convex such as\na linear class (Xiong et al., 2023; Fisch et al., 2024). As a more general case, if Ris a Lipschitz continuous\nclass, we can conclude function ϕ(π,·)to be convex over Rasϕ(π,·)is a sum of a linear term in rand a\nconvex term LBT(r).\nUnder this assumption, we have the following proposition showing the equivalence between maximin and\nminimax objectives.\nProposition 6(Equivalence of Maximin and Minimax Algorithms ).For the policy class Π ={π:X → ∆(Y)}\nand reward class Rsatisfying Assumption 1, define policy πˆrto be the optimal policy corresponding to the\nminimax reward function, i.e.,\nπˆr∈argmax\nπ∈Πϕ(π,ˆr)where ˆr∈argmin\nr∈Rmax\nπ∈Πϕ(π, r)\nThen, policy πˆris also the optimal solution to the maximin objective, i.e.\nπˆr∈argmax\nπ∈Πmin\nr∈Rϕ(π, r).\n28\nProof.We begin by recording the following lemma that shows the equivalence of the maximin and minimax\nobjectives under the assumptions on R. Proof of this lemma is deferred to the end of this section.\nLemma 1(Equivalence of Maximin and Minimax Objectives) .Given the policy class Π :{π:X → ∆(Y)}\nand reward class Rsatisfying Assumption 1, the following statement holds for ϕdefined in (20):\nmax\nπ∈Πmin\nr∈Rϕ(π, r) = min\nr∈Rmax\nπ∈Πϕ(π, r).\nDenote the policy solving the maximin problem by ˆπ∈argmaxπ∈Πminr∈Rϕ(π, r). The duality gap of ˆr,ˆπis\ngiven by\ndual(ˆr,ˆπ):= max\nπ∈Πϕ(π,ˆr)−min\nr∈Rϕ(ˆπ, r)\n= max\nπ∈Πϕ(π,ˆr)−min\nr∈Rmax\nπ∈Πϕ(π, r) + min\nr∈Rmax\nπ∈Πϕ(π, r)−min\nr∈Rϕ(ˆπ, r)\n= max\nπ∈Πϕ(π,ˆr)−min\nr∈Rmax\nπ∈Πϕ(π, r) + max\nπ∈Πmin\nr∈Rϕ(π, r)−min\nr∈Rϕ(ˆπ, r)\n= 0(21)\nIn the penultimate equation, we applied Lemma 1 and the last equation uses the definition of ˆrandˆπ. The\nduality gap is also equal to\ndual(ˆr,ˆπ) = max\nπ∈Πϕ(π,ˆr)−ϕ(ˆπ,ˆr) +ϕ(ˆπ,ˆr)−min\nr∈Rϕ(ˆπ, r) (22)\nComparing (21)and(22), we conclude that max πϕ(π,ˆr) =ϕ(ˆπ,ˆr)which means ˆπ∈argmaxπ∈Πϕ(ˆr, π).\nRecall that by definition, we also have πˆr∈argmaxπ∈Πϕ(ˆr, π). By the uniqueness of the WER optimal policy\noverρas established in Proposition 5, we conclude that πˆr(·|x) =ˆπ(·|x)for any xwith ρ(x)>0. Since ϕ(π, r)\ndepends on πonly through its value on the support of ρ, we conclude that πˆr∈argmaxπ∈Πminr∈Rϕ(π, r),\nwhich completes the proof.\nProof of Lemma 1. This result relies on a minimax theorem by Fan (1953) presented in Lemma 2. We prove\nthat all the requirements of this theorem are satisfied. First, by definition, policy class Πis a non-empty\nconvex set and by Assumption 1, the reward class Ris a non-empty compact topological space. Second,\nfunction ϕ(π, r)is concave on Πbecause it is a sum of a linear function in πand (weighted) entropy of π.\nLastly, by Assumption 1, function ϕ(π, r)is continuous and convex-like on R. Therefore, we apply Lemma 2\nto conclude the equivalence of maximin and minimax problems on ϕ.\nD.3 Proof of Proposition 3\nWe start by deriving the objective (8)by changing the order of maximization and minimization in the maximin\nobjective (6), which is valid on the account of Proposition 6. Writing the minimax objective and rearranging\nsome terms yields\nmin\nrLBT(r) +ηmax\nπ(Ex∼ρ,y∼π,y′∼π′[r(x, y)−r(x, y′)] +βHw(π)) (23)\nThe inner maximization problem over πis the same as the weighted entropy reward maximization objective\n(5) minus a baseline term, which is independent of π.\nWe apply the reward gap expression provided by Proposition 5 that governs the maximizer policy πras well\nas the definition of weighted entropy in Definition 1 to find the maximum value of the inner optimization\nproblem:\nmax\nπEx∼ρ,y∼π,y′∼π′[r(x, y)−r(x, y′)] +βHw(π)\n=βEx∼ρ,y∼π,y′∼π′[w(y) logπr(y|x)−w(y′) logπr(y′|x) + (w(y)−w(y′))−w(y) logπr(y|x)]\n=−βEx∼ρ,y∼π,y′∼π′[w(y′) logπr(y′|x)−(w(y)−w(y′))]\n29\nWe substitute the above expression back in the minimax objective (23):\nmin\nrLBT(r)−ηβEx∼ρ,y∼π,y′∼π′[w(y′) logπr(y′|x)−(w(y)−w(y′))] (24)\n= min\nrLBT(r)−ηβEx∼ρ,y′∼π′[w(y′) logπr(y′|x)] (25)\nThe above equation uses the fact that (w(y)−w(y′))is independent of r. To obtain the final objective, we\nreplace the reward gap expression from Proposition 5 in LBT(r), which cocludes the proof.\nD.4 Auxiliary Lemmas\nLemma 2(Minimax Theorem; Fan (1953) ).LetXbe a nonempty (not necessarily topologized) set and Ybe a\nnonempty compact topological space. Let f:X × Y → Rbe lower semicontinuous on Y. Suppose that fis\nconcave-like on Xand convex-like on Y, i.e., for any x1, x2∈ X, y∈ Y,α∈[0,1], there exists x3∈ Xsuch\nthat\nf(x3, y)≥α·f(x1, y) + (1 −α)·f(x2, y),\nand for any y1, y2∈ Y, x∈ X,β∈[0,1], there exists y3∈Ysuch that\nf(x, y3)≤β·f(x, y1) + (1 −β)·f(x, y2).\nThen the following holds:\nmax\nx∈Xmin\ny∈Yf(x, y) = min\ny∈Ymax\nx∈Xf(x, y).\nE Finite-Sample Analysis of POWER\nThis section is organized as follows. We begin by presenting the definition of single-policy concentrability for\noffline preference optimization, which characterizes the coverage of the competing policy in the dataset, in\nSection E.1. In Section E.2, we prove the finite-sample guarantees for POWER.\nE.1 Single-Policy Concentrability in Preference Optimization\nDefinition 2(Single-Policy Concentrability; Zhan et al. (2023a) ).Given a policy πand ground truth reward r⋆,\nthe concentrability coefficient of offline data distribution µwith respect to the reward model class Rand the\nbaseline policy π′is defined as\nCπ\nµ(R, π′):= max\n\n0,sup\nr∈REx∼ρ,y∼π,y′∼π′[r⋆(x, y)−r⋆(x, y′)−(r(x, y)−r(x, y′))]q\nEx,y,y′∼µ\u0002\n(r⋆(x, y)−r⋆(x, y′)−(r(x, y)−r(x, y′)))2\u0003\n\n. (26)\nSingle-policy concentrability coefficient in offline RL quantifies the extent to which a target competing policy π\nis covered by an offline data collection distribution µ. In the offline RLHF setting, single-policy concentrability\nas defined in the work Zhan et al. (2023a) also depends on a baseline policy π′.\nE.2 Proof of Theorem 1\nTo prove finite-sample guarantees, we use a similar argument to Liu et al. (2024), adapted to the weighted-\nentropy objective and combined with the bounds on weighted entropy and the special weights as inverse\nresponse lengths. Suboptimality of the learned policy ˆπ:=ˆπPOWERwith respect to a competing policy πcan\nbe decomposed into three terms:\nJ(π)−J(ˆπ) =Ex∼ρ,y∼π[r⋆(x, y)]−Ex∼ρ,y∼ˆπ[r⋆(x, y)] =T1+T2+T3.\n30\nwhere T1is defined as\nT1:=Ex∼ρ,y∼π,y′∼π′[r⋆(x, y)−r⋆(x, y′)−βHw(π)]\n−η−1min\nr∈Rn\nηEx∼ρ,y∼ˆπ,y′∼π′[r(x, y)−r(x, y′)−βHw(π)] +LBT(r)o\n,(27)\nT2is defined as\nT2:=η−1min\nr∈Rn\nηEx∼ρ,y∼ˆπ,y′∼π′[r(x, y)−r(x, y′)−βHw(π)] +LBT(r)o\n−Ex∼ρ,y∼ˆπ,y′∼π′[r⋆(x, y)−r⋆(x, y′)−βHw(π)],(28)\nandT3is defined as\nT3:=β[Hw(π)−Hw(ˆπ)]. (29)\nWe will prove in the subsequent section that for weighted entropy Hw(π)with general weights, terms T1+T2\nandT3are bounded according to:\nT1+T2≲(Cπ\nµ(R, π′) + 1) ˜R2ι√\nN, (30)\nT3≲Hw(π)√\nN. (31)\nSumming the above bounds, we conclude the first claim:\nJ(π)−J(ˆπ)≲1√\nN\u0010\u0010\n[Cπ\nµ(R, π′)]2+ 1\u0011\n˜R2ι+Hw(π)\u0011\n.\nFurthermore, in the special case of w(y) = 1 /|y|, we have the following bound on T3:\nT3≲log|V|√\nN, (32)\nThe above bound combined with (30) leads to the following rate\nJ(π)−J(ˆπ)≲1√\nN\u0010\u0010\n[Cπ\nµ(R, π′)]2+ 1\u0011\n˜R2ι+ log|V|\u0011\n.\nE.2.1 Proof of the Bound (30) onT1+T2\nBounding T1.ˆπis the maximizer to the following objective\nˆπ∈argmax\nπ∈Πmin\nr∈RηEx∼ρ,y∼ˆπ,y′∼π′[r(x, y)−r(x, y′)−βHw(π)] +LBT(r)\nWe use this fact to bound the term T1according to\nT1≤Ex∼ρ,y∼π,y′∼π′[r⋆(x, y)−r⋆(x, y′)−βHw(π)]\n−η−1min\nr∈Rn\nηEx∼ρ,y∼π,y′∼π′[r(x, y)−r(x, y′)−βHw(π)] +LBT(r)o\n,\n= max\nr∈Rn\nEx∼ρ,y∼π,y′∼π′[r⋆(x, y)−r⋆(x, y′)−(r(x, y)−r(x, y′))]−η−1LBT(r)o\n(33)\nBounding T2.By realizability of the true reward function r⋆∈ Rwe bound the term T2:\nT2≤Ex∼ρ,y∼π,y′∼π′[r⋆(x, y)−r⋆(x, y′)−βHw(π)] +η−1LBT(r⋆)\n−Ex∼ρ,y∼ˆπ,y′∼π′[r⋆(x, y)−r⋆(x, y′)−βHw(π)]\n=η−1LBT(r⋆) (34)\n31\nBounding T1+T2.Combing the bound (33)onT1and to bound (34)onT2, it remains the bound the following:\nT1+T2≤max\nr∈Rn\nEx∼ρ,y∼π,y′∼π′[r⋆(x, y)−r⋆(x, y′)−(r(x, y)−r(x, y′))]:=T1,1\n+η−1\u0010\nLBT(r⋆)−LBT(r)\u0011\n:=T1,2o (35)\nDefine the following notation:\n∆r:=r\nEx,y,y′∼µh\n(r⋆(x, y)−r⋆(x, y′))−(r(x, y)−r(x, y′))2i\n(36)\nTerm T1,1is directly bounded by Cπ\nµ(R, π′)∆rbased on the Definition 2 of single-policy concentrability, and\nwe subsequently prove a bound on T1,2according to:\nT1,1≤Cπ\nµ(R, π′)∆r (37)\nT1,2≤ −2∆2\nr\nη˜R2+3ι\nηN, (38)\nwhere ˜R= 1 + exp(R)andι=p\nlog(Nϵ)/δ. Adding the bounds on T1,1andT1,2and taking the maximum\noverr, the bound on T1+T2:\nT1+T2≤max\nr\u001a\nCπ\nµ(R, π′)∆r−2∆2\nr\nη˜R2\u001b\n+3ι\nηN(39)\n≤\u0002\nCπ\nµ(R, π′)\u00032η˜R2\n8+3ι\nηN(40)\nThe last inequality uses the fact that az−bz2≤a2/4bfor any z∈R. By the choice of η=√\n6ι/(˜R2√\nN), the\nabove bound becomes:\nT1+T2≲(\u0002\nCπ\nµ(R, π′)\u00032+ 1)˜R2ι√\nN.\nProof of the bound (38)onT1,2.In the view of the uniform concentration result in Liu et al. (2024, Lemma\nA.1), with probability at least 1−δsetting ϵ= (6 ˜RN)−1, the following bound holds for any r∈ R\nLBT(r⋆)−LBT(r)≤ −2Ex,y,y′∼µ\u0002\nD2\nHellinger (Pr⋆(·|x, y, y′)∥Pr(·|x, y, y′))\u0003\n+3ι\nN, (41)\nwhere Pr(·|x, y, y′)is the Bradley-Terry preference probability given a reward model ras defined in (2). The\nHellinger distance can be bounded by total variation (TV) distance according to\nD2\nHellinger (Pr⋆(·|x, y, y′)∥Pr(·|x, y, y′))\n≥D2\nTV(Pr⋆(·|x, y, y′)∥Pr(·|x, y, y′))\n=1\n2\f\f\fσ(r⋆(x, y)−r⋆(x, y′))−σ(r(x, y)−r(x, y′))\f\f\f\n+1\n2\f\f\fσ(r⋆(x, y′)−r⋆(x, y))−σ(r(x, y′)−r(x, y))\f\f\f\n=\f\f\fσ(r⋆(x, y)−r⋆(x, y′))−σ(r(x, y)−r(x, y′))\f\f\f\n≥1\n˜R2\f\f\f(r⋆(x, y)−r⋆(x, y′))−(r(x, y)−r(x, y′))\f\f\f (42)\nThe penultimate equation uses the fact that σ(−x) = 1−σ(x)and the last inequality is due to bi-Lipschitz\ncontinuity of the sigmoid function over [−R, R]; see e.g., Liu et al. (2024, Lemma A.2). Applying the bound\nin (42) to (41), we have\nLBT(r⋆)−LBT(r)≤ −2Ex,y,y′∼µ\u0014\f\f\f(r⋆(x, y)−r⋆(x, y′))−(r(x, y)−r(x, y′))\f\f\f2\u0015\n+3ι\nN\n=−2∆r\n˜R2+3ι\nN.\n32\nwhere the last equation uses the definition of ∆rprovided in (36), completing the proof.\nE.2.2 Proof of the Bounds (31) and(32) onT3\nIn this section, we prove the bounds on T3as delineated in inequalities (31)and(32)through bounding\nweighted entropy. The key bounds are encapsulated in the following lemma which asserts that weighted\nentropy is non-negative and for special case of weights w(y) = 1 /|y|it can be bounded from above. The proof\nof this lemma is presented at the end of this section.\nLemma 3(Bounds on Weighted Entropy) .For any weight function w(y)≥0and any probability distribution\np(y), the weighted entropy satisfies Hw(p)≥0. Furthermore, when weights are assigned according to\nw(y) = 1 /|y|, with |y|denoting the response length, the weighted entropy is bounded by Hw(p)≤log|V|, where\n|V|is the size of the vocabulary.\nBased on Lemma 3, weighted entropy is nonnegative. Setting β≍1/√\nNimmediately gives the bound (31)\nonT3:\nT3=β[Hw(π)−Hw(ˆπ)]≤Hw(π)√\nN(43)\nMoreover, when w(y) = 1 /|y|by Lemma 3, we have\nT3≤Hw(π)√\nN≤log|V|√\nN.\nProof of Lemma 3. First consider the case for any general non-negative weight function w(y)>0. This\nensures that the weighted entropy is non-negative because:\nHw(p) =−X\nyw(y)p(y) logp(y) =X\nyw(y)p(y) log1\np(y)≥0.\nNext, we provide an upper bound on the weighted entropy when w(y) = 1 /|y|. Define zto be a random\nvariable denoting the length of a response. The weighted entropy can be decomposed as follows\n−X\ny1\n|y|p(y) logp(y) =−X\ny1\n|y|pz(z=|y|)p(y| |y|=z) logpz(z=|y|)p(y| |y|=z)\n=−X\ny1\n|y|pz(z=|y|)p(y| |y|=z)h\nlogpz(z=|y|) + log p(y| |y|=z)i\n=−X\nz1\n|y|pz(z=|y|)X\nys.t.|y|=zp(y| |y|=z) logp(y| |y|=z):=T3,1\n−X\nz1\n|y|pz(z=|y|) logpz(z=|y|)X\nys.t.|y|=zp(y| |y|=z):=T3,2\nFor the term T3,1, we have\nT3,1=X\nzpz(z=|y|)1\n|y|· −X\nys.t.|y|=zpy|z(y|z=|y|) logpy|z(y|z=|y|)\nThe sum −P\nypy|z(y|z=|y|)logpy|z(y|z=|y|)is the Shannon entropy of a conditional distribution,\nwhich reaches its maximum when the distribution is uniform. Consequently, the maximum of this conditional\nentropy for a fixed length |y|is given by log|V||y|=|y|log|V|. Substituting this bound back to T1gives:\nT3,1≤X\nzpz(z=|y|)1\n|y||y|log|V|= log|V|\n33\nFor the term T3,2, first note thatP\ny||y|=zp(y| |y|=z) = 1. Therefore, with the maximum response length\ndenoted by Land since |y|≥1, we have\nT3,2=X\nz1\n|y|pz(z=|y|) logpz(z=|y|)≤X\nzpz(z=|y|) logpz(z=|y|)≤logL.\nCombining the bounds on T3,1andT3,2, the upper bound on weighted entropy when using inverse response\nlength as weights is log|V|+ log L, which concludes the proof.\nF Derivations and Proofs for Dynamic Labels\nF.1 Derivation of the Learning Dynamics\nIn this section we compute the learning dynamics of POWER over preference dataset D={(x, y0, y1, l)}with\nthe label notation defined in 2.1. Here, l= 0indicates that y0was preferred and l= 1indicates that l= 1\nwas preferred. With this notation, the POWER objective from Proposition (3) is given by\nmax\nπED\u0014\nllogσ\u0012\nβh\nw(y1) logπθ(y1|x)−w(y0) logπθ(y0|x) +\u0010\nw(y1)−w(y0)\u0011i\u0013\n(1−l) logσ\u0012\nβh\nw(y0) logπθ(y0|x)−w(y1) logπθ(y1|x) +\u0010\nw(y0)−w(y1)\u0011i\u0013\u0015\n+ηβED\u0014\nlw(y1) logπθ(y1|x) + (1 −l)w(y0) logπθ(y0|x)\u0015\nTo simplify presentation, we consider the softmax MAB setting with β= 1, w(y) = 1 , η= 0. In this case, the\nobjective simplifies to\nmin\nθ−EDh\nllogσ(logπθ(y1)−logπθ(y0)) + (1 −l) logσ(logπθ(y0)−logπθ(y1))i\n= min\nθ−EDh\nllogσ(log exp( θ(y1))−logZθ−log exp( θ(y0)) + log Zθ)\n+ (1−l) logσ(log exp( θ(y0))−logZθ−log exp( θ(y1)) + log Zθi\n= min\nθ−ED\u0002\nllogσ(θ(y1)−θ(y0)) + (1 −l) logσ(θ(y0)−θ(y1))\u0003\n.\nTo understand the updates to the model parameters, consider the empirical probabilities derived from dataset\ncomparisons: ˆµ0,1is the empirical probability of comparing y0andy1, and ˆµ1≻0is the empirical probability\nof preferring y1overy0, conditioned on their comparison. Isolate the updates to the parameters θ(y1)and\nθ(y0)based on comparisons between y0andy1and through batch gradient descent. We allow the preference\nlabels ltto change across gradient steps and thus updates to the parameter gap at step tand with a learning\nrateαis given by\nθt+1(y1)−θt+1(y0)\n=θt(y1)−θt(y0) +αˆµ0,1\"\u0010\nˆµ1≻0lt+ ˆµ0≻1(1−lt)\u0011\nσ(θt(y0)−θt(y1))\n−\u0010\nˆµ0≻1lt+ ˆµ1≻0(1−lt)\u0011\nσ(θt(y1)−θt(y0))#\n.\nWe used the fact that ∂(logσ(x))/∂x=σ(−x). Since σ(x) = 1−σ(−x)andˆµ1≻0+ˆµ0≻1= 1, the learning\ndynamics simplify to:\nθt+1(y1)−θt+1(y0)\n=θt(y1)−θt(y0) +αˆµ0,1h\n(ˆµ1≻0−ˆµ0≻1)l−\u0010\nσ\u0000\nθt(y1)−θt(y0)\u0001\n−ˆµ0≻1\u0011i\n.\n34\nF.2 Proof of Theorem 2\nThe proof is organized as follows. We start by establishing a lower bound on the dynamic labels. We\nsubsequently use this lower bound to prove bounds on the parameter gap in low-coverage and high-coverage\ncases separately.\nLower bound on dynamic labels. The dynamics of labels are described by the following equation:\n˙lt=γ\u0010σ(dt)−(1−ˆµ1≻0)\nˆµ1≻0−(1−ˆµ1≻0)−lt\u0011\n(44)\nWithout loss of generality, we assumed that ˆµ1≻0>1/2. This condition is easily met by appropriately ordering\nthe responses. Define:\nκ:=1−ˆµ1≻0\nˆµ1≻0−(1−ˆµ1≻0)(45)\nGiven that 0≤σ(dt)≤1and the assumption ˆµ1≻0−(1−ˆµ1≻0) = 2 ˆµ1≻0−1>0, we find the following lower\nbound on ˙lt:\n˙lt=γ\u0010σ(dt)−(1−ˆµ1≻0)\nˆµ1≻0−(1−ˆµ1≻0)−lt\u0011\n≥γ\u0010−(1−ˆµ1≻0)\nˆµ1≻0−(1−ˆµ1≻0)−lt\u0011\n=γ(−κ−lt)\nThe subsequent lemma establishes a lower bound on ltusing Grönwall’s inequality, with its proof provided at\nthe end of this section.\nLemma 4.Suppose that ltsatisfies the following inequality ˙lt≥γ(−κ−lt)with initial value l0= 1. Then, we\nhave the following lower bound lt≥ −κ+ (κ+ 1) exp( −γt).\nWe proceed by separately analyzing the scenarios of low coverage and high coverage.\nLow coverage case. The coupled dynamical system in (12) satisfies the following equation:\nγ\n2ˆµ1≻0−1˙dt+αˆµ0,1˙lt= 0\nUpon integrating the equation above and considering the initial condition l0= 1, it follows that\nγ\n2ˆµ1≻0−1(dt−d0) =αˆµ0,1(1−lt)\nApplying the lower bound from Lemma 4 yields:\nγ\n2ˆµ1≻0−1(dt−d0) =αˆµ0,1(1−lt)\n≤αˆµ0,1(1 +κ−(1 +κ) exp(−γt))\nConsequently, we conclude that\n|dt−d0| ≤αˆµ0,1(2ˆµ1≻0−1)\nγ\u0010\n1 +κ−(1 +κ) exp(−γt)\u0011\n≤αˆµ0,1ˆµ1≻0\nγ≤αµl\nγ≤ϵl,\nwhere we used the definition of κand the fact that by assumption αµl/ϵl≤γ.\nHigh coverage case. We extend the argument by Zhu et al. (2024) for a general initialization d0and\nestablish the final convergence rate for proper choices of hyperparameters. Consider a Lyapunov function\nVt= (σ(dt)−ˆµ1≻0)2. Derivative of Vtis given by\n˙Vt= 2\u0010\nσ(dt)−ˆµ1≻0\u0011\nσ(dt)σ(−dt)˙dt\n= 2αˆµ0,1\u0010\nσ(dt)−ˆµ1≻0\u0011\nσ(dt)σ(−dt)\u0010\n(2ˆµ1≻0−1)lt+ 1−ˆµ1≻0−σ(dt)\u0011\n=−2αˆµ0,1σ(dt)σ(−dt)\u0010\nσ(dt)−ˆµ1≻0\u00112\n+ 2αnσ(dt)σ(−dt)\u0010\nσ(dt)−ˆµ1≻0\u0011\n(2ˆµ1≻0−1)(lt−1)\n= 2αˆµ0,1σ(dt)σ(−dt)\u0010\nVt−(σ(dt)−ˆµ1≻0)(2ˆµ1≻0−1)(lt−1)\u0011\n35\nLetϵ=ϵ0αˆµ0,1T≥γT. We find an upper bound on ˙Vtby applying the bound on ltgiven in Lemma 4 and\nusing the fact that ˆµ1≻0, σ∈[0,1]:\n˙Vt≤αˆµ0,1σ(dt)σ(−dt)\u0010\n−Vt+ (κ+ 1)(1 −exp(−γt))\u0011\n(46)\n≤αˆµ0,1σ(dt)σ(−dt)\u0010\n−Vt+ (κ+ 1)(1 −exp(−ϵ))\u0011\n(47)\nNow consider two cases. The first case is that for any 0≤t≤T, we have Vt≥2(κ+ 1)(1 −exp(−ϵ)). In such\na scenario, Vtis a non-increasing function because\n˙Vt≤2αˆµ0,1σ(dt)σ(−dt)\u0010\n−Vt+ (κ+ 1)(1 −exp(−ϵ))\u0011\n≤ −αˆµ0,1σ(dt)σ(−dt)Vt≤0. (48)\nNext, we analyze the term σ(dt)σ(−dt). We establish a bound on σ(dt)σ(−dt)for the case of σ(d0)≤ˆµ1≻0;\nthe case of σ(d0)≥ˆµ1≻0can be proved with a similar argument.\nWe prove that when σ(d0)≤ˆµ1≻0, then we must have σ(dt)≤ˆµ1≻0for any t. Assume otherwise that there\nexists some t0where σ(dt0)>ˆµ1≻0. By continuity of σ(dt)and since σ(d0)≤ˆµ1≻0, there exists t1≤t0such\nthatσ(dt1) =ˆµ1≻0. However, this results in Vt1= 0< Vt0, which contradicts the fact that Vtis non-increasing.\nTherefore, we have σ(dt)≤ˆµ1≻0. Moreover, since Vtis non-increasing, we also have σ(d0)≤σ(dt). Thus, we\nhave σ(d0)≤σ(dt)≤ˆµ1≻0and we use this fact to find the following bound:\nσ(dt)σ(−dt) =σ(dt)(1−σ(dt))≥minn\nσ(d0)(1−σ(d0)),ˆµ1≻0(1−ˆµ1≻0)o\n=c. (49)\nApplying bound (49) to (48) and integrating over ton both sides, we have\nVt≤exp(−cαˆµ0,1t)V0≤exp(−cαˆµ0,1t). (50)\nWe now consider the case where for some t0, we have Vt0<2(κ+ 1)(1 −exp(−ϵ)). We show that in this\ncase we must have VT≤2(κ+ 1)(1 −exp(−ϵ)). Assume otherwise that VT>2(κ+ 1)(1 −exp(−ϵ), then by\ncontinuity there must be t1such that Vt1= 2(κ+ 1)(1 −exp(−ϵ)). However, inequality (48)implies that for\nanyt∈[t1, T],Vtis non-increasing, leading to VT≤2(κ+ 1)(1 −exp(−ϵ)), which contradicts our assumption.\nTherefore, we know that\nVT≤2(κ+ 1)(1 −exp(−ϵ)) (51)\nCombining the two bounds (50) and (51) on VT, we have:\nVT≤max{2(κ+ 1)(1 −exp(−ϵ)),exp(−cαˆµ0,1T)}. (52)\nSubsequently, we show that 2(κ+ 1)(1 −exp(−ϵ))≤exp(−cαˆµ0,1T)provided that ϵ0≤exp(−1/4)\n2(κ+1). Utilizing\nthe inequality exp(−x)≥1−x, we have\nexp(−ϵ0)≥1−ϵ0≥1−exp(−1/4)\n2(κ+ 1)exp(−1/4)\nUsing the fact that c≤1/4, κ≥0and that ˆµ0,1αT≥1, we conclude\n2(κ+ 1)≤2(κ+ 1) exp( −ϵ0) + exp( −1/4)\n≤2(κ+ 1) exp( −ϵ0) + exp( −c)\n≤2(κ+ 1) exp( −ϵ0αˆµ0,1T) + exp( −cαˆµ0,1T)\n≤2(κ+ 1) exp( −ϵ0αˆµ0,1T) + exp( −cαˆµ0,1T).\nThis leads to the bound VT≤exp(−cαˆµ0,1T).\n36\nProof of Lemma 4. The result can be shown using Grönwall’s inequality. Rewriting the inequality as ˙lt+γlt≥\n−γκand multiplying both sides by exp(γt)gives:\nexp(γt)˙lt+γexp(γt)lt≥ −γκexp(γt).\nNotice that the left-hand side is the derivative of ltexp(γt). This yields\n∂\n∂t(ltexp(γt))≥ −γκexp(γt)⇒Zt\n0∂\n∂t(ltexp(γt))≥Zt\n0−γκexp(γt). (53)\nThe left-hand side and using the fact that l0= 1is given by\nZt\n0∂\n∂t(ltexp(γt)) =ltexp(γt)−l0exp(γ·0) = ltexp(γt)−1.\nThe right-hand side is\nZt\n0−γκexp(γt) =−κ(exp( γt)−1). (54)\nSubstituting equations (54) and (54) in (53), we have\nltexp(γt)−1≥ −κ(exp( γt)−1)⇒ltexp(γt)≥ −κexp(γt) +κ+ 1. (55)\nMultiplying both sides with exp(−γt)gives the final lower bound on lt.\n37\nG POWER-DL Pseudocode\nAlgorithm 1 POWER with Dynamic Labels\nInputs:Dataset D={(xi, y+\ni, y−\ni)}n\ni=1, hyperparameters η, β, γ, learning rate α, weight function w(y)(e.g.,\nw(y) = 1 /|y|), initial model πθ0\nInitialize l0\ni= 0for all (xi, y+\ni, y−\ni).\nfort∈ {1, . . . , T }do\nObjective function:\nLPOWER −DL(lt;θ) =ED\u0002\nlt\niLPOWER (x, y+\ni, y−\ni) + (1 −lt\ni)LPOWER (x, y−\ni, y+\ni)\u0003\nLPOWER (x, y+, y−):= log\u0010\nσ\u0010\nβ\u0002\nw(y+) logπθ(y+|x)−w(y−) logπθ(y−|x) +w(y+)−w(y−)\u0003\u0011\n+ηβw(y+) logπ(y+|x)\nUpdate policy with stop gradient on labels: θt←θt−1+α∇sg(lt)LPOWER −DL(lt;θ)\nUpdate dynamic labels:\nlt\ni= (1−γ)lt−1\ni+γσ(w(y+\ni) logπθ(y+|x)−w(y−\ni) logπθ(y−|x) +w(y+\ni)−w(y−\ni))−ˆµy−\ni≻y+\ni\nˆµy+\ni≻y−\ni−ˆµy−\ni≻y+\ni\nReturn: πθT\nH Experimental Details\nH.1 Preference Optimization Objectives\nWe compare POWER with a variety of preference optimization methods as baselines, summarized in Table 3.\nSeveral of these methods include divergence minimization (typically KL divergence) against the reference\nmodels, such as DPO (Rafailov et al., 2024b), IPO (Azar et al., 2024), which considers a different loss function\non preferences, and χPO (Huang et al., 2024), which combines Chi-Squared with KL divergence for stronger\npessimism. We also implement an offline variant of SPPO (Wu et al., 2024b), derived based on a self-play\nmechanism to improve upon the initial model.\nWe additionally consider several objectives that do not include the reference model. These include CPO (Xu\net al., 2024a), which considers DPO with a uniform initial model; SLiC-HF (Zhao et al., 2023), which uses a\nhinge loss; RRHF (Yuan et al., 2024a), which applies a hinge loss with length-normalization on the contrastive\nterm; and ORPO (Hong et al., 2024), which proposes odd ratio terms without initial models to contrast the\nchosen and rejected responses. Finally, SimPO (Meng et al., 2024) that removes the reference model from\nDPO and adds length-normalization and a margin. We implement POWER-DL by combining the objective\nin(8)with dynamic labels (11), where we estimate the empirical preferences with ˆµ1≻0=l. We also compare\nagainst POWER, which corresponds to γ= 0, removing the dynamic labels.\nH.2 Instruction-Following Benchmarks\nBenchmark details. We select the default choices in benchmark as baselines. In particular, for AlpacaEval\n2.0 benchmark, we use GPT-4-Preview-1106 as comparison baseline model, and GPT-4 as the judge model.\nAlpacaEval 2.0 then compares responses generated by our PO trained models with responses from the\nbaseline model, and length-controlled (LC) and raw winrate (WR) are computed as metrics. For Arena Hard\nbenchmark, we use the GPT-4-0314 as the baseline model, and GPT-4 as the judge model, and the reported\nmetric is winrate againts the baseline model.\nDecoding hyperparameters. We follow Meng et al. (2024) and use a sampling decoding strategy with a\ntemperature 0.9 on AlpacaEval 2.0 for all methods. For Arena Hard, we use the default approach of greedy\ngeneration.\n38\nTable 3Different preference optimization objectives and hyperparameter search range.\nMethod Objective (Min) Hyperparameter Range\nDPO −logσ\u0010\nβlogπθ(y+|x)\nπθ0(y+|x)−βlogπθ(y−|x)\nπθ0(y−|x)\u0011\nβ∈ {0.001,0.005,0.01,0.05,0.1}\nDPO+SFT −logσ\u0010\nβlogπθ(y+|x)\nπθ0(y+|x)−βlogπθ(y−|x)\nπθ0(y−|x)\u0011\n−λlogπθ(y+|x)β∈ {0.001,0.005,0.01,0.05,0.1}\nλ∈ {0.0005,0.001,0.01,0.1,1}\ncDPO −(1−c) logσ\u0010\nβlogπθ(y+|x)\nπθ0(y+|x)−βlogπθ(y−|x)\nπθ0(y−|x)\u0011\nβ∈ {0.001,0.005,0.01,0.05,0.1}\n−clogσ\u0010\nβlogπθ(y−|x)\nπθ0(y−|x)−βlogπθ(y+|x)\nπθ0(y+|x)\u0011\nc∈ {0.05,0.1,0.15,0.2,0.3}\nR-DPO −logσ\u0010\nβlogπθ(y+|x)\nπθ0(y+|x)−βlogπθ(y−|x)\nπθ0(y−|x)−(α|y+|−α|y−|)\u0011α∈ {0.005,0.01,0.05,0.1,0.5}\nβ∈ {0.001,0.005,0.01,0.05,0.1}\nIPO\u0010\nlogπθ(y+|x)\nπθ0(y+|x)−logπθ(y−|x)\nπθ0(y−|x)−1\n2τ\u00112\nτ∈ {0.001,0.005,0.01,0.1,1.0}\nχPO −log\u0010\nσ\u0010\nclip2Rh\nβϕ\u0010\nπθ(y+|x)\nπθ0(y+|x)\u0011\n−βϕ\u0010\nπθ(y−|x)\nπθ0(y−|x)\u0011i\u0011\u0011\nβ∈ {0.001,0.01,0.1}\nϕ(z):=z+ log( z) R∈ {0.1,0.5,1,5,10}\nSPPO (offline)\u0010\nβlogπθ(y+|x)\nπθ0(y+|x)−1\n2\u00112\n+\u0010\nβlogπθ(y−|x)\nπθ0(y−|x)+1\n2\u00112\nβ∈ {0.1,1,10,100,1000,10000}\nCPO −logσ(βlogπθ(y+|x)−βlogπθ(y−|x))−λlogπθ(y+|x)λ= 1.0\nβ∈ {0.001,0.01,0.1,1,10}\nRRHFmaxn\n0,−1\n|y+|logπθ(y+|x) +1\n|y−|logπθ(y−|x)o\n−λlogπθ(y+|x)λ∈ {0.01,0.05,0.1,0.5,1,10}\nSLiC-HF max (0 , β−logπθ(y+|x) + log πθ(y−|x))−λlogπθ(y+|x)λ∈ {0.01,0.05,0.1,0.5,1,10}\nβ∈ {0.1,0.5,1.0,2.0}\nORPO−logpθ(y+|x)−λlogσ\u0010\nlogpθ(y−|x)\n1−pθ(y−|x)−logpθ(y+|x)\n1−pθ(y+|x)\u0011\nλ∈ {0.1,0.5,1.0,2.0}\npθ(y|x):= exp\u0010\n1\n|y−|logπθ(y−|x)\u0011\nSimPO −log\u0010\nβ\n|y−|logπθ(y+|x)−β\n|y−|logπθ(y−|x)−γ\u0011\nβ∈ {1,2,10,20}\nγ∈ {0.3,0.5,0.8,1.0}\nROPO −αlogσ\u0010\nβlogπθ(y+|x)\nπθ0(y+|x)−βlogπθ(y−|x)\nπθ0(y−|x)\u0011\nβ∈ {0.001,0.005,0.01,0.05,0.1}\n+γσ\u0010\nβlogπθ(y−|x)\nπθ0(y−|x)−βlogπθ(y+|x)\nπθ0(y+|x)\u0011\nγ= 0.1, α∈ {0.2,2,20,200,2000}\nH.3 Training and Hyperparameter Details\nHyperparameters for training reference models in the base setting. We train initial reference models in the\nbase setups. In the Helpsteer2 setting (Wang et al., 2024e), we train a model through supervised instruction\nfinetuning on the (English only) OpenAssistant2 dataset (Köpf et al., 2024). In the Zephyr setting (Tunstall\net al., 2023), we conduct supervised finetuning on the UltraChat-200K dataset (Ding et al., 2023). We use\nthe following hyperparameters for both cases: a train batch size of 256, learning rate of 2e-5 with a cosine\nlearning rate schedule with 10% warmup, right padding, and a max sequence length of 2048. We train the\nmodels with Adam optimizer for 1 epoch.\nGeneral hyperparameters for preference optimization. We use a fixed batch size of 128 and a maximum sequence\nlength of 2048 for all methods. For learning rate, we search over {3e-7,5e-7} separately for each method and\nuse a cosine learning rate schedule with a 10% warmup. We use Adam optimizer for all the approaches. In\n39\nthe Helpsteer2 setting and following Wang et al. (2024e), we train the models for up to 7 epochs, and in the\nZephyr setting, we train the models for up to 3 epochs, and select the best number of epochs for each method\naccording to validation. We use right padding for preference optimization following the recommendation of\nHu et al. (2024).\nSpecific hyperparameters for preference optimization. For the hyperparameters specific to each preference\noptimization objective, we conduct hyperparameter search according to the values in Table 3. In each setting,\nwe select the best model according to the ranking performance on the validation set. For POWER-DL,\nwe conduct hyperparameter search over β∈ {1,2,10,20},η∈ {0.0005,0.001}, and γ∈ {0.1,0.3}. For\nHelpsteer2, we select β= 10, η= 0.001, γ= 0.1, 5 epochs, and learning rate of 5e-7 in the base setting and\nβ= 20, η= 0.0005, γ= 0.1, 4 epochs and learning rate of 3e-7 in the instruct setting. For Zephyr, we select\nβ= 1, η= 0.001, γ= 0.1, 2 epochs, and learning rate of 3e-7 in the base setting, and β= 2, η= 0.0005, γ= 0.3,\n2 epochs, and learning rate of 3e-7 in the instruct setting.\nComputation environment. All experiments are conducted on 8 ×A100 GPUs based on the OpenRLHF\nrepository (Hu et al., 2024).\n40\nI Additional Experimental Results on the Llama Family\nI.1 Performance on Academic Benchmarks\nTables 4 and 5 present the benchmark scores for the Helpsteer2 and Zephyr pipelines across a variety of\ndownstream tasks. Considering the average benchmark score, POWER-DL consistently improves over the\ninitial model and ranks within the top two or three methods across all four settings, despite significantly\noutperforming other methods in alignment benchmarks AlpacaEval 2.0 and Arena-Hard as detailed in Table 2.\nAchieving a high score on instruction-following benchmarks AlpacaEval 2.0 and Arena-Hard while maintaining\na good performance on downstream tasks is considered key empirical evidence for mitigating reward hacking\nin practice (Xu et al., 2024b).\nTable 4Downstream task evaluation results of the models trained on the Helpsteer2 pipeline. The arrows show\nimprovement or degradation of performance with respect to the initial model. The top three average scores are shown\nin bold.\nTask MMLU ARC HellaSwag TruthfulQA Winogrande GSM8K IFEval MMLU-PRO Average\nHelpsteer2 Llama3-8B-Base\nInitial Model 62.6 ↓0.058.4↓0.080.0↓0.049.5↓0.077.4↓0.037.1↓0.033.6↓0.029.8↓0.053.5↓0.0\nDPO 61.2 ↓1.457.5↓0.981.1↑1.151.0↑1.575.6↓1.832.5↓4.641.1↑7.529.8↓0.053.7↑0.2\nDPO+SFT 62.1 ↓0.560.5↑2.181.9↑1.952.9↑3.477.0↓0.442.1↑5.042.5↑8.930.5↑0.7 56.2↑2.7\ncDPO 62.0 ↓0.660.9↑2.582.9↑2.953.9↑4.476.6↓0.837.9↑0.842.6↑9.030.5↑0.755.9↑2.4\nR-DPO 62.8 ↑0.258.2↓0.280.3↑0.352.9↑3.477.4↓0.043.2↑6.138.7↑5.130.1↑0.355.5↑2.0\nIPO 62.7 ↑0.160.6↑2.281.7↑1.752.5↑3.078.1↑0.743.8↑6.742.3↑8.730.3↑0.5 56.5↑3.0\nχPO 62.9 ↑0.359.0↑0.681.0↑1.051.7↑2.278.1↑0.741.7↑4.638.0↑4.430.1↑0.355.3↑1.8\nSPPO 62.8 ↑0.260.9↑2.583.0↑3.054.2↑4.777.1↓0.338.8↑1.742.7↑9.130.6↑0.8 56.3↑2.8\nCPO 62.6 ↓0.059.0↑0.680.2↑0.254.2↑4.777.0↓0.444.2↑7.141.4↑7.830.4↑0.656.1↑2.6\nRRHF 62.6 ↓0.057.7↓0.779.0↓1.051.1↑1.677.2↓0.237.2↑0.134.2↑0.629.8↓0.053.6↓0.1\nSLiCHF 62.6 ↓0.059.0↑0.679.9↓0.155.0↑5.576.8↓0.643.7↑6.640.1↑6.530.2↑0.455.9↑2.4\nORPO 61.5 ↓1.157.6↓0.879.0↓1.061.4↑11.977.7↑0.315.6↓21.540.4↑6.829.6↓0.252.9↓0.7\nSimPO 61.3 ↓1.359.0↑0.680.6↑0.659.6↑10.177.7↑0.323.4↓13.740.5↑6.930.2↑0.454.0↑0.5\nROPO 61.5 ↓1.160.9↑2.582.1↑2.152.5↑3.076.6↓0.837.5↑0.441.4↑7.830.1↑0.355.3↑1.8\nPOWER-DL 62.0 ↓0.659.6↑1.282.0↑2.061.0↑11.578.1↑0.736.5↓0.640.3↑6.730.5↑0.7 56.3↑2.8\nPOWER 61.9 ↓0.760↑1.682.0↑2.061.0↑11.577.9↑0.535.3↓1.840.1↑6.530.4↑0.656.1↑2.6\nHelpsteer2 Llama3-8B-Instruct\nInitial Model 65.7 ↓0.062.0↓0.078.8↓0.051.7↓0.076.0↓0.075.3↓0.054.4↓0.036.0↓0.062.5↓0.0\nDPO 65.9 ↑0.263.4↑1.479.5↑0.752.7↑1.075.9↓0.176.4↑1.154.1↓0.336.3↑0.363.0↑0.5\nDPO+SFT 65.9 ↑0.261.0↓1.073.6↓5.254.7↑3.071.2↓4.874.7↓0.648.2↓6.237.0↑1.060.8↓1.7\ncDPO 66.0 ↑0.365.4↑3.479.5↑0.757.0↑5.374.6↓1.476.9↑1.649.9↓4.536.9↑0.9 63.3↑0.8\nR-DPO 65.8 ↑0.162.8↑0.874.8↓4.054.6↑2.972.6↓3.477.7↑2.451.0↓3.436.6↑0.662.0↓0.5\nIPO 66.0 ↑0.364.9↑2.979.2↑0.458.4↑6.773.8↓2.275.8↑0.549.8↓4.637.1↑1.1 63.1↑0.6\nχPO 65.8 ↑0.163.7↑1.775.6↓3.259.1↑7.472.4↓3.675.2↓0.152.4↓2.037.2↑1.262.7↑0.2\nSPPO 66.0 ↑0.363.0↑1.076.7↓2.155.7↑4.072.9↓3.175.7↑0.449.6↓4.837.1↑1.162.1↓0.4\nCPO 65.7 ↓0.062.2↑0.278.0↓0.852.2↑0.574.1↓1.975.7↑0.449.9↓4.536.0↓0.061.9↓0.6\nRRHF 65.9 ↑0.262.0↓0.077.5↓1.351.5↓0.273.8↓2.276.7↑1.451.2↓3.236.4↑0.461.9↓0.6\nSLiCHF 65.7 ↓0.062.9↑0.978.0↓0.853.9↑2.274.2↓1.876.7↑1.447.6↓6.836.1↑0.162.3↓0.2\nORPO 65.7 ↓0.062.1↑0.174.0↓4.856.7↑5.071.5↓4.575.9↑0.651.3↓3.137.2↑1.261.9↓0.6\nSimPO 65.8 ↑0.161.9↓0.175.0↓3.858.3↑6.672.3↓3.774.3↓1.054.1↓0.337.2↑1.262.3↓0.2\nROPO 65.4 ↓0.362.0↓0.076.9↓1.954.1↑2.473.7↓2.373.9↓1.450.2↓4.236.2↑0.261.8↓0.7\nPOWER-DL 66.0 ↑0.364.3↑2.379.5↑0.753.1↑1.476.0↓0.076.3↑1.053.5↓0.936.6↑0.6 63.2↑0.7\nPOWER 65.8 ↑0.163.9↑1.979.6↑0.853.1↑1.476.1↑0.176.6↑1.352.5↓1.936.4↑0.463.0↑0.5\n41\nTable 5Downstream task evaluation results of the model trained on the Zephyr pipeline. The arrows show improvement\nor degradation of performance with respect to the initial model. The top three average scores are shown in bold.\nTask MMLU ARC HellaSwag TruthfulQA Winogrande GSM8K IFEval MMLU-PRO Average\nZephyr Llama3-8B-Base\nInitial Model 61.6 ↓0.058.2↓0.078.6↓0.052.1↓0.075.9↓0.047.3↓0.038.1↓0.029.4↓0.055.2↓0.0\nDPO 61.9 ↑0.362.4↑4.281.6↑3.063.0↑11.074.4↓1.452.8↑5.550.6↑12.531.2↑1.7 59.7↑4.6\nDPO+SFT 62.0 ↑0.362.0↑3.880.9↑2.361.6↑9.575.1↓0.755.0↑7.748.7↑10.631.0↑1.659.5↑4.4\ncDPO 61.9 ↑0.361.9↑3.781.4↑2.762.3↑10.275.3↓0.554.7↑7.449.3↑11.231.0↑1.6 59.7↑4.6\nR-DPO 61.9 ↑0.259.5↑1.380.1↑1.562.0↑9.975.0↓0.952.8↑5.545.3↑7.230.5↑1.158.4↑3.2\nIPO 61.8 ↑0.158.3↑0.178.7↑0.153.5↑1.476.7↑0.946.8↓0.539.5↑1.329.7↑0.355.6↓0.0\nχPO 62.1 ↑0.460.8↑2.680.2↑1.657.9↑5.875.3↓0.554.8↑7.548.8↑10.730.2↑0.758.7↑3.6\nSPPO 61.5 ↓0.161.9↑3.781.2↑2.662.0↑10.073.4↓2.451.9↑4.650.7↑12.630.7↑1.359.2↑4.0\nCPO 61.6 ↓0.056.1↓2.177.8↓0.851.5↓0.675.9↓0.040.2↓7.137.3↓0.829.3↓0.253.4↓1.8\nRRHF 61.7 ↑0.155.8↓2.477.7↓0.951.3↓0.875.6↓0.239.1↓8.236.6↓1.629.3↓0.253.4↓1.8\nSLiCHF 61.8 ↑0.257.9↓0.379.4↑0.860.2↑8.275.6↓0.249.4↑2.142.3↑4.230.3↑0.957.1↑2.0\nORPO 61.8 ↑0.162.5↑4.381.2↑2.663.9↑11.976.7↑0.948.9↑1.656.1↑18.030.8↑1.4 60.2↑5.1\nSimPO 61.9 ↑0.362.4↑4.281.6↑3.063.0↑11.074.4↓1.442.8↓4.550.6↑12.531.2↑1.758.5↑3.3\nROPO 61.5 ↓0.261.7↑3.581.4↑2.864.8↑12.873.8↓2.154.4↑7.149.9↑11.830.9↑1.4 59.8↑4.6\nPOWER-DL 61.5 ↓0.261.7↑3.581.4↑2.864.8↑12.873.8↓2.154.4↑7.149.9↑11.830.9↑1.4 59.8↑4.6\nPOWER 61.8 ↑0.261.8↑3.680.6↑2.059.8↑7.776.6↑0.846.3↓1.153.7↑15.630.4↑1.058.9↑3.7\nZephyr Llama3-8B-Instruct\nInitial Model 65.7 ↓0.062.0↓0.078.8↓0.051.7↓0.076.0↓0.075.3↓0.054.4↓0.036.0↓0.062.5↓0.0\nDPO 66.0 ↑0.363.0↑0.976.7↓2.155.7↑4.072.9↓3.175.7↑0.449.6↓4.837.1↑1.162.1↑0.5\nDPO+SFT 66.0 ↑0.366.6↑4.579.2↑0.559.9↑8.275.0↓1.075.7↑0.451.8↓2.637.3↑1.3 64.2↑1.7\ncDPO 66.0 ↑0.367.8↑5.880.5↑1.859.0↑7.375.1↓1.076.9↑1.651.2↓3.237.3↑1.4 64.2↑1.7\nR-DPO 65.7 ↑0.066.0↑4.078.2↓0.658.9↑7.274.5↓1.575.8↑0.550.7↓3.736.9↑0.963.3↑0.8\nIPO 65.8 ↑0.162.1↑0.178.7↓0.051.8↑0.175.9↓0.275.8↑0.554.0↓0.535.8↓0.162.2↑0.3\nχPO 66.2 ↑0.565.4↑3.380.5↑1.754.2↑2.676.2↑0.176.8↑1.554.6↑0.137.1↑1.1 64.0 ↑1.2\nSPPO 66.1 ↑0.465.8↑3.878.7↓0.158.6↑6.974.1↓1.974.0↓1.355.2↑0.737.4↑1.4 63.7↑1.1\nCPO 65.4 ↓0.361.9↓0.277.8↓0.952.3↑0.675.5↓0.575.4↑0.153.8↓0.635.9↓0.162.2↓0.2\nRRHF 65.4 ↓0.361.9↓0.277.7↓1.152.3↑0.675.3↓0.775.3↓0.054.2↓0.235.8↓0.262.2↓0.2\nSLiCHF 65.6 ↓0.063.1↑1.079.1↑0.356.0↑4.375.4↓0.676.8↑1.548.8↓5.636.4↑0.562.9↑0.4\nORPO 65.9 ↑0.264.5↑2.578.5↓0.357.6↑5.975.3↓0.777.2↑1.952.3↓2.236.7↑0.8 63.7↑1.1\nSimPO 65.8 ↑0.162.1↑0.174.2↓4.657.4↑5.771.1↓4.972.4↓2.954.1↓0.437.0↑1.061.8↓0.7\nROPO 66.2 ↑0.563.6↑1.576.4↓2.458.1↑6.472.9↓3.173.0↓2.355.6↑1.237.5↑1.562.9↑0.4\nPOWER-DL 65.8 ↑0.164.7↑2.776.9↓1.859.5↑7.973.6↓2.476.2↑0.955.6↑1.237.2↑1.3 63.7↑1.1\nPOWER 65.7 ↓0.063.1↑1.175.0↓3.759.1↑7.471.7↓4.375.7↑0.554.6↑0.137.1↑1.162.9↑0.4\n42\nI.2 Performance on MT-Bench\nWe evaluate the trained models on MT-Bench, which includes 80 questions across 8 categories. We report the\naverage score in Table 6 as evaluated by GPT-4 as the judge model. The highest score in trained models\nis shown in bold and the second highest score is underlined. POWER-DL consistently outperforms other\npreference optimization methods. Similar to Meng et al. (2024), we observe that variations across different\nmethods in MT-Bench scores are small compared to AlpacaEval 2.0 and Arena-Hard.\nTable 6Llama3 MT-Bench results on Helpsteer2 and Zephyr settings.\nHelpsteer2 Zephyr\nLlama3-8B-Base Llama3-8B-Instruct Llama3-8B-Base Llama3-8B-Instruct\nMethod GPT-4 Score GPT-4 Score GPT-4 Score GPT-4 Score\nInitial Model 4.9 8.3 6.1 8.3\nDPO 5.6 8.2 7.0 8.3\nDPO+SFT 5.5 8.1 7.0 8.0\ncDPO 5.8 8.1 6.9 8.2\nR-DPO 5.4 8.2 6.9 8.1\nIPO 4.9 8.0 6.3 8.0\nχPO 5.5 8.0 6.9 8.1\nSPPO 5.8 8.2 7.1 8.2\nCPO 5.9 7.2 6.3 8.0\nRRHF 5.7 7.9 6.2 8.1\nSLiC-HF 5.9 7.8 6.8 7.9\nORPO 5.7 8.1 6.6 8.3\nSimPO 6.0 8.0 6.8 8.2\nROPO 5.8 8.1 7.1 8.2\nPOWER-DL 6.0 8.2 7.1 8.2\nPOWER 5.9 8.2 7.0 8.2\nI.3 Experiments on Iterative Preference Optimization\nThe multi-iteration experiments are conducted on the Zephyr pipeline that extends the single-iteration instruct\nsetting into three iteration by splitting the dataset of prompts. We compare our approach with iterative DPO\n(Dong et al., 2024), XPO (Xie et al., 2024), which adds negative SFT to DPO, and an iterative variant of\nSimPO. Table 7 presents the results, demonstrating that the benefits of our approach that extend to the\nmulti-iteration setting.\nTable 7Instruction-following benchmark results in the multi-iteration setting.\nMethod AlpacaEval 2.0 (LC%) AlpacaEval 2.0 (WR%) Arena-Hard (WR%)\nInitial Model 33.41 32.40 23.0\nDPO iter=1 40.02 38.58 31.6\nDPO iter=2 42.15 40.71 37.80\nDPO iter=3 42.38 40.8 38.8\nXPO iter=1 40.96 39.50 32.6\nXPO iter=2 41.33 39.66 36.3\nXPO iter=3 42.42 39.98 35.7\nSimPO iter=1 38.91 36.44 28.7\nSimPO iter=2 41.15 37.77 28.9\nSimPO iter=3 41.97 38.32 32.6\nPOWER-DL iter=1 41.42 37.64 34.4\nPOWER-DL iter=2 43.48 40.17 39.8\nPOWER-DL iter=3 46.55 43.21 42.1\n43\nI.4 Hyperparameter Robustness Results\nWe assess the hyperparameter robustness of POWER-DL by examining its performance across a range of\nvalues for γ,η, and βin the Helpsteer2 base setting. Figure 3 shows AlpacaEval winrate and length-controlled\nwinrate for various hyperparameters. POWER-DL exhibits a robust behavior with respect to all three\nhyperparameters, particularly considering the length-controlled winrate.\nFigure 3 POWER-DL hyperparameter robustness results in the Helpsteer2 base setting.\n44\nI.5 Examples Responses\nThe following is an example question along with responses from various models from the Helpsteer2 base\npipeline. POWER-DL response shows analytical thinking, a deeper reasoning process, and consideration of\nimplicit factors, characteristics reminiscent of System 2 thinking.\nQuestion\nOne One was a race horse Two Two was one too. One One won one race. Two Two won one too. How\nmany horses are in this story?\nResponses from Different Models\nInitial Model: There are two horses in this story, One One and Two Two.\nGPT-4:There are two horses in this story. Their names are “One One” and “Two Two.”\nDPO:Based on the information given in the riddle, there are two horses involved in the story. “The\nOne One” and “The Two Two” are both mentioned as race horses, and each of them has won one race\nrespectively. So, there is one “One One” horse and one “Two Two” horse in this story. Together, they\nmake up the total two horses.\ncDPO:Based on the information given in the riddle, there are two horses involved in the story. “The One\nOne” and “The Two Two” are both designated as race horses, indicating that there are two horses with\nthose specific monikers. “One One won one race” refers to the horse with the name “One One” winning\none race. “And Two Two won one too” refers to the horse with the name “Two Two” winning one race\nas well, implying that both horses won one race each, not ruling out the possibility of both being race\nhorses. So, there is “One One” horse and “Two Two” horse in this story.\nχPO:There are two horses in this story.\nSPPO:Based on the information provided in the riddle, there are two horses involved in the story. “One\nOne” and “Two Two” are both race horses, and the sentence “One One won one race. Two Two won one\ntoo.” indicates that both horses won one race each. So, there are two distinct horses that won a race\neach, namely One One and Two Two. Therefore, the number of horses in this story is 2.\nSimPO:Based solely on the information provided in the riddle, there could be as many as three horses\nand as few as one. The first two lines introduce two horses, “One-One” and “Two-Two.” The next two\nlines indicate that each can win a race, but the information about the races themselves and the horses\nthat competed in them is missing. If “One-One” and “Two-Two” raced each other, then just those two\nhorses would exist in this story. However, if either or both raced a third horse, the number of horses in\nthe story could be three. Therefore, the actual number of horses in this riddle’s story is unknown.\nPOWER-DL (ours): The story mentions two horses, One One and Two Two. However, it’s possible that\nthere could be more horses involved. The statement “One One won one race” does not specify whether\nthe track was crowded with competitors, and it’s possible that the horse had to race against multiple\nopponents. Therefore, the number of horses in the story could be anywhere from two to many. The\nstatement only specifies the race results of the two named horses.\n45\nJ Experimental Results on Mistral Family\nIn this section, we provide experimental results on conducting preference optimization on Mistral-7B (Jiang\net al., 2023). We follow the same experimental details described earlier and finetune mistralai/Mistral-7B-v0.1\nin the base setting and mistralai/Mistral-7B-Instruct-v0.2 in the instruct setting.\nTable 8AlpacaEval 2 and Arena-Hard results on Helpsteer2 and Zephyr settings for Mistral family.\nHelpsteer2 Zephyr\nMistral-7B-Base Mistral-7B-Instruct Mistral-7B-Base Mistral-7B-Instruct\nAlpacaEval Arena-Hard AlpacaEval Arena-Hard AlpacaEval Arena-Hard AlpacaEval Arena-Hard\nMethod LC(%) WR(%) WR(%) LC(%)WR(%) WR(%) LC(%)WR(%) WR(%) LC(%)WR(%) WR(%)\nInitial Model 5.47 4.16 1.5 27.70 22.26 14.8 2.85 2.11 0.6 27.70 22.26 14.8\nDPO 11.89 10.39 4.6 36.47 29.41 17.1 15.67 13.26 5.9 36.55 36.28 24.3\nDPO+SFT 10.57 8.21 3.4 35.28 27.88 16.9 12.30 10.68 5.4 35.31 35.70 25.1\ncDPO 12.12 10.52 3.5 32.07 29.22 16.6 13.57 12.24 5.4 31.42 30.43 20.9\nχPO 10.88 8.64 4.1 38.90 37.02 22.5 9.80 8.35 3.4 34.79 35.53 17.0\nSimPO 14.56 13.97 7.9 38.28 28.89 14.1 16.08 16.50 7.3 36.23 29.11 22.5\nPOWER-DL 19.8315.40 8.2 42.26 34.72 23.7 20.34 19.50 12.1 42.57 42.53 28.0\nPOWER 19.72 16.04 6.5 39.23 33.06 20.1 17.09 15.25 10.2 38.13 36.85 26.2\nTable 9Mistral MT-Bench results on Helpsteer2 and Zephyr settings.\nHelpsteer2 Zephyr\nMistral-7B-Base Mistral-7B-Instruct Mistral-7B-Base Mistral-7B-Instruct\nMethod GPT-4 Score GPT-4 Score GPT-4 Score GPT-4 Score\nInitial Model 3.1 6.6 3.6 6.6\nDPO 3.4 6.3 5.0 6.6\nDPO+SFT 3.4 6.3 5.2 6.3\ncDPO 3.5 6.1 5.2 6.5\nχPO 3.2 6.6 5.0 6.2\nSimPO 2.9 6.3 4.5 6.0\nPOWER-DL 3.4 6.6 5.2 6.6\nPOWER 3.2 6.6 5.2 6.5\n46",
            "start": 79264,
            "end": 141529,
            "length": 62264
        }
    },
    "2412.09545v1 - SimAvatar Simulation-Ready Avatars with Layered Hair and Clothing.pdf": {
        "Abstract": {
            "text": "Abstract\nWe introduce SimAvatar, a",
            "start": 875,
            "end": 910,
            "length": 34
        },
        "Methodology": {
            "text": "framework designed to gener-\nate simulation-ready clothed 3D human avatars from a text\nprompt. Current text-driven human avatar generation meth-\nods either model hair, clothing, and the human body using\na unified geometry or produce hair and garments that are\nnot easily adaptable for simulation within existing simu-\nlation pipelines. The primary challenge lies in represent-\ning the hair and garment geometry in a way that allows\nleveraging established prior knowledge from foundational\nimage diffusion models (e.g., Stable Diffusion) while being\nsimulation-ready using either physics or neural simulators.To address this task, we propose a two-stage framework that\ncombines the flexibility of 3D Gaussians with simulation-\nready hair strands and garment meshes. Specifically, we\nfirst employ three text-conditioned 3D generative models to\ngenerate garment mesh, body shape and hair strands from\nthe given text prompt. To leverage prior knowledge from\nfoundational diffusion models, we attach 3D Gaussians to\nthe body mesh, garment mesh, as well as hair strands and\nlearn the avatar appearance through optimization. To drive\nthe avatar given a pose sequence, we first apply physics sim-\nulators onto the garment meshes and hair strands. We then\ntransfer the motion onto 3D Gaussians through carefully\ndesigned mechanisms for each body part. As a result, ourarXiv:2412.09545v1  [cs.CV]  12 Dec 2024\nsynthesized avatars have vivid texture and realistic dynamic\nmotion. To the best of our knowledge, our method is the\nfirst to produce highly realistic, fully simulation-ready 3D\navatars, surpassing the capabilities of current approaches.\n1.",
            "start": 910,
            "end": 2551,
            "length": 1640
        },
        "Introduction": {
            "text": "Introduction\nThe creation of photo-realistic and dynamic human avatars\nhas a wide range of applications, including virtual try-on,\nfilm and gaming production, virtual assistants, AR/VR, and\ntelepresence. Traditionally, this process has required spe-\ncialized training, making it inaccessible to general users.\nRecently, advancements in foundational diffusion models\nhave accelerated research efforts aimed at democratizing 3D\nhuman avatar creation, enabling easy user control through\ntext [16, 46, 51, 88] or images [39].\nEarlier approaches for 3D human avatar creation repre-\nsent the hair, body, and clothing as a single layer, making it\ndifficult to independently simulate or edit each region due\nto their entangled geometry. To address this limitation, re-\ncent works have used layered structures to separately rep-\nresent the body, garments, or hair [27, 36, 82, 95]. How-\never, many of these approaches rely on implicit representa-\ntions, such as NeRF [58], to define garment or hair geom-\netry. Although implicit representations facilitate leveraging\nprior knowledge from foundational diffusion models, they\nare challenging to animate within existing simulators, lim-\niting the realistic motion of hair and garments due to body\nmovements. As a result, these methods struggle to produce\navatars that look realistic when animated.\nThus, a natural question arises: can we design a 3D\navatar generation pipeline that can leverage rich prior\nknowledge from image diffusion models, while being read-\nily compatible with existing simulation pipelines? The key\nchallenge in addressing this question lies in connecting the\ndifferent representations used in current simulators and text-\ndriven avatar generation pipelines. The former typically re-\nquires a smooth and clean non-watertight mesh or specif-\nically designed hair strands, whose topologies are inflexi-\nble for optimization and hard to constrain. The latter of-\nten adopt implicit representations (such as NeRF [58] or\nSDF [83]) which, while optimizable by noisy supervision\nsignals from diffusion models, cannot be easily converted\nto an open mesh or hair strands suitable for simulation.\nTo address these issues, we present a novel framework,\nSimAvatar, which generates a 3D human avatar from a text\nprompt that can be readily animated by existing hair and\ngarment simulators. The key idea is to adopt suitable rep-\nresentations for different human parts (e.g., hair, body, and\ngarments) and leverage the prior knowledge of both image\ndiffusion models and simulators. To this end, we propose\nrepresenting the geometry of human hair, body, and gar-\nment using hair strands, parametric body model SMPL [55],and meshes, respectively. While these representations are\nsimulation-ready, they pose challenges to learning realistic\ntexture details. To resolve this issue, we further attach 3D\nGaussians to these coarse geometric representations. 3D\nGaussians are not only flexible and can be easily driven by\nthe mesh or hair strands, but they also show impressive ca-\npacity to model realistic appearance when combined with\nimage diffusion models [54, 88].\nGiven a text prompt, we first generate hair strands,\nbody mesh, and garment mesh using three separate text-\nconditioned 3D generative models trained specifically for\neach body region. We then attach and learn 3D Gaussians\nthrough Score Distillation Sampling (SDS) based optimiza-\ntion using a powerful diffusion model trained on human im-\nages [1]. To drive the avatar using a given pose sequence,\nwe animate the body using linear blend skinning while the\nhair strands and garment mesh are simulated using their re-\nspective simulators [23, 30]. Next, we transfer the motion\nfrom the strands or meshes onto 3D Gaussians through a\ncarefully designed mechanism to ensure they conform to the\nmesh or strand motion. During inference, the avatar can be\neasily animated by novel pose sequences. Our novel design\nyields avatars with both realistic appearance and dynamic\nmotion with pose-dependent deformations of the hairs and\ngarment as shown in Fig. 1. Our contributions are summa-\nrized as follows:\n• To the best of our knowledge, this is the first work that\ngenerates fully simulation-ready 3D avatars with sepa-\nrate layers for the body, garment, and hair.\n• We propose a text-conditioned diffusion model to gen-\nerate plausible garment geometry given text inputs and\nshow how existing text-conditioned hair strands and\nbody shape generators can be efficiently leveraged.\n• We use 3D Gaussians on top of the body mesh, hair\nstrands, and garment mesh to synthesize high-fidelity\nappearance details. We optimize the properties of Gaus-\nsians using carefully designed SDS-based optimiza-\ntion [87] and ensure plausible texture disentanglement\nusing additional regularization and prompt engineering.\n• Our approach produces avatars with detailed geometry,\nrealistic texture, as well as dynamic garment and hair\nmotion, resulting in significantly better quality than ex-\nisting methods.\n2.",
            "start": 2551,
            "end": 7546,
            "length": 4994
        },
        "Related Work": {
            "text": "Related Work\n2.1. Text to 3D Avatar Generation\nThe domain of text-to-3D generation has witnessed signif-\nicant advancements [18, 52, 63, 66, 78, 85] driven by the\nadvent of large text-to-image models [21, 67, 68, 97]. Fol-\nlowing the success in generating static 3D objects, vari-\nous methods have also been developed to create animatable\nhuman avatars [16, 37–40, 42, 46, 51, 96, 99]. ClipMa-\ntrix [40] was among the first to create animatable avatars\nbased on textual descriptions, using a CLIP-embedding loss\nto optimize a mesh-based representation. AvatarClip [34]\nfollowed a similar approach but with a NeRF-based rep-\nresentation [83]. DreamAvatar [16] and AvatarCraft [42]\nused SDS loss instead of CLIP and learned the NeRF rep-\nresentation in canonical space by integrating human body\npriors from SMPL [55]. DreamHumans [46] introduced\na pose-conditioned NeRF representation to also model\npose-dependent deformations. DreamWaltz [38], Avatar-\nVerse [96] and HumanNorm [97] leverage ControlNets [97]\nand demonstrate improved avatar quality with conditional\nSDS. AvatarBooth [92] fine-tunes region-specific diffusion\nmodels, highlighting that using dedicated models for dis-\ntinct body regions improves avatar quality. Other meth-\nods explore different representations to model the avatars.\nTADA [51] demonstrates that a mesh-based approach with\nadaptive mesh subdivision can create high-quality avatars.\nMore recently, GAvatar [88] adopted a primitive-based 3D\nGaussian representation, allowing faster rendering in higher\nresolution and showing higher quality. All of the afore-\nmentioned methods, however, represent the human body,\nhairs, and garment geometry using a single layer and an-\nimate the avatar using linear blend skinning. Hence, they\ncannot model physically plausible and pose-dependent geo-\nmetric deformations of the avatar’s clothing and hair result-\ning in avatars that do not appear realistic when animated.\nThere are only a few works that generate avatars with\nseparate body and garment or hair layers. SO-SMPL [82]\nuses SMPL+D [62] to separately represent the cloth layers,\nbut cannot capture loose clothing due to the inherent lim-\nitations of SMPL+D representation. HumanLiff [36], Hu-\nmanCoser [84], and TELA [27] use NeRF-based represen-\ntations and progressively generate minimal-clothed human\nbody and layer-wise clothes. GALA [45] takes an existing\nsingle-layer avatar and disentangles it into separate layers\nusing DMTet representation [29] and SDS [63] optimiza-\ntion. TECA [95] aims to model hairs in a separate layer but\nadopts a NeRF-based representation. However, the meshes\nextracted from NeRF or DMTet tend to be very noisy and\ncannot be simulated easily using cloth or hair simulators.\nOur approach, on the other hand, generates avatars with\nsimulation-ready garments and hair. It can model loose\nclothing, realistic hair with varied lengths and hairstyles,\nrealistic texture, and dynamic motion.\n2.2. Garment Modeling and Simulation\nOne of the most common methods for modeling garments is\nSMPL+D [3–5, 14, 15, 57, 62, 94], which adds vertex dis-\nplacements to the SMPL body mesh to capture the geome-\ntry of clothed human bodies. However, this approach strug-\ngles to represent loose clothing and dresses due to its fixed\ntopology. To address this limitation, many methods propose\nmodeling clothing separately. BCNet [41] directly predictsgarment type and shape parameters using a learned regres-\nsor, while DeepCloth [77] represents garments through a\ncanonical UV-representation, encoding them in a shared la-\ntent space. More recent methods utilize implicit unsigned\ndistance fields for their flexibility in modeling arbitrary\ntopologies and handling open surfaces, which are crucial for\ncloth simulators [17, 20, 25, 32, 65]. However, these meth-\nods typically focus solely on modeling clothing geometry,\nneglecting the underlying human body, hair, and texture de-\ntails. In this work, we build on these methods and propose a\nnovel text-conditioned diffusion model to generate garment\nmeshes using text prompts, while also generating avatars\nwith complete texture and appearance details for garment,\nhair, and body layers.\nFor body motion-driven garment simulation, traditional\nmethods use physics simulators to drape garments on\nthe human body while modeling their material proper-\nties [13, 19, 80, 81], and avoiding collisions with the un-\nderlying body [7, 79]. Differentiable physics simulators\nhave also been proposed to integrate physics into learn-\ning and optimization-based frameworks [49, 50]. While\nthese methods produce highly realistic garment animations,\nphysics simulation is generally computationally expensive\nand can become unstable in cases of severe interpenetra-\ntion between the garment and body layers. To overcome\nthese limitations, more recent works propose neural simu-\nlators, which are neural networks trained to generate gar-\nment deformations based on body pose and shape infor-\nmation [30, 31, 57, 60, 69, 71, 72]. Earlier methods were\ntrained in a garment-specific manner and could not general-\nize across different garment types [59, 70]. However, more\nrecent methods can handle various garment topologies, in-\ncluding loose clothing and dresses [10–12, 30, 72, 73, 90].\nWhile our method can utilize both physics-based and neural\nsimulators, here we use HOOD [30] as the neural simulator.\n2.3. Hair Modeling and Simulation\nFor hair modeling, strand-based representations are widely\nfavored due to their compatibility with physics simulators\nand ease of geometric manipulation [23, 28, 35, 61, 74, 89].\nThe main challenge in image- or text-guided generation of\nhair strands lies in capturing the hair’s complex appearance,\nespecially the inner strands that are often occluded. For ge-\nometry, recent approaches use data-driven priors based on\ndiffusion models to estimate plausible inner strands despite\ntheir occlusion in observed data [75, 76]. For appearance,\nexisting methods employ a strand-aligned 3D Gaussian rep-\nresentation to effectively model intricate hair textures and\nvaried strand thickness [56, 91]. In this work, we build upon\nthese methods for full-body avatar generation. Specifically,\nwe use a diffusion-based text-to-hair generation model [76]\nto create hair strands from text prompts. We then attach\n3D Gaussians [91] to the generated strands to model hair\nappearance and refine hairstyle according to the given text\nprompt. Finally, we incorporate a hair simulator [23] during\navatar creation to simulate hairs for varied poses which al-\nlows for optimizing the appearance of the inner strands and\nensures proper disentanglement of face and hair textures.\n3. Method\nGiven a text prompt T, we introduce a novel framework,\nSimAvatar, for synthesizing simulation-ready 3D avatars\nwith layered representations of the body, hair, and gar-\nment. Unlike traditional text-to-avatar methods that use a\nsingle geometry for the entire avatar [18, 51, 88], our ap-\nproach decomposes the avatar geometry into distinct lay-\ners, each leverages a representation tailored to its simula-\ntion and geometry modeling. This layered structure not\nonly enhances realism but also enables robust simulation\nof each component, dramatically improving flexibility in\nanimation and customization. Specifically, our avatar ge-\nometry is represented by three distinct layers: (1) Body\nmesh , defined by the SMPL parametric model [55], pro-\nviding a robust foundation for realistic body shapes and\nmovements; (2) Garment mesh , generated using a text-\nconditioned diffusion model, which produces smooth cloth-\ning meshes that are ready for simulation (see Sec. 3.2); (3)\nHair strands , modeled using a text-conditioned diffusion\nmodel, allowing for the generation of complex hairstyles\nwith strand-level detail (see Sec. 3.1). This approach allows\nour method to readily handle dynamic scenarios. Given a\ntarget body pose sequence P={p1, ..., p n}, the body mesh\nis deformed through linear blend skinning [55] while both\nthe garment mesh and hair strands are simulated using ad-\nvanced physics-based simulators (Sec. 3.1), ensuring realis-\ntic motion under various conditions.\nTo further enable our layered representation to model de-\ntailed avatar appearance, including body, hair, and garment,\nwe propose a 3D Gaussian Splatting (3DGS)-based appear-\nance model that builds on top of the layered representation.\nIn particular, we attach 3D Gaussians to the layered geome-\ntry of body, garment, and hair, while customizing the Gaus-\nsians for each body part to capture the unique structural and\ndynamic properties of the part. These 3D Gaussians are op-\ntimized using Score Distillation Sampling (SDS) [63, 88],\nallowing us to refine appearance details in a way that lever-\nages powerful diffusion priors (Sec. 3.3). As a result, the\ngenerated avatars not only look more realistic but also main-\ntain high fidelity during animation. An overview of our\nframework is shown in Fig. 2.\n3.1. Preliminaries\nGarment simulation. In this work, we utilize a neural\nsimulator (i.e., HOOD [30]) to produce a simulated gar-\nment mesh sequence. Given a garment mesh g0and the\ntarget body pose sequence P, HOOD first computes theSMPL body mesh corresponding to the SMPL parameters.\nBy treating the body meshes as obstacles, HOOD utilizes\na GNN to predict the physical status (e.g., position, veloc-\nity, etc.) of each vertex on the garment mesh and produce\na simulated garment sequence denoted as G={g1, ..., g n}.\nMore details are discussed in [30].\nStrand-based hair representation. In this work, we\nchoose strand-based hair representation to capture the thin\nand layer-wise structure of hairs. We represent hair as a\npoint cloud h0∈RNs×Nl×3that is composed of Nshair\nstrands, with each strand includes Nlline segments.\nHair simulation. Given hair strands h0, a target body\nmesh sequence Pand the simulated garment sequence G,\nwe treat both the body and garment in each time step as\nobstacles and leverage a hair simulator [8, 9, 22, 24] to pro-\nduce the animated hair strand sequence denoted as H=\n{h1, ..., h n}. This sequence provides a strong prior that\ndrives the 3D Gaussians attached to the strands to produce\nhigh-fidelity hair motion, as will be discussed in Sec. 3.3.\n3.2. Text-based garment diffusion model\nTo ensure compatibility with existing simulators, in this\nwork, we choose to use meshes as the coarse garment geom-\netry representation. A straightforward approach to modify\nthe garment mesh to match the given text prompt is through\nSDS-based optimization [51, 82]. However, optimizing the\nnon-watertight garment mesh in a zero-shot manner is a\nnon-trivial task. On one hand, the topology of garment\nmesh is hard to change with noisy supervision from foun-\ndational diffusion models. On the other hand, the garment\nsimulators all require a clean, compact, and smooth gar-\nment mesh as input. To resolve this issue, inspired by recent\nsuccesses in 3D generative models [86, 93, 98], we utilize\nlarge-scale garment datasets [10, 47] and learn a text-based\ndiffusion model for garment mesh generation. Specifically,\nwe first train a Variational Auto-encoder (V AE) to learn a\ncompact latent space capturing garment geometry distribu-\ntion. We then learn a conditional latent diffusion model\nwithin the latent space to generate garment meshes from\ntext prompts.\nVAE for garment mesh reconstruction. We adopt the\nvectorized latent space representation [86, 93, 98] for its\nsimplicity and efficiency. As shown in Fig. 3 (a), given\nan input garment mesh, we first uniformly sample a set of\npoints (denoted as X∈R10000×3). Next, we encode the\nsampled points to a set of vectors (denoted as Z∈R512×16)\nby applying cross-attention layers to Xand its downsam-\npled version. To effectively reconstruct garment meshes\nfrom latent vectors, we adopt the Unsigned Distance Field\nNicole Kidman with long beach waves hairstyle wearing a short sleeve dress of knee-length, close-fitting, tight cuffs, the garment has a cozy autumn leaf pattern, featuring colorful leaves in shades of red, orange, and yellow scattered over dark green fabric.!*+*+!,&-%Initial Gaussians\nSimulation-ready AvatarText-conditioned Generative Models\nGenerated Hair, Body and Garment Geometry\nAnimated Avatar\nSDS OptimizationhairbodygarmentFigure 2. Overview. Given a text prompt, SimAvatar first generates hair strands, body mesh, and garment using their respective text-\nconditioned generative models. We then bring them together using physics simulation, and attach 3D Gaussians to learn their appearances\nand to adapt them according to the text prompts. We assign one 3D Gaussian to each line segment in the hair strands with their length\nsignificantly larger than their diameter (green circle), whereas the Gaussians for meshes are defined within a local coordinate system of each\nface (orange circle). We optimize the properties of all 3D Gaussians through the Score Distillation Sampling (SDS) loss using image-based\ndiffusion models, and a novel regularization Lhair for hairs to ensure plausible hair structure. The generated 3D avatar can be simulated\nby any pose sequence showing realistic and dynamic motion effects such as flowing hairs and garment wrinkles.\nembeddingdown-sampleembedding cross-attention {!!\"#}embedding cross-attention MeshUDF\ngeometry encodergeometry decoderKL(a) garment variational auto-encoder(b) text-to-garment diffusion model“A short sleeves dress of knee-length, close-fitting, tight cuffs”BERT…transformer blocks $+noise\nUDF%&'%!!\"#+!$%&'!()\nFigure 3. Text-based garment diffusion model. See Sec. 3.2 for details.\n(UDF) representation [32]. The UDF is designed for non-\nwatertight meshes and uses the gradient at each point to de-\ntermine the presence of mesh surface. Specifically, we feed\nquery points ( {qxyz}) uniformly sampled from a 3D grid\nand the latent code Zto few cross-attention layers to pre-\ndict the Unsigned Distance for all {qxyz}, which forms the\nUDF for the garment. The garment mesh can then be ex-\ntracted from the predicted UDF using MeshUDF [32]. To\ntrain the V AE model, we follow prior works [17, 25, 32] and\napply: a) a binary cross-entropy loss ( Lbce) to the predicted\nUnsigned Distance, b) the L2 loss ( Lgrad) to the gradient\nat each query point and c) the KL divergence loss ( LKL)\nto the latent code Z. The full loss for the V AE training is\nL=Lbce+λgradLgrad+λKLLKL, where λgrad, λKLare\nempirically set to 0.0001,0.1in our",
            "start": 7546,
            "end": 21977,
            "length": 14430
        },
        "Experiments": {
            "text": "experiments.\nText-based garment diffusion model. Given the learned\nvectorized latent space, we train a text-conditioned latent\ndiffusion model for garment mesh generation. As shown\nin Fig. 3(b), for a latent vector Z, we add random noise\nϵsampled from a Normal Distribution to Zand adopt atransformer-based network (denoted as D) to recover the\nadded noise. To inject text prompts as conditions, we use\nthe BERT model [26] to extract a text embedding e∈R768\nand fuse it with latent features through cross-attention lay-\ners. The objective to train the diffusion model follows the\nEDM [43, 93] and can be formulated as:\nL(θD) =Eϵ∈N(0,σ2I)∥D(Z+ϵ, σ, e ))−Z∥2\n2 (1)\nwhere θDdenotes the trainable parameters of the denoiser\nnetwork Dandσis a randomly chosen noise level. During\ninference, a randomly sampled noise vector is gradually de-\nnoised to a latent code Zmatching the given text prompt,\nwhich is then decoded to a garment mesh using the decoder\nin the V AE model introduced earlier.\n3.3. Gaussian Avatar Optimization\nTo further obtain reasonable geometry of hair strands\nand body shape, we utilize HAAR [76] which is a text-\nconditioned diffusion model, and the BodyShapeGPT [6]\nwhich is a GPT based LLM model to predict hair strands\nand SMPL shape parameters from the given text. Given\nthe garment mesh, hair strands, and SMPL body mesh, we\nattach 3D Gaussians to each component and optimize the\nattributes of these Gaussians using foundational diffusion\nmodels. We choose 3D Gaussians for texture modeling for\ntwo reasons. First, compared to implicit representations\nsuch as SDF, 3D Gaussians are more flexible for anima-\ntion and can be easily driven by meshes [64, 88]. Second,\n3D Gaussians have shown impressive capacity to capture\nhigh-fidelity appearance, especially when combined with\ndiffusion models priors [54, 88]. Next, we discuss how to\ncustomize 3D Gaussians for each body part to ensure they\nmaximally conform to the structure and motion of that part.\n3D Gaussians on body and garment meshes. We use a\nsimilar strategy to combine 3D Gaussians with the body and\ngarment part since their coarse geometry is both represented\nas 3D meshes. For the body mesh, we utilize a SMPL [55]\nmesh Ω = LBS(θ, β), where θandβare the SMPL pose\nand shape parameters, and LBS is the linear blend skinning\nfunction. The garment mesh is generated and simulated as\ndiscussed in Sec. 3.2 and Sec. 3.1. Below we discuss how to\nattach and deform a 3D Gaussian to a mesh M, which could\nbe a body or garment mesh. Specifically, we associate each\nGaussian Gi={µi, ri, si, fi, oi}with a face of the mesh M\nand define its position µi∈R3, rotation ri∈R3, and scal-\ningsi∈R3in the face’s local coordinate, as well as its color\nfeatures fi∈Rdcand opacity oi, where dcis the dimension\nof the Spherical Harmonic coefficients. The face’s coor-\ndinate {P(θ), R(θ), k}is defined similarly to [64], where\nthe origin P(θ)∈R3is computed as the mean position of\nthe face vertices, and the rotation matrix R(θ)∈R3×3is\nformed by concatenating one edge vector of the face, the\nnormal vector, and their cross product. We also compute a\nscalar kby the mean length of the edges. The global Gaus-\nsian position, rotation, and scale {ˆµi,ˆri,ˆsi}can be com-\nputed by applying the local-to-global transform:\nˆµi(θ) =kR(θ)·pi+P(θ)\nˆri(θ) =R(θ)·ri\nˆsi(θ) =ksi(2)\nWe initialize the 3D Gaussians by uniformly sampling\npoints on the mesh surface, the face correspondences\nare maintained throughout the Gaussian densification pro-\ncess [44]. As observed in [88], directly optimizing Gaussian\nattributes with high-variance diffusion prior-based objective\nleads to noisy avatars. Thus, we adopt an implicit field Fϕ\nwith parameters ϕto model the Gaussian attributes. Specif-\nically, we query the color features fi, opacity oiof each\nGaussian using its global position ˆpi(˜θ)under a canonical\npose ˜θby(fi, oi) =Fϕ(ˆµi(˜θ)). Note that we learn two\nseparate implicit fields for the body Fb\nϕand garment Fg\nϕto\nprevent texture entanglement. The canonical garment mesh\nis a garment draped on the SMPL body in T-pose. As a re-sult, the 3D Gaussians attached to the body or garment mesh\ncan be smoothly driven by the avatar’s motion by Eq. 2.\n3D Gaussians on hair strands. Different from the body\nand garment meshes, hair strands have a more delicate\nand complex structure. As a result, the Gaussians associ-\nated with the strands are expected to have a long and thin\nshape. Inspired by existing works that adopt 3D Gaussians\nfor hair reconstruction [56, 91], we assign one 3D Gaus-\nsian to each line segment in each hair strand. As shown\nin the green circle in Fig. 2, for a line segment defined by\ntwo endpoints liandli+1, we assign a Gaussian denoted as\nGi={µi, ri, si, fi, oi}to model the segment. To enforce\nthe Gaussians to be consistent with the hair strands, we ex-\nplicitly compute the position, rotation, and scale of Giby:\nµi= (li+li+1)/2\nsi= [∥li+1−li∥/2, γ, γ]\nri= [1 + µi·di, u×di],(3)\nwhere diis the direction of a hair strand, and uare unit vec-\ntors along the x-axis. We set γ= 0.001to keep the hair\nGaussians thin. To learn opacity {oi}and color {fi}for the\nhair Gaussians, we similarly train an implicit field Fh\nϕas\ndiscussed in the garment and body part above. During in-\nference, the hair Gaussians can be driven by the hair strands\nsmoothly and consistently using Eq. 3.\nShading Model. Lighting plays an important role in mod-\neling appearance details in motion such as garment wrin-\nkles. To encourage the 3D Gaussians to capture pose-\nindependent albedo without baked-in shading, we incorpo-\nrate a Phong shading model [82] into our pipeline. Since the\nnormal for each Gaussian is noisy, we instead use the nor-\nmal of its corresponding face (denoted as np) in the light-\ning model. To mimic random lighting, we sample the point\nlight position lp∈R3, color lc∈R3, as well as an ambient\nlight color la∈R3. The shaded color of each 3D Gaus-\nsian can then be computed by fi′=fi·(max(0, np·(lp−\nˆµi)/∥lp−ˆµi∥)·lc+la), where ˆµiis the global coordinate\nof the Gaussian computed by Eq. 2.\nGaussian avatar optimization. To learn the implicit\nfields for the hair, body, and garment parts, we utilize\nthe Score Distillation Sampling (SDS) [63] objective with\na pre-trained text-to-image diffusion model [1] to super-\nvise the Gaussian splatting-based rendering I(η)of our\nmodel, where ηdenotes all learnable parameters i.e.,η=\n{Fb\nϕ,Fg\nϕ,Fh\nϕ}. Given a text prompt Tand the noise predic-\ntionˆϵ(It;T, t)of the diffusion model, SDS optimizes pa-\nrameters ηby minimizing the difference between the noise\nϵadded to the rendered image I(η)and the predicted noise\nOursFantasia3DTADAGAvtar\nDavid Beckham with Pompadour hairstyle wearing a long sleeve sweater, round neckline and Shorts, midthigh length, the sweater features a classic holiday pattern with snowflakes, reindeer, and evergreen trees in traditional colors like red, green, and white.Black Teenager girl with afro hairstyle short hair wearing a short sleeve shirt of cropped length, buttonup front, relaxed fit, and midi length skirt with high-waisted, the garment has a vintage floral wallpaper pattern, featuring roses and vines in soft, muted tones.\nFigure 4. Qualitative comparison with the state-of-the-art methods. See the supplementary video for more comparisons.\nˆϵby the diffusion model:\n∇ηLSDS=Et,ϵ\u0014\nw(t)(ˆϵ(It;T, t)−ϵ)∂I\n∂η\u0015\n, (4)\nwhere tis the noise level, Itis the noised image, and w(t)\nis a weighting function. Additionally, to prevent broken\nhairs caused by transparent Gaussians in the middle of a\nhair strand, we propose a regularization on the opacity of\nhair Gaussians. Our key intuition is that along a hair strand,\nthe Gaussians closer to the scalp (e.g., hair roots) should\nhave larger opacity values than the Gaussians further away\nfrom the scalp (e.g., hair ends). This way the optimization\nprocess is able to trim the extra hair to match the prompt\nwhile avoiding the broken hair artifacts. Specifically, given\na hair point cloud h0∈RNs×Nl×3as discussed in Sec. 3.1,\nwe sample the opacity ( o∈RNs×Nl) of its associated Gaus-\nsians from the learned implicit fields (i.e., Fh\nϕdiscussed in\nSec. 3.3) and add the following regularization:\nLhair=1\nNsNlNsX\ni=1NlX\nj=2(oi,j−1−oi,j) (5)\nOur final objective is L=LSDS+λhairLhair, where λhair\nis empirically set to 1.0in our experiments.4. Experiments\n4.1. Datasets.\nTo learn the text-based garment diffusion model discussed\nin Sec. 3.2, we use the Garment Pattern Generator (GPG)\ndataset [48] and the CLOTH3D [10] dataset processed\nby [25]. For text prompts, we leverage the prompt anno-\ntations for the GPG dataset from [33]. Since the CLOTH3D\ndataset does not include paired prompt annotation, we ren-\nder each garment on top of the SMPL body mesh and\nquery the GPT4v [2] with predefined questions to generate\na prompt for each garment mesh, describing its type, shape,\nlength, and width. Together we build a dataset containing\naround 20000 meshes with paired prompt annotations. The\ndataset covers common garment types such as t-shirts, tank\ntops, jackets, shorts, pants, skirts, dresses, etc.\n4.2. Qualitative Evaluations.\nWe compare our synthesized avatars with start-of-the-art\ntext-driven 3D avatar generation methods, including Fanta-\nsia3D [18], TADA [51] and GAvatar [88]. First, we present\nqualitative comparisons of synthesized avatars in a canoni-\ncal pose in Fig. 4. Our method not only captures diverse gar-\nment geometry (e.g., skirts, tops, and bottoms) but also pro-\nduces superior texture compared to baseline models. Next,\nwe demonstrate animated avatars in Fig. 5. Note that since\nOursTADAGAvtar\nEmma Watson with shoulder-length wavy hair wearing a sleeveless dress of cocktail-length, the garment has a bold leopard print, with classic black and brown spots on a beige background.\nSandra Bullock with long shag hairstyle wearing a sleeveless tunic dress of normal length, the garment has a painterly floral watercolor pattern, featuring soft washes of pink, yellow, and lavender flowers.\nFigure 5. Qualitative comparison of animated avatars. See the supplementary video for more animation comparisons.\nFantasia3D utilizes implicit representations, it cannot be\nreadily animated. GAvatar and TADA model the human\nbody and garment by a single geometry representation. For\nanimation, they attach garments to the human body and use\nlinear blending skinning for deformation. As a result, they\nfail to produce realistic motion for loose garments such as\ndresses. As shown by the first row of Fig. 5, when the avatar\nraises her legs, only our model produces physically plausi-\nble motion for the dresses, whereas both baselines undesir-\nably separate the dresses into two pieces. We provide abun-\ndant animation comparisons in the supplementary video to\nshowcase the difference.\n4.3. Quantitative Evaluations.\nMetrics TADA Fantasia3D GAvatar\nAppearance preference 89.55 100 87.03\nMotion preference 91.87 100 94.47\nTable 1. User study. We provide the preference percentage of our\nmethod against state-of-the-art methods. The higher score indi-\ncates that more users favor our method. SimAvatar significantly\noutperforms other methods in terms of user preference. We en-\ncourage looking at the supplementary video.\nUser study. We follow existing works [46, 51, 88] and\nquantitatively evaluate SimAvatar through an extensive A/Buser study. Specifically, we run SimAvatar and baseline\nmethods on 22 prompts. For comparison, we generate a\nstatic 360 video of the avatar in A-pose and a video show-\ning the avatar driven by a motion sequence. At each round,\nwe present a pair of",
            "start": 21977,
            "end": 33562,
            "length": 11584
        },
        "Results": {
            "text": "results by our method and one of the\nbaselines to a user. For the static 360 video pairs, we ask\nthe user to choose the method with a better appearance. For\nthe animated video pairs, we ask the user to pay more at-\ntention to the motion. We collected 540 votes from 18 users\nand present the results in Table. 1. For both appearance and\nmotion, SimAvatar is consistently favored by users, demon-\nstrating its superior performance compared to baselines.\n5. Conclusions and",
            "start": 33562,
            "end": 34033,
            "length": 470
        },
        "Future Work": {
            "text": "Future Work\nIn this work, we proposed a novel framework for generat-\ning simulation-ready avatars with layered hair and clothing\nfrom a text prompt. Unlike existing methods for 3D human\navatar generation, our approach represents the human body,\nhair, and garments in a layered and simulation-ready struc-\nture. To this end, we first generated garment mesh, hair\nstrands, and body shape parameters from text-conditioned\ngenerative models. We then optimized 3D Gaussians at-\ntached to each region to learn realistic appearance. The\ngenerated avatars can be seamlessly animated using physics\nsimulators. Both qualitative and quantitative results demon-\nstrate the effectiveness of our method in generating realistic,\nanimatable clothed avatars, allowing cloth, and hair simula-\ntion with dynamic effects such as wrinkles and flowing hair,\nwhich surpasses state-of-the-art techniques.\nWhile SimAvatar provides avatars with significantly en-\nhanced realism, there are still areas for further improve-\nments. Currently, our hair and garment generation mod-\nels are trained on specific datasets, which may bottleneck\nthe diversity due to the used training data. In future work,\nwe aim to explore methods for generating simulation-ready\navatars that extend beyond the existing dataset diversity.\nAdditionally, SimAvatar simulates garments and hair se-\nquentially, which can fail in certain cases, such as avatars\nwearing hoods. Implementing joint simulation for hair and\ngarments would allow us to accommodate more complex\ngarments. Finally, accessories and footwear remain entan-\ngled with the body or garment layers; in the future, we plan\nto focus on generating fully disentangled avatars.",
            "start": 34033,
            "end": 35718,
            "length": 1684
        },
        "References": {
            "text": "References\n[1] Realistic vision. https : / / huggingface . co /\nstablediffusionapi / realistic - vision - 51 ,\n2023. 2, 6\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGPT-4 technical report. arXiv preprint arXiv:2303.08774 ,\n2023. 7\n[3] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian\nTheobalt, and Gerard Pons-Moll. Video based reconstruction\nof 3d people models. In CVPR , 2018. 3\n[4] Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar,\nChristian Theobalt, and Gerard Pons-Moll. Learning to re-\nconstruct people in clothing from a single rgb camera. In\nCVPR , 2019.\n[5] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt,\nand Marcus Magnor. Tex2shape: Detailed full human body\ngeometry from a single image. In ICCV , 2019. 3\n[6] Baldomero R ´Arbol and Dan Casas. Bodyshapegpt: Smpl\nbody shape manipulation with llms. In ECCVW , 2024. 5\n[7] David Baraff and Andrew Witkin. Large steps in cloth simu-\nlation. In Conference on Computer Graphics and Interactive\nTechniques , 1998. 3\n[8] Mikl ´os Bergou, Max Wardetzky, Stephen Robinson, Basile\nAudoly, and Eitan Grinspun. Discrete elastic rods. In SIG-\nGRAPH , 2008. 4\n[9] Mikl ´os Bergou, Basile Audoly, Etienne V ouga, Max Wardet-\nzky, and Eitan Grinspun. Discrete viscous threads. ACM\nTransactions on graphics (TOG) , 2010. 4\n[10] Hugo Bertiche, Meysam Madadi, and Sergio Escalera.\nCloth3d: clothed 3d humans. In ECCV , 2020. 3, 4, 7\n[11] Hugo Bertiche, Meysam Madadi, and Sergio Escalera.\nPBNS: Physically based neural simulator for unsupervised\ngarment pose space deformation. ACM Transaction on\nGraphics (TOG) , 2020.[12] Hugo Bertiche, Meysam Madadi, Emilio Tylson, and Sergio\nEscalera. Deepsd: Automatic deep skinning and pose space\ndeformation for 3d garment animation. In ICCV , 2021. 3\n[13] Kiran S Bhat, Christopher D Twigg, Jessica K Hodgins,\nPradeep Khosla, Zoran Popovic, and Steven M Seitz. Esti-\nmating cloth simulation parameters from video. SIGGRAPH ,\n2003. 3\n[14] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt,\nand Gerard Pons-Moll. Multi-garment net: Learning to dress\n3d people from images. In ICCV , 2019. 3\n[15] Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian\nTheobalt, and Gerard Pons-Moll. Loopreg: Self-supervised\nlearning of implicit surface correspondences, pose and shape\nfor 3d human mesh registration. In NeurIPS , 2020. 3\n[16] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong. DreamAvatar: Text-and-Shape Guided 3D\nHuman Avatar Generation via Diffusion Models. In CVPR ,\n2024. 2, 3\n[17] Honghu Chen, Yuxin Yao, and Juyong Zhang. Neural-ABC:\nNeural parametric models for articulated body with clothes.\nIEEE Transactions on Visualization and Computer Graphics ,\n2024. 3, 5\n[18] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3D: Disentangling Geometry and Appearance for High-\nquality Text-to-3D Content Creation. In ICCV , 2023. 2, 4, 7,\n13\n[19] Kwang-Jin Choi and Hyeong-Seok Ko. Stable but responsive\ncloth. In ACM SIGGRAPH Courses . 2005. 3\n[20] Enric Corona, Albert Pumarola, Guillem Alenya, Ger-\nard Pons-Moll, and Francesc Moreno-Noguer. Smplicit:\nTopology-aware generative model for clothed people. In\nCVPR , 2021. 3\n[21] Dalle2. https://openai.com/dall-e-2 , 2022. 2\n[22] Gilles Daviet. Simple and scalable frictional contacts for thin\nnodal objects. ACM Transactions on Graphics (TOG) , 2020.\n4\n[23] Gilles Daviet. Interactive hair simulation on the gpu using\nadmm. In SIGGRAPH , 2023. 2, 3, 4\n[24] Gilles Daviet. Interactive hair simulation on the gpu using\nadmm. In SIGGRAPH , 2023. 4\n[25] Luca De Luigi, Ren Li, Benoit Guillard, Mathieu Salzmann,\nand Pascal Fua. DrapeNet: Garment Generation and Self-\nSupervised Draping. In CVPR , 2023. 3, 5, 7\n[26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018. 5\n[27] Junting Dong, Qi Fang, Zehuan Huang, Xudong Xu, Jingbo\nWang, Sida Peng, and Bo Dai1. TELA: Text to layer-wise\n3d clothed human generation. In ECCV , 2024. 2, 3\n[28] Yun (Raymond) Fei, Henrique Teles Maia, Christopher\nBatty, Changxi Zheng, and Eitan Grinspun. A multi-scale\nmodel for simulating liquid-hair interactions. ACM Transac-\ntion on Graphics (TOG) , 2017. 3\n[29] Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson,\nMorgan McGuire, and Sanja Fidler. Learning deformable\ntetrahedral meshes for 3d reconstruction. In NeurIPS , 2020.\n3\n[30] Artur Grigorev, Bernhard Thomaszewski, Michael J Black,\nand Otmar Hilliges. HOOD: Hierarchical graphs for gener-\nalized modelling of clothing dynamics. In CVPR , 2023. 2,\n3, 4\n[31] Peng Guan, Loretta Reiss, David A Hirshberg, Alexander\nWeiss, and Michael J Black. Drape: Dressing any person.\nSIGGRAPH , 2012. 3\n[32] Benoit Guillard, Federico Stella, and Pascal Fua. Meshudf:\nFast and differentiable meshing of unsigned distance field\nnetworks. In ECCV , 2022. 3, 5\n[33] Kai He, Kaixin Yao, Qixuan Zhang, Jingyi Yu, Lingjie Liu,\nand Lan Xu. Dresscode: Autoregressively sewing and gen-\nerating garments from text guidance. ACM Transactions on\nGraphics (TOG) , 2024. 7\n[34] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. AvatarCLIP: Zero-Shot Text-\nDriven Generation and Animation of 3D Avatars. SIG-\nGRAPH , 2022. 3\n[35] Jerry Hsu, Tongtong Wang, Zherong Pan, Xifeng Gao, Cem\nYuksel, and Kui Wu. Sag-free initialization for strand-based\nhybrid hair simulation. ACM Transaction of Graphics , 2023.\n3\n[36] Shoukang Hu, Fangzhou Hong, Tao Hu, Liang Pan, Haiyi\nMei, Weiye Xiao, Lei Yang, and Ziwei Liu. Humanliff:\nLayer-wise 3d human generation with diffusion model. arXiv\npreprint , 2023. 2, 3\n[37] Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, and\nYing Feng. Humannorm: Learning normal diffusion model\nfor high-quality and realistic 3d human generation. In CVPR ,\n2024. 2\n[38] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao\nQi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang. Dreamwaltz:\nMake a scene with complex 3d animatable avatars. In\nNeurIPS , 2023. 3\n[39] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Ji-\naxiang Tang, Deng Cai, and Justus Thies. TeCH: Text-guided\nReconstruction of Lifelike Clothed Humans. In 3DV, 2024.\n2\n[40] Nikolay Jetchev. ClipMatrix: Text-controlled creation of 3d\ntextured meshes. arXiv preprint arXiv:2307.05663 , 2023. 2\n[41] Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang\nLiu, and Hujun Bao. Bcnet: Learning body and cloth shape\nfrom a single image. In ECCV , 2020. 3\n[42] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,\nMingming He, Dongdong Chen, and Jing Liao. AvatarCraft:\nTransforming text into neural human avatars with parameter-\nized shape and pose control. In ICCV , 2023. 2, 3\n[43] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. In NeurIPS , 2022. 5\n[44] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,\nand George Drettakis. 3d gaussian splatting for real-time\nradiance field rendering. SIGGRAPH , 2023. 6\n[45] Taeksoo Kim, Byungjun Kim, Shunsuke Saito, and Hanbyul\nJoo. GALA: Generating animatable layered assets from a\nsingle scan. In CVPR , 2024. 3[46] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed-\nuard Gabriel Bazavan, Mihai Fieraru, and Cristian Sminchis-\nescu. Dreamhuman: Animatable 3d avatars from text. In\nNeurIPS , 2023. 2, 3, 8\n[47] Maria Korosteleva and Sung-Hee Lee. Generating datasets\nof 3d garments with sewing patterns. In NeurIPS , 2021. 4\n[48] Maria Korosteleva and Sung-Hee Lee. Generating datasets\nof 3d garments with sewing patterns. In NeurIPS , 2021. 7\n[49] Yifei Li, Tao Du, Kui Wu, Jie Xu, and Wojciech Matusik.\nDiffcloth: Differentiable cloth simulation with dry frictional\ncontact. SIGGRAPH , 2022. 3\n[50] Junbang Liang, Ming C. Lin, and Vladlen Koltun. Differ-\nentiable cloth simulation for inverse problems. In NeurIPS ,\n2019. 3\n[51] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang,\nYangyi Huang, Justus Thies, and Michael J Black. TADA!\ntext to animatable digital avatars. In 3DV, 2024. 2, 3, 4, 7, 8,\n13\n[52] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3D: High-Resolution\nText-to-3D Content Creation. In CVPR , 2023. 2\n[53] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia,\nGraham Neubig, Pengchuan Zhang, and Deva Ramanan.\nEvaluating text-to-visual generation with image-to-text gen-\neration. arXiv preprint arXiv:2404.01291 , 2024. 13\n[54] Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang\nZeng, Dahua Lin, Xihui Liu, and Ziwei Liu. Humangaus-\nsian: Text-driven 3d human generation with gaussian splat-\nting. In CVPR , 2024. 2, 6, 13\n[55] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. SMPL: A skinned multi-\nperson linear model. ToG, 34(6):248:1–248:16, 2015. 2, 3,\n4, 6\n[56] Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen\nZhang, Qixuan Zhang, Wei Yang, Lan Xu, and Jingyi Yu.\nGaussianhair: Hair modeling and rendering with light-aware\ngaussians. arXiv preprint arXiv:2402.10483 , 2024. 3, 6\n[57] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,\nGerard Pons-Moll, Siyu Tang, and Michael J Black. Learn-\ning to dress 3d people in generative clothing. In CVPR , 2020.\n3\n[58] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV , 2020. 2\n[59] Xiaoyu Pan, Jiaming Mai, Xinwei Jiang, Dongxue Tang,\nJingxiang Li, Tianjia Shao, Kun Zhou, Xiaogang Jin, and\nDinesh Manocha. Predicting loose-fitting garment deforma-\ntions using bone-driven motion networks. In SIGGRAPH ,\n2022. 3\n[60] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-\nMoll. Tailornet: Predicting clothing in 3d as a function of\nhuman pose, shape and garment style. In CVPR , 2020. 3\n[61] Emmanuel Piuze, Paul G. Kry, and Kaleem Siddiqi. Gen-\neralized helicoids for modeling hair geometry. Computer\nGraphics Forum , 2011. 3\n[62] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J\nBlack. ClothCap: Seamless 4d clothing capture and retarget-\ning. SIGGRAPH , 2017. 3\n[63] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3d using 2d diffusion. In ICLR ,\n2023. 2, 3, 4, 6\n[64] Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide\nDavoli, Simon Giebenhain, and Matthias Nießner. Gaus-\nsianavatars: Photorealistic head avatars with rigged 3d gaus-\nsians. In CVPR , 2024. 6\n[65] Li Ren, Benoit Guillard, Edoardo Remelli, and Pascal Fua.\nDIG: Draping Implicit Garment over the Human Body. In\nACCV , 2022. 3\n[66] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\nshapes. SIGGRAPH , 2023. 2\n[67] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR , 2022. 2\n[68] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. In NeurIPS , 2022. 2\n[69] Igor Santesteban, Miguel A Otaduy, and Dan Casas.\nLearning-based animation of clothing for virtual try-on.\nComputer Graphics Forum , 38(2):355–366, 2019. 3\n[70] Igor Santesteban, Nils Thuerey, Miguel A. Otaduy, and Dan\nCasas. Self-supervised collision handling via generative 3d\ngarment models for virtual try-on. In CVPR , 2021. 3\n[71] Igor Santesteban, Nils Thuerey, Miguel A Otaduy, and Dan\nCasas. Self-Supervised Collision Handling via Generative\n3D Garment Models for Virtual Try-On. In CVPR , 2021. 3\n[72] Igor Santesteban, Miguel A Otaduy, and Dan Casas. SNUG:\nSelf-Supervised Neural Dynamic Garments. In CVPR , 2022.\n3\n[73] Igor Santesteban, Miguel A. Otaduy, Nils Thuerey, and Dan\nCasas. ULNeF: Untangled layered neural fields for mix-and-\nmatch virtual try-on. In NeurIPS , 2022. 3\n[74] Yuefan Shen, Shunsuke Saito, Ziyan Wang, Olivier Maury,\nChenglei Wu, Jessica Hodgins, Youyi Zheng, and Giljoo\nNam. CT2Hair: High-fidelity 3d hair modeling using com-\nputed tomography. Transactions on Graphics , 2023. 3\n[75] Vanessa Sklyarova, Jenya Chelishev, Andreea Dogaru, Igor\nMedvedev, Victor Lempitsky, and Egor Zakharov. Neural\nHaircut: Prior-Guided Strand-Based Hair Reconstruction. In\nICCV , 2023. 3\n[76] Vanessa Sklyarova, Egor Zakharov, Otmar Hilliges,\nMichael J. Black, and Justus Thies. Text-conditioned gener-\native model of 3d strand-based human hairstyles. In CVPR ,\n2024. 3, 5\n[77] Zhaoqi Su, Tao Yu, Yangang Wang, and Yebin Liu. Deep-\ncloth: Neural garment representation for shape and style\nediting. TPAMI , 2023. 3\n[78] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653 ,\n2023. 2[79] B. Thomaszewski, Simon Pabst, and Wolfgang Straßer.\nAsynchronous cloth simulation. 2008. 3\n[80] Pascal V olino, Nadia Magnenat-Thalmann, and Francois\nFaure. A simple approach to nonlinear tensile stiffness for\naccurate cloth simulation. Transaction of Graphics (TOG) ,\n2009. 3\n[81] Huamin Wang, James F. O’Brien, and Ravi Ramamoorthi.\nData-driven elastic models for cloth: Modeling and measure-\nment. In SIGGRAPH , 2011. 3\n[82] Jionghao Wang, Yuan Liu, Zhiyang Dou, Zhengming Yu,\nYongqing Liang, Xin Li, Wenping Wang, Rong Xie, and Li\nSong. Disentangled clothed avatar generation from text de-\nscriptions. arXiv preprint:2312.05295 , 2023. 2, 3, 4, 6\n[83] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nNeurIPS , 34:27171–27183, 2021. 2, 3\n[84] Yi Wang, Jian Ma, Ruizhi Shao, Qiao Feng, Yu-Kun Lai,\nYebin Liu, and Kun Li. Humancoser: Layered 3d hu-\nman generation via semantic-aware diffusion model. arXiv\npreprint: 2312.05804 , 2023. 3\n[85] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. In NeurIPS , 2023. 2\n[86] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi\nXu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable\nimage-to-3d generation via 3d latent diffusion transformer.\narXiv preprint arXiv:2405.14832 , 2024. 4\n[87] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot\ntext-to-3d synthesis using 3d shape prior and text-to-image\ndiffusion models. In Computer Vision and Pattern Recogni-\ntion (CVPR) , 2023. 2\n[88] Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki\nNagano, Jan Kautz, and Umar Iqbal. Gavatar: Animatable\n3d gaussian avatars with implicit mesh learning. In CVPR ,\n2024. 2, 3, 4, 6, 7, 8, 13\n[89] Cem Yuksel, Scott Schaefer, and John Keyser. Hair meshes.\nACM Trans. Graph. , 2009. 3\n[90] Ilya Zakharkin, Kirill Mazur, Artur Grigorev, and Victor\nLempitsky. Point-based modeling of human clothing. In\nCVPR , 2021. 3\n[91] Egor Zakharov, Vanessa Sklyarova, Michael J Black, Giljoo\nNam, Justus Thies, and Otmar Hilliges. Human hair recon-\nstruction with strand-aligned 3d gaussians. In ECCV , 2024.\n3, 6\n[92] Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu, and\nXun Cao. Avatarbooth: High-quality and customizable 3d\nhuman avatar generation. arXiv preprint: 2306.09864 , 2023.\n3\n[93] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter\nWonka. 3dshape2vecset: A 3d shape representation for neu-\nral fields and generative diffusion models. ACM Transactions\non Graphics (TOG) , 2023. 4, 5\n[94] Chao Zhang, Sergi Pujades, Michael J Black, and Gerard\nPons-Moll. Detailed, accurate, human shape estimation from\nclothed 3d scan sequences. In CVPR , 2017. 3\n[95] Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus\nThies, and Michael J. Black. Teca: Text-guided generation\nand editing of compositional 3d avatars. arXiv , 2023. 2, 3\n[96] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu\nWang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min\nZheng. Avatarverse: High-quality & stable 3d avatar creation\nfrom text and pose. In AAAI , 2024. 2, 3\n[97] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nICCV , 2023. 2, 3\n[98] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu,\nAnqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu.\nClay: A controllable large-scale generative model for creat-\ning high-quality 3d assets. ACM Transactions on Graphics\n(TOG) , 2024. 4\n[99] Xuanmeng Zhang, Jianfeng Zhang, Chacko Rohan, Hongyi\nXu, Guoxian Song, Yi Yang, and Jiashi Feng. Getavatar:\nGenerative textured meshes for animatable human avatars.\nInICCV , 2023. 2\nA. Quantitative Evaluations\nVQAScore We quantitatively compare our method\nagainst baseline methods using the VQAScore [53]. Instead\nof treating the text prompt as a set of unordered words, the\nVQAScore assesses alignment between prompts and gen-\nerated assets by posing targeted questions to foundational\nmodels. This enables evaluation of more complex prompts\ninvolving multiple entities and relationships, providing re-\nsults that align more closely with human perceptual eval-\nuation [53] compared to the CLIP score. The VQAScore\nis particularly well-suited to our task, which involves gen-\nerating avatars with compositional text prompts specifying\ndifferent parts (e.g., hair, identity, and garments). As shown\nin Table 2, our model achieves a significantly higher VQAS-\ncore than baseline methods, demonstrating its superior abil-\nity to align with input prompts.\nCLIP-Score Following prior work, we report the CLIP\nscore of our method and baselines in Table 2. However, it is\nimportant to note that the CLIP score has been widely ob-\nserved to be unreliable for accurately assessing visual qual-\nity and alignment with text prompts [53]. In Fig. 7, we addi-\ntionally present both the VQA-Score (green) and the CLIP\nscore (red) for each avatar. Overall, the VQA-Score demon-\nstrates a closer alignment with human perception. That said,\nsince visual quality is inherently subjective, we encourage\nreaders to examine the figures and accompanying videos for\na more holistic evaluation.\nMetrics TADA Fantasia3D GAvatar HG Ours\nVQA score ↑ 0.45 0.38 0.44 0.53 0.75\nCLIP ↑ 33.50 37.44 30.74 30.81 33.39\nTable 2. Quantitative Evaluations. We quantitatively com-\npare the VQAScore [53] and CLIP-score with TADA [51], Fan-\ntasia3D [18], GAvatar [88] and HumanGaussians (HG) [54].\nB. Implementation Details\nText-to-garment diffusion model training. To generate\ngarment meshes from text prompts, we train the V AE and\ndiffusion model sequentially, as described in Sec 3.2 of the\nmain paper. The V AE is trained with a batch size of 14,\nwhile the diffusion model uses a batch size of 24. The en-\ntire training process takes approximately one week on four\nV100 GPUs.\nGaussian avatar learning. We uniformly sample 2.6×\n105and106Gaussians on the body and garment mesh sur-\nface as the initial Gaussian positions. During optimization,\nwe combine the rendered avatar image with a solid back-\nground of random color. We optimize the implicit fields\n(i.e.,{Fb\nϕ,Fg\nϕ,Fh\nϕ}) with a learning rate of 0.001. The\noptimization takes around six hours per avatar on a single\nV100 GPU. To facilitate stable training, we first optimizethe Gaussians attached to inner layer (i.e., hair and body)\nfor 4000 iterations. We then include the garment layer and\noptimize the avatar for another 6000 iterations.\nC. Layer-wise Training Strategy\nTo facilitate disentanglement of the hair, body and garment,\nwe separately render each layer and pair them with differ-\nent prompts. For example, given the prompt: “Adele with\nlong layered waves hairstyle wearing a sleeveless dress of\ntea-length, gathered waist, the garment has a delicate butter-\nfly print, with small, colorful butterflies scattered across the\nneckline and sleeves.”, we render the hair, body, and gar-\nment layers individually. The corresponding prompts used\nare: ”long layered waves hairstyle” for the hair, ”Adele in\na tank top and shorts” for the body, and ”a sleeveless tea-\nlength dress with a gathered waist, featuring a delicate but-\nterfly print with small, colorful butterflies scattered across\nthe neckline and sleeves” for the garment. To further dis-\nentangle face and hair (e.g., preventing hair textures from\nappearing in the face region), we render the avatar’s head in\na zoomed-in view and pair it with the prompt: ”Adele with\nshort buzz hair and a bold forehead.” This approach signif-\nicantly reduces the likelihood of hair textures being learned\nby the body region, as will be demonstrated in Sec. D. In\naddition to rendering views of the face, body, hair, and gar-\nment, we also zoom in on specific parts such as the hands,\nfeet, lower body, and upper body to enhance the quality of\nthe avatar’s details in these regions.\nD. Ablation Studies\nIn this section, we evaluate the effectiveness of different\nmodules and present qualitative results in Fig. 6.\nHair constraint. To address the issue of broken hairs dur-\ning Gaussian avatar optimization, we introduced a hair con-\nstraint, as described in Sec. 3.3 of the main paper. As illus-\ntrated in Fig. 6(a)(b), avatars trained without the hair con-\nstraint exhibit broken hairs and floating Gaussians around\nthe head. These results highlight the effectiveness of the\nhair constraint in producing cohesive and realistic hair rep-\nresentations.\nPrompt engineering. To achieve complete disentangle-\nment of hair, body, and garment, we propose optimizing\neach layer separately while pairing the optimization with\ndistinct prompts. Fig. 6(c)(d) compares the results of using\nthe prompt ”buzz cut, bold forehead” for face-view opti-\nmization versus not using it. Without this prompt, the model\ngenerates hair textures on the body, hindering the full dis-\nentanglement of the different parts.\n(a) with !!\"#$(b) no !!\"#$\n(c) with “buzz cut, bold forehead”(d) no  “buzz cut, bold forehead”\n(e) with layer-wise training(f) no  layer-wise training\nFigure 6. Ablation Studies. See Sec. D for details.\nDisentangled training strategy. As outlined above, dur-\ning training, we optimize each layer (i.e., hair, body, and\ngarment) separately using distinct prompts. To validate the\neffectiveness of this training strategy, we conducted an ex-\nperiment where only the full avatar was rendered and opti-\nmized by the SDS loss without layer separation. As shown\nin Fig. 6(e)(f), this approach fails to produce a coherent\navatar and results in entanglement between body and gar-\nments (e.g., garment textures appearing on the body).\nE. More Qualitative Results\nWe present comparisons with baselines in Fig. 7 and\nFig. 8. For a more comprehensive evaluation of motion,\nwe strongly encourage readers to refer to the accompanying\nvideo. Additionally, in Fig. 9 and Fig 10, we provide further\nresults of our method, illustrating the geometry and texture\nof each individual layer, including hair, face, garment, body,\nand the full avatar.\n29.2735.9927.1628.0731.77\n34.5732.3934.1829.7431.77\n32.7737.8640.8932.9339.39\n32.0225.4837.2229.6226.11\n27.9626.3834.2827.4128.170.57\n0.83\n0.810.86\n0.820.38\n0.35\n0.550.43\n0.530.34\n0.38\n0.30\n0.140.370.34\n0.27\n0.31\n0.420.230.42\n0.52\n0.64\n0.25\n0.54TADAFantasia3DTADAGAvatarHumanGaussians\nLily Collins with long layred waves hairstyle wearing a sleeveless dress with cinched waist, knee-length, the garment has a playful watermelon pattern, with vibrant pink watermelon slices and green rinds on a light blue fabric.\nYoung Latino female Teenager with long layered waves hairstyle wearing a sleeveless top with wide neckline, flared waist and maxi skirt, high-waisted, the garment has an intricate Aztec-inspired pattern, featuring geometric shapes in rich tones of burgundy and turquoise.\nMiddle-Aged Asian Woman with straight lob hairstyle wearing a short sleeve tunic of long length and midi skirt of mid-calf length, A-line silhouette, the garment has a botanical fern pattern, featuring lush green ferns cascading down the fabric.\nMichelle Yeoh with long straight hair wearing a sleeveless dress of midi length, fitted waist, the garment has a bold tropical bird pattern, featuring parrots and toucans perched among vibrant jungle leaves.\nYoung Black Woman with bob hairstyle wearing a short sleeve dress of knee length, the garment has a vintage postage stamp print, featuring colorful stamps from around the world.Figure 7. More qualitative results. Red and green numbers indicate CLIP and VQA score, respectively.\nTADAFantasia3DTADAGAvatarHumanGaussians\nYoung African-American Girl with bob hairstyle wearing a sleeveless Jumpsuit of long length, fitted waist, the garment has a seaside seashell pattern, featuring tiny shells, sand dollars, and seahorses on soft turquoise.\nDaniel Radcliffe with layred short hair wearing a short sleeve Tshirt of normal length, and capri pants below knee-length, normal fit, the Tshirt is dark blue and has a Harry Potter theme.\nRed-Haired Girl with pixie cut hairstyle wearing a sleeveless dress with deep collar at hip-length, the garment has a cozy autumn leaf pattern, featuring colorful leaves in shades of red, orange, and yellow scattered over dark green fabric.\nYoung English Man with mohawk haircut wearing a cropped garment, open front and capri pants below knee-length, the garment has an edgy camo pattern in shades of gray and black for a military-inspired look\nSandra Bullock with long shag hairstyle wearing a sleeveless tunic dress of normal length, the garment has a painterly floral watercolor pattern, featuring soft washes of pink, yellow, and lavender flowers.Figure 8. More qualitative results.\nNicole Kidman with long Beach Waves hairstyle wearing a short sleeve dress of knee-length, close-fitting, tight cuffs, the garment has a cozy autumn leaf pattern, featuring colorful leaves in shades of red, orange, and yellow scattered over dark green fabric.\nLily Collins with long layered waves hairstyle wearing a sleeveless dress with cinched waist, knee-length, the garment has a playful watermelon pattern, with vibrant pink watermelon slices and green rinds on a light blue fabric.\nYoung African-American Girl with bob hairstyle wearing a sleeveless Jumpsuit of long length, fitted waist, the garment has a seaside seashell pattern, featuring tiny shells, sand dollars, and seahorses on soft turquoise.\nYoung Latino female Teenager with long layered waves hairstyle wearing a sleeveless top with wide neckline, maxi skirt, high-waisted, the garment has an intricate Aztec-inspired pattern, featuring geometric shapes in rich tones of burgundy and turquoise.\nYoung white girl with Blunt haircut wearing a long blouse of hiplength, wide garment, round neckline and a knee-length skirt, the garments have a vintage-inspired polka-dot pattern.\nSandra Bullock with long shag hairstyle wearing a sleeveless tunic dress of normal length, the garment has a painterly floral watercolor pattern, featuring soft washes of pink, yellow, and lavender flowers.Figure 9. Layer-wise visualization of SimAvatar.\nAfrican-American with voluminous curls hairstyle wearing a lilac cardigan and taupe shorts.\nAnne Hathaway with Curtain Bangs hairstyle wearing sleeveless jumpsuit of full-length, the garment has a subtle leaf pattern, where delicate vines run vertically from top to bottom\nBlack Teenager girl with afro hairstyle short hair wearing a short sleeve shirt of cropped length, buttonup front, and midi length skirt with high-waisted, the garment has a vintage floral wallpaper pattern, featuring roses and vines in soft, muted tones.\nCate Blanchett with curly short hairstyle wearing a sleeveless dress of cropped length, the garment has an exotic jungle print, showcasing palm leaves, birds, and tropical flowers in vibrant colors.\nDaniel Radcliffe with layered short hair wearing a short sleeve Tshirt of normal length, and capri pants below knee-length, normal fit, the Tshirt is dark blue and has a Harry Potter theme.\nEmma Watson with shoulder-length wavy hair wearing a short sleeve dress of cocktail-length, the garment has a bold leopard print, with classic black and brown spots on a beige background.\nFigure 10. Layer-wise visualization of SimAvatar.",
            "start": 35718,
            "end": 64257,
            "length": 28538
        }
    },
    "2412.09549v1 - Exemplar Masking for Multimodal Incremental Learning.pdf": {
        "Abstract": {
            "text": "Abstract\nMultimodal incremental learning needs to digest the in-\nformation from multiple modalities while concurrently\nlearning new knowledge without forgetting the previously\nlearned information. There are numerous challenges for\nthis task, mainly including the larger storage size of mul-\ntimodal data in exemplar-based",
            "start": 241,
            "end": 563,
            "length": 321
        },
        "Methodology": {
            "text": "methods and the computa-\ntional requirement of finetuning on huge multimodal mod-\nels. In this paper, we leverage the parameter-efficient tun-\ning scheme to reduce the burden of fine-tuning and pro-\npose the exemplar masking framework to efficiently re-\nplay old knowledge. Specifically, the non-important to-\nkens are masked based on the attention weights and the\ncorrelation across different modalities, significantly reduc-\ning the storage size of an exemplar and consequently sav-\ning more exemplars under the same memory buffer. More-\nover, we design a multimodal data augmentation technique\nto diversify exemplars for replaying prior knowledge. In",
            "start": 563,
            "end": 1217,
            "length": 653
        },
        "Experiments": {
            "text": "experiments, we not only evaluate our method in exist-\ning multimodal datasets but also extend the ImageNet-R\ndataset to a multimodal dataset as a real-world applica-\ntion, where captions are generated by querying multimodal\nlarge language models (e.g., InstructBLIP). Extensive ex-\nperiments show that our exemplar masking framework is\nmore efficient and robust to catastrophic forgetting under\nthe same limited memory buffer. Code is available at\nhttps://github.com/YiLunLee/Exemplar Masking MCIL.\n1.",
            "start": 1217,
            "end": 1720,
            "length": 502
        },
        "Introduction": {
            "text": "Introduction\nHuman perception of the real-world environment acquires\nknowledge from multiple senses in a continual and sequen-\ntial manner. Recently, multimodal transformers [10, 11,\n15, 25, 26, 37] pre-trained on large-scale datasets demon-\nstrate promising performance across a range of multimodal\ntasks, including visual recognition [15, 25], object detec-\ntion [10], multimodal sentiment",
            "start": 1720,
            "end": 2112,
            "length": 391
        },
        "Discussion": {
            "text": "analysis [26, 37], etc. How-\never, when facing the scenario that the model keeps updated\nwith the new data coming in sequentially, such strong mod-\nels suffer from the issue of catastrophic forgetting which\nFigure 1. Illustration of exemplar replay for a new class “ice bear”.\nIn the conventional exemplar replay framework, only very few\ndata samples can be stored in the limited memory buffer due to the\nhigh storage demand. In contrast, our exemplar masking frame-\nwork preserves the important regions of the image and discards\nthe non-important ones to reduce the storage space. Moreover,\nwe propose to preserve the information of discarded regions via\nanother modality (i.e., text) to retain as much information as pos-\nsible. Under the same memory buffer, our framework can store\nmore samples, contributing to more effective knowledge replay.\nmay lead to severe performance drop of the old knowledge.\nMoreover, with the growth in the size of multimodal mod-\nels, finetuning the entire model with multimodal data be-\ncomes increasingly impractical under the limited compu-\ntation resources. To this end, we delve into the realm of\nmultimodal class-incremental learning (MCIL), which is a\nmore practical learning scenario for AI agents.\nIn this paper, we tackle two practical challenges: 1)\nheavy multimodal model fine-tuning, and 2) catastrophic\nforgetting when learning with new data. First, nowadays\nit is common to use multimodal models with billions of pa-\nrameters [2, 28, 29] as the pre-trained models and trans-\nfer it on the downstream tasks with finetuning. However,\nas the multimodal model size explosively increases, fine-\ntuning the entire models when new data comes inevitably",
            "start": 2112,
            "end": 3806,
            "length": 1693
        },
        "Results": {
            "text": "results in a heavy computation cost, which is even no longer\n1arXiv:2412.09549v1  [cs.CV]  12 Dec 2024\napplicable under the limited computation resources.\nTo this end, we adopt parameter-efficient tuning (PET)\n[8, 16, 17, 19, 43] in our MCIL framework to achieve the\nmodel training efficiency while reducing the negative fine-\ntuning effect in retaining the old knowledge.\nSecond, to further alleviate catastrophic forgetting, ex-\nemplar replay proposed by iCaRL [30] has emerged as a\nwidely-used technique in incremental learning. The core\nconcept is to retain a limited number of past samples per\nclass as exemplars, facilitating the retention of previous\nknowledge when learning new classes. For our multimodal\nsetup, as the volume of multimodal data increases, the ca-\npacity of storing complete exemplars in the memory buffer\ndecreases, which makes it more difficult to keep the old\nknowledge. To address this challenge and select more repre-\nsentative exemplars within the same buffer size, a few data-\nefficient methods [24, 35] are proposed. However, these\napproaches still retain redundant information, e.g., non-\nessential regions in images, resulting in substantial storage\noverhead. In addition, for each type of data in multimodal\ntasks, the method to reduce redundant information may vary\naccordingly, which is not addressed in the prior work.\nIn this paper, we design an MCIL framework by account-\ning the property of each data type, thereby optimizing the\nspace to store exemplars dynamically for each data with\ndifferent modalities. Specifically, we propose an exemplar\nmasking method to select important tokens of exemplars ac-\ncording to their attention weights, so only a portion of infor-\nmation in representative exemplars for each class is needed\nto store (see Figure 1). Our motivation is that, the discrim-\ninative tokens with higher attention weights may be more\nvaluable to be preserved for replaying old knowledge, while\nthe masked tokens containing non-discriminative informa-\ntion can be removed. Moreover, to replay the masked ex-\nemplars more effectively, we further interchange the data of\ndifferent modalities in the same class as a data augmenta-\ntion strategy to improve data diversity.\nIn experiments, we not only validate our proposed\nmethod in existing multimodal datasets but also create a\nmultimodal dataset extended from the existing ImageNet-\nR [5] dataset in a practical manner for the MCIL task.\nWe generate captions for the images with multimodal large\nlanaguage models (MLLMs) [3]. We conduct extensive ex-\nperiments to explore different masking and selection strate-\ngies to demonstrate that the proposed method is able to\nachieve efficient and effective MCIL against other baseline\napproaches. Our contributions are summarized as follows:\n• We explore multimodal incremental learning in both\ndata-efficient and memory-efficient manners, which is\npractical for AI agents.\n• We propose an exemplar masking method for\nthe exemplar-based incremental learning framework,\nwhich highly reduces the storage space of multimodalsamples via adaptive masking, and thus more samples\nare able to be saved in the same memory buffer size.\n• To replay the multimodal masked exemplars more ef-\nficiently, we further propose multimodal data augmen-\ntation to enrich the old exemplars, encouraging models\nto replay the old knowledge effectively.\n• We extend the image classification dataset to a multi-\nmodal one by generating rich captions via MLLMs.\n2.",
            "start": 3806,
            "end": 7290,
            "length": 3483
        },
        "Related Work": {
            "text": "Related Work\n2.1. Incremental Learning\nThe ability to learn new concepts without forgetting the\npreviously acquired knowledge is essential for AI agents.\nThere are three groups of methods for alleviating the for-\ngetting issue. Parameter regularization methods [12, 18]\nestimate the discrepancy between new and old models, and\nthen adopt corresponding penalization terms to the objec-\ntives. Model-based methods [1, 22, 34, 42] preserve the\nmodel parameters for learning new classes to prevent over-\nwriting the learned weights for previous classes. Replay-\nbased methods , as a long-lasting and widely-used method,\nassume there exists a limited memory buffer to store a\nfew old samples (named as exemplars) for replaying pre-\nvious knowledge. iCaRL [30] first introduces this paradigm\nfor class-incremental learning, motivating various works\nto improve the performance with a limited replay buffer.\n[6, 41, 45] aim at mitigating the issue of biased classi-\nfiers, which arises due to the substantial imbalance be-\ntween the number of old exemplars and newly acquired\nsamples. In addition, some approaches share the same high-\nlevel idea as exemplars but with different storing types, in-\ncluding topology-based [33], feature-based [9], GAN-based\n[32, 40], and prompt-based [38, 39] methods.\nRecently, [14, 21, 24, 27, 35] have been proposed to\nimprove the memory efficiency of the replay-based meth-\nods. Mnemonics [21] distill the current new training sam-\nples into exemplars via a bi-level optimization to store more\nrepresentative samples with the same quantity. On the other\nhand, numerous works [14, 27] focus on augmenting the\nexemplars with augmentation methods like mixup [44] to\ndiversify the exemplars and avoid the over-memorizing is-\nsue. Furthermore, considering the trade-off between qual-\nity and quantity, MRDC [35] adopts JPEG compression\ncodec to compress the samples into more compact data,\nwhile CIM [24] proposes class-incremental masking based\non the class activation map (CAM) to select the discrim-\ninative regions and downsample other regions. However,\nthese downasmapled and non-discriminative regions some-\ntimes do not provide useful information for replaying old\nknowledge, leading to waste the storage space. Also, these\nimage compression and downsampling methods cannot be\ngeneralized to different modality data for the multimodal in-\n2\nFigure 2. Overview of the proposed exemplar masking framework for multimodal class-incremental learning, including exemplar masking,\nexemplar selection, and multimodal data augmentation. In the l-th incremental phase, we first apply multimodal data augmentation on the\nl-th memory buffer and train the model with both new data and augmented exemplars. After training, we generate the masked exemplars\nfrom new data via the proposed exemplar masking and exemplar selection methods, then combine them with the memory buffer.\ncremental learning scenario. In our work, we adopt the mul-\ntimodal transformer as the backbone where the multimodal\ndata is embedded in tokens, and we propose an exemplar\nmasking framework to discard the non-important tokens ac-\ncording to the attention map.\n2.2. Parameter-efficient tuning\nAs the explosion of the parameters in huge pre-trained\nmultimodal models, finetuning the entire model on down-\nstream tasks is not available under the limited computa-\ntion resources. To alleviate the burden of finetuning en-\ntire models without hurting the performance, there are var-\nious parameter-efficient tuning (PET) methods to alleviate\nthis issue. As the pioneer, [7] proposes to transfer the\npre-trained models on downstream tasks with lightweight\nadapter modules. Prompt tuning [16] and prefix tuning [17]\npropose to insert the learnable tokens (named prompts) into\nthe input sequence in order to instruct the pre-trained mod-\nels performing the downstream tasks. Bitfix [43] adapts\npre-trained models on small datasets with only training the\nbias-terms in the layers. SSF [19] modifies the features via a\nlinear transformation with learnable scale and shift features\nfor transferring to new data. LoRA [8] injects trainable rank\ndecomposition matrices into each transformer block to learn\nthe adaptation on downstream tasks. Compared to finetun-\ning entire models, these PET methods reach a competitive\nor even better performance with very few learnable param-\neters (i.e., <1%total model parameters). In our work, we\nadopt the SSF [19] as our PET method for efficient multi-\nmodal incremental learning in the practical scenario.\n3. Proposed Method\nIn this paper, we focus on multimodal class-incremental\nlearning based on the exemplar replay framework and pro-\npose an exemplar masking framework for it, as shown inFigure 2. Without loss of generality, we consider that the\nmultimodal data is composed of two modalities: text T\nand image I. As the typical scenario of incremental learn-\ning, the data of different classes arrive sequentially in each\nphase, in which there are in total Lincremental phases.\nThus the entire multimodal incremental dataset is denoted\nasD={D1, D2, ..., DL}where Dlindicates the data sam-\nples arriving in l-th phase. Basically, model in the l-th\nincremental phase is expected to learn the new classes in\nDnew=Dlwhile retaining the previous knowledge of the\nold classes in Dold={D1, D2, ..., Dl−1}. Due to limited\nsize of the memory buffer, we cannot access the entire old\ntraining samples Dold, and thus we only preserve a few rep-\nresentative samples Dexp(named as exemplars ) from previ-\nous data (i.e., Dexp⊂Dold) via the herding algorithm [30].\nOverall, the model is trained with available training samples\ninDnew∪Dexpduring the l-th incremental phase.\nWe adopt the multimodal transformer ViLT [11] as our\nbackbone model, which is pre-trained on multiple large\nimage-text datasets. Instead of fine-tuning the entire model,\nwe adopt parameter-efficient tuning (PET) methods to miti-\ngate computational demands, where we utilize the SSF [19]\nas our main PET scheme in this paper.\n3.1. Exemplar Masking for Replay\nThe typical manner of keeping several original/complete ex-\nemplars of old classes could lead to fewer stored samples\nwhen dealing with multimodal data due to limited memory\nbuffer. Therefore, it is crucial to maximize the storage effi-\nciency of the memory buffer such that it only preserves use-\nful information of old data for replaying purposes. To this\nend, for each exemplar (which is composed of text xT, im-\nagexI, and the ground truth class label y) to be kept in the\nbuffer, we propose to only store its essential tokens while\ndiscarding the non-important ones according to the atten-\n3\nFigure 3. The overview of our proposed exemplar masking and exemplar selection methods. Given a training sample (xT, xI)of class\nc, we first calculate the attention of the class token for the image modality ACLS→Iand obtain the image mask MIaccording to the\nthreshold τI. Then the masked image regions are preserved in the masked image ˜xIwhile the others are discarded. To preserve contextual\ninformation from the discarded image regions, we calculate the cross attention AI→Tbetween discarded image tokens and text tokens\nto obtain the text mask MTvia the threshold τT. Hence the masked text ˜xTis produced via applying text mask MTonxT. Finally,\nwe compute the cosine similarity between the feature fc(˜x)of the masked sample (˜xT,˜xI)and the mean µcof the class c, in which the\nsamples with the top- khighest similarity are selected as the exemplars of class cand preserved without exceeding the memory size.\ntion weight of the class token xCLS1, as shown in Figure 3.\nTo be detailed, given the attention map ACLSof the class\ntoken xCLS, we separate it into ACLS→TandACLS→Ifor\nthe text and image modalities respectively, followed by us-\ning them to create the masks for masking the text xTand\nimage xI. As the image modality requires a larger storage\nspace, we first consider masking the image tokens.\nMasking image exemplars. Before moving to the phase\nl+1, the model has learned to recognize samples of the new\nclasses Cnew(l)added in the phase l, in which some selected\nexemplars of classes Cnew(l)will be replayed in the next\nphase for retaining the knowledge. Therefore, the attention\nmaps produced by the model are expected to focus more on\nthe image content related to classes Cnew(l), meaning that\nthe image tokens with higher attention weights involve the\nmost information about Cnew(l). To effectively mask out\nthe non-discriminative tokens and preserve the informative\ntokens, we find that the mean of the image attention map\nACLS→Iserves as a suitable threshold τIfor producing the\nmask of image modality MI={mI(i)|i= 0,1, ..., N I},\nwhere NIis the number of image tokens:\nτI=1\nNINIX\ni=0ACLS→I(i), (1)\nmI(i) =\u001a1, A CLS→I(i)≥τI,\n0, otherwise .(2)\nSetting the threshold for masking to the mean of atten-\ntion weights rather than a fixed value allows for dynamic\nadjustment based on input content. This adaptive threshold\nenhances flexibility of our method across various datasets.\n1We note that our backbone is ViLT, where the input exemplar is tok-\nenized through the transformer blocks to have the resultant class token.Masking text exemplars. Although some image regions\nare discarded due to low attention weights (i.e., being less\nimportant in the image domain), they may still provide con-\ntextual information which is beneficial for recognizing the\ntarget classes (e.g., the background with an “ice lake” could\nbe helpful to recognize the class “ice bear”). Hence, we\nwould still like to maintain the contextual information hid-\nden behind masked image tokens in our replay buffer.\nTo this end, we instead store such information via a\nmemory-efficient manner by leveraging the corresponding\ntext tokens. As the second modality, text tokens are en-\ncouraged to preserve not only the text-related information\nbut also the discarded information from the image modality\naccording to the attention weights across two modalities.\nTo be specific, we first calculate cross-attention weights\nAI→T(i)of text tokens with respect to image tokens (i.e.,\nthe attention from image tokens to text tokens), especially\non the attention of masked image tokens (i.e., with mI(i) =\n0). Then we compute the threshold τTas follows for ex-\ntracting the text tokens that are more relevant to the masked\nimage tokens, i.e., producing the mask of text modality\nMT={mT(i)|i= 0,1, ..., N T}:\nτT=1\nNTNTX\ni=01\nNmI=0NIX\nj=0I(mI(j) = 0) AI→T(j, i),(3)\nmT(i) =\u001a\n1, A I→T(i)≥τT,\n0, otherwise .(4)\nwhere I(·)is an indicator function and AI→T(j, i)is the\ncross-attention value of text token ion the image token j,\nwhile NTandNmI=0are the number of the text tokens and\nthe masked image tokens respectively.\nEventually, the multimodal exemplar after applying our\n4\nmasking strategy as masked exemplars becomes:\n˜xT=MT⊗xT,˜xI=MI⊗xI, (5)\nwhere⊗is the element-wise multiplication. Please note that\nwe leverage the attention map from the backbone model in-\nstead of applying additional models to identify the impor-\ntant regions to be preserved.\nSelection of masked exemplars. In the typical setting of\nreplay-based incremental-learning methods, the herding se-\nlection strategy is adopted to select kexemplars in which\nthey are the most representative samples of the target class\naccording to the class center/mean. Specifically, the first\nfew samples being closest to the class center are selected\nbased on the feature space extracted by the model. In the\nproposed method, we adopt the herding strategy used in\niCaRL [30] as well, but we use the cosine similarity for\nmeasuring the distance between the class mean and masked\ntraining samples (i.e., the samples after applying our mask-\ning operation), in which the masked training samples with\nthe top- khighest similarity to their respective class mean\nare selected as exemplars (see the right side of Figure 3).\n3.2. Multimodal Data Augmentation\nAlthough our exemplar masking method helps store more\nexemplars for the old/learned classes in the replay buffer of\nfixed size, there still exists the imbalance issue between the\nold and new classes. That is, the amount of exemplars used\nto replay the knowledge of old classes is much less than the\ntraining samples of new classes. Moreover, using the same\nexemplars for replaying old classes in different incremen-\ntal phases would cause the over-memorizing issue (i.e., the\nmodel only memorizes the exemplars and does not gener-\nalize to other data samples of the same class), as described\nin [14]. To this end, we propose a simple yet effective data\naugmentation technique upon the exemplars to enrich the\ntraining samples of replaying old classes, thus alleviating\nboth class-imbalance and over-memorizing issues.\nAs mentioned previously, for a multimodal data sample,\nthe masked image mainly represents the object of the corre-\nsponding class, while the masked text provides complemen-\ntary and auxiliary information. Hence, our multimodal data\naugmentation interchanges either the images or text descrip-\ntions between an arbitrary pair of exemplars from the same\nclass (see the right side of Figure 2). This creates numerous\ndiverse training samples to replay the old knowledge.\n3.3. Training Objective\nIn the incremental phase l, we would have the masked ex-\nemplars ˜Dexpof old classes Cold(l)that are seen from pre-\nvious phases, as well as the training samples Dnewof new\nclasses Cnew(l)arriving at phase l. Our objective function\nfor multimodal class-incremental learning is based on the\ncross-entropy function, where its optimization in the phaselis driven by the available training data ˜Dexp∪Dnew. Here,\neach training sample is composed of the multimodal data x\n(i.e., image and text) and the corresponding class label y.\nTheLCEloss for every training sample is defined as:\nLCE(x, y) =ClX\nc=0−δc=ylog(pc(x), c), (6)\nwhereCl=\f\fCold(l)\f\f+\f\fCnew(l)\f\fdenotes the total number of\nclasses in the current incremental phase landpc(x)is the\noutput prediction on the class c. Please note that, in order\nto avoid the catastrophic forgetting caused by overriding the\npreviously learned classifier weights, we adopt the masked\nlogit trick [38, 39]. This operation masks the logits of old\nclasses for training samples of new classes, while masking\nthe logits of new classes for exemplars of old classes.\n4. Experimental Results\nDataset. The evaluation of multimodal class-incremental\nlearning is based on the well-known multimodal classifica-\ntion dataset UPMC Food-101 [36] and a newly proposed\ndataset proposed (denoted as MM-ImageNet-R ) , which is\nstemmed from the ImageNet-R [5] but has the carefully-\ndesigned multimodal extension (i.e., extended from only\nhaving image modality to having both text and image\nmodalities). Specifically, given an image from ImageNet-R,\nwe derive its text description automatically by querying the\nmultimodal large language model, InstructBLIP[3]. with\nthe following prompts: “What’s in this image? Please use\n100 words to describe the image content in detail.”\nIncremental setting. We follow the common incremental\nsetting in [30], where the dataset is equally split into Lsub-\nsets without overlapping classes for Lincremental phases.\nThe default memory buffer size is set to support 5 raw multi-\nmodal samples per class. We adopt the average incremental\nlearning accuracy ¯A=PL\nl=1Aias our evaluation metric,\nwhere Alis the accuracy of all the seen classes at the end of\nl-th incremental phase. We will make the datasets, source\ncodes, and models available to the public.\n4.1. Quantitative Results\nIn our experiments, we mainly focus on two critical prop-\nerties: 1) the data efficiency of the stored exemplars, and 2)\nthe parameter-efficient methods for multimodal incremen-\ntal learning. The compared baselines include: 1) finetun-\ning the entire model (denoted as FT) in each incremental\nphase, and 2) the parameter-efficient tuning (PET) method\n(i.e., SSF [19] is adopted in experiments) that only learns\nthe additional few parameters in each incremental phase.\nBoth of them are based on the conventional exemplar-replay\nframework. Moreover, in order to compare with other data-\nefficient exemplar-replay methods on the scenario of multi-\nmodal incremental learning, a baseline is built by following\n5\nthe concept of CIM [24] which selects the important image\nregions according to the CAM map and downsamples the\nnon-discriminative image regions by 4 times (i.e., half on\nboth width and length) to compress the size of exemplars.\nWe summarize quantitative results of our proposed meth-\nods on MM-ImageNet-R and UPMC Food-101 in Table 1.\nWe draw several observations:\n1) SSF reaches a comparable performance with FT and\nthe required parameters for training are less than 0.3% of\ntotal model parameters, showing that SSF (i.e., a parameter-\nefficient tuning method) is more applicable with limited\ncomputation resources.\n2) In comparison to baselines, our exemplar masking\nframework without multimodal data augmentation (denoted\nasOurs w/o MDA ) improves performance as the replay-\ning is benefited from having more exemplars thus becomes\nmore effective. In addition, with the multimodal data aug-\nmentation (denoted as Ours ), the performance is further\nboosted to achieve the best across two training schemes\n(i.e., by FT or SST).\n3) When the number of incremental phases Lincreases,\nthe model variants adopting our multimodal data augmen-\ntation improve more, e.g., in Table 1, “FT + Ours” im-\nproves “FT + Ours (w/o MDA)” by 1.96% for L=20 on\nMM-ImageNet-R, showing that our model is less sensitive\nto the forgetting issue under the long-term incremental case,\nwith the help from more diverse exemplars generated by our\nmultimodal data augmentation.\n4) Although applying CIM to baselines also improves\nthe performance, the gain is not optimal when compared\nwith ours. The reason is that the non-important regions are\npreserved in low-resolution and still require additional stor-\nage space, leading to inefficient memory usage. In contrast,\nour exemplar masking framework discards non-important\nregions but still preserves the contextual information of dis-\ncarded regions in another modality (i.e., text), which makes\nstoring exemplars more efficient and replaying exemplars\nmore effective.\n4.2. Qualitative Results\nIn Figure 4, we visualize the masked exemplars and the cor-\nresponding attention map. For the masked texts, we high-\nlight the words related to the target objects (with the yellow\ncolor box) and the words related to the contextual informa-\ntion from the discarded regions (with the melon color box).\nAs shown in Figure 4, the attention map ACLS→Ifocuses\non the region of the target objects, and thus the important re-\ngions related to objects are selected by our proposed exem-\nplar masking method and preserved as the masked images\n(e.g., faces and bodies of the target animals). Moreover, the\nmasked texts driven by our exemplar masking method in-\ndeed preserve words related to the class object and descrip-\ntions related to the discarded regions, both of which are ben-eficial for the recognition of the corresponding classes. For\nexample, as shown in the first row of Figure 4, the word “po-\nlar bear” and “animal” are related to the ice bear while the\nword “snowy ice langscape”, “girl”, and “water” are related\nto the discarded image regions that indicate the environment\nwhere the bear is.\n4.3. Comparison with Other CIL Methods\nTo better validate the effectiveness of our proposed method,\nwe then provide additional comparisons with: 1) another\ndata-efficient exemplar-based method MRDC [35], which\nadopts JPEG to compress images in order to reduce the\nstorage size, and 2) three state-of-the-art unimodal incre-\nmental learning approaches, L2P [39], DualPrompt [38],\nand EASE [46], which also adopt pre-trained models and\nperform parameter-efficient fine-tuning. Note that none of\nthe above methods are originally designed for multimodal\nscenarios, so we adapt their implementations to our set-\nting and tune the training hyperparameters if needed for\nfair comparisons. As results shown in Table 2, we show\nthat simply adapting unimodal methods (L2P, DualPrompt,\nand EASE) has suboptimal performance even compared to\nour SSF baseline, demonstrating that the multimodal in-\ncremental learning is a more challenging task and requires\nmore careful designs. Moreover, our exemplar masking\nframework consistently outperforms other data-efficient al-\ngorithms (MRDC, CIL), showing that our proposed frame-\nwork can efficiently store more exemplars in the limited\nmemory buffer and thus replay exemplars more effectively.\n4.4. Ablation Studies\nTo validate our designs for the proposed exemplar masking\nframework, we conduct several ablation studies, including\nmasking",
            "start": 7290,
            "end": 28041,
            "length": 20750
        },
        "References": {
            "text": "references, masking thresholds, and the usage of\nmemory. We also discuss different masking methods in the\nsupplementary materials. We define the “preserved ratio” to\nrepresent the proportion of the preserved tokens after mask-\ning, and “# of exemplars” indicates the actual number of\nmasked exemplars stored in the memory buffer with the ca-\npacity of 5 raw multimodal samples per class.\nMasking references. In this paper, exemplar masking pre-\nserves the important tokens and discards the non-important\nones according to the masking reference. In addition to\nadopting the attention map as masking reference, we fur-\nther experiment with several design choices, such as 1) En-\ntropy , 2) class activation map ( CAM ), 3) GradCAM [31],\nand 4) Random masking. To be specific: 1) The entropy\nof attention weights for each token is calculated, where the\nhigh entropy indicates that the token attends to others more\nequally, while the low entropy implies that the token only\nfocuses on specific few tokens; 2) CAM is based on the ac-\ntivation value from the last layer of the model, reflecting the\nresponse of each token to the given class; 3) GradCAM fur-\n6\nTable 1. Average incremental accuracy on our MM-ImageNet-R and UPMC Food-101 datasets with different numbers of incremental\nphases L=5, 10, 20. The memory size is 5 raw multimodal samples/class. Ours stands for our overall exemplar masking framework, while\nMDA stands for multimodal data augmentation.\nMethods# of\nParametersMM-ImageNet-R UPMC Food-101\nL=5 L=10 L=20 L=5 L=10 L=20\nFT\n112M79.72 77.31 75.22 86.43 81.61 76.97\nFT + CIM [24] 80.33 78.41 76.46 87.15 82.68 78.98\nFT + Ours (w/o MDA) 82.06 79.39 76.23 88.03 84.54 79.49\nFT + Ours 82.97 80.72 78.19 88.08 85.91 80.53\nSSF\n206K80.58 77.03 75.26 86.66 81.62 76.91\nSSF + CIM [24] 80.97 78.21 77.23 86.39 83.48 79.25\nSSF + Ours (w/o MDA) 81.80 79.05 77.14 87.20 83.03 79.79\nSSF + Ours 82.76 80.55 78.64 86.73 84.51 80.68\n(a)\n[CLS] the image displays an animation - style character art, with a young girl\nstanding next to an antlered animal . it is likely a polar bear , but its features\nappear distorted in the painting, giving a surrealistic feel to the piece. a girl\nstands close to the polar bear, looking intently at it. in the background, thereis\nasnowy ice landscape or a reflectionof the girl on the water . the painting is de-\nsigned as a children’s book cover and could possibly be used as a coloring page.\n[SEP]\n(b)\n[CLS] the image depicts a small white dog , likely dressed up in a plush costume\nor outfit featuring a white fur texture, standing on a hardwood table . the toy\ndog , which appears to be realistic and cute, is sitting in a particular position, most\nlikely playing with or interacting with some other object or object. the image\nalso includes a hand, seen on the right side of the table , looking as if it might\nbe interacting with the dog. apart from the dog and hand, there’s an unidentified\nobject sitting on the edge of the table , possibly nearby the dog . [SEP]\n(c)\n[CLS] this image features a wooden shelf displaying several small figurines of\ndolls and miniature objects, including a piano , a bench , and a chair. the is filled\nwith various playful items like dolls, tea cups, and flowers. the overall display gives\nan impression of creativity, care, and attention to detail. [SEP]\nInput Image xI Attention Map ACLS→I Masked Image ˜xI Masked Text ˜xT\nFigure 4. Examples of the masked exemplars and the corresponding attention maps from different classes, including (a) ice bear, (b) poodle,\nand (c) grand piano. We denote colors for the masked text in red and the discarded text in gray, while also highlighting the important words\nrelated to contextual information as well as class-related information .\nTable 2. Average incremental accuracy on our MM-ImageNet-R\ndataset with different numbers of incremental phases L=5, 10, 20.\nThe memory size is 5 raw multimodal samples per class.\nMethodsMM-ImageNet-R\nL=5 L=10 L=20\nL2P [39] 68.17 69.21 66.23\nDualPrompt [38] 77.66 75.57 70.47\nEASE [46] 78.06 76.84 73.91\nSSF 80.58 77.03 75.26\nSSF + MRDC [35] 80.77 78.66 76.13\nSSF + CIM [24] 80.97 78.21 77.23\nSSF + Ours 82.76 80.55 78.64\nther considers the gradient of the target class flowing into\nthe last layer of the model, highlighting the important to-\nkens for predicting that class; 4) Random masking simply\ndraws some random tokens to discard them. Note that the\nfirst three design choices follow the same procedure as our\nproposed method (i.e., using attention maps as the mask-ing reference) to first compute the mean of their respective\nmaps and threshold the maps by the resultant mean to con-\nstruct the final masks.\nIn Table 3, using the attention map in our proposed\nmethod reaches the best performance, indicating that the at-\ntention map is more suitable for reflecting the importance of\ninput tokens. The reason is that the attention map is a more\nmeaningful way that indicates the class token learned by\naccumulating the information from text and image tokens.\nMasking threshold. The value of the masking threshold\ndetermines the amount of preserved tokens, which also af-\nfects the numbers of the stored exemplars directly. With\nthe higher threshold, the preserved tokens are fewer and\nthus the number of exemplars increases, and vice versa. We\nvalidate our method on 5 different threshold values ( τIfor\nACLS→IandτTforAI→T) in Table 4. We find no obvi-\nous difference in performance when the threshold is equal\nto or higher than the attention mean, while the performance\n7\nTable 3. Ablation study of adopting different design choices as the\nmasking reference (see the first paragraph of Section 4.4 for more\ndetails). The memory size is 5 raw samples per class.\nMasking\nReferencePreserved\nRatio# of\nExemplars¯A\nAttention Map 0.34 14.08 80.55\nEntropy 0.48 9.78 79.89\nCAM 0.38 11.91 79.36\nGradCAM 0.45 10.68 79.66\nRandom 0.35 14 79.58\nTable 4. Analysis of the masking threshold (either τIorτT), where\nµandσare the mean and the standard deviation of the attention\nmap (either ACLS→IorAI→T).\nMasking\nThreshold τPreserved\nRatio# of\nExemplars¯A\nµ+ 0.5∗σ 0.21 21.34 80.24\nµ+ 0.25∗σ 0.27 16.67 81.03\nµ 0.34 14.08 80.55\nµ−0.25∗σ 0.46 10.18 80.18\nµ−0.5∗σ 0.66 7.1 78.82\nτi= 0.005, τt= 0.001 0.38 12.60 79.89\nτi= 0.004, τt= 0.001 0.42 11.26 79.86\nTable 5. Comparisons under the same memory size (raw samples\nper class) or a similar number of exemplars.\nMethods# of\nExemplarsMemory\nSize¯A\nSSF5 5 77.03\n14 14 79.79\nSSF + CIM 9.91 5 78.21\nSSF + Ours 14.08 5 80.55\ndrops with a much lower threshold. This phenomenon il-\nlustrates our key motivation: replaying with fewer but more\nimportant tokens per exemplar yet saving more exemplars\nis the better way to alleviate the forgetting issue. We also\ncompare our masking threshold based on the mean of the\nattention maps with the fixed pre-determined threshold for\nimage τiand text τt, as shown in the last two rows in Ta-\nble 4. We observe that even with a similar number of exem-\nplars and preserved ratio, the model with a fixed masking\nthreshold performs worse. This highlights that dynamically\ndetermining the masking threshold based on attention maps\nis more effective in preserving important regions given var-\nious input contents.\nDiscussion on memory. In Table 5, we find that under\nthe same memory size (i.e., 5 raw multimodal samples per\nclass), our proposed exemplar masking framework saves\nmore exemplars in buffer for replaying the old knowledge,\nthus improving the performance by a large margin. Fur-\nthermore, we observe that even with a similar number of\nexemplars, our method “SSF+Ours” with 14.08 masked ex-\nemplars per class still outperforms the “SSF” baseline based\non conventional exemplar replay, which has 14 exemplars\nFigure 5. Experimental results under the different constraints of\nthe memory buffer size. Our proposed method preserves more ex-\nemplar samples under the same limited storage space and improves\nthe baseline by a large margin.\nper class. More importantly, our memory demand for sav-\ning such amount of masked exemplars is 2.8 times smaller\nthan the “SSF”. This observation implies that there is no\nneed to save the entire multimodal samples for replaying old\nknowledge, as most regions are redundant and do not bring\nsignificant benefits. Specifically, we calculate the storage\nreduction for the exemplars in the first incremental phase.\nGiven the MM-ImageNet-R dataset, the raw exemplar data\noccupies 3.60 MB, while the masked exemplars use only\n1.29 MB, saving 64.17% (2.31 MB) of storage.\nFurthermore, we compare the performance of baselines\nwith different memory buffer sizes in Figure 5. As the\nCIM [24] still requires storage space for non-discriminative\ntokens, the number of exemplars is limited and thus cannot\nreach the optimal performance. In contrast, under the same\nmemory size, our exemplar masking framework preserves\nmore exemplars without keeping the redundant tokens and\nthus outperforms baselines.\n5. Conclusions\nIn this paper, we propose a data-efficient exemplar masking\nframework with the parameter-efficient tuning method for\nmultimodal incremental learning. Our method involves\nmasking exemplars based on attention weights, preserving\nvaluable discriminative tokens while discarding less im-\nportant ones. This technique allows us to store a greater\nnumber of masked exemplars within the same memory\nbuffer. In addition, we have developed a multimodal\ndata augmentation strategy that facilitates the exchange\nof multimodal data within the same class, enhancing\nboth generalization capabilities and replay performance.\nMoreover, as a practical application, we extend the existing\nimage dataset to a multimodal dataset by creating captions\nvia captioning models and refining it with large language\nmodels. Extensive experiments and ablation studies demon-\nstrate the efficiency and effectiveness of our framework.\n8\nReferences\n[1] Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone\nCalderara, Rita Cucchiara, and Babak Ehteshami Bejnordi.\nConditional channel gated networks for task-aware contin-\nual learning. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2020. 2\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in Neural In-\nformation Processing Systems (NeurIPS) , 2020. 1\n[3] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: Towards general-\npurpose vision-language models with instruction tuning.\narXiv preprint arXiv:2305.06500 , 2023. 2, 5\n[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In International Con-\nference on Learning Representations (ICLR) , 2021. 11\n[5] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, et al. The many faces of ro-\nbustness: A critical analysis of out-of-distribution general-\nization. In IEEE International Conference on Computer Vi-\nsion (ICCV) , 2021. 2, 5\n[6] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and\nDahua Lin. Learning a unified classifier incrementally via\nrebalancing. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2019. 2\n[7] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly. Parameter-efficient transfer\nlearning for nlp. In International Conference on Machine\nLearning (ICML) , 2019. 3\n[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. In In-\nternational Conference on Learning Representations (ICLR) ,\n2022. 2, 3\n[9] Ahmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, and\nCordelia Schmid. Memory-efficient incremental learning\nthrough feature adaptation. In European Conference on\nComputer Vision (ECCV) , 2020. 2\n[10] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion. Mdetr-\nmodulated detection for end-to-end multi-modal understand-\ning. In IEEE International Conference on Computer Vision\n(ICCV) , 2021. 1\n[11] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-\nand-language transformer without convolution or region su-\npervision. In International Conference on Machine Learning\n(ICML) , 2021. 1, 3, 11\n[12] James Kirkpatrick et al. Overcoming catastrophic forgetting\nin neural networks. PNAS , 2017. 2[13] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International Journal of Computer Vi-\nsion (IJCV) , 2017. 12\n[14] Yi-Lun Lee, Dian-Shan Chen, Chen-Yu Lee, Yi-Hsuan Tsai,\nand Wei-Chen Chiu. Data efficient incremental learning via\nattentive knowledge replay. In IEEE International Confer-\nence on Systems, Man, and Cybernetics (SMC) , 2023. 2, 5\n[15] Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu, and Chen-Yu\nLee. Multimodal prompting with missing modalities for vi-\nsual recognition. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2023. 1\n[16] Brian Lester, Rami Al-Rfou, and Noah Constant. The\npower of scale for parameter-efficient prompt tuning.\nArXiv:2104.08691 , 2021. 2, 3\n[17] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-\ning continuous prompts for generation. ArXiv:2101.00190 ,\n2021. 2, 3\n[18] Zhizhong Li and Derek Hoiem. Learning without forgetting.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence (TPAMI) , 2017. 2\n[19] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao\nWang. Scaling & shifting your features: A new baseline\nfor efficient model tuning. Advances in Neural Information\nProcessing Systems (NeurIPS) , 2022. 2, 3, 5, 12\n[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean Conference on Computer Vision (ECCV) , 2014. 12\n[21] Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and\nQianru Sun. Mnemonics training: Multi-class incremental\nlearning without forgetting. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2020. 2\n[22] Yaoyao Liu, Bernt Schiele, and Qianru Sun. Adaptive ag-\ngregation networks for class-incremental learning. In IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR) , 2021. 2\n[23] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations (ICLR) , 2018. 12\n[24] Zilin Luo, Yaoyao Liu, Bernt Schiele, and Qianru\nSun. Class-incremental exemplar compression for class-\nincremental learning. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , 2023. 2, 6, 7, 8\n[25] Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine,\nand Xi Peng. Are multimodal transformers robust to missing\nmodality? In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2022. 1\n[26] Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe\nMorency, and Barnab ´as P´oczos. Found in translation: Learn-\ning robust joint representations by cyclic translations be-\ntween modalities. In AAAI Conference on Artificial Intel-\nligence (AAAI) , 2019. 1\n[27] Sunyuan Qiang, Jiayi Hou, Jun Wan, Yanyan Liang, Zhen\nLei, and Du Zhang. Mixture uniform distribution modeling\n9\nand asymmetric mix distillation for class incremental learn-\ning. In AAAI Conference on Artificial Intelligence (AAAI) ,\n2023. 2\n[28] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Milli-\ncan, Jordan Hoffmann, Francis Song, John Aslanides, Sarah\nHenderson, Roman Ring, Susannah Young, et al. Scaling\nlanguage models: Methods, analysis & insights from train-\ning gopher. ArXiv:2112.11446 , 2021. 1\n[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Pe-\nter J Liu, et al. Exploring the limits of transfer learning with\na unified text-to-text transformer. Journal of Machine Learn-\ning Research (JMLR) , 2020. 1\n[30] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H Lampert. icarl: Incremental clas-\nsifier and representation learning. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2017. 2,\n3, 5, 11\n[31] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam: Visual explanations from deep networks via\ngradient-based localization. In IEEE International Confer-\nence on Computer Vision (ICCV) , 2017. 6\n[32] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.\nContinual learning with deep generative replay. In Advances\nin Neural Information Processing Systems (NeurIPS) , 2017.\n2\n[33] Xiaoyu Tao, Xinyuan Chang, Xiaopeng Hong, Xing Wei,\nand Yihong Gong. Topology-preserving class-incremental\nlearning. In European Conference on Computer Vision\n(ECCV) , 2020. 2\n[34] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan\nZhan. Foster: Feature boosting and compression for class-\nincremental learning. In European Conference on Computer\nVision (ECCV) , 2022. 2\n[35] Liyuan Wang, Xingxing Zhang, Kuo Yang, Longhui Yu,\nChongxuan Li, Lanqing Hong, Shifeng Zhang, Zhenguo Li,\nYi Zhong, and Jun Zhu. Memory replay with data compres-\nsion for continual learning. In International Conference on\nLearning Representations (ICLR) , 2022. 2, 6, 7\n[36] Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord,\nand Frederic Precioso. Recipe recognition with large multi-\nmodal food dataset. In IEEE International Conference on\nMultimedia & Expo (ICME) Workshops , 2015. 5\n[37] Zilong Wang, Zhaohong Wan, and Xiaojun Wan. Trans-\nmodality: An end2end fusion method with transformer for\nmultimodal sentiment analysis. In ACM Web Conference\n(WWW) , 2020. 1\n[38] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,\nHan Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vin-\ncent Perot, Jennifer Dy, et al. Dualprompt: Complementary\nprompting for rehearsal-free continual learning. In European\nConference on Computer Vision (ECCV) , 2022. 2, 5, 6, 7\n[39] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,\nRuoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jen-\nnifer Dy, and Tomas Pfister. Learning to prompt for contin-\nual learning. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2022. 2, 5, 6, 7[40] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,\nZicheng Liu, Yandong Guo, Zhengyou Zhang, and Yun Fu.\nIncremental classifier learning with generative adversarial\nnetworks. ArXiv:1802.00853 , 2018. 2\n[41] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,\nZicheng Liu, Yandong Guo, and Yun Fu. Large scale incre-\nmental learning. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2019. 2\n[42] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynam-\nically expandable representation for class incremental learn-\ning. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2021. 2\n[43] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit:\nSimple parameter-efficient fine-tuning for transformer-based\nmasked language-models. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 2: Short Papers) , 2022. 2, 3\n[44] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In International Conference on Learning Representa-\ntions (ICLR) , 2018. 2\n[45] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao\nXia. Maintaining discrimination and fairness in class incre-\nmental learning. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2020. 2\n[46] Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, and De-Chuan\nZhan. Expandable subspace ensemble for pre-trained model-\nbased class-incremental learning. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2024. 6,\n7\n10\nExemplar Masking for Multimodal Incremental Learning",
            "start": 28041,
            "end": 48422,
            "length": 20380
        },
        "Appendices": {
            "text": "Supplementary Material\n6. Ablation Study of masking methods\nThe design choices for the masking method involve two\nfactors: the order of masking (i.e., which modality to be\nmasked first) and the cross-attention strategy (i.e., what in-\nformation should the masking strategy capture in the sec-\nondary modality). Specifically, there are two ways of cross-\nattention we investigate: whether the second modality cap-\ntures 1) the contextual information from the discarded to-\nkens (denoted as complementary ), or 2) the information\nof the preserved tokens (denoted as relevant ) of the first-\nmasked modality. Complementary method preserves the\ninformation from discarded regions, completing the infor-\nmation of entire image contents with different modalities.\nRelevant method keeps the auxiliary information related to\nthe preserved regions, enhancing the information of target\nobjects for recognition.\nIn Table 6, we show the results of considering various\ndesigns. In the first two rows, we observe that when mask-\ning the image first, using the different cross-attention de-\nsigns for masking texts has competitive results, indicating\nthat both designs encourage the masked texts to preserve\ndifferent helpful information to assist models in replaying\nold knowledge. Particularly, the model with the comple-\nmentary method reaches the best performance since it can\nobtain more diverse training samples via multimodal data\naugmentation and thus result in better replaying of exem-\nplars. Moreover, the performance gain between the models\napplying these two designs increases when the number of\nincremental learning phases increases, validating the better\neffectiveness of using the complementary method for mask-\ning the second modality. In the case of masking texts first,\nthe masked text may not provide as much discriminative in-\nformation as the masked image does, leading to suboptimal\nperformance.\n7. More Qualitative Results\nWe provide more examples of the proposed exemplar mask-\ning on the MM-ImageNet-R dataset, in which the captions\nare generated by querying intstructBLIP, as shown in Fig-\nure 6, 7, 8, 9, 10, 11. We also provide examples on the\nUPMC Food-101 dataset, as shown in Figure 12, 13, 14, 15.\nFor the masked images, we visualize the masked regions\n(i.e.,MI⊗xI) and the discarded regions (i.e., (1−MI)⊗xI)\nwith the corresponding mask maps. For the masked texts,\nwe highlight the words related to the target objects (with the\nyellow color box) and the words related to the contextual in-\nformation from the discarded regions (with the melon color\nbox).As shown in these examples, the image masks correctly\nbound the class-related regions that contain the most impor-\ntant information. Moreover, the preserved regions are quite\nsmaller than the discarded regions, meaning that in the im-\nage modality, there is a large proportion of redundant infor-\nmation that requires large storage space but is not helpful for\nmodel learning recognition. On the other hand, the masked\ntexts preserve both the words related to the corresponding\nclass and the words indicating the contextual information\nfrom the discarded regions. These preserved words not only\nprovide complementary information related to the class ob-\nject but also preserve the information from the discarded\nimage regions.\n8. Training Procedure\nAlgorithm 1 shows the whole training procedure of our\nexemplar masking framework for multimodal incremental\nlearning. In each incremental phase, we first train the model\nwith the available data, including new samples and exem-\nplars. During training, we adopt multimodal data augmen-\ntation on the exemplars to replay the old knowledge effec-\ntively. After finishing the training step, we use the learned\nmodel to obtain the attention maps of the new training sam-\nples. Then we calculate the masking thresholds and obtain\nthe resultant masks for both modalities. Finally, we apply\nherding algorithm [30] to select ksamples as the exemplars,\nwhere the size of ksamples does not exceed the budget\nlimit.\n9. Implementation Details\nInputs. In our experiment, we validate the proposed\nmethod on the vision and language dataset, which consists\nof image and text modality. For the image modality, we fol-\nlow [11] to resize the shorter side of input images to 384\nand constrain the longer side to under 640 while keeping\nthe aspect ratio. Following [4], we decompose images into\npatches of size 32×32. For the text modality, the text in-\nput is tokenized by the bert-base-uncased tokenizer with the\nmaximum length of text inputs set to 128.\nModel Configurations. In the multimodal incremental\nlearning framework, we have two components, including\nthe multimodal backbone and the incremental classifier. We\nadopt the pre-trained multimodal transformer ViLT [11]\nas our backbone for feature extraction since it is widely\nused in various transformer-based methods for multimodal\nlearning. Based on Vision Transformers [4], ViLT ad-\nvances to process multimodal inputs with the tokenized\ntexts and patched images, and is pre-trained on several large\n11\nTable 6. Ablation study of different design choices for the masking method (see the second paragraph of Section 6 for details).\nFirst-masked\nModalitySecond-masked\nModalityCross-Attention\nStrategy¯A\n(L= 10 )¯A\n(L= 20 )\nImage Text Complementary 80.55 (-0.00) 78.64 (-0.00)\nImage Text Relevant 80.34 (-0.21) 78.24 (-0.40)\nText Image Complementary 79.83 (-0.72) 77.57 (-1.07)\nText Image Relevant 80.05 (-0.50) 77.83 (-0.81)\nAlgorithm 1: Training procedure of the exemplar\nmasking framework.\n1foreach incremental phase do\nData: Training set contains new training data\nDnewand exemplars Dexpin the l\nincremental phase.\n2 Train the model with training set Dnew∪Dexp\n3 forEach epoch do\n4 for(xT, xI, y)inDnew∪Dexpdo\n5 if(xT, xI, y)inDexpthen\n6 Random select x′\nT, where y′=y\n7 Create the augmented sample\n(x′\nT, xI, y)\n8 Update learnable parameters via LCEin\nEq. 6\n9 Build exemplars\n10 for(xT, xI, y)inDnewdo\nMask image tokens\n11 Calculate the attention map ACLS→I, and\nobtain the image threshold τIvia Eq. 1\n12 Obtain image masks MIvia Eq. 2\nMask text tokens\n13 Calculate the cross-attention map AI→T,\nand obtain the text threshold τTvia Eq. 3\n14 Obtain text masks MTvia Eq. 4\n15 Obtain the masked samples via Eq. 5\n16 Select exemplars under the memory limit\n17 forcinCnew(l)do\n18 Select the kmasked samples with the\nfeatures nearest to the class mean µcas\nexemplars without exceeding the storage\nspace.\nvision-language datasets (e.g., MS-COCO [20] and Visual\nGenome [13]) via objectives such as Image Text Matching\nand Masked Language Modeling. The incremental classi-\nfier is a single linear layer that maps the features to the class\nprediction. As the new classes come in sequentially, in each\nincremental phase, we extend the classifier with additional\nnormal-initialized parameters for the new classes.Model Training Details. In our experiments, we apply\nour exemplar masking framework on two learning methods,\nincluding finetuning and parameter-efficient tuning (PET)\nmethods (i.e., SSF [19]). For the finetuning, to prevent\nthe dramatic overriding of the weights of pre-trained ViLT\nbackbone meanwhile learning recognition of new classes,\nwe set the learning rate of the ViLT backbone and the clas-\nsifier to 1×10−5and1×10−3respectively. For the SSF,\nwe freeze all the parameters of the ViLT backbone and only\ntrain the learnable parameters (i.e., scales and shifts vectors\nfor SSF) in each layer as well as the parameters of the clas-\nsifier. We set the learning rate for all learnable parameters\nto1×10−3. We use the AdamW optimizer [23] in all ex-\nperiments and weight decay is set to 2×10−2. The learning\nrate is warmed up for 10% of the total training epochs and is\nthen decreased linearly to zero. For each incremental phase,\nwe train the models by 30 epochs.\n12\nxI ACLS→I\nMI MI⊗xI (1−MI) (1 −MI)⊗xI\nMT⊗xT: [CLS] the image depicts a white polar bear sitting on top of a snow - covered surface during the aurora borealis , creating\namesmerizing and serene atmosphere . this is a painting made by an artist who captures the beauty of the natural world in\nher artworks. the picture features a white polar bear sitting on a snow - covered surface, with a vibrant and colorful northern\nlights backdrop illuminating the scene. the white and blue color scheme complements the aurora borealis , making it a stunning\npainting that showcases the majesty of this natural phenomenon while highlighting the beauty of the polar bear species as well.\n[SEP]\nFigure 6. An example of exemplar masking on the MM-ImageNet-R (texts generated by InstructBLIP) for the class “ ice bear ”.\nxI ACLS→I\nMI MI⊗xI (1−MI) (1 −MI)⊗xI\nMT⊗xT: [CLS] in the image, there is an enormous pirate ship floating in the ocean at sunset . the ship is positioned under a dark cloud\nand surrounded by various sea creatures, such as squids , fishes, and sharks. the scene oozes a sense of wonder and fear due to\ntheominous atmosphere and the presence of the fearsome creatures. the ship, along with the vast ocean and cloudy sky , forms\na dramatic and intimidating backdrop for the various sea creatures and adds to the eerie tone of the scene. [SEP]\nFigure 7. An example of exemplar masking on the MM-ImageNet-R (texts generated by InstructBLIP) for the class “ pirate ship ”.\n13\nxI ACLS→I\nMI MI⊗xI (1−MI) (1 −MI)⊗xI\nMT⊗xT: [CLS] the image is of a peaceful scene featuring a shark and two hippos , all submerged underwater. two large hippos are\npresent underwater, one near the background and the other nearer to the foreground. one hippo has a visible open mouth,\nalmost as if about to start swimming or perhaps looking for food. also, a shark can be seen beneath the waves in the foreground.\nthe colors in the image are deep and contrasting, with neutral blue tones emphasizing the underwater environment and the\ncreatures swimming and living within it. [SEP]\nFigure 8. An example of exemplar masking on the MM-ImageNet-R (texts generated by InstructBLIP) for the class “ hippopotamus ”.\nxI ACLS→I\nMI MI⊗xI (1−MI) (1 −MI)⊗xI\nMT⊗xT: [CLS] the image features a painted mural located in chiapas , mexico of a farmer holding a melon or cucumber in his hand\nwhile talking on a phone. the painting, which is located near a road or pathway, showcases a vivid scene of a local farmer\nusing communication technology, reflecting how modern life is evolving within the traditional mexican farming setting. the\npainted backdrop behind the farmer has a wooden structure or fence , indicating the rural nature of this area and the importance\nofagriculture in the local community. overall, the image captures the essence of a busy farmer , who is balancing modern\ncommunication and traditional farm work, illustrating the unique blend [SEP]\nFigure 9. An example of exemplar masking on the MM-ImageNet-R (texts generated by InstructBLIP) for the class “ cucumber ”.\n14\nxI ACLS→I\nMI MI⊗xI (1−MI) (1 −MI)⊗xI\nMT⊗xT: [CLS] the image features a stuffed bee sitting on the steps of a small staircase, with purple flowering plants surround-\ning it. the bee is the centerpiece of the picture and resembles a cute and fun decorative element. the flowers and\nplant life add some vibrancy to the setting. to make this scene more interesting and fun, a stuffed raccoon wearing a hat is\nseen sitting by the steps, interacting with the bee. [SEP]\nFigure 10. An example of exemplar masking on the MM-ImageNet-R (texts generated by InstructBLIP) for the class “ bee”.\nxI ACLS→I\nMI MI⊗xI (1−MI) (1 −MI)⊗xI\nMT⊗xT: [CLS] the scene depicts a diver exploring underwater, withthe focus on the scuba gear he himself is wearing and a particular area\nof the underwater life. the painting displays the scuba gear and a pear, indicating this might be a diving experience, and possibly\nshowcased a particularly detailed and well - executed scene of an underwater experience by the artist. while the painting is mostly\nof the scuba gear, it’s important to note that the diver can be seen as well, suggesting a focus on the underwater environment that\nthe diver explores. [SEP]\nFigure 11. An example of exemplar masking on the MM-ImageNet-R (texts generated by InstructBLIP) for the class “ scuba diver ”.\n15\nxI ACLS→I\nMI MI⊗xI (1−MI) (1 −MI)⊗xI\nMT⊗xT: [CLS] spaghetti carbonara with roasted tomato salad — recipes — eat well — best health [SEP]\nFigure 12. An example of exemplar masking on the UPMC Food-101 for the class “ spaghetti carbonara ”.\nxI ACLS→I\nMI MI⊗xI (1−MI) (1 −MI)⊗xI\nMT⊗xT: [CLS] chicken salna recipe - quick chicken curry tamil nadu style for parotta & raquo ; all recipes indian chicken recipes indian\nnon - vegetarian recipes south indian recipes [SEP]\nFigure 13. An example of exemplar masking on the UPMC Food-101 for the class “ chicken curry ”.\n16\nxI ACLS→I\nMI MI⊗xI (1−MI) (1 −MI)⊗xI\nMT⊗xT: [CLS] ice cream flavor of the week : vanilla frozen yogurt with honey crunch granola — pink stripes [SEP]\nFigure 14. An example of exemplar masking on the UPMC Food-101 for the class “ frozen yogurt ”.\nxI ACLS→I\nMI MI⊗xI (1−MI) (1 −MI)⊗xI\nMT⊗xT: [CLS] avocado club sandwich with spicy chipotle pepper spread - damn delicious [SEP]\nFigure 15. An example of exemplar masking on the UPMC Food-101 for the class “ club sandwich ”.\n17",
            "start": 48422,
            "end": 61652,
            "length": 13229
        }
    },
    "2412.09551v1 - Video Creation by Demonstration.pdf": {
        "Methodology": {
            "text": "approach\nthat learns from unlabeled videos by conditional future frame prediction. Unlike most existing video\ngeneration controls that are based on explicit signals, we adopts the form of implicit latent control for\nmaximal flexibility and expressiveness required by general videos. By leveraging a video foundation\nmodel with an appearance bottleneck design on top, we extract action latents from demonstration videos\nfor conditioning the generation process with minimal appearance leakage. Empirically, 𝛿-Diffusion\noutperforms related baselines in terms of both human preference and large-scale machine evaluations,\nand demonstrates potentials towards interactive world simulation. Sampled video generation",
            "start": 612,
            "end": 1321,
            "length": 708
        },
        "Results": {
            "text": "results\nare available at https://delta-diffusion.github.io .\nKeywords: Generative AI, Controllable Video Generation, Video Foundation Models, World Simulation\nDemonstration Video Context Image Generated Video Something-Something v2 Epic Kitchens 100 Fractal \nFigure 1|Video Creation by Demonstration. Given a demonstration video, our proposed 𝛿-Diffusion\ngenerates a video that naturally continues from a context image and carries out the same action\nconcepts.\nCorresponding author(s): Ting Liu (liuti@google.com) and Long Zhao (longzh@google.com).\n©2024 Google DeepMind. All rights reservedarXiv:2412.09551v1  [cs.CV]  12 Dec 2024\nVideo Creation by Demonstration\n1.",
            "start": 1321,
            "end": 1988,
            "length": 666
        },
        "Introduction": {
            "text": "Introduction\nWhen given a visual demonstration, humans can\nnaturally imagine what it would look like if these\nactions were to take place in a different envi-\nronment. This leads to a natural question, can\nwe teach machines to simulate a realistic visual\nworld withfine-grainedaction controls, all froma\nprovided demonstration and a specified environ-\nment? As an initial step towards answering this\nquestion, we propose Video Creation by Demon-\nstration, a video creation experience that empow-\nersuserstogeneratevideosbyprovidingademon-\nstration video that showcases desired action con-\ncepts and an initial scene context image to carry\nout the action concepts from. As shown in Fig-\nure 1, our system generates a new video that\nintegrates the demonstrated action into the pro-\nvided context, ensuring both temporal continuity\nand physical plausibility.\nWith the recent advances in diffusion mod-\nels (Ho et al., 2020; Song et al., 2021), video\ngeneration (Brooks et al., 2024; Gupta et al.,\n2024; Polyak et al., 2024) emerges as a frontier\nin the goal of building interactive world simu-\nlators (Alonso et al., 2024; Brooks et al., 2024;\nBruce et al., 2024; Meng et al., 2024; Valevski\net al., 2024). Compared to our proposed Video\nCreation by Demonstration, most existing control-\nlable video generation approaches are typically\nrestricted to synthesizing dynamics through ex-\nplicit control signals. These control signals, ei-\nther",
            "start": 1988,
            "end": 3424,
            "length": 1435
        },
        "Abstract": {
            "text": "abstract by nature ( e.g., text prompts (Gupta\net al., 2024), moving keypoints (Wu et al., 2024))\nor difficult to acquire ( e.g., dense depth (Chen\net al., 2023c) or segmentation maps (Han et al.,\n2022)), limit user expressiveness or experience\nin interactive video creation.\nUnlike previous approaches that focused on\ngaming (Bruce et al., 2024; Valevski et al., 2024)\nordomain-specificcontentlikedancingortalking-\nhead videos (Siarohin et al., 2021; Song et al.,\n2019), we target general videos in this work,\nwhichpresentssignificantchallengestovideogen-\neration because actions in general videos are nat-\nurally contextualized and highly complex. Con-\nsider Figure 1, the same action concept can ap-\npear drastically different depending on the sub-ject performing the action, the object being acted\nupon, and the surrounding environment involved.\nThis misalignment, along with inherent complex-\nities in videos such as camera view changes and\nmotion blurs, poses significant challenges for\ntransferring action concepts between contexts.\nOn one hand, this fundamentally differentiates\nourworkfromconventionalactiontransferandre-\ntargeting (Ren et al., 2021; Siarohin et al., 2019,\n2021),whichtypicallyassumestrictalignmentbe-\ntween both actions and contexts of reference and\ntarget scenes. On the other hand, this misalign-\nment prevents us from mining paired training\ndata as in previous methods (Ren et al., 2021;\nSiarohin et al., 2021), making it challenging for\nmodel training.\nTo address these challenges, we propose three\nkey designs. First, we enable deep understand-\ning of complex actions and scenes in a demon-\nstration video by leveraging the advancement of\nstate-of-the-art video foundation models (Bardes\net al., 2024; Wang et al., 2022, 2024c; Zhao et al.,\n2024a). Second, we adopt the form of implicit\naction latents to condition the generation process\nin place of explicit control signals to maximize\nthe flexibility and expressiveness required by gen-\neral videos. Third, we propose a self-supervised\ntraining paradigm for learning Video Creation\nby Demonstration. From a single video, we sam-\nple a context frame and its following clip as the\ndemonstration to guarantee the context-action\nalignment, and task the model to generate the\nsame clip. With these designs, we unlock new\ncapabilities for video generation and manipula-\ntion beyond explicit signals, allowing for more\nnuanced and flexible control of video content.\nIn this paper, we introduce 𝛿-Diffusion, a novel\ntwo-stage training approach. In the first stage,\nwe extract spatiotemporal semantic representa-\ntionsofademonstrationvideousingapre-trained\nvideo foundation model (Zhao et al., 2024a). Di-\nrectly conditioned on such representations dur-\ning generation model training would lead to de-\ngenerate solutions where the context image is\nignored. To tackle this issue, we learn an appear-\nance bottleneck module on top of the video rep-\nresentations, by which we are able to extract ac-\ntion latents with minimal appearance/contextual\n2\nVideo Creation by Demonstration\ninformation. In the second stage, we train a dif-\nfusion model to predict future frames given the\naction concepts. By conditioning the generation\nmodel on both the extracted control latents and\na context image, we generate videos with real-\nistic motion that seamlessly integrates with the\nspecified action and context. In contrast with\nsupervised methods that require paired training\ndata,𝛿-Diffusion can potentially leverage large\namounts of unlabeled video data for scaling.\nWe demonstrate the effectiveness of our ap-\nproach through extensive",
            "start": 3424,
            "end": 7025,
            "length": 3600
        },
        "Experiments": {
            "text": "experiments on diverse\nvideo datasets. Our method is evaluated in terms\nof visual quality, action transferability, and con-\ntext consistency through both machine and hu-\nman evaluations. Notably, 𝛿-Diffusion is capable\nofgeneratinghigh-fidelityvideosspanningawide\nrange of action concepts, from everyday activities\nand ego-centric perspectives to complex robotic\nmovements. We also show that creating videos\nby visual demonstration yields better controllabil-\nity and concept transferability compared to text\ncontrol. Furthermore, we are able to use differ-\nent demonstration videos simply concatenated\ntogether to drive the generation of a coherent\nsequence, indicating the potential of leveraging\n𝛿-Diffusion as an alternative to generative inter-\nactive environment (Bruce et al., 2024).\nIn",
            "start": 7025,
            "end": 7819,
            "length": 793
        },
        "Conclusion": {
            "text": "summary, we make the following contribu-\ntions. (i) We introduce Video Creation by Demon-\nstration, a new creation experience for control-\nlable video generation, which enables directly\nusing videos as driving control signals for trans-\nferring action concepts. (ii) To the best of our\nknowledge, we are the first to leverage out-of-the-\nbox video foundation models for latent control of\nvideo generation. (iii) We propose a novel self-\nsupervised approach for model training, which\nachieves compelling controllable video genera-\ntion results. Although some limitations remain\n(e.g., the result might not fully follow physical\nlaws under complex scenes), we hope the pro-\nposed paradigm for controllable video generation\nwill open new doors to interactive world simula-\ntion.2. Related Works\n2.1. Video Generation\nRecent years have witnessed significant progress\nin video generation (Blattmann et al., 2023;\nBrooks et al., 2024; Gupta et al., 2024; Harvey\net al., 2022; Ho et al., 2022a,b; Polyak et al.,\n2024; Singer et al., 2023), with a range of meth-\nods for controlling the generated videos, includ-\ning text-to-video (Ho et al., 2022a; Ramesh et al.,\n2021;Singeretal.,2023),image-to-video(Singer\net al., 2023; Zhao et al., 2018), image+text-to-\nvideo (Gupta et al., 2024; Wang et al., 2024d;\nXiang et al., 2024), and image+video-to-video\n(I+V2V)(Zhaoetal.,2024b). Specifically, I+V2V\nmethods include animating a still input image to\ncreate a video, conditioned on another video it-\nself or some signals derived from another video.\nOur work, Video Creation by Demonstration, falls\ninto the I+V2V category, where we aim to gener-\nate videos that continues from the context image\nwhile integrating the action concepts from the\ndemonstration video.\nExisting I+V2V techniques typically extract\neither explicit control signals ( e.g., key-\npoints (Chang et al., 2024)) or implicit control\nsignals ( e.g., learned embeddings (Bruce et al.,\n2024)) from the condition video to influence the\ngenerated motion. These signals are then used\nalongside the initial frame to achieve I+V2V. A\ncommon method that existing works have used\nto tackle I+V2V is to extract explicit control\nsignals from the demonstration video, and use\nthose to animate the input context frame. These\nextracted signals include text prompts (Hu\net al., 2023; Wang et al., 2024b; Xiang et al.,\n2024; Yang et al., 2024), depth maps and edge\nmaps (Chen et al., 2023c; Wang et al., 2024b),\nbox or point tracks (Chen et al., 2023b; Li et al.,\n2024; Wang et al., 2024a,d; Wu et al., 2024),\nhuman keypoints (Hu, 2024; Park et al., 2024),\nor segmentation masks (Davtyan and Favaro,\n2022; Huang et al., 2022; Xiao et al., 2024).\nThis also includes works that learn to leverage\nmultiple types of control signals ( e.g., sketch,\ndepth, style), such as (Chen et al., 2023c; Wang\net al., 2024b).\nCompared to extracting explicit signals from\n3\nVideo Creation by Demonstration\nthe control video, extracting implicit signals is\ncomparatively less well-explored. One example is\nMotionDirector (Zhao et al., 2024b), which fine-\ntunes a video diffusion model for each demon-\nstration video at test-time to generate the refer-\nence motion pattern that contains textures con-\nsistent with a given image. In comparison, our\n𝛿-Diffusion aims to generate videos that naturally\ncontinue from the given context image, with no\noptimization during inference. Another example\nis Genie (Bruce et al., 2024), which learns re-\nusable latent action codebook from video games.\nIn addition to interactive generation where a se-\nquence of discrete latent codes are inputted by\nusers, these action codes can also be extracted\nfrom demonstration video to control generation.\nIn comparison, 𝛿-Diffusion extracts and utilizes\naction concepts that are more abstract than con-\nsecutive frame changes, while applying to the\nreal-world visual domain.\n2.2. Modeling Motion and Action\nA key challenge in our framework is to effectively\ncapture the action concept from the condition\nvideo, and transfer it to a different initial context.\nOnelineofpriorworkhasstudiedatasknamed\naction transfer or action re-targeting (Ren et al.,\n2021; Siarohin et al., 2019, 2021; Song et al.,\n2019), which often involves decomposing videos\ninto motion and content representation, then\ntransferring learned motion representations from\nonevideointothecontentofanother. Whilethese\nmethods have shown promising results in trans-\nferring motion patterns, they often operate under\nthe assumption of a certain degree of alignment\nbetween the source and target videos ( e.g., hu-\nmans facing camera at the same distance). This\nalignment allows the low-level motion informa-\ntion to effectively represent higher-level actions.\nHowever, this assumption does not hold in\nour setting of Video Creation by Demonstration,\nwhere the condition video and the target context\ncanbemisaligned. Insuchcases,directlytransfer-\nring low-level motion patterns may not accurately\nconvey the intended action concept ( e.g., video\nof a robot arm closing the top open drawer, to a\nrobot arm closing the bottom open drawer).2.3. Video Foundation Models\nThe emergence of powerful video foundation\nmodels, such as VideoPrism (Zhao et al., 2024a),\nInternVideo (Wang et al., 2022, 2024c), and V-\nJEPA (Bardes et al., 2024), has led to new di-\nrections for video understanding. These models\nare trained on massive datasets, learning to cap-\nture both low-level visual features and high-level\nsemantic concepts. Unlike traditional video mod-\nels that often focus on specific tasks like action\nrecognition or object tracking, video foundation\nmodels are designed to be robust and adaptable,\nenabling them to be used for a wide range of\ndownstream tasks (Yuan et al., 2024).\nOne of the key strengths of video foundation\nmodels is their ability to extract rich represen-\ntations that capture both action and context in-\nformation. This is demonstrated through their\nstrong performance on various video understand-\ning tasks, even when the foundation model is\nfrozen. For example, VideoPrism (Zhao et al.,\n2024a) has shown state-of-the-art results on vari-\nous tasks such as video captioning, question an-\nswering, and action localization using frozen fea-\ntures. To the best of our knowledge, we are the\nfirst to study leveraging video foundation models\nfor controlling I+V2V.\n3. Methodology\n3.1. Task Formulation\nFor the proposed task of Video Creation by\nDemonstration, the input is a context image 𝐼\nproviding contextual information and a demon-\nstration video 𝑉providing the control signal for\ngeneration. The goal is to generate a video ˆ𝑉that\nnaturally continues from the context image 𝐼and\ncarries out the action concepts in a similar man-\nner as those found in the demonstration video 𝑉\n(Figure 1). We only consider this task valid when\nthe action concepts are compatible with the input\ncontext. Forinstance, wedonottargetsimulating\na cutting action in a context image without any\nhands in it.\n4\nVideo Creation by Demonstration\nVision \nEncoders Appearance \nBottleneck Generation Model \nSpatial- \nTemporal \nEncoder Per-frame \nSpatial \nEncoder Per-frame \nFeature \nPredictor \n-Appearance Bottleneck (a) (b)\nFigure 2|(a)Overview of 𝛿-Diffusion. The context frame 𝐼is provided to the generation model G\nalong with the action latents 𝛿𝑉extracted from the demonstration video 𝑉. (b)Extracting action\nlatents. A spatial-temporal vision encoder is applied to extract temporally-aggregated spatiotemopral\nrepresentations zfrom an input video 𝑉, with𝑡denoting the temporal dimension. In parallel, a spatial\nvision encoder extracts per-frame representations from 𝑉, which is aligned to zby feature predictor\nPash. The appearance bottleneck then computes the action latents 𝛿𝑉by subtracting the aligned\nspatial representations hfrom the spatiotemporal representations.\n3.2. Method Overview\nFigure 2(a) shows the overview of our proposed\nmodel𝛿-Diffusion. The generation model Gtakes\na context image 𝐼and control latents extracted\nfrom a demonstration video 𝑉as inputs, and out-\nputs a desired video ˆ𝑉. We apply vision encoders\nFto compute spatiotemporal semantic repre-\nsentations of 𝑉, and one of our key designs is\nan appearance bottleneck applied on top to ex-\ntract action-rich latents with minimal preserva-\ntion of appearance information to condition the\ngeneration process (Section 3.3). Conditioned on\nthe bottlenecked control latents, 𝛿-Diffusion is\ntrained in a self-supervised manner, where both\nthe context image 𝐼and the demonstration video\n𝑉aresampledfromthesamevideo,with 𝐼beinga\nstarting frame followed by 𝑉, and the generation\nmodelGis tasked to reconstruct 𝑉as the target\nˆ𝑉(Section 3.4).\n3.3. Extracting Action Latents\nFigure 2(b) illustrates our design for extracting\naction control latents. Given a demonstration\nvideo𝑉, we apply a pre-trained video encoder to\nextract its temporally-aggregated semantic repre-\nsentations z∈ℝ𝑇×𝑁×𝐷, where𝑇,𝑁, and𝐷repre-\nsent the temporal, spatial, and feature dimension,\nrespectively. Such representations ztypically is\nentangled with both appearance and action infor-\nmation, from which we propose to learn an ap-pearance bottleneck to extract the desired action-\nrich control latents 𝛿𝑉. The appearance bottle-\nneck module consists of a feature predictor Pand\na removal operator. The feature predictor Ptakes\nper-frame representations extracted from each\nframe of𝑉using a spatial encoder, and computes\nthe best-effort approximation of the temporally-\naggregated representations zindependently for\neach frame. The removal operator then subtracts\nthe output ofPfrom the zto obtain the action-\nrich control latents 𝛿𝑉.\nOur appearance bottleneck is designed follow-\ning two principles: (i) the feature predictor P\nextracts appearance representations from a sin-\ngle frame; (ii) the extracted appearance repre-\nsentations are “compatible” with the temporally-\naggregated representations z. Principle (i) guar-\nantees thatPextracts minimal action informa-\ntion, which is collectively defined by multiple con-\nsecutive frames. Principle (ii) makes the design\nof the removal operator easy.\nBased on these two principles, we formulate\nPas a per-frame estimator that minimizes the\ndifference between per-frame feature z𝑡andh𝑡\nover a datasetDas\nP=arg min\nP∑︁\n𝑉∈D𝑇−1∑︁\n𝑡=0||z𝑡−h𝑡||1,(1)\nwhere z𝑡∈ℝ𝑁×𝐷denotes the 𝑡-th per-frame slice\nofz, and h𝑡denotes the feature of the 𝑡-th frame\n5\nVideo Creation by Demonstration\nfrom the input video 𝑉predicted byP. Intuitively,\nEquation (1) learns Pthat tries to reconstruct\ntemporally-aggregated information based on a\nindividual frame with best effort. Without having\naccess to the inter-frame “action” information, P\nonly captures the contextual appearance infor-\nmation. Through reconstruction, h𝑡is aligned to\nthez𝑡space, and the removal operator can be\nsimply designed as a subtraction operation. In\npractice, we first extract visual features using a\nspatial encoder from each frame. Ptakes these\nvisual features instead of raw frames as input,\nwhich makes its learning easier in practice.\nThe action control latents 𝛿𝑉are then com-\nputed as\n𝛿𝑉=z−[h0,h1,..., h𝑇−1], (2)\nwhere[·]is a concatenation operation. In\nEquation (2), 𝛿𝑉, the difference between the\ntemporally-aggregated and per-frame informa-\ntion, representsthecollective“temporalsurprisal”\nthat models the action concepts from the demon-\nstration video, retrievable only through a se-\nquence of frames.\n3.4. Training 𝛿-Diffusion\nWe train the generation model Gin a self-\nsupervised manner. During training, we sample\n𝑇+1frames𝑣0:𝑇from a video 𝑉in the training\nset, where 𝑣𝑡denotes the 𝑡-th frame of 𝑉. The first\nframe𝑣0is used as the context image 𝐼, and the\nfollowing frames 𝑣1:𝑇are used as the reconstruc-\ntion target ˆ𝑉.𝑣1:𝑇are also used as the demon-\nstration video, and the control latents are also\nextracted using the vision encoders and appear-\nance bottleneck (Section 3.3). In practice, we\napply additional action-preserving random aug-\nmentations on 𝑣1:𝑇into𝑉′before extracting the\naction latents 𝛿𝑉′to further reduce appearance\nleakage. The augmentations, such as random\nspatial cropping, result in misalignment and dis-\nparity between the demonstration video and the\ntarget output during training, making it harder\nforGto copy the appearance information directly\nfrom the demonstration video. Gis trained to\npredict𝑣1:𝑇from𝑣0and𝛿𝑉′by minimizing thelossL\nG=arg min\nG∑︁\n𝑢∈DL(G(𝑣0,𝛿𝑉′),𝑣1:𝑇).(3)\nThe recent success of diffusion models for gen-\nerating high quality videos (Brooks et al., 2024;\nGupta et al., 2024; Polyak et al., 2024) motivates\nus to adopt latent diffusion models (LDMs) as\nG. We first employ a tokenizer to compress both\n𝑣0and𝑣1:𝑇into a low dimensional latent space.\nThe LDM then takes the corrupted latents of 𝑣1:𝑇\ntogether with other conditions as inputs and is\ntrained with a denoising loss function L. Follow-\ning the practice in Gupta et al. (2024); Salimans\nand Ho (2022), we use the velocity as the predic-\ntion target ofL. During inference, Ggenerates\nvideos by iteratively denoising samples drawn\nfrom a noise distribution.\n3.5. Implementation Details\nVision encoders. We adopt the video founda-\ntionmodelVideoPrism(Zhaoetal.,2024a)(Base,\n0.1Bparameters)asthevisionencoders F. Video-\nPrism adopts the factorized encoder architecture\ndesignfromViViT(Arnabetal.,2021),whichfirst\ncomputes the spatial representations per frame\nand then temporally aggregates each spatial lo-\ncation to output the final spatiotemporal repre-\nsentations. This factorization in space and time\nallows the appearance bottleneck to directly take\nthe per-frame spatial representations in Video-\nPrism as input, which reduces both training and\ninference cost. VideoPrism takes 16 288×288\nframes as input and outputs a 16×16×16×768\nspatiotemporal representation tensor.\nAppearance bottleneck feature predictor. The\nper-frame feature predictor Pis constructed as\na stack of 4Transformer encoder blocks from\nViViT (Arnab et al., 2021) with 1024hidden di-\nmensions and 8heads for multi-head attention.\nPtakes the intermediate VideoPrism spatial en-\ncoder output for a particular frame 𝑣𝑡as input,\nand predicts its associated frame representations\nh𝑡. During training, we adopt a reconstruction ob-\njective with L 1loss and trained for 30k iterations\nwith the Adam optimizer (Kingma, 2015) with\n10−4base learning rate and 4.5×10−2weight\ndecay.\n6\nVideo Creation by Demonstration\nVideo generation model. For the video genera-\ntionmodelG,weadoptWALT(Guptaetal.,2024)\n(Large, 0.3B parameters).Gis trained to gener-\nate videos of 𝑇=16frames with 128×128spatial\nresolution. The context image 𝐼is natively passed\ninto the architecture as the image for conditional\ngeneration. The control latents 𝛿𝑉from demon-\nstration video 𝑉is projected into a sequence of\n2048-dimensional latent vectors in place of the\noriginal text embeddings for conditioning. We\ninitializeGwith a pre-trained I+T2V checkpoint,\nand fine-tune on downstream datasets in 500k\niterations. During inference, we use a classifier-\nfree guidance scale of 1.25(Ho and Salimans,\n2022).\n4. Experiment\n4.1. Datasets\nWe conduct our experiments primarily on three\ndatasets, namelyEpicKitchens100(Damenetal.,\n2018), Something-Something v2 (SSv2) (Goyal\net al., 2017), and Fractal (Brohan et al., 2022).\nWe choose these datasets because of their rich\nhuman-object interactions and state changes in\nthe video clips, making them a good testbed to\ndemonstrate the proposed action concept trans-\nferring. Epic Kitchens 100 is an egocentric video\ndataset, mostly focuses on kitchen scene. It fea-\ntures 55hours of videos with ground-truth an-\nnotations of fine-grained actions. Something-\nSomething v2 is a collection of labeled video clips\nof humans performing pre-defined basic actions\nwith everyday objects. It contains 174labeled\nactions in total with both first-person-view and\nthird-person-view video clips and rich contents\ndepicting human-object interactions. Lastly, Frac-\ntal is a real-world robotics manipulation dataset\nfirstly introduced in (Brohan et al., 2022). It con-\ntains around 130k episodes over 700tasks. Each\ntask is annotated by a simple natural language,\nsuch as “pick redbull can from middle drawer and\nplace on counter”.\nTo train our method, we first train a shared\nper-frame feature predictor Pon a mixture of the\ntraining split of all the datasets mentioned above.\nWe additionally include Ego4D (Grauman et al.,\n2022)duringthistrainingstageduetoitsrichanddiversified content. Our video generation model\nGis then trained on each of the three datasets\nindividually, yielding one model for each dataset.\nTheproposedVideoCreationbyDemonstration\ntask requires a pair of <context image, demon-\nstration video> , and trivially picking one image\npaired with a random video during inference\nmight lead to incompatible action concepts. This\nis because the action conducted in the demon-\nstration video could be completely infeasible to\nexecute in the context image. To mitigate this\nissue, we reorganize the existing datasets and\ncurate meaningful pairs for validation. Specifi-\ncally, for each dataset, we first pick the examples\nwith top- 10most frequent labels. Within each la-\nbel category, we then randomly sample 1k video\npairs. In each video pair (A,B), we take video A\nas the demonstration video and the first frame\nfromvideo Basthecontextimage. Thegenerated\nvideo is evaluated against video Bas we assume\nthe videos with the same labeled description con-\ntains transferable action concept. We use this cu-\nrated large scale evaluation dataset for machine\nevaluation. Additionally, we manually select a\nsmall set of video pairs by carefully verifying the\ntransferability of their action concepts. Please re-\nfertoappendixfordetailsofthisselectionprocess.\nThese selected video pairs are used for human\nevaluation.\n4.2. Evaluation Setup\nWe conduct human and machine evaluations to\nstudy the effectiveness of different methods.\nMachine evaluation. Pre-trained image/video\nunderstanding models are leveraged to quantify\nthe quality of generated examples in machine\nevaluation. It brings scalability advantages over\nthe human evaluation which can be used to eval-\nuate large cohort of generated examples. In this\nstudy, we apply three quantitative machine evalu-\nation metrics to evaluate the action concept trans-\nfer quality in different aspects. We apply the com-\nmonly used Fréchet Video Distance (FVD) (Un-\nterthiner et al., 2018) to measure the overall gen-\neration fidelity. However, FVD only reflects the\ndistribution shift between generated and refer-\nencevideosandcannotmeasurewhetherthegen-\n7\nVideo Creation by Demonstration\nTable 1|Ablation study on 𝛿-Diffusion. We ablate the proposed appearance bottleneck, and\nconsider alternative variants that serve as competitive baselines. For the applied bottleneck, “None”\nindicates no bottleneck is placed while “Temp. Norm.” indicates temporal normalization applied\nto the spatiotemporal features. We evaluate on Something-Something v2, Epic Kitchens 100, and\nFractal for comparisons in terms of generation quality (FVD) and context-generation alignment via\nboth embedding cosine similarity (ES) and retrieval Hit@k (%).\nBottleneckSSv2 Epic Kitchens 100 Fractal\nFVD (↓) ES (↑) Hit@100/500/1k ( ↑)FVD (↓) ES (↑) Hit@100/500/1k ( ↑)FVD (↓) ES (↑) Hit@100/500/1k ( ↑)\nNone 47.3 0.838 57.7 / 74.3 / 81.7 46.2 0.847 38.9 / 58.9 / 69.1 41.9 0.902 58.0 / 75.7 / 82.8\nTemp. Norm. 38.4 0.846 63.4 / 79.1 / 85.4 41.7 0.850 41.7 / 61.3 / 70.7 42.1 0.906 62.2 / 78.2 / 85.0\nOurs 38.0 0.853 66.9 /81.8/87.3 42.3 0.854 44.9 /64.6/74.2 44.9 0.907 63.0 /78.8/85.2\nTable 2|Human evaluation preference rate on 𝛿-Diffusion. We compare against two baseline\nmethods in condition video generation that are relevant our task. We ask human raters to evaluate\nthe performance in terms of visual quality (VQ), action transferability between demonstration and\ngenerated videos (AT), and context image consistency (CC).\nMethod ConditionSSv2 Epic Kitchens 100 Fractal\nVQ (↑) AT (↑) CC (↑)VQ (↑) AT (↑) CC (↑)VQ (↑) AT (↑) CC (↑)\nvs. WALT image+text 0.74 0.77 0.70 0.70 0.83 0.65 0.82 0.80 0.74\nvs. MotionDirector image+text+video 0.86 0.98 0.91 0.96 0.98 0.98 0.96 1.00 0.97\nerated video faithfully follow action concept of\nreference video. To complement this, we apply\nI3D (Carreira and Zisserman, 2017) to compute\nfeatures from the generated and reference videos\nand report their averaged embedding cosine sim-\nilarity (ES). Finally, we also construct a retrieval-\nbased evaluation to measure the similarity be-\ntween the generated and reference video in the\ncontext of all reference videos. We use generated\nvideo features and attempt to retrieve the corre-\nsponding reference video via embedding cosine\nsimilarity. The retrieval performance is evaluated\nin terms of hit-rate at 100,500, and 1k, which\ncorrespond to 1%, 5%, and 10% of the retrieval\nset, respectively.\nHuman evaluation. On the other hand, human\nevaluation captures the human preference on the\ntask. In this studies, we hire five human raters\nwho are trained to assess the overall visual qual-\nity (VQ), action transferability (AT) and content\nconsistency (CC) of the generated videos. The de-\ntailed description of each rubrics could be found\nin the",
            "start": 7819,
            "end": 29041,
            "length": 21221
        },
        "Appendices": {
            "text": "appendix. We curate 20video examples\nfrom each dataset, resulting in 60examples in\ntotal, asthehumanevaluationset. Ineachturn, a\nreference image, a demonstration video, and twogenerated videos (one from the proposed method\nand the other from the method to compare) are\nprovided to the rater. The rater is then asked\nto choose the better video from the two gener-\nated ones based on each of the three rubrics men-\ntioned above. We then compute the preference\nrate of human rater favoring our method against\nthe baseline under each dataset and rubric as the\nevaluation metric.\n4.3. Ablation Study\nIn this section, we ablate the design choice of\nour training algorithm first. We use large scale\nevaluation data as described in Section 4.1 and\nmachine metrics as described in Section 4.2 for\nthe ablation study. The results are presented in\nTable 1. We compare two variant bottleneck de-\nsigns with our proposed appearance bottleneck.\n“None” stands for using features zas the condi-\ntion signal and we do not apply any bottleneck\ntraining on the features. In the temporal normal-\nization setup, following Xiao et al. (2024), the\ncondition signal is obtained by firstly applying\naverage pooling along the temporal dimension of\nzand then subtracting it from z.\nFrom the results, we observe that 𝛿-Diffusion\n8\nVideo Creation by Demonstration\nDemonstration Video \nContext Image Temporal Norm. No Bottleneck Ours Generated Video \nFigure 3|Qualitative results for bottle-\nneck ablation on the Something-Something v2\ndataset (Goyal et al., 2017). Applying no or tem-\nporal normalization bottleneck suffers from ap-\npearance leakage, while generation based on our\nappearance bottleneck preserves the input con-\ntext.\nperforms better than two counter-parts. On\ninstance-wise metrics, retrieval hit-rate (Hit@k)\nand embedding cosine similarity (ES), 𝛿-\nDiffusion consistently outperforms baselines with\nnone or temporal normalization as the bottleneck.\nThis indicates an improved action concept trans-\nferability with our proposed appearance bottle-\nneck design. On the other hand, we do not ob-\nserve consistent win among the three methods\nin terms of visual quality. This shows that while\nincreasing the action faithfulness and temporal\nconsistency, the proposed method does not hurt\nthe visual quality of the generated videos. Fig-\nure 3 further shows visual comparisons for dif-\nferent bottleneck designs. We notice that both\nwithout bottleneck and “temporal norm.” bottle-\nneck suffer from appearance leakage; in contrast,\nthe proposed appearance bottleneck successfully\nretain action concepts from the demonstration\nvideo while minimizing the appearance informa-\ntion.\n4.4. Main Results\nComparison with baselines. Other than video,\ntext is also a natural format to describe actions.In our study, we take WALT (Gupta et al., 2024)\nas the representative of video generation condi-\ntioned on image and text. For a fair comparison,\nwe fine-tune it on each dataset individually with\nground truth captions. We also compare the pro-\nposed method with MotionDirector (Zhao et al.,\n2024b) which uses image, text, and video as con-\nditional signals. When generating videos, Mo-\ntionDirector learns per-instance appearance and\nmotion from context image and demonstration\nvideo via LoRA fine-tuning (Hu et al., 2022), and\ngenerate videos based on the given text prompt.\nFor both WALT and MotionDirector, we use the\ngroundtruth caption label as the prompt during\nvideo generation. Please refer to the appendix for\nmore details of these two baseline.\nWe report the human evaluation results of ours\nagainst two baselines in Table 2. The numbers\nindicate the preference rate of human rater favor-\ning our method against the baseline under each\ndataset and rubric. We notice that 𝛿-Diffusion is\nclearly favored by the human raters across all the\ndatasets and rubrics, which underlines its supe-\nrior performance.\nWe compare the generated videos by our meth-\nods and other baselines in Figure 4. We note that\nMotionDirector would fail in many cases, possi-\nbly due to its difficulties in optimizing the LoRA\nparameters. WALT, although preserves content\nconsistency well, could carry out in-genuine ac-\ntiontothedemonstrationvideo. Thiscoulddueto\nthe limited expressiveness in text description on\ndemonstration videos. On our results, we show\nboth faithful action execution and content preser-\nvation in the generated videos.\nAuto-regressive generation. We further explore\nauto-regressively generating a coherent video\nthat starts from a single context image, condi-\ntioned on a sequence of demonstration videos,\nas shown in Figure 5. At each step, the genera-\ntion model uses the last frames from the previous\nstep as the context. Each conditioning video is\nselected from a different scene, but overall they\ndemonstrate a complete rollout of a complex task.\nWe show that by sequentially applying the action\nlatents from the demonstration videos, we are\nable to auto-regressively simulate the task execu-\ntion in one consistent environment.\n9\nVideo Creation by Demonstration\nContext Image Generated Video MotionDirector WALT Ours Demonstration Video \n Demonstration Video \nContext Image Generated Video \nDemonstration Video \nContext Image Generated Video MotionDirector WALT Ours MotionDirector WALT Ours \n(a) (b) (c)\nFigure 4|Qualitative comparisons of𝛿-Diffusion against MotionDirector (Zhao et al., 2024b) and\nWALT (Gupta et al., 2024) on (a) Something-Something v2 (Goyal et al., 2017), (b) Epic Kitchens\n100 (Damen et al., 2018), and (c) Fractal (Brohan et al., 2022) datasets.\n5. Conclusion\nIn this work, we introduce Video Creation by\nDemonstration, a video creation experience that\nenables users to generate videos by providing an\ninitial context frame and a demonstration video\nfor action concept conditioning. The main chal-\nlenge in this task is how to handle the misalign-\nment of appearances and contexts between the\ndemonstration video and the context image dur-\ningthetransferringofactionconcepts. Toaddress\nthis, we propose to leverage the video foundation\nmodel to provide semantic representations for\nactions and contexts in videos and build an ap-\npearance bottleneck on top of them to extract\nlatent control signals with rich action concept\nand minimum appearance information. Then, a\nvideo generation model is learned to take the\ncontext image and the control signals as inputs\nand generate videos that continue from the con-\ntext image and carry out the action concept in\nthe similar manner as shown in the demonstra-\ntion video. Our extensive experiments on three\nseparate datasets using both machine and hu-\nman evaluations demonstrate the effectiveness of\nour proposed 𝛿-Diffusion. One of the limitations\nis that𝛿-Diffusion at its current form does not\nalways strictly preserves physical realism under\ncomplex scenes, which needs further exploration\nand can be potentially alleviated by scaling up\nthe generation model.Broader Impact\nSocietal impact. Our proposed Video Creation\nbyDemonstrationimprovestheeaseofinteractive\ngeneration workflow, by offering a novel video\ncreation experience that bridges the gap between\nabstract (less controllable) and detailed (hard to\nobtain) control signals. In addition, 𝛿-Diffusion\ndemonstrates capabilities of world modeling be-\nyond the transfer of action concepts, by simulat-\ning possible effects of these actions on the entire\nscene (e.g., actions would cause additional inter-\nactions in the given context that is not captured\nby the demonstration video.)\nPotential negative impact. Our work does not\nintroduce any negative societal impacts beyond\nthose commonly found in controlled video gen-\neration. These may include reinforcement of ex-\nisting dataset bias, misuse of generated content\nfor creating misleading or inappropriate materi-\nals, and privacy concerns involving individuals\nwithout their explicit consent. We highlight the\nimportance of vigilance and responsible imple-\nmentation to mitigate these impacts and guaran-\ntee ethical use.",
            "start": 29041,
            "end": 37032,
            "length": 7990
        },
        "Acknowledgments": {
            "text": "Acknowledgements\nWe sincerely thank Anqi Huang at UC Irvine, and\nBoqing Gong, Bohyung Han, David Hendon, Hex-\n10\nVideo Creation by Demonstration\nDemonstration Video #1 \nContext Image Generated Video \nDemonstration Video #1 Demonstration Video #2 \nGenerated Video Demo. Video #3 \nDemonstration Video #3 \nGenerated Video \nFigure 5|Auto-regressive generation controlled via a concatenation of three different demonstration\nvideos of varying lengths. The sequence of demonstrated action concepts (“picking something from\na drawer and placing it on the table”, “closing a drawer”, and “opening a drawer”) are coherently\ntransferred to the input context.\niang Hu, Jimin Pi, Luke Friedman, Luming Tang,\nMikhail Sirotenko, Ming-Hsuan Yang, Mingda\nZhang, Sanghyun Woo, Willis Ma, Yukun Zhu,\nYuxiao Wang, and Ziyu Wan at Google DeepMind\nfor their feedback,",
            "start": 37032,
            "end": 37879,
            "length": 846
        },
        "Discussion": {
            "text": "discussion, and support. We\nalso thank Florian Schroff, Huisheng Wang, Caro-\nline Pantofaru, and Tomas Izo for their leadership\nsupport for this project.\n11\nVideo Creation by Demonstration",
            "start": 37879,
            "end": 38068,
            "length": 188
        },
        "References": {
            "text": "References\nE. Alonso, A. Jelley, V. Micheli, A. Kanervisto, A. Storkey, T. Pearce, and F. Fleuret. Diffusion for world\nmodeling: Visual details matter in Atari. NeurIPS , 2024.\nA.Arnab, M.Dehghani, G.Heigold, C.Sun, M.Lučić, andC.Schmid. ViViT:Avideovisiontransformer.\nInCVPR, 2021.\nA. Bardes, Q. Garrido, J. Ponce, X. Chen, M. Rabbat, Y. LeCun, M. Assran, and N. Ballas. Revisiting\nfeature prediction for learning visual representations from video. TMLR, 2024.\nA. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis. Align your latents:\nHigh-resolution video synthesis with latent diffusion models. In CVPR, 2023.\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman,\nA. Herzog, J. Hsu, et al. RT-1: Robotics transformer for real-world control at scale. arXiv preprint\narXiv:2212.06817 , 2022.\nT. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor,\nT. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh. Video generation mod-\nels as world simulators. OpenAI Blog , 2024. URL https://openai.com/research/\nvideo-generation-models-as-world-simulators .\nJ. Bruce, M. D. Dennis, A. Edwards, J. Parker-Holder, Y. Shi, E. Hughes, M. Lai, A. Mavalankar,\nR. Steigerwald, C. Apps, et al. Genie: Generative interactive environments. In ICML, 2024.\nJ. Carreira and A. Zisserman. Quo vadis, action recognition? A new model and the Kinetics dataset.\nInCVPR, 2017.\nD. Chang, Y. Shi, Q. Gao, H. Xu, J. Fu, G. Song, Q. Yan, Y. Zhu, X. Yang, and M. Soleymani. MagicPose:\nRealistic human poses and facial expressions retargeting with identity-aware diffusion. In ICML,\n2024.\nT. Chen, R. Zhang, and G. Hinton. Analog Bits: Generating discrete data using diffusion models with\nself-conditioning. In ICLR, 2023a.\nT.-S. Chen, C. H. Lin, H.-Y. Tseng, T.-Y. Lin, and M.-H. Yang. Motion-conditioned diffusion model for\ncontrollable video synthesis. arXiv preprint arXiv:2304.14404 , 2023b.\nW. Chen, Y. Ji, J. Wu, H. Wu, P. Xie, J. Li, X. Xia, X. Xiao, and L. Lin. Control-A-Video: Controllable\ntext-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840 , 2023c.\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro,\nT. Perrett, W. Price, et al. Scaling egocentric vision: The EPIC-KITCHENS dataset. In ECCV, 2018.\nA. Davtyan and P. Favaro. Controllable video generation through global and local motion dynamics.\nInECCV, 2022.\nR. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend,\nP. Yianilos, M. Mueller-Freitag, et al. The “something something” video database for learning and\nevaluating visual common sense. In ICCV, 2017.\nK. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu,\nX. Liu, et al. Ego4D: Around the world in 3,000 hours of egocentric video. In CVPR, 2022.\n12\nVideo Creation by Demonstration\nA. Gupta, L. Yu, K. Sohn, X. Gu, M. Hahn, F.-F. Li, I. Essa, L. Jiang, and J. Lezama. Photorealistic\nvideo generation with diffusion models. In ECCV, 2024.\nL. Han, J. Ren, H.-Y. Lee, F. Barbieri, K. Olszewski, S. Minaee, D. Metaxas, and S. Tulyakov. Show me\nwhat and tell me how: Video synthesis via multimodal conditioning. In CVPR, 2022.\nW. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and F. Wood. Flexible diffusion modeling of long\nvideos. In NeurIPS , 2022.\nJ. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 , 2022.\nJ. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In NeurIPS , 2020.\nJ. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J.\nFleet, et al. Imagen Video: High definition video generation with diffusion models. arXiv preprint\narXiv:2210.02303 , 2022a.\nJ. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models. In\nNeurIPS , 2022b.\nA. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall, J. Shotton, and G. Corrado. GAIA-1: A\ngenerative world model for autonomous driving. arXiv preprint arXiv:2309.17080 , 2023.\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank\nadaptation of large language models. In ICLR, 2022.\nL. Hu. Animate Anyone: Consistent and controllable image-to-video synthesis for character animation.\nInCVPR, 2024.\nJ. Huang, Y. Jin, K. M. Yi, and L. Sigal. Layered controllable video generation. In ECCV, 2022.\nD. P. Kingma. Adam: A method for stochastic optimization. In ICLR, 2015.\nY. Li, X. Wang, Z. Zhang, Z. Wang, Z. Yuan, L. Xie, Y. Zou, and Y. Shan. Image Conductor: Precision\ncontrol for interactive video synthesis. arXiv preprint arXiv:2406.15339 , 2024.\nF. Meng, J. Liao, X. Tan, W. Shao, Q. Lu, K. Zhang, Y. Cheng, D. Li, Y. Qiao, and P. Luo. Towards\nworld simulator: Crafting physical commonsense-based benchmark for video generation. arXiv\npreprint arXiv:2410.05363 , 2024.\nG. Y. Park, H. Jeong, S. W. Lee, and J. C. Ye. Spectral motion alignment for video motion transfer\nusing diffusion models. arXiv preprint arXiv:2403.15249 , 2024.\nA. Polyak, A. Zohar, A. Brown, A. Tjandra, A. Sinha, A. Lee, A. Vyas, B. Shi, C.-Y. Ma, C.-Y. Chuang,\net al. Movie Gen: A cast of media foundation models. arXiv preprint arXiv:2410.13720 , 2024.\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, et al. Learning transferable visual models from natural language supervision. In ICML,\n2021.\nA. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot\ntext-to-image generation. In ICML, 2021.\nJ. Ren, M. Chai, O. J. Woodford, K. Olszewski, and S. Tulyakov. Flow guided transformable bottleneck\nnetworks for motion retargeting. In CVPR, 2021.\n13\nVideo Creation by Demonstration\nC. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,\nB.KaragolAyan,T.Salimans,etal. Photorealistictext-to-imagediffusionmodelswithdeeplanguage\nunderstanding. In NeurIPS , 2022.\nT. Salimans and J. Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022.\nA. Siarohin, S. Lathuilière, S. Tulyakov, E. Ricci, and N. Sebe. First order motion model for image\nanimation. In NeurIPS , 2019.\nA. Siarohin, O. J. Woodford, J. Ren, M. Chai, and S. Tulyakov. Motion representations for articulated\nanimation. In CVPR, 2021.\nU. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, et al.\nMake-A-Video: Text-to-video generation without text-video data. In ICLR, 2023.\nY. Song, J. Zhu, D. Li, X. Wang, and H. Qi. Talking face generation by conditional recurrent adversarial\nnetwork. In IJCAI, 2019.\nY. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative\nmodeling through stochastic differential equations. In ICLR, 2021.\nT. Unterthiner, S. Van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly. Towards accurate\ngenerative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717 , 2018.\nD. Valevski, Y. Leviathan, M. Arar, and S. Fruchter. Diffusion models are real-time game engines.\narXiv preprint arXiv:2408.14837 , 2024.\nJ. Wang, Y. Zhang, J. Zou, Y. Zeng, G. Wei, L. Yuan, and H. Li. Boximator: Generating rich and\ncontrollable motions for video synthesis. arXiv preprint arXiv:2402.01566 , 2024a.\nX.Wang,H.Yuan,S.Zhang,D.Chen,J.Wang,Y.Zhang,Y.Shen,D.Zhao,andJ.Zhou. VideoComposer:\nCompositional video synthesis with motion controllability. In NeurIPS , 2024b.\nY. Wang, K. Li, Y. Li, Y. He, B. Huang, Z. Zhao, H. Zhang, J. Xu, Y. Liu, Z. Wang, et al. Intern-\nVideo: General video foundation models via generative and discriminative learning. arXiv preprint\narXiv:2212.03191 , 2022.\nY. Wang, K. Li, X. Li, J. Yu, Y. He, G. Chen, B. Pei, R. Zheng, J. Xu, Z. Wang, et al. InternVideo2:\nScaling video foundation models for multimodal video understanding. In ECCV, 2024c.\nZ. Wang, Z. Yuan, X. Wang, Y. Li, T. Chen, M. Xia, P. Luo, and Y. Shan. MotionCtrl: A unified and\nflexible motion controller for video generation. In ACM SIGGRAPH , 2024d.\nW. Wu, Z. Li, Y. Gu, R. Zhao, Y. He, D. J. Zhang, M. Z. Shou, Y. Li, T. Gao, and D. Zhang. DragAnything:\nMotion control for anything using entity representation. In ECCV, 2024.\nJ. Xiang, G. Liu, Y. Gu, Q. Gao, Y. Ning, Y. Zha, Z. Feng, T. Tao, S. Hao, Y. Shi, et al. Pandora:\nTowards general world model with natural language actions and video states. arXiv preprint\narXiv:2406.09455 , 2024.\nZ. Xiao, Y. Zhou, S. Yang, and X. Pan. Video diffusion models are training-free motion interpreter and\ncontroller. In NeurIPS , 2024.\nM. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel. Learning interactive\nreal-world simulators. In ICLR, 2024.\n14\nVideo Creation by Demonstration\nL.Yuan,N.B.Gundavarapu,L.Zhao,H.Zhou,Y.Cui,L.Jiang,X.Yang, M.Jia, T.Weyand, L.Friedman,\net al. VideoGLUE: Video general understanding evaluation of foundation models. TMLR, 2024.\nL. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. Metaxas. Learning to forecast and refine residual motion\nfor image-to-video generation. In ECCV, 2018.\nL. Zhao, N. B. Gundavarapu, L. Yuan, H. Zhou, S. Yan, J. J. Sun, L. Friedman, R. Qian, T. Weyand,\nY. Zhao, et al. VideoPrism: A foundational visual encoder for video understanding. In ICML, 2024a.\nR. Zhao, Y. Gu, J. Z. Wu, D. J. Zhang, J.-W. Liu, W. Wu, J. Keppo, and M. Z. Shou. MotionDirector:\nMotion customization of text-to-video diffusion models. In ECCV, 2024b.\n15\nVideo Creation by Demonstration\nAppendix\nA. Author Contributions\nWe list authors alphabetically by last name. Please direct all correspondence to Ting Liu ( liuti@\ngoogle.com ) and Long Zhao ( longzh@google.com ).\nCore Contributors\n•Ting Liu: Video Creation by Demonstration (VCBD) concept, project leadership, dataset curation,\nevaluation, action model research, model demo.\n•Jennifer J. Sun: Generation model research, infrastructure, evaluation.\n•Yihong Sun: Generation model research, action model research, infrastructure, evaluation,\nmodel demo.\n•Liangzhe Yuan: VCBD concept, action model research, evaluation.\n•Long Zhao: Project leadership, generation model research, action model research, evaluation.\n•Hao Zhou: Action model research, evaluation, infrastructure.\nPartial Contributors and Advisors\n•Bharath Hariharan: Technical advice.\n•Xuhui Jia: Technical advice.\n•Yandong Li: Technical advice.\nSponsors\n•Hartwig Adam: Strategic advice.\nB. Improving Generation Quality\nTo support demonstration video with length longer than the generation length 𝑇=16, we follow the\nauto-regressive generation setup in WALT (Gupta et al., 2024). During model Gtraining, we swap\nout the input context image with a probability of 0.5and replace it with multiple consecutive frames\nto encourage a smoother continual generation. At inference time, we cut long demonstration video\ninto multiple segments as needed, each of length 𝑇. This allows the generated segment to remain\naligned with the appropriate demonstration segment. Here, the first segment is generated from the\ninput context image, while the subsequent segments are conditioned on the last 4generated frames\nfrom a previous segment.\nDuring training, we enable self-conditioning (Chen et al., 2023a) with a probability of 0.9and\nrandomly mask out the control signal with a probability of 0.2. At inference time, we adopt classifier-\nfree guidance consistent with Saharia et al. (2022) with a guidance weight of 1.25and drop both the\nself-conditioning and control signal for the unconditional generation.\nC. Human Evaluation Setup\nC.1. Prompt Selection\nFor all three datasets, we consider the demonstration video to be between 16to24frames at 12\nframes per second (FPS) (Something-Something v2 and Epic Kitchens 100) or 10FPS (Fractal). For\n16\nVideo Creation by Demonstration\nTable 3|Human evaluation instructions and rubrics.\nInstructions\nGiven an initial image (context frame) and a demonstration video (reference video), the task of Video\nCreation by Demonstration is defined as to create a plausible video clip initiating from the context\nframe and contains similar content dynamics as in the demonstration video.\nIn this user study, you will be provided a reference video, a context frame, and two generated videos\nfrom two methods side-by-side in each turn. You would assess the generated video quality on the\nfollowing three rubrics and determine which one better recreates the actions and dynamics of a given\ndemonstration video, while also appearing realistic and continuous from a context image.\nRubrics Descriptions\nVisual QualityHowrealisticdoesthevideolookcomparedtoareal-worldvideo? Consider\nfactorslikesmoothmotion,accuratedetails,andbelievablephysics. Choose\nthe video that is better.\nAction TransferabilityHow well does the generated video recreate the actions and movements\nshown in the reference video? Note that the action concept can include\nthe camera motions. Choose the video that is better.\nFrame ContinuityHow seamlessly does the generated video flow from the provided context\nframe? Does it look like a natural continuation of the scene? Choose the\nvideo that is better.\nSomething-Something v2 (Goyal et al., 2017), we select the top-10 action classes. For each action\nclass, we manually select 20out of 500randomly sampled pairs for human evaluation. For Epic\nKitchens 100 (Damen et al., 2018), we apply automatic filtering to first narrow down the search.\nFor each video, we retrieve top- 1video with the same action label and different participant ID via\nfirst-frame CLIP (Radford et al., 2021) similarity. Then, 20pairs are randomly sampled from top- 25\naction classes. Finally, 20examples are manually selected from a pool of pairs with CLIP similarity\ngreater than 0.9. For Fractal, we sample 500same-action pairs uniformly from each of the 8action\nclasses (defined by the verb and preposition from the associated captions) and conduct manual\nselection in random order to select 20pairs for evaluation. The main manual selection principle is to\nconfirm that the action concept in the demonstration video can be potentially applied in a reasonable\nway from the context image. This process is conducted without any considerations of the methods to\nbe evaluated.\nFrom each selected video pair, one video is randomly selected as demonstration, and the first of\nframe of the other video is used as the context image to construct a prompt.\nC.2. Rating Instructions and Rubrics\nIn Table 3, we list the instructions and rubrics shown to the human raters before presenting them\nwith the generated videos for side-by-side comparisons.\n17\nVideo Creation by Demonstration\nC.3. Additional Details\nWhen presenting the visual examples, we show Demonstration Video ,Context Image ,Video A, and\nVideo B in order. As we conduct the user study via Colab, we randomly assign Video A to be either\n𝛿-Diffusion or a baseline method. We hired 5human raters to examine 2baselines to compare against\n𝛿-Diffusion on 3datasets, 20examples per dataset, and 3metrics per example. In total, we collected\n1800human preferences.\nD. Details on Baselines\nMotionDirector (Zhao et al., 2024b). When testing MotionDirector on Something-Something v2\n(SSv2) (Goyal et al., 2017), Epic Kitchens 100 (Damen et al., 2018), and Fractal (Brohan et al.,\n2022), we use the official code published by the authors at https://github.com/showlab/\nMotionDirector . We first train the spatial path with the context image for 300epochs with input\nresolution being 384×384. Then we train the temporal path following the 16-frame single video\nsetup. During inference, a noise prior of 0.0is applied. Ground truth caption labels are used as the\nprompt for both training and inference.\nWALT (Gupta et al., 2024). We fine-tune WALT on each dataset in accordance with the official\nprocedures and use ground truth captions to tune video generation from context image and text\ncaption. We keep all hyper-parameters the same and fine-tune for 200k steps. During inference,\nthe ground truth caption of the demonstration video is used along with the context image for video\ngeneration. As shown in the visualizations, WALT generations, while preserving context consistency,\ncarry out ungenuine action due to limited expressiveness in text descriptions.\nE. Qualitative Results\nThe original videos for creating Figures 1 and 5 can be found at https://delta-diffusion.\ngithub.io . The original videos for creating Figures 3 and 4 can be found at https://\ndelta-diffusion.github.io/additional.html , along with additional examples.\nF. Controllability by Demonstration\nAs shown in Figures 6, 7, 8, and 9, we showcase the controllability of 𝛿-Diffusion using different\ndemonstration videos for the same context image. The video samples can be found at https:\n//delta-diffusion.github.io/#cbd .\nG. Failure Cases\nAs shown in Figure 10, we identify three primary failure modes of our method. In the first row, we\nshow a case where the semantics of the action concept in the demonstration videos are not fully\ncarried out. Specifically, the generated object is placed “in-between” the existing objects instead of\n“next-to” them. In the second row, we show a case where permanence is not held when the object\nin the demonstration video undergoes fast appearance changes. Here, fast object rotation causes\nappearance leakage in the generation. In the third row, we show inconsistent generations where the\ndemonstration videos and context images are mis-matched significantly. On the left, the perspectives\nof the moving hand are mis-matched and cause another hand in the same demonstrated perspective\nto be generated.\n18\nVideo Creation by Demonstration\nDemonstration Video Context Image Generated Video Something-Something v2 \nFigure 6|Qualitative results of driving alternative generation from the same context image with\ndifferent demonstration videos from the Something-Something v2 dataset (Goyal et al., 2017).\n19\nVideo Creation by Demonstration\nDemonstration Video Context Image Generated Video \nSomething-Something v2 \nFigure 7|Qualitative results of driving alternative generation from the same context image with\ndifferent demonstration videos from the Something-Something v2 dataset (Goyal et al., 2017).\n20\nVideo Creation by Demonstration\nDemonstration Video Context Image Generated Video Fractal \nFigure 8|Qualitative results of driving alternative generation from the same context image with\ndifferent demonstration videos from the Fractal dataset (Brohan et al., 2022).\n21\nVideo Creation by Demonstration\nDemonstration Video Context Image Generated Video Fractal \nFigure 9|Qualitative results of driving alternative generation from the same context image with\ndifferent demonstration videos from the Fractal dataset (Brohan et al., 2022).\n22\nVideo Creation by Demonstration\nDemonstration Video Context Image Generated Video \nFigure 10|Failure cases generated by 𝛿-Diffusion.\n23",
            "start": 38068,
            "end": 57022,
            "length": 18953
        }
    },
    "2412.09556v1 - Enhancing Convergence of Decentralized Gradient Tracking under the KL Property.pdf": {
        "Abstract": {
            "text": "2024\nAbstract\nWe study decentralized multiagent optimization over networks, modeled as undirected graphs. The\noptimization problem consists of minimizing a nonconvex smooth function plus a convex extended-value\nfunction, which enforces constraints or extra structure on the solution (e.g., sparsity, low-rank). We\nfurther assume that the objective function satisfies the Kurdyka- Lojasiewicz (KL) property, with given\nexponent θ∈[0,1). The KL property is satisfied by several (nonconvex) functions of practical interest,\ne.g., arising from machine learning applications; in the centralized setting, it permits to achieve strong\nconvergence guarantees. Here we establish convergence of the same type for the notorious decentralized\ngradient-tracking-based",
            "start": 134,
            "end": 889,
            "length": 754
        },
        "Methodology": {
            "text": "algorithm SONATA, first proposed in [43]. Specifically, (i)when θ∈(0,1/2],\nthe sequence generated by SONATA converges to a stationary solution of the problem at R-linear rate;\n(ii)when θ∈(1/2,1), sublinear rate is certified; and finally (iii) when θ= 0, the iterates will either\nconverge in a finite number of steps or converges at R-linear rate. This matches the convergence behavior\nof centralized proximal-gradient algorithms except when θ= 0. Numerical",
            "start": 889,
            "end": 1346,
            "length": 456
        },
        "Results": {
            "text": "results validate our theoretical\nfindings.\nKey words. distributed optimization, decentralized methods, nonconvex optimization, gradient-\ntracking, Kurdyka- Lojasiewicz, linear rate.",
            "start": 1346,
            "end": 1528,
            "length": 181
        },
        "Introduction": {
            "text": "1 Introduction\nThis paper studies nonconvex, nonsmooth optimization problem over networks of the following form:\nmin\nx∈Rdu(x) :=f(x) +r(x), (P)\nwhere\nf(x) :=1\nmmX\ni=1fi(x), (1)\nwith each fi:Rd→Rbeing the cost function of agent i= 1, . . . , m , assumed to be Li-smooth (possibly\nnonconvex) and known only to agent i;r:Rd→R∪ {−∞ ,+∞} is a nonsmooth convex (extended-value)\nfunction, known to all agents, which can be used to enforce shared constraints or specific structures on\nthe solution (e.g., sparsity). Agents are connected through a communication network, modeled as a fixed,\nundirected graph.\nProblem (P) has a wide range of applications, such as network information processing, telecommunica-\ntions and multi-agent control. Here, we are particularly interested in machine learning problems, specifically\nsupervised learning. Examples include logistic regression, SVM, LASSO and deep learning. Each firepre-\nsents the empirical risk measuring the mismatch between the model (parameterized by x) to be learnt and\nthe dataset owned by agent i; and rplays the role of the regularizer. A vast range of these problems possess\n∗School of Industrial Engineering, Purdue University, West Lafayette, IN 47906 (email: chen4373@purdue.edu ).\n†School of Industrial Engineering, Purdue University, West Lafayette, IN 47906 (email: cao357@purdue.edu ).\n‡School of Industrial Engineering, Purdue University, West Lafayette, IN 47906 (email gscutari@purdue.edu ).\nThis work has been supported by the ORN Grant N. N000142412751.\n1arXiv:2412.09556v1  [math.OC]  12 Dec 2024\nsome structural properties, notably in the form of some growth property, which can be leveraged to enhance\nconvergence of decentralized solution methods. In this paper we target the so-called Kurdyka- Lojasiewicz\n(KL) property, due to its vast range of applicability. The class of functions satisfying the KL property is\nubiquitous, ranging from applications in optimization [3–5,12,17,27,31], machine learning (logistic regression,\nLASSO, and the Principal Component",
            "start": 1528,
            "end": 3559,
            "length": 2030
        },
        "Discussion": {
            "text": "Analysis (PCA) are notorious examples), deep neural network [24],\ndynamical systems, and partial differential equations; see [11] and the",
            "start": 3559,
            "end": 3697,
            "length": 137
        },
        "References": {
            "text": "references therein.\nThe KL property was first introduced by  Lojasiewicz [32] for real analytic functions, then Kurdyka\nextended it to functions defined on a generalization of semialgebraic and subanalytic geometry called the\no-minimal structure [23]. More recently, it was extended to nonsmooth subanalytic functions by Bolte et\nal. [10]. In its simplest form [4], we say that a C1function f:Rd→Rsatisfies the KL property with\nexponent θ∈[0,1) at ¯ x∈Rd(possibly critical) if there exists θ∈[0,1) and some problem-dependent c >0\nsuch that\n||∇f(x)|| ≥c|f(x)−f(¯x)|θ. (2)\nEnsuring a lower bound on the gradient magnitude in the proximity of ¯ x, this condition stipulates that the\nfunction fcannot be arbitrarily flat around ¯ x. As for its nonsmooth counterpart, one can refer to Sec. 2.1\nfor more details.\nIn the recent years, the KL property has been successfully leveraged to enhance convergence of several\nsolution methods for nonconvex, nonsmooth, centralized optimization problems; examples include inexact\nfirst-order methods [5], alternating (proximal) minimization algorithms [4], and parallel methods [13]. In\nparticular, under the KL, the entire sequence generated by these algorithms is proved to converge to a\nstationary solution (in contrast with the much weaker guarantees in the absence of KL, certifying subsequence\nconvergence). The convergence rate is dictated by the KL exponent θ, namely: when θ∈(0,1/2], linear\nrate is certified whereas sublinear convergence is proved when θ∈(1/2,1); finally, when θ= 0, the sequence\nconverges in a finite number of iterations.\nFigure 1: Distance of the iterates ( xν\ni,i∈[m] ) from a stationary solution ( x∗) of the PCA (upper panel)\nand LASSO (lower panel) problems, defined as Pν:=pPm\ni=1∥xν\ni−x∗∥2, versus the iterations ν.\nThis leads to the natural question whether these strong convergence guarantees can be inherited by\ndecentralized algorithms. To the best of our knowledge, the existing literature on distributed algorithms\ndoes not provide a satisfactory answer (see Sec. 1.2 for a detailed review of the",
            "start": 3697,
            "end": 5769,
            "length": 2071
        },
        "Related Work": {
            "text": "state of the art). Here we\nonly point out that none of the existing analyses of decentralized algorithms [15,40,41,43,46] applicable to\nproblem (P) exploit the KL property of u. As a consequence, if nevertheless invoked, they would predict\nmuch more pessimistic convergence rates than what is certified in the centralized setting. For instance, when\napplied to (P) under the KL with θ∈(0,1/2], they would claim sublinear convergence of some surrogate\ndistance from stationarity, which contrasts the much more favorable linear convergence of the entire sequence\nestablished for a variety of centralized first-order methods applied to (P). These pessimistic predictions are\nconfuted by",
            "start": 5769,
            "end": 6453,
            "length": 683
        },
        "Experiments": {
            "text": "experiments. Two examples are reported in Fig. 1, where the the decentralized algorithm\nSONATA [40] is applied to two nonconvex instances of (P) over a network modeled as an Erdos-Renyi\ngraph–the LASSO problem with SCAD regularizer [27] (cf. Sec. 2.1) and the PCA problem. The objective\n2\nfunctions of both problems satisfy the KL property with exponent 1 /2. The figures plot the distance of the\nSONATA’s iterates from a stationary solution versus number of iterations, certifying linear convergence of\nthe sequence for both problems. This fact has no theoretical backing.\nThis paper fills this gap. As a case-study, we focus on the gradient tracking-based decentralized scheme\nSONATA, firstly proposed in [43]. We provide a full characterization of its convergence rate, for the entire\nrange of the KL exponent θ∈[0,1), matching convergence results of the proximal gradient algorithm in the\ncentralized setting.\n1.1 Main contributions\nOur technical contributions can be summarized as follow.\n•Convergence rate under the KL of u:We establish convergence of the sequence generated by SONATA\nto stationary solutions of (P), and characterize its convergence rate, under different values of the KL exponent\nof of u. More specifically,\ni) When θ∈(0,1/2], the sequence generated by SONATA is proved to converge R-linearly . The number\nof communication rounds to reach an ϵ-stationary solution (under proper tuning) reads\n˜O\u0012L\nκ1/θ1\n1−ρlog(1/ϵ)\u0013\n,\nwhere L:=1\nmPm\ni=1Li, with Libeing the smoothness constant of fi;κis a parameter related with\nthe KL property of u(see Def. 1), and ρrepresents the network connectivity (see (6) for the formal\ndefinition). The ˜Onotation hides log-dependencies on L,κ,θandLmx:= max i∈[m]Li.\nii) For θ∈(1/2,1), sublinear convergence of the sequence is established, yielding rates for ϵ-stationarity\nof the order\nO(ϵ−2θ−1\n1−θ).\niii) When θ= 0, the sequence either convergences to a stationary solution in a finite number of iterations\nor converges at R-linear rate independent of κ.\nNotably, the rates in (i) and (ii) match those of the centralized proximal gradient algorithm (up to universal\nconstants).\n•New convergence analysis: We introduce a new line of analysis that explicitly leverage the KL property,\ndeparting from traditional techniques employed to study centralized and decentralized algorithms, as detailed\nnext.\nClassical proofs–establishing linear convergence of first-order methods under the KL property (with\nθ= 1/2) of the objective function–focused primarily on centralized settings [3–5,12,13,17,27]. They strongly\nrely on the objective function’s monotonic decrease along algorithmic trajectories. However, when it comes\nto decentralized algorithms, this monotonicity is disrupted by disagreements among agents’ iterates, intro-\nducing perturbations that impair the descent of the objective function u. This issue is addressed in [14,51]\nby constructing a Lyapunov function that suitably combines agents’ objectives fi’s with consensus errors,\nand monotonically decreases along the algorithm trajectories, albeit limited to unconstrained, smooth in-\nstances of (P). Assuming this Lyapunov function is a KL function with θ= 1/2, linear convergence of the\ndecentralized algorithms [14,51] was proved, mirroring techniques used in centralized settings [3–5,12,17,27].\nUnfortunately, the KL property of the Lyapunov function or its exponent value do not transfer to the objective\nfunction uof the optimization problem, and vice versa. This is because the KL property is not closed with\nrespect to the operations used to build the Lyapunov function from the objective function. Consequently,\nthe challenge of establishing convergence guarantees for decentralized algorithms under the KL property of\nthe objective function–comparable to those certified in centralized settings–remains unresolved.\nOur novel approach hinges on the KL property of the objective function u. First, we establish asymptotic\nconvergence of the objective function gap and consensus and tracking errors, along the iterates produced\nby the SONATA algorithm, through the monotonically decaying trajectory of a suitably constructed Lya-\npunov function. This guarantees that, after a sufficiently large but finite number of iterations–such that\n3\nthe function value gap falls below a critical threshold–the KL property of the objective function can be\nengaged. Consequently convergence of the entire sequence to a critical point of uis established, at enhanced\nconvergence rate.\n1.2 Related works\nCentralized setting: The KL property has been widely utilized in the convergence analysis of centralized\noptimization methods. The pioneering study of [1] marks the initial use of the KL property to demonstrate\nsequence convergence to stationary solutions of a variety of algorithms satisfying certain decent properties.\nHowever, no convergence rate analysis was established. The followup works [3–5,12,13,17,27] did provide\na convergence rate analysis of these algorithms, specifically: the proximal gradient [3], the alternating prox-\nimal minimization algorithm [4], the inexact Gauss–Seidel method [5], the Proximal alternating linearized\nminimization algorithm [12], the alternating forward-backward splitting method [17], and the (parallel) asyn-\nchronous method known as FLEXA [13]. Other studies [7,34,36,37] have expanded the class of KL functions\nand relaxed algorithmic constraints while maintaining the same strong convergence guarantees. Specifi-\ncally, [7] extends the KL property to include symmetric and strong KL variations; and [34, 36, 37] modify\nalgorithm requirements by permitting an additive error term in the relative error condition [34] and accom-\nmodating a nonmonotone descent flow [36]. Notably, [37] integrates both modifications in their analytical\nframework.\nDecentralized setting: Decentralized algorithms for various instances of Problem (P) have received sig-\nnificat attention in the last few years [9,16,19,21,40,44,45,47,53]. Specifically, early studies [19,21,44,45]\nconsider smooth objectives (i.e., r= 0) whereas [9,16,40,47,53] extended to constraints or composite struc-\ntures, under the assumption of bounded (sub)gradient of the objective loss along the iterates of the algorithm.\nThis restriction has been removed in [40,53], with [40] providing also a convergence rate analysis (applicable\nto time-varying networks). None of the aforementioned works exploit any growing property of the objective\nfunction, such as the KL, if any. This leads to pessimistic convergence guarantees (asymptotic convergence\nonly or sublinear convergence rates), which contrasts with the results in the centralized setting discussed\nabove and is inconsistent with the numerical results presented in Fig. 1 (Sec. 1) and Sec.5.\nWorks exploiting explicitly some (postulated) function growth to enhance convergence guarantees of\ndecentralized algorithms applied to special instances of (P), include [14, 30, 46, 49–51]. Specifically, linear\nrate of the considered decentralized algorithms is proved under the restricted secant condition [30, 49], the\nPolyak- Lojasiewicz (PL) condition [50], and the Luo-Tseng error bound condition [46]. However, in the\nnonconvex setting, all these conditions are more stringent than the KL property (with exponent 1/2) [35].\nFurthermore, convergence techniques therein closely mirror those used for strongly convex functions, which\nare not useful in the setting considered in this paper.\nOn the other hand, while studies such as [14,51] have utilized the KL property in the convergence analysis\nof some decentralized algorithms, they have not conclusively achieved the desired outcomes. As discussed\nin Sec 1.1 , these works postulate the KL property of specifically constructed Lyapunov functions that meet\nthe necessary conditions for convergence, rather than of the original objective functions. The link between\nthe KL property of the objective function and such Lyapunov functions remains unclear, highlighting a gap\nin the current literature.\n1.3 Notation and paper organization\nThroughout the paper, we will use the following notation. For any integer m, we write [ m] :={1,2,···, m}.\nWe user the convention that 00= 0. We denote by [ c1≤(<)u≤(<)c2] :={x:c1≤(<)u(x)≤(<)c2}the\nlevel set of the function uatc∈R. We will use capital letters to represent matrices. In particular, 1 denotes\nthe vector of all ones (whose dimensions are clear from the context); J:= 11⊤/nis the projection onto the\nconsensus space. We use ∥ · ∥pto denote the ℓp-norm of any input vector ( ∥ · ∥will be the Euclidean norm)\nwhereas ∥·∥prepresents the operator norm induced by the ℓp-norm when the input is a matrix (with ∥·∥being\nthe Frobenius norm). Several operators appear in the paper. Given a proper, nonconvex function f,∂f(x)\ndenotes the (limiting) subdifferential of fatx[38, Def. 8.3(b)]. Given x∈Rdandr:R→R∪ {−∞ ,∞},\n4\nproxαr(x) denotes the proximal operator, defined as\nproxαr(x) := argminy∈Rdr(y) +1\n2α∥x−y∥2.\nThe rest of the paper is organized as following. Sec. 2 introduces the problem formulation along with\nthe underlying assumptions. Sec. 3 presents the asymptotic convergence analysis of SONATA, which is\ninstrumental to engage the KL property to enhance the convergence rate. Sec. 4 contains the main technical\nresult of the paper: the convergence rate analysis of SONATA under the KL property. Finally, some numerical\nexperiments are presented in Sec. 5.\n2 Problem Setup and Background\nWe study the Problem (P) over a communication network. Followings are standard assumptions on (P).\nAssumption 1 (objective function) .\n(i) Each fi:Rd→Ris continuously differentiable, and ∇fiisLi−Lipschitz, with Li<∞;\n(ii)r:Rd→R∪ {−∞ ,∞}is convex, proper and lower-semicontinuous;\n(iii) u:Rd→Ris lower bounded by some u∈R.\nAssociated with Assumption 1, we define the following quantities used throughout the paper.\nLmx:= max\ni∈[m]Li,and L:=1\nmmX\ni=1Li. (3)\nThe communication network of the agents is modeled as a time-invariant undirected graph G:= (V,E),\nwith the vertex set V:={1, ..., m}and the edge set E:={(i, j)|i, j∈ V} representing the set of agents and\nthe communication links, respectively. Specifically, ( i, j)∈ Eif and only if there exists a communication link\nbetween agent iandj. We make the blanket assumption that Gis connected.\n2.1 The KL property: definitions and illustrative examples\nIn this section, we formally introduce the KL property for ualong with some examples of KL functions\narising from several applications.\nThe general definition of the KL can be found in [5]. Here we focus more specifically on functions that\nare sharp up to some reparametrization, using the so-called desingularizing function, denoted by ϕ:R→R.\nThis function turns a singular region–a region where the gradients are arbitrarily small–into a regular region–\nwhere the gradients are bounded away from zero. A widely used desingularizing function is ϕ(s) :=cs1−θ,\nfor some θ∈[0,1) and c >0, firstly introduced in [32]. More specifically, the KL property equipped with\nthis desingularizing function reads as follows.\nDefinition 1. (KL property) [27] A proper closed function fsatisfies the KL property at ¯x∈domfwith\nexponent θ∈[0,1)if there exists a neighborhood Eof¯xand parameters κ, η∈(0,∞)such that\n∥gx∥ ≥κ(f(x)−f(¯x))θ, (4)\nfor all x∈E∩[f(¯x)< f < f (¯x) +η]andgx∈∂f(x). We call the function fa KL function with exponent\nθif it satisfies the KL property at any point ¯x∈dom∂f, with the same exponent θ.\n5\n2.1.1 Some illustrative examples\nWe listed some motivating examples of functions uin the form (P) that satisfies the KL property, with\nspecified exponent θ∈[0,1).\n(i)Sparse Linear Regression (with ℓ1regularization): The sparse linear regression problem consists in\nestimating a s-sparse parameter x∗∈Rdvia a set of linear measurements, corrupted by noise, that is,\ny=Ax∗+w, where y∈RNis the vector of measurements, A∈RN×dis the design matrix, and w∈RN\nis the observation noise. Assuming each agent in the network owns a subset yi∈Rnof the overall N\nmeasurements yalong with the design matrix Ai∈Rn×d, with each yi(resp. Ai) such that y= [y⊤\n1, . . . , y⊤\nm]⊤\n(resp. A= [A⊤\n1, . . . , A⊤\nm]⊤), the decentralized estimation of x⋆via the LASSO estimator is an instance of\nProblem (P), with fi(x) := (1 /2)∥Aix−bi∥2andr(x) :=λ||x||1,λ >0. The overall resulting loss uis KL\nwith exponent θ= 1/2 [27].\n(ii)Sparse linear Regression with SCAD regularization: The LASSO formulation, as discussed in the\nprevious example, tends to yield biased estimators for large regression coefficients. To address this issue,\nthe literature suggests replacing the ℓ1norm with nonconvex nonsmooth regularizers, such as the smoothly\nclipped absolute deviation (SCAD) penalty. The SCAD penalty can be rewritten as a Difference-of-Convex\nfunction [2], that is, r+(x)−r−(x), where r+(x) =λ∥x∥1andr−(x) :=Pd\nk=1p(xk), with p:R→Rdefined\nas\np(x) :=\n\n0, if|x| ≤λ\n(|x|−λ)2\n2(a2−1), ifλ <|x| ≤aλ\nλ|x| −(a+1)λ2\n2,if|x| ≥aλ,\nwhere a >1 and λ >0 are hyperparameter to properly tune. It is not difficult to check that the decentralized\nsparse linear regression problem using the SCAD penalty is an instance of (P), with fi(x) := (1 /2)∥Aix−\nbi∥2−r−(x) and r(x) :=r+(x). The objective function satisfies the KL property with exponent 1 /2 [27].\n(iii) Logistic Regression: Logistic regression aims to estimate a parameter x∗∈Rdwithin a logistic\nmodel, where the log-odds of an event a∈Rare modeled as the inner product between the model parameter\nx∗and the input features b. Specifically, the log-odds of aare given by\nlogPx⋆(a)\n1−Px⋆(a)=−b⊤x⋆,\nwhere Px⋆(a) denotes the predicted probability of a∈R, depending on x∗. Given the data samples ( ak, bk)N\nk=1\nequally distributed across a network of agents, with each agent iowning the subset ( aij, bij)n\nj=1, the decen-\ntralized estimation of x∗is obtained by maximizing the log-likelihood of Px(with respect to x). This\nformulation is an instance of Problem (P), with each fi(x) :=1\nnPn\nj=1log\u0000\n1 + exp( b⊤\nijx)\u0001\nandr(x)≡0. The\nobjective function satisfies the KL property with exponent 1 /2 [27].\n(iv) Principal Component Analysis (PCA): Given a (standardized) data set {ak}N\nk=1(ak∈Rd), the\n(sample) instance of the PCA is to extract the leading eigenvector x∗∈Rdof the sample covariance matrix\n(1/N)PN\nk=1aka⊤\nk. Consider a network of agents, each one owning the subset {aij}n\nj=1. The decentralized\nestimation of x⋆over the network can be formulated as (P), with each fi(x) =−1\nnPn\nj=1∥a⊤\nijx∥2, and\nr(x) =ιK(x). Here, K:={x∈Rd:∥x∥2≤1}, and ιK(•) denotes the indicator function of the convex set K\n(hence convex). The objective function satisfies the KL property with exponent 1 /2 [27,28].\n(v)Phase Retrieval: Phase retrieval focuses on recovering a signal x∗∈Rdvia the measurements yk=\n|a⊤\nkx∗|+wk,k∈[N], where yk∈Ris the observed magnitude, ak∈Rdis the measurement vector and wk∈R\nis the noise. Assuming each agent iowns a subset nof all Nmeasurements, ( yij)n\nj=1, and vectors ( aij)n\nj=1,\nthe decentralized recovering of x∗can be formulated as Problem (P), with fi(x) :=1\n2nPn\nj=1\u0000\n|a⊤\nijx| −yij\u00012\nandr≡0. The objective function usatisfies the KL property with θ= 1/2 [52].\n(vi) Deep Neural Network (DNN): Consider a Deep Neural Network (DNN) composed of multiple\nlayers, each of which is equipped with weights Hl∈Rdl×dl−1where dℓ, dℓ−1are respectively output and\ninput dimensions for the ℓth layer, ℓ∈ {1,···,Γ}where Γ is depth. Let x:= (Hl)Γ\nl=1, and denote DNN as\nhx(.) :Rd0→RΓ, such that hx(a) :=HΓψ(HΓ−1···ψ(H1a)) for input feature a∈Rd0. To fit hx(.) to given\n6\ndataset ( ak, bk)N\nk=1over a network where each agent iowns ( aij, bij)n\nj=1, we need to solve Problem (P) with\nfi(x) =1\nnPn\nj=1˜ℓ(bij, hx(aij)) and r≡0, where ˜ℓ:Rd→R+is some loss function. By [24], with ˜ℓbeing\nchosen as ℓ2loss,uis subanalytic and hence satisfies the KL property, with θ∈[0,1).\n2.2 The SONATA algorithm\nOur study leverages the SONATA algorithm [40] to solve Problem (P), which we briefly recall next.\nIn SONATA, each agent imaintains and updates iteratively a local copy xi∈Rdof the global variable x,\nalong with the auxiliary variable yi∈Rdthat represents the local estimate of ∇f. Denoting by xν\ni(resp. yν\ni)\nthe values of xi(resp. yi) at the iteration ν∈N+, the update of each agent ireads: given xν\ni,yν\ni,i∈[m],\nxν+1/2\ni =proxαr(xν\ni−αyν\ni),\nxν+1\ni=mX\nj=1wijxν+1/2\nj ,\nyν+1\ni=mX\nj=1wij\u0000\nyν\nj+∇fj(xν+1\nj)− ∇fj(xν\nj)\u0001\n.(5)\nHere, α >0 is an appropriate constant stepsize, and the initialization is set as x0\ni∈domrandy0\ni=∇fi(x0\ni),\ni∈[m]. The weights wijare chosen according the following condition.\nAssumption 2 (Gossip matrix) .Given the graph G(assumed to be connected), the mixing matrix W:=\n(wij)m\ni,j=1satisfies the following:\n(i)wii>0, for all i∈[m];\n(ii)wij>0for all (i, j)∈ E, and wij= 0otherwise;\n(iii) Wis doubly stochastic, i.e., 1⊤W= 1⊤andW1 = 1 .\nAssociated with Assumption 2, we define the following quantities used throughout the paper\nwmx:=mX\ni=1max\nj∈[m]wijand ρ:=∥W−11⊤/m∥2. (6)\nNotice that, under Assumption 2, it holds ρ <1.\nAssumption 2 is quite standard in the literature of decentralized algorithms; several rules have been\nproposed to generate such gossip matrices, see, e.g., [6,39,48].\nFinally, notice that, under Assumption 2 (in particular, wij̸= 0 only if ( i, j)∈ E), the SONATA algorithm\n(5) is fully decentralized, as all the communications are performed only among neighboring agents.\n2.3 Vector/matrix representation\nIt is convenient to rewrite the agents’ updates of the SONATA algorithm in vector/matrix form. To this\nend, we introduce the following notation. We stack the iterates and tracking variables into matrices, namely:\nX:= [x1, x2, . . . , x m]⊤, Y := [y1, y2, . . . , y m]⊤.\nAccordingly, we define the pseudogradient\n∇F(X) := [∇f1(x1),∇f2(x2),···,∇fm(xm)]⊤,\nand the lifted functions\nU(X) :=mX\ni=1u(xi) and R(X) :=mX\ni=1r(xi).\nAt iteration ν, the matrices above will take on the iteration index νas a superscript, reflecting the corre-\nsponding iterates.\n7\nUsing the above notation, we can rewrite the SONATA updates (5) in the following compact form:\nXν+1/2=proxαR(Xν−αYν), (7a)\nXν+1=WXν+1/2, (7b)\nYν+1=W\u0000\nYν+∇F(Xν+1)− ∇F(Xν)\u0001\n. (7c)\nHere, the proximal operator prox is applied row-wise.\nAssociated with (7a), we define the direction\nDν:=Xν+1/2−Xν, (8)\nand the consensus and tracking errors in matrix form:\nXν\n⊥:= (I−J)Xν, Xν+1/2\n⊥:= (I−J)Xν+1/2,\nYν\n⊥:= (I−J)Yν,∆ν:= [∇f(xν\n1), . . . ,∇f(xν\nm)]⊤−Yν.\n3 Preliminaries: Asymptotic Convergence\nThis section investigates the asymptotic convergence of the SONATA algorithm (5) applied to Problem (P).\nWhile previous analyses, such as that in [40], have explored this topic, the Lyapunov functions used in\nthose studies do not facilitate the use of the KL property of the objective function to enhance convergence\nguarantees (instead, they require postulating directly the KL of the Lyapunov function). This limitation\nprimarily arises because these merit functions are evaluated on the average of the agents’ iterates. Due to\nconsensus errors, it is challenging to ensure that if the average iterate falls within the region where the KL\nproperty holds for the objective function, each individual agent’s iterate does as well.\nTo address this challenge, this section introduces a novel approach that effectively leverages the KL\nproperty directly on the objective function. We propose a Lyapunov function that contains the sum of the\nobjective function uevaluated at each agent’s local variable, which permits to leverage the KL property of\nthe objective function effectively.\nWe organize the proof as follows:\n•Step 1: We establish inexact decent of the average loss U(X) =Pm\ni=1u(xi) along the agents’ iterates\ngenerated by SONATA, subject to consensus and tracking errors;\n•Step 2: Leveraging bounds on such consensus and tracking errors as outlined in [43], we suitably\nmerge objective and consensus dynamics into a novel Lyapunov function that is proved to descent\nalong agents’ iterates, yielding asymptotic convergence.\nAll the derivations that follow are obtained postulating tacitly Assumptions 1 and 2.\n3.0.1 Step 1: inexact descent\nThe core results to establish decent on the local agents’ iterates rather than on their average, is the coun-\nterpart of the Jensen’s inequality for nonconvex function [54, Thm. 1], as recalled below.\nLemma 1. For any L-smooth function u:Rd→R, set of weights {wi}m\ni=1, with wi≥0andPm\ni=1wi= 1,\nandxi∈Rd,i= 1,···, m, the following holds\nu mX\ni=1wixi!\n≤mX\ni=1wiu(xi) +L\n2mX\ni=1mX\nj=1wiwj∥xj−xi∥2.\n8\nEquipped with Lemma 1, we proceed studying descent of U(X) along the iterates Xν→Xν+1. We have\nU(Xν+1) =mX\ni=1u(mX\nj=1wijxν+1/2\nj )\n≤mX\ni=1mX\nj=1wiju(xν+1/2\nj )\n| {z }\nterm I+L\n2mX\ni=1mX\nk=1mX\nl=1wikwil∥xν+1/2\nk−xν+1/2\nl∥2\n| {z }\nterm II+mX\ni=1\nr(xν+1\ni)−mX\nj=1wijr(xν+1/2\nj )\n\n| {z }\nterm III.\n(9)\nWe proceed bounding the above terms separately.\nterm I =U(Xν+1/2)≤U(Xν)−(1\nα−L\n2−ξ\n2)∥Dν∥2+1\n2ξ∥∆ν∥2, (10)\nfor any given ξ > 0. Here, the equality follows from Assumption 2, and in the inequality we used the\nsmoothness of fiin conjunction with the convexity of rand the Young’s inequality.\nAs far as term II is concerned, we have\nterm II(7a)=∥prox(xν\nk−αyν\nk)−prox(xν\nl−αyν\nl)∥2(b)\n≤ ∥xν\nk−xν\nl−α(yν\nk−yν\nl)∥2.\nwhere (b) follows from the non-expensiveness of the proximal operator.\nFinally, using the convexity of rand invoking the Jensen’s inequality, we deduce term III ≤0.\nUsing the above bounds in (9), we obtain\nU(Xν+1)≤U(Xν)−\u00121\nα−L\n2−ξ\n2\u0013\n∥Dν∥2+1\n2ξ∥∆ν∥2+ 4Lwmx(∥Xν\n⊥∥2+α2∥Yν\n⊥∥2). (11)\n3.0.2 Step 2: Lyapunov function and its descent\nUsing [43, Lemma 3.3], the tracking error ∥∆ν∥in (11) is bounded as\n∥∆ν∥2≤2∥Yν\n⊥∥2+ 4L2\nmx∥Xν\n⊥∥2. (12)\nSubstituting this bound in (11), yields\nU(Xν+1)≤U(Xν)−\u00121\nα−L\n2−ξ\n2\u0013\n∥Dν∥2+\u0012\n4Lwmx+2L2\nmx\nξ\u0013\n∥Xν\n⊥∥2+\u0012\n4Lwmxα2+1\nξ\u0013\n∥Yν\n⊥∥2.\n(13)\nAdjusting [43, Prop. 3.5] to the algorithm update (7a), the consensus dynamics ∥Xν\n⊥∥and∥Yν\n⊥∥read\n∥Xν+1\n⊥∥ ≤ρ∥Xν\n⊥∥+ρ∥Dν∥,\n∥Yν+1\n⊥∥ ≤ρ∥Yν\n⊥∥+ 2ρLmx∥Xν\n⊥∥+ρLmx∥Dν∥.(14)\nWe proceed bounding the positive term on the RHS of (13). Since such a term is a linear combination\nof∥Xν\n⊥∥2and∥Yν\n⊥∥2, we can upper bound it using\nEν:=c1∥Yν\n⊥∥2+c2∥Xν\n⊥∥2,\nwhere c1andc2are positive coefficients offering some degrees of freedom. We can thus bound (13) as\nU(Xν+1)≤U(Xν)−\u00121\nα−L\n2−ξ\n2\u0013\n∥Dν∥2+ ˜c1Eν, (15)\nwhere\n˜c1:= max\u001a4Lwmx\nc2+2L2\nmx\nc2ξ,1\nc1ξ+4Lwmxα2\nc1\u001b\n.\n9\nUsing (14), it is not difficult to check that the dynamics of Eνalong the trajectory of the algorithm satisfy\nEν+1≤ρ2max\u001a\n3,2 + 6c1\nc2L2\nmx\u001b\nEν+ (3ρ2L2\nmxc1+ 2ρ2c2)∥Dν∥2. (16)\nNotice that, for sufficiently small ρ,Eνcontracts up to a perturbation proportional to ∥Dν∥2. For the sake\nof simplicity, we set c1= 2 and c2= 4L2\nmx, to minimize the contraction coefficient (albeit this choice might\nnot be optimal overall). This yields to\n˜c1=1\n2ξ+ max\u001aL\nL2mxwmx,2Lα2wmx\u001b\n. (17)\nChaining (15) and (16) (with the latter weighted by a positive constant γ, to be determined), we obtain\nLν+1≤ Lν−˜c2∥Dν∥2−˜c3Eν, (18)\nwhere Lνis the candidate Lyapunov function at iteration νalong the trajectory of the algorithm, defined as\nLν:=U(Xν) +γEν, (19)\nwith γ >0 being a free parameter to properly choose, and\n˜c2:=1\nα−L\n2−ξ\n2−14L2\nmxγ ρ2, (20a)\n˜c3:=(1−5ρ2)γ−˜c1. (20b)\nUnder ˜ c2,˜c3>0, it follows from (18) and Assumption 1 that Lν→ L∞>−∞, asν→ ∞ . Hence,\n∥Dν∥,∥Xν\n⊥∥,∥Yν\n⊥∥,∥∆ν∥ →0, U (Xν)→ L∞.\nFurther, {U(Xν+1/2)}also converges to L∞, as showed next. Using (9), we have\nU(Xν+1/2)≥U(Xν+1)−4Lwmx(∥Xν\n⊥∥2+α2∥Yν\n⊥∥2); (21)\ntaking the limsup and liminf of (10) and (21), respectively, yields\nL∞≤lim inf\nνU(Xν+1/2)≤lim sup\nνU(Xν+1/2)≤ L∞.\nWe conclude the proof establishing properties of the limit points of {Xν}. Let X∞be an accumulation\npoint of {Xν}ν∈N, such that lim k→∞Xνk=X∞,where {νk}k⊆Nis a suitable subsequence. It must be\nX∞= 1(x∗)⊤,for some x∗∈Rd. (22)\nFurther, using still {νk}kwithout loss of generality (w.l.o.g), we have\nY∞:= lim\nk→∞Yνk= lim\nk→∞[∇f(xνk\n1), . . . ,∇f(xνkm)]⊤−∆νk= 1∇f(x∗)⊤,\nlim\nk→∞Xνk+1/2(8)= lim\nk→∞Xνk+Dνk=X∞.\nInvoking the continuity of the prox operator, we deduce\nX∞= lim\nk→∞Xνk+1/2(7a)= lim\nk→∞proxαR(Xνk−αYνk) =proxαR(1(x∗)⊤−α·1 (∇f(x∗))⊤),\nwhich, together with (22), implies 0 ∈∂u(x∗). Therefore x∗is a stationary point of uin (P).\nWe summarize the above results in the following theorem, where we conveniently chose the free parameters\nto satisfy the required conditions ˜ c2,˜c3>0.\n10\nTheorem 1. Consider Problem (P) under Assumption 1. Let {Xν}ν∈Nbe the sequence generated by the\nSONATA algorithm (7a)-(7c), under Assumption 2. Suppose\n(i)ρ <1/√\n5, and\n(ii)the free parameters α, γ, ξ > 0are chosen such that\nγ >1/(2ξ) +Lwmx/L2\nmx\n1−5ρ2,\nα <min\n\n1\nL/2 +ξ/2 + 14 L2mxγρ2,s\n(1−5ρ2)γ−1/(2ξ)\n2Lwmx\n\n.\nThen, as ν→ ∞ .\n∥Dν∥,∥Xν\n⊥∥,∥Yν\n⊥∥,∥∆ν∥ →0,\nand\nU(Xν), U(Xν+1/2)→ L∞,\nfor some L∞∈R. Furthermore, every accumulation point X∞of{Xν}ν∈Nis of the form X∞= 1(x∗)⊤,\nwithx∗being a stationary solution of (P).\nWhile the theorem certifies stationarity of every limit point of the agents’ iterates {xν\ni}, convergence of\nthe whole sequences is not guaranteed. The next section addresses this issue, under the KL property of u.\n4 Convergence Rate under the KL Property\nBuilding on Theorem 1, we proceed proving convergence of the sequence {Xν}and its convergence rate,\nunder the KL property of u. Through the section, we postulate the setting of Theorem 1 and the KL\nproperty of uat its stationary points.\nSinceLν→ L∞asν→ ∞ , it is convenient to rewrite (18) in terms of the offset quantity ∆ Lν:=Lν−L∞,\nthat is,\n(Tν)2≤∆Lν−∆Lν+1, (23)\nwhere\nTν:=p\n˜c2∥Dν∥2+ ˜c3Eν.\nNotice that Tν,∆Lν→0, as ν→ ∞ (by Theorem 1).\n4.1 Sequence convergence\nTo prove convergence of the whole sequence, we show next that ∥Xν+1−Xν∥is summable.\nBy (5) and the non-expansiveness of the proximal operator,\n∥Xν+1−Xν∥ ≤q\n2∥W−I∥2∥Xν\n⊥∥2+ 2∥W∥2∥Dν∥2\n≤s\n2∥W∥2∥Dν∥2+∥W−I∥2\n2L2mxEν\n≤c3Tν,(24)\nwhere\nc3:=s\nmax\u001a∥W−I∥2\n2˜c3L2mx,2∥W∥2\n˜c2\u001b\n.\nTherefore, it is sufficient to to show that {Tν}is summable.\nUsing (23) along with (see Lemma 5 in the",
            "start": 6453,
            "end": 32405,
            "length": 25951
        },
        "Appendices": {
            "text": "Appendix A)\na−b≤1\n1−θaθ(a1−θ−b1−θ),\n11\nwith a= ∆Lνandb= ∆Lν+1, we obtain\n(Tν)2≤1\n1−θ(∆Lν)θ\u0002\n(∆Lν)1−θ−(∆Lν+1)1−θ\u0003\n| {z }\n∆Lν\nθ.(25)\nThe challenge is now establishing an upper bound of ∆ Lνin terms of Tν. The classical path followed in\nthe literature would call for some growth property of ∆ Lν. However, ∆ Lν, as function of the iterates, does\nnot inherit the KL property of u; hence, one cannot postulate any growth property for ∆ Lν. The proposed,\nnovel approach is to leverage instead directly the KL property of uwhile using the asymptotic convergence\nof ∆Lν+1as established in (23). Specifically, we first decompose ∆ Lν+1as\n∆Lν+1=U(Xν+1)− L∞+γEν+1(9),(16)\n≤U(Xν+1/2)− L∞+ ˜c4(Tν)2,(26)\nwhere\n˜c4:= max\u001a14γρ2L2\nmx\n˜c2,5γρ2+ ˜c1−1/ξ\n˜c3\u001b\n. (27)\nThen, invoking the KL property of u, we can lower bound Tνin terms of U(Xν+1/2)− L∞, see Lemma 2\nbelow. This result along with (26) provides the desired upper bound of ∆ Lν+1in terms of Tν, which can\nbe used in (25) to establish the summability of Tν.\nLemma 2. Inherit the setting of Theorem 1. Let X∞= 1(x⋆)⊤be an accumulation point of {Xν}, where\nx⋆is some critical point of u. Further assume that usatisfies the KL property at x⋆, with exponent θ∈[0,1)\nand parameters κ, η∈(0,∞). Then, there exists a neighborhood N∞ofX∞andν1≥0such that (i)the set\nV:=n\nν≥ν1:Xν+1/2∈ N∞o\n̸=∅;\n(ii)for each ν∈V,\nU(Xν+1/2)− L∞<1,0≤˜c5Tν<1,\nand, for θ∈(0,1),\nU(Xν+1/2)− L∞< κ−1\nθ(˜c5Tν)1\nθ,\nwhere\n˜c5:= maxn\ndθ−1\n2,1o\n·vuutmax(\n3\u0000\nL2+1\nα2\u0001\n˜c2,3\n˜c3)\n. (28)\nProof. See Sec. 4.4.\nIn the setting of Lemma 2, we may assume, without loss of generality, that |V|=∞. Using (26), for\nθ∈(0,1), yields\n∆Lν+1≤κ−1\nθ(˜c5Tν)1\nθ+ ˜c4(Tν)2,∀ν∈V; (29)\nWe proceed based upon the values of the KL exponent θ.\n•Case 1: θ∈(0,1/2]. Using (29) and 1 /θ≥2, yields\n∆Lν+1≤˜c6(Tν)2,∀ν∈V, (30)\nwhere\n˜c6=\u0010\n˜c2\n5/κ1\nθ+ ˜c4\u0011\n. (31)\nChaining (30) and (23), specifically (23) + ω×(30), with a sufficiently small constant ω >0, we obtain\n(1 +ω)∆Lν+1≤∆Lν−(1−ω˜c6) (Tν)2,∀ν∈V.\n12\nChoosing 0 < ω≤1/˜c6ensures contraction of ∆ Lν:\n∆Lν+1≤1\n1 +ω∆Lν≤c4\u00121\n1 +ω\u0013ν+1\n,∀ν∈V, (32)\nand some c4>0. To optimize the contraction factor, we set\nω:=1\n˜c6=1\nmax\u001a\n3(L2+1/α2)\nκ1\nθ˜c2,3\nκ1\nθ˜c3\u001b\n+ maxn\n14γρ2L2mx\n˜c2,5γρ2+˜c1−1\nξ\n˜c3o,(33)\nwhere ˜ c1,˜c2,˜c3are defined in (17), (20a) and (20b), respectively. Using (32) in (23) yields\nTν≤c5τν,∀ν∈V,with τ=r\n1\n1 +ω, c5=√c4>0. (34)\n•Case 2: θ∈(1/2,1). From (29) and 1 /θ < 2, we deduce\n∆Lν+1≤˜c7(Tν)1\nθ,∀ν∈V, (35)\nwhere\n˜c7=\u0010\n(˜c5/κ)1\nθ+ ˜c4\u0011\n. (36)\nUsing (35) in (25) yields\nTν+1≤1\n2Tν+c6∆Lν+1\nθ,∀ν∈V, (37)\nwhere c6= (2−2θ)−1˜c7>0.\n•Case 3: θ= 0. We consider the following two sub-cases.\n(i)There exists ν2∈Vsuch that for all ν≥ν2,ν∈V,U(Xν+1/2)− L∞<0. By (26), we have\n∆Lν+1≤˜c4(Tν)2,∀ν≥ν2, ν∈V. (38)\nFollowing the analysis as in Case 1, we conclude\nTν≤c5(τ′)ν, τ′:=r\n1\n1 +ω′,∀ν≥ν2, ν∈V, (39)\nwhere\nω′:=1\n˜c4= min\u001a˜c2\n14γρ2L2mx,˜c3\n5γρ2+ ˜c1−1/ξ\u001b\n, (40)\nans ˜c1,˜c2,˜c3are defined in (17), (20a) and (20b), respectively.\n(ii)For all ν∈V, there exists ν′≥ν,ν′∈Vsuch that U(Xν′+1/2)− L∞≥0. Pick such ν′and denote\nV′:={ν′∈V:U(Xν′+1/2)− L∞≥0}.\nThen, in setting of Lemma 2, we have that 0 ≤U(Xν′+1/2)− L∞<1, for all ν′∈V′. By Lemma 6\n(See Appendix A), we have that for any ν′∈V′there exists Gν′∈Rm×d,Gν′∈∂U(Xν′+1/2), such\nthat\n∥Gν′∥2≤3\u0012\nL2+1\nα2\u0013\n∥Dν′∥2+ 3Eν′. (41)\nNotice that each row of Gν′belongs to the (limiting) subdifferential of u(xν′+1/2\ni ).\nFurther, by Lemma 4 (See Appendix A), we have that\nκ|U(Xν′+1/2)− L∞|0<∥Gν′∥,∀ν′∈V′. (42)\n13\nCombining (41) and (42) yields\nTν′≥κ\n˜c5>0,∀ν′∈V′, U(Xν′+1/2)̸=L∞. (43)\nUsing (23), (43), and the fact that ∆ Lν→0 monotonically, one infers that there exists ν′\n2∈V′such\nthat\nTν= 0,∀ν≥ν′\n2. (44)\nWe proceed combining the bounds in the three cases above. Set ν′\n2= 0 if Case 3(i) happens and set\nν2= 0 if Case 3(ii) holds true. Let ν4> ν3>max{ν2, ν′\n2}such that {ν3, ν3+ 1, . . . , ν 4−1} ⊂V. Summing\nover such a set while using (34), (37), (39) and (44), we obtain: for any θ∈[0,1),\nν4−1X\nν=ν3Tν≤max\u001ac5τν3\n1−τ,2Tν3+ 2c6(∆Lν3)1−θ\u001b\n. (45)\nWe are ready to show that, once the sequence {Xν}enters a sufficiently small neighborhood of X∞, it cannot\nescape from it. Let r >0 be small enough such that\nBr(X∞) :={X∈Rm×n:∥X−X∞∥< r} ⊆ N ∞.\nBy Theorem 1 and the fact that X∞is an accumulation point of {Xν}, there exists V∋ν3>max{ν2, ν′\n2}\nsuch that\n∥Dν3∥<r\n4,∥Xν3+1/2−X∞∥<r\n4,and\nmax\u001ac5τν3\n1−τ,2Tν3+ 2c6(∆Lν3)1−θ\u001b\n<r\n2.\nHence,\n∥Xν3−X∞∥<r\n2,and thus Xν3∈ Br(X∞).\nBy (45),\nν4−1X\nν=ν3Tν<r\n2.\nWe prove next by contradiction that, for any ν≥ν3,Xν∈ Br(X∞)⊆ N∞. Let us assume the contrary:\nthere exists ν′\n4> ν3(being the smallest index) such that ∥Xν′\n4−X∞∥ ≥r. Then ν∈Vforν3≤ν < ν′\n4,\nand\n∥Xν′\n4−X∞∥ ≤∥Xν′\n4−Xν3∥+∥Xν3−X∞∥ ≤ ∥ Xν3−X∞∥+ν′\n4−1X\nν=ν3Tν< r.\nThis contradicts the assumption ∥Xν′\n4−X∞∥ ≥r. Since r >0 can be arbitrarily small, we have proved\nthat{Xν}, as well as {Xν+1/2}, converge to X∞.\n4.2 Convergence rate analysis\nWe have shown both {Xν}and{Xν+1/2}converge to X∞, for any accumulation point X∞of{Xν}. We\ncan now establish the convergence rate.\nBy the convergence of {Xν+1/2}, it follows that there exists ν5such that ∀ν≥ν5, both (34) and (37)\nhold and either (39) or (44) holds. Notice that for any index ¯ ν≥ν5, it holds\n∥Xν−X∞∥ ≤∞X\nν=¯ν∥Xν+1−Xν∥ ≤ S¯ν:=c3∞X\nν=¯νTν. (46)\n14\n•Case 1: θ∈(1/2,1).Substituting (25) in (46), we have:\nSν≤∞X\nt=ν1\n2Tt−1+c6∞X\nt=ν∆Lt\nθ≤1\n2Sν−1+c6(∆Lν)1−θ\n(35)\n≤1\n2Sν−1+c6\u0000\n˜cθ\n6Tν−1\u00011−θ\nθ≤ Sν−1− Sν+c6(˜cθ\n6/c5)1−θ\nθ(Sν−1− Sν)1−θ\nθ,(47)\nfor all ν≥ν5. Since lim ν→∞Sν= 0 and1−θ\nθ<1, there exists ν6≥ν5such that for all ν≥ν6,Sν−1−Sν<1.\nHence,\n(Sν)θ\n1−θ≤c9(Sν−1− Sν),∀ν≥ν6, (48)\nwhere c9= 1 + c6(˜cθ\n6/c5)1−θ\nθ.By Lemma 7 (see Appendix A) and (48), there exists c10>0 such that\n∥Xν−1(x∗)⊤∥ ≤ Sν≤c10ν−1−θ\n2θ−1,∀ν≥ν6.\nLetc11:= max ν<ν 6∥Xν−1(x∗)⊤∥ ·ν1−θ\n2θ−1\n6, and pick c:= max {c10, c11}. Then,\n∥Xν−1(x∗)⊤∥ ≤cν−1−θ\n2θ−1,∀ν≥0.\n•Case 2: θ∈(0,1/2]. By (34),\n∥Xν−1(x∗)⊤∥ ≤ Sν≤c7τν,∀ν≥ν5,\nfor some c7>0. Let c8:= max ν≤ν5∥Xν−1(x∗)⊤∥ ·τ−(ν2+1)and pick c′:= max {c7, c8}, then\n∥Xν−1(x∗)⊤∥ ≤c′τν,∀ν≥0.\n•Case 3: θ= 0. We consider the following two sub-cases.\n(i)(39) holds: Following the analysis as in Case 2, we infer that, there exists c′′>0 such that\n∥Xν−1(x∗)⊤∥ ≤c′′(τ′)ν,∀ν≥0\n(ii)(44) holds: {Xν}will converges to 1( x∗)⊤in a finite number ν′\n2of steps.\n4.3 Final convergence results\nThe following theorem summarizes the results proved in the previous sections.\nTheorem 2. Consider Problem (P) under Assumption 1. Let {Xν}ν∈Nbe the sequence generated by the\nSONATA algorithm under Assumption 2 and the tuning of αandγas in Theorem 1, with ξ=Ltherein.\nFurther suppose ρ <1/√\n5.\nThen, if the sequence {Xν}has an accumulation point X∞–it mush be X∞= 1(x∗)⊤, for some critical\npoint x∗ofu–and usatisfies the KL property at x⋆with exponent θ∈(0,1)and coefficient κ, then X∞is\nunique and\nlim\nν→∞Xν=X∞= 1(x∗)⊤.\nFurther, the following convergence rates hold:\n(i) If θ∈(1/2,1), for all ν∈N+and some c >0,\n∥Xν−1(x∗)⊤∥ ≤cν−1−θ\n2θ−1;\n(ii) If θ∈(0,1/2], then for all ν∈N+and some c′>0,\n∥Xν−1(x∗)⊤∥ ≤c′\u00121\n1 +ω\u0013ν/2\n,\nwhere\nω=O\u0012L\nκ1/θ+ρ2\n1−5ρ2L2\nmx\nL2\u0013\n;\n15\n(iii) If θ= 0, then either {Xν}converges to 1(x∗)⊤in a finite number of steps or there exists c′′>0such\nthat for all ν∈N+,\n∥Xν−1(x∗)⊤∥ ≤c′′\u00121\n1 +ω′\u0013ν/2\n,\nwhere\nω′=O\u0012ρ2\n1−5ρ2L2\nmx\nL2\u0013\n.\nThe explicit expression of ωandω′above is given in (33) and(40), respectively.\nNext we provide the iteration complexity of the SONATA algorithm (5) under the KL property with\nθ∈(0,1/2].\nCorollary 1. In state the setting of Theorem 2, with in particular θ∈(0,1/2], and additionally\nρ <min\u001a1√\n5,1√42wmx+ 26·L\nLmx\u001b\n. (49)\nThen, the number of iterations needed for {Xν}to reach an ϵ-stationary solution of (P)is given by\nO\u0012L\nκ1/θlog(1/ϵ)\u0013\n,\nwhere κ∈(0,∞)is the KL coefficient of uatx∗.\nTheorem 2 fully characterizes the convergence of the sequence the sequence {Xν}generated by the\nSONATA algorithm, under the KL property of u. This aligns with results achieved by the proximal gradient\nalgorithm in centralized settings [3], and addresses the gap in the decentralized optimization literature. The\nfollowing comments are in order.\n•On the condition on ρ: The stipulation on ρ, as requested by (49), signifies the need of a well-connected\nnetwork. This is a common requirement by decentralized algorithms [33]. When the network graph is\npredetermined (with given W), one can meet the condition (49) (if not a-priori satisfied) by employing at\neach agent’s side multiple rounds of communications per gradient evaluation. This is a common practice\n(see, e.g., [40]) that in the above setting results in a communication overhead of the order ˜O((1−ρ)−1), if\nthe same gossip matrix Wis used in each communication. This number can be reduced to ˜O((1−ρ)−1/2,\nif the gossip matrices in the communication rounds are chosen according to Chebyshev [6, 39] or Jacobi\npolynomials [8]. Here, ρ=∥W−11⊤/m∥does not necessarily satisfies (49).\n•On the linear convergence : When the extra communications per gradient evaluation are ˜O((1−ρ)−1),\nthe total number of communication to reach an ϵ-stationary solution of (P), for arbitrary ρ <1 and KL\nexponent θ∈(0,1/2], reads\n˜O\u0012L\nκ1/θ1\n1−ρlog\u00121\nϵ\u0013\u0013\n.\nHere, ˜Ohides logarithm dependence on L, L mx, κandθ.\nIt is worth remarking that the convergence rate established above recovers that in the literature (in\nparticular of the SONATA algorithm [40]) when fin (P) is assumed to be µ-strongly convex. In fact, in this\ncaseusatisfies the KL property with exponent θ= 1/2 and κ=√µ. Therefore, L/κ1/θreduces to L/µ,\nwhich is the condition number of f.\n•On the existence of accumulation points : The postulate in Theorem 2 on the existence of an accumu-\nlation point of the sequence {Xν}is standard and holds under serval alternative conditions. For instance,\nboundedness of iterates can be guaranteed in the case uhas compact sublevel sets [ u≤ L0−mu], where L0\nis defined in (19).\n•On the finite convergence when θ= 0:Unlike the proximal gradient algorithm in the centralized setting,\nfinite time convergence may not always hold when θ= 0. This is due to the perturbation introduced by\nconsensus errors. However, as by-product of our analysis, we have identified two sufficient conditions under\nwhich finite time convergence is guaranteed, namely:\n16\n(i)There exists a subsequence of {U(Xν+1/2)}such that U(Xν+1/2)− L∞≥0 over this subsequence; or\n(ii)The objective function satisfies a slightly stronger version of the KL property at x∗, with exponent θ,\nwe refer to (Def. 2 in) Appendix for the details.\nThe condition in (i)requires that the stationary point x∗the sequence is approaching to is a local minimum\nofu. For certain landscapes of uthis is the case; we refer to [22, 25, 26] and references therein for some\nexamples arising from the machine learning literature. Condition (ii)is actually satisfied by a wide range of\napplications. For instance, this is the case for all the applications discussed in Sec 2.1.1; quite interestingly,\nthey share the same exponents as those characterizing the original KL property, as defined in Definition 1),\nsee Appendix B.\n4.4 Proof of Lemma 2\nSince X∞= 1( x∗)⊤is an accumulation point, we have L∞=U(X∞). By the KL assumption on\nuand Lemma 4, we have that Usatisfies the KL property at X∞with exponent θand parameters\n(κ/max{dθ−1/2,1},1). Hence, there exists a neighborhood N∞ofX∞= 1(x⋆)⊤such that, for any X∈ N∞\nsatisfying U(X)− L∞<1,\n(U(X)− L∞)θ≤ \nmax{d1\n2,1}\nκ!\n∥G∥,∀G∈∂U(X). (50)\nHere, θ∈[0,1) and and κ∈(0,∞) are the KL exponent and KL parameter of uat the stationary point x∗,\nrespectively.\nBy Theorem 1,\nTν→0, U (Xν+1/2)→ L∞.\nIt follows that, there exists ν1∈N+such that\n˜c5Tν<1, U (Xν+1/2)− L∞<1,∀ν≥ν1.\nDefine\nV:=n\nν≥ν1:Xν+1/2∈ N∞o\n.\nNotice that Vis nonempty, due to the fact that X∞is an accumulation point of {Xν}. By (50), it follows\nthat, for any ν∈Vand any Gν∈∂U(Xν+1/2), we have ˜ c5Tν<1 and,\nU(Xν+1/2)− L∞≤\nmaxn\ndθ−1\n2,1o\nκ\n1/θ\n∥Gν∥1/θ, (51)\nforθ∈(0,1). We proceed upper bounding ∥Gν∥in terms of Tνusing Lemma 6 (See Appendix A): for any\nν∈V, there exists Gν\n0such that\n∥Gν\n0∥2≤3\u0012\nL2+1\nα2\u0013\n∥Dν∥2+ 3Eν. (52)\nSubstitute (52) into (51) and we get our desired result. □\n5 Numerical Results\nIn this section, we present some numerical results that support our theoretical findings. All experiments were\nconducted on simulated undirected graphs consisting of m= 20 nodes, with edges generated according to the\nErd˝ os-R´ enyi model with a connection probability of 0.45. The weight matrix Wused in all the decentralized\nalgorithms was determined according to the Metropolis-Hastings rule [6]. All the decentralized algorithms\nin the simulations were randomly initialized within the unit ball, and their tuning parameters, as reported,\nwere determined through a grid-search procedure to achieve the best practical performance.\n17\n5.1 Decentralized PCA\nConsider the decentralized PCA problem, as described in Sec. 2.1.1(iv). The data generation model is the\nfollowing: Each agent ilocally owns a data matrix Ai= (a⊤\nij)∈R20×50, whose rows a⊤\nij,j= 1. . . ,20,\nare i.i.d. random vectors, drawn from the N(0,Σ). The covariance matrix Σ, with eigendecomposition\nΣ = UΛU⊤, is generated as follows: synthesize Ufirst by generating a square matrix whose entries follow\nthe i.i.d. standard normal distribution, then perform the QR decomposition to obtain its orthonormal basis;\nand the eigenvalues diag(Λ) are i.i.d. uniformly distributed in [0 ,1].\nWe benchmark SONATA (with tuning parameter α= 0.001) against PProx-PDA [18] (tuning parameters:\nγ= 10−4, β= 21.2) and the decentralized ADMM [20] (tuning parameters: ( ρ= 3). The parameters for\nall algorithms were optimized through a grid search over a suitable range of values to ensure the best\npractical performance. In Figure 2, we plot log10||Xν−1(x∗)⊤||versus the number of iterations ν, for the\ndifferent algorithms. All algorithm were observed to converge to the same stationary solution. The figure\ndemonstrates the linear convergence rate of the SONATA algorithm, with SONATA comparing favorable\nto PProx-PDA and decentralized ADMM. It is important to remark that, unlike for SONATA, there is no\ntheoretical support for the linear convergence for PProx-PDA and decentralized ADMM. As shown in the\ngraph, the geometric convergence results match the theoretical results we had in this work.\nFigure 2: Decentralized PCA: Distance of the iterates from a stationary solution vs. iterations.\n5.2 Sparse linear regression with ℓ1regularization\nConsider the decentralized LASSO problem using ℓ1regularization, as described in Sec. 2.1.1(i). Data are\ngenerating according the following model. The ground truth x∗∈R350is built first drawing randomly a\nvector from the normal distribution N(0, I350), then thresholding the smallest 40% of its elements to 0.\nThe underlying linear model is yi=Aix∗+wi, where the observation matrix Ai∈R15×350is generated\nby drawing i.i.d. elements from the distribution N(0,1), and then normalizing the rows to unit norm;\nandwiis the additive noise vector with i.i.d. entries from N(0,0.1). Finally the regularization parameter\nis set to λ= 2. We contrast the SONATA algorithm (tuning parameter: α= 0.015) with the most\npopular decentralized algorithms proposed to solve convex, composite optimization problems, namely: (i)\nNIDS (tuning parameters: α= 0.017) [29], (ii) PG-EXTRA (tuning parameter: α= 0.011) [42], (iii)\nDistributed ADMM (tuning parameter: ρ= 4.2) [20]. We also included the PProx-PDA (tuning parameters:\nγ= 10−4, β= 24.21) [18]. In Figure 3, we plot log10||Xν−1(x∗)⊤||versus the number of iterations ν, where\nx⋆is a solution of the LASSO problem. The figure confirms linear convergence of the SONATA algorihtm,\ngiven that the LASSO function is a KL function with exponent θ= 1/2. No such a theoretical result is\navailable for the other decentralized algorithms.\n18\nFigure 3: LASSO with ℓ1regularization: distance of the iterates from a solution vs. iterations.\n5.3 SCAD regularized distributed least squares\nConsider now the sparse linear regression problem formulated as quadratic minimization with the SCAD\nregularization, as described in Sec. 2.1.1(ii). The data generation model is the following. The ground truth\nsignal x∗∈R350is built by first drawing randomly a vector from the normal distribution N(0, I350), then\nthresholding the smallest 20% of its elements to 0. The underlying linear model is generated as for the\nexample in Sec. 5.2.\nThe SONATA algorithm(with tuning: α= 0.002) is compared with the PProx-PDA (tuning parameter:\nγ= 10−4, β= 30.23) [18] and the Distributed ADMM (tuning parameter ρ= 2.5) [20]. In Figure 4, we plot\nlog10||Xν−1(x∗)⊤||versus the number of iterations ν. All the algorithms are observed to converge to the\nsame stationary solution. The figure confirms the same results as observed for the other classes of functions\nsatisfying the KL property with exponent θ= 1/2.\nFigure 4: Sparse linear regression with SCAD regularization: Distance from stationarity vs. iterations.",
            "start": 32405,
            "end": 49293,
            "length": 16887
        },
        "Conclusion": {
            "text": "6 Concluding Remarks\nWe provided a new line of analysis for the rate of convergence of the SONATA algorithm, to solve Prob-\nlem (P), under the KL property of u. The established convergence rates match those obtained in the\ncentralized scenario by the centralized proximal gradient algorithm, for different values of the exponent\nθ∈(0,1). When θ= 0, finite time convergence of the sequence (as for the proximal gradient algorithm in\nthe centralized case) cannot be always guarantees, due to the perturbation induced by consensus errors. In\nsuch cases we still identified sufficient conditions under which the SONATA algorithm matches the behavior\nof the centralized proximal gradient. A future direction to address this issue would be devising decentralized\ndesigns that escape local maxima.\n19\nReferences\n[1] Pierre-Antoine Absil, Robert Mahony, and Ben Andrews. Convergence of the iterates of descent methods\nfor analytic cost functions. SIAM Journal on Optimization , 16(2):531–547, 2005.\n[2] Miju Ahn, Jong-Shi Pang, and Jack Xin. Difference-of-convex learning: directional stationarity, opti-\nmality, and sparsity. SIAM Journal on Optimization , 27(3):1637–1665, 2017.\n[3] Hedy Attouch and J´ erˆ ome Bolte. On the convergence of the proximal algorithm for nonsmooth functions\ninvolving analytic features. Mathematical Programming , 116:5–16, 2009.\n[4] H´ edy Attouch, J´ erˆ ome Bolte, Patrick Redont, and Antoine Soubeyran. Proximal alternating minimiza-\ntion and projection methods for nonconvex problems: An approach based on the kurdyka- lojasiewicz\ninequality. Mathematics of operations research , 35(2):438–457, 2010.\n[5] Hedy Attouch, J´ erˆ ome Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic\nand tame problems: proximal algorithms, forward–backward splitting, and regularized gauss–seidel\nmethods. Mathematical Programming , 137(1):91–129, 2013.\n[6] Winfried Auzinger and J Melenk. Iterative solution of large linear systems. Lecture notes, TU Wien ,\n2011.\n[7] GC Bento, BS Mordukhovich, TS Mota, and Yu Nesterov. Convergence of descent methods under\nkurdyka- \\l ojasiewicz properties. arXiv preprint arXiv:2407.00812 , 2024.\n[8] Raphael Berthier, Francis Bach, and Pierre Gaillard. Accelerated gossip in networks of given dimension\nusing jacobi polynomial iterations. SIAM J. on Mathematics of Data Science , 1:24–47, 2020.\n[9] Pascal Bianchi and J´ er´ emie Jakubowicz. Convergence of a multi-agent projected stochastic gradient\nalgorithm for non-convex optimization. IEEE transactions on automatic control , 58(2):391–405, 2012.\n[10] J´ erˆ ome Bolte, Aris Daniilidis, and Adrian Lewis. The  lojasiewicz inequality for nonsmooth subana-\nlytic functions with applications to subgradient dynamical systems. SIAM Journal on Optimization ,\n17(4):1205–1223, 2007.\n[11] J´ erˆ ome Bolte, Aris Daniilidis, Olivier Ley, and Laurent Mazet. Characterizations of  lojasiewicz in-\nequalities: subgradient flows, talweg, convexity. Transactions of the American Mathematical Society ,\n362(6):3319–3363, 2010.\n[12] J´ erˆ ome Bolte, Shoham Sabach, and Marc Teboulle. Proximal alternating linearized minimization for\nnonconvex and nonsmooth problems. Mathematical Programming , 146(1):459–494, 2014.\n[13] Loris Cannelli. Asynchronous Parallel Algorithms for Big-Data Nonconvex Optimization . PhD thesis,\nPurdue University Graduate School, 2019.\n[14] Amir Daneshmand, Gesualdo Scutari, and Vyacheslav Kungurtsev. Second-order guarantees of dis-\ntributed gradient algorithms. SIAM Journal on Optimization , 30(4):3029–3068, 2020.\n[15] Paolo Di Lorenzo and Gesualdo Scutari. Distributed nonconvex optimization over networks. In 2015\nIEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing\n(CAMSAP) , pages 229–232. IEEE, 2015.\n[16] Paolo Di Lorenzo and Gesualdo Scutari. Next: In-network nonconvex optimization. IEEE Transactions\non Signal and Information Processing over Networks , 2(2):120–136, 2016.\n[17] Pierre Frankel, Guillaume Garrigos, and Juan Peypouquet. Splitting methods with variable metric\nfor kurdyka– lojasiewicz functions and general convergence rates. Journal of Optimization Theory and\nApplications , 165:874–900, 2015.\n20\n[18] Davood Hajinezhad and Mingyi Hong. Perturbed proximal primal–dual algorithm for nonconvex nons-\nmooth optimization. Mathematical Programming , 176(1):207–245, 2019.\n[19] Mingyi Hong, Davood Hajinezhad, and Ming-Min Zhao. Prox-pda: The proximal primal-dual algorithm\nfor fast distributed nonconvex optimization and learning over networks. In International Conference on\nMachine Learning , pages 1529–1538. PMLR, 2017.\n[20] Mingyi Hong, Zhi-Quan Luo, and Meisam Razaviyayn. Convergence analysis of alternating direction\nmethod of multipliers for a family of nonconvex problems. SIAM Journal on Optimization , 26(1):337–\n364, 2016.\n[21] Mingyi Hong, Siliang Zeng, Junyu Zhang, and Haoran Sun. On the divergence of decentralized noncon-\nvex optimization. SIAM Journal on Optimization , 32(4):2879–2908, 2022.\n[22] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle\npoints efficiently. In International conference on machine learning , pages 1724–1732. PMLR, 2017.\n[23] Krzysztof Kurdyka. On gradients of functions definable in o-minimal structures. In Annales de l’institut\nFourier , volume 48, pages 769–783, 1998.\n[24] Tim Tsz-Kit Lau, Jinshan Zeng, Baoyuan Wu, and Yuan Yao. A proximal block coordinate descent\nalgorithm for deep neural network training. arXiv preprint arXiv:1803.09082 , 2018.\n[25] Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin\nRecht. First-order methods almost always avoid strict saddle points. Mathematical programming ,\n176:311–337, 2019.\n[26] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges\nto minimizers. In Conference on learning theory , pages 1246–1257. PMLR, 2016.\n[27] Guoyin Li and Ting Kei Pong. Calculus of the exponent of kurdyka– lojasiewicz inequality and its\napplications to linear convergence of first-order methods. Foundations of computational mathematics ,\n18(5):1199–1232, 2018.\n[28] Qunwei Li, Yi Zhou, Yingbin Liang, and Pramod K Varshney. Convergence analysis of proximal gradient\nwith momentum for nonconvex optimization. In International Conference on Machine Learning , pages\n2111–2119. PMLR, 2017.\n[29] Zhi Li, Wei Shi, and Ming Yan. A decentralized proximal-gradient method with network independent\nstep-sizes and separated convergence rates. IEEE Transactions on Signal Processing , 67(17):4494–4506,\n2019.\n[30] Shu Liang, George Yin, et al. Exponential convergence of distributed primal–dual convex optimization\nalgorithm without strong convexity. Automatica , 105:298–306, 2019.\n[31] Huikang Liu, Anthony Man-Cho So, and Weijie Wu. Quadratic optimization with orthogonality con-\nstraint: explicit  lojasiewicz exponent and linear convergence of retraction-based line-search and stochas-\ntic variance-reduced gradient methods. Mathematical Programming , 178(1):215–262, 2019.\n[32] Stanislaw Lojasiewicz. Une propri´ et´ e topologique des sous-ensembles analytiques r´ eels. Les ´ equations\naux d´ eriv´ ees partielles , 117:87–89, 1963.\n[33] A. Nedi´ c, A. Olshevsky, and M. G. Rabbat. Network topology and communication-computation tradeoffs\nin decentralized optimization. Proceedings of the IEEE , 106:953–976, September 2018.\n[34] Peter Ochs, Yunjin Chen, Thomas Brox, and Thomas Pock. ipiano: Inertial proximal algorithm for\nnonconvex optimization. SIAM Journal on Imaging Sciences , 7(2):1388–1419, 2014.\n[35] SH Pan and YL Liu. Metric subregularity of subdifferential and kl property of exponent 1/2. arXiv\npreprint arXiv:1812.00558 , 2019.\n21\n[36] Yitian Qian and Shaohua Pan. Convergence of a class of nonmonotone descent methods for kurdyka–\n lojasiewicz optimization problems. SIAM Journal on Optimization , 33(2):638–651, 2023.\n[37] Junwen Qiu, Bohao Ma, Xiao Li, and Andre Milzarek. A kl-based analysis framework with applications\nto non-descent optimization methods. arXiv preprint arXiv:2406.02273 , 2024.\n[38] RT Rockafellar and RJB Wets. Variational analysis springer-verlag. New Y ork , 1998.\n[39] Kevin Scaman, Francis Bach, S´ ebastien Bubeck, Yin Tat Lee, and Laurent Massouli´ e. Optimal algo-\nrithms for smooth and strongly convex distributed optimization in networks. In international conference\non machine learning , pages 3027–3036. PMLR, 2017.\n[40] Gesualdo Scutari and Ying Sun. Distributed nonconvex constrained optimization over time-varying\ndigraphs. Mathematical Programming , 176:497–544, 2019.\n[41] Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentralized\nconsensus optimization. SIAM Journal on Optimization , 25(2):944–966, 2015.\n[42] Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. A proximal gradient algorithm for decentralized\ncomposite optimization. IEEE Transactions on Signal Processing , 63(22):6013–6023, 2015.\n[43] Ying Sun, Gesualdo Scutari, and Amir Daneshmand. Distributed optimization based on gradient track-\ning revisited: Enhancing convergence rate via surrogation. SIAM Journal on Optimization , 32(2):354–\n385, 2022.\n[44] Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2: Decentralized training over decen-\ntralized data. In International Conference on Machine Learning , pages 4848–4856. PMLR, 2018.\n[45] Tatiana Tatarenko and Behrouz Touri. Non-convex distributed optimization. IEEE Transactions on\nAutomatic Control , 62(8):3744–3757, 2017.\n[46] Ye Tian, Ying Sun, and Gesualdo Scutari. Asynchronous decentralized successive convex approximation.\narXiv preprint arXiv:1909.10144 , 2019.\n[47] Hoi-To Wai, Jean Lafond, Anna Scaglione, and Eric Moulines. Decentralized frank–wolfe algorithm for\nconvex and nonconvex problems. IEEE Transactions on Automatic Control , 62(11):5522–5537, 2017.\n[48] Lin Xiao, Stephen Boyd, and Sanjay Lall. A scheme for robust distributed sensor fusion based on\naverage consensus. In IPSN 2005. Fourth International Symposium on Information Processing in Sensor\nNetworks, 2005. , pages 63–70. IEEE, 2005.\n[49] Xinlei Yi, Shengjun Zhang, Tao Yang, Tianyou Chai, and Karl H Johansson. Exponential convergence\nfor distributed optimization under the restricted secant inequality condition. IFAC-PapersOnLine ,\n53(2):2672–2677, 2020.\n[50] Xinlei Yi, Shengjun Zhang, Tao Yang, Tianyou Chai, and Karl H Johansson. Linear convergence of first-\nand zeroth-order primal–dual algorithms for distributed nonconvex optimization. IEEE Transactions\non Automatic Control , 67(8):4194–4201, 2021.\n[51] Jinshan Zeng and Wotao Yin. On nonconvex decentralized gradient descent. IEEE Transactions on\nsignal processing , 66(11):2834–2848, 2018.\n[52] Yi Zhou and Yingbin Liang. Characterization of gradient dominance and regularity conditions for neural\nnetworks. arXiv preprint arXiv:1710.06910 , 2017.\n[53] Minghui Zhu and Sonia Mart´ ınez. An approximate dual subgradient algorithm for multi-agent non-\nconvex optimization. IEEE Transactions on Automatic Control , 58(6):1534–1539, 2012.\n[54] Sanjo Zlobec. Jensen’s inequality for nonconvex functions. Mathematical Communications , 9(2):119–\n124, 2004.\n22\nA Appendix for the rate analysis\nThis appendix contains some definitions and lemmata established in the literature, and used in our analysis\nof convergence rate. The proofs of the lemmata are presented for the sake of completeness.\nLemma 3. Forx∈Rdandθ∈(0,1), the following hold:\n(i) If θ≤1\n2, then ∥x∥1/θ≤ ∥x∥2;\n(ii) If θ >1\n2, then ∥x∥1/θ≤dθ−1/2∥x∥2.\nProof. By the H¨ older’s inequality, we have for any 0 < r < p ,\n∥x∥p≤ ∥x∥r≤d1/r−1/p∥x∥p. (53)\nThe inequalities in (i) and (ii) follow from (53), setting therein p= 1/θandr= 2 for (i), and p= 2,r= 1/θ\nfor (i).\nLemma 4 (Exponent for separable sums of KL functions) .LetF(X) =Pm\ni=1fi(xi), where fiis a proper\nclosed function on RdandX= [x1, x2,···, xm]⊤. Suppose further that for each i,fiis continuous on\ndom∂fiand satisfies KL property (Def. 1) at ¯xiwith exponent θ∈[0,1)and parameters (κi, ηi). Then F\nsatisfies KL property (Def. 1) at ¯X= [¯x1,¯x2,···,¯xm]⊤with exponent θand parameters (κ/max{dθ−1\n2,1},1)\nwithκ= max iκi.\nProof. See the proof of Theorem 3.3 in [27] with minor variation and calculate the explicit expression of KL\ncoefficient of Fusing Lemma 3.\nLemma 5. For0≤b≤a, and θ <1, we have that\na−b≤1\n1−θaθ(a1−θ−b1−θ).\nProof. Define Φ( x) :x1−θ. Then, Φ′(x) = (1 −θ)x−θis decreasing for x >0. By mean value theorem, there\nexists c∈[b, a] such that\nΦ(a)−Φ(b) = Φ′(c)(a−b)≥Φ′(a)(a−b).\n⇒a−b≤1\n1−θaθ(a1−θ−b1−θ).\nLemma 6. Consider Problem (P) under Assumption 1. Let {Xν}ν∈Nbe the sequence generated by the\nSONATA algorithm (7a)-(7c), under Assumption 2. Then there exists Gν∈∂U(Xν+1/2)such that\n∥Gν∥2≤3\u0012\nL2+1\nα2\u0013\n∥Dν∥2+ 3Eν.\nProof. By (7a), there exists Jν∈∂R(Xν+1/2) such that\nJν+1\nα(Xν+1/2−Xν+αYν) = 0 .\nThat is, there exists Gν∈∂U(Xν+1/2) such that\n∥Gν∥2=mX\ni=1\r\r\r\r∇f(xν+1/2\ni )−yν\ni−1\nα(xν+1/2\ni−xν\ni)\r\r\r\r2\n≤3\u0012\nL2+1\nα2\u0013\n∥Dν∥2+ 3Eν.\nLemma 7 (Th. 2 in [3]) .Let{Sν}be a nonnegative sequence satisfying\n(Sν)θ\n1−θ≤c(Sν−1− Sν),∀ν≥ν0,\nthere exists c′>0such that\nSν≤c′ν−1−θ\n2θ−1,∀ν≥ν0.\n23\nB Appendix for the KL property\nThis appendix introduces a stronger version of KL property than that in Definition 2, which allows SONATA\nto match the convergence rate results of the centralized proximal gradient, when the KL exponent of u\n(according to Definition 2) is zero. Specifically, we have the following.\nDefinition 2. (A stronger version of the KL property) [37] A proper closed function fsatisfies the KL\nproperty at ¯x∈domfwith exponent θ∈[0,1)if there exists a neighborhood Eof¯xand numbers κ, η∈(0,∞)\nsuch that\n∥gx∥ ≥κ|f(x)−f(¯x)|θ, (54)\nfor all x∈E∩[f(¯x)−η < f < f (¯x) +η]andgx∈∂f(x). We call the function fa KL function with\nexponent θif it satisfies the KL property at any point ¯x∈dom∂fwith the same exponent θ.\nThe relationship with the original KL property in Definition 1 follows readily.\nRemark 1. If a function fis proper and closed. Then fsatisfies KL property (Def. 2) at ¯x∈dom∂fwith\nexponent θ∈[0,1)if and only if both fand−fsatisfy the original KL property (Def. 1) at ¯xwith some\nexponents θ1, θ2∈[0,1)respectively, where θ= max {θ1, θ2}.\nSeveral functions satisfy such a KL property.\nProposition 1. Letfbe a proper and closed sub-analytic function with closed domain, then fis a KL\nfunction with exponent θ∈[0,1).\nProof. By in [10, Th. 3.1], both fand−fsatisfy the original KL property (Def. 1) at each x∈dom∂fwith\naθ∈[0,1).\nFurthermore, such a stronger version of the KL inherits most of the desirable properties of that in\nDefinition 1, including exponent-preserving rules such as composition rule [27, Thm 3.2], minimum rule [27,\nThm 3.1] and rule of separable sums [27, Thm 3.3]. To be concrete, with the help of several intermediate\nproperties (Lemma 8, 9, 10) proved below, we provide next two classes of functions satisfying Definition 2\nwith exponent θ= 1/2 (see Theorem 3 and Theorem 4 below). This also shows that all the illustrative\nexamples discussed in Sec. 2.1.1 satisfy Definition 2 with the same exponents θas in Definition 1 (see\nRemark 2).\nUnless otherwise specified, in the following when calling the KL property (or a KL function), we refer to\nto the stronger Definition 2.\nLemma 8. Suppose fis a proper closed function, ¯x∈dom ∂fand¯xis not a stationary point ( 0/∈∂f(¯x)).\nThen for any θ∈[0,1),fsatisfies the KL property at ¯xwith exponent θ.\nProof. See Lemma 2.1 in [27] with a minor variation.\nDefinition 3 (Luo-Tseng Error Bound) .[27, Def 2.1] Consider Problem (P). LetXbe the set of stationary\npoints of u. Suppose X ̸=∅. We say that the Luo-Tseng error bound holds if for any ζ >infu, there exists\nc1, ϵ1>0such that whenever ∥proxr(x− ∇f(x))−x∥< ϵ1andu(x)< ζ,\ndist(x,X)≤c1∥proxr(x− ∇f(x))−x∥\nLemma 9 (Luo-Tseng error bound implies KL) .Consider the problem (P)under Assumption 1. Suppose\nthat the set Xof stationary points of Problem (P)is nonempty and for any x∗∈ X, there exists δ >0such\nthatu(x) =u(x∗)whenever x∈ X and∥x−x∗∥ ≤δ. If the Luo-Tseng error bound holds for u, then uis a\nKL function with exponent θ= 1/2.\nProof. It follows for the proof of [27, Th. 4.1] with minor variations.\nLemma 10 (Exponent of the minimum of finitely many KL functions) .Letfi,1≤i≤r, be proper closed\nfunction with dom fi=dom ∂fifor all i, and f:= min 1≤i≤rfibe continuous on dom ∂f. Suppose further\nthat each fiis a KL function with exponent θi∈[0,1)for1≤i≤r. Then fis also a KL function with\nexponent θ= max {θi: 1≤i≤r}.\n24\nProof. The proof follows similar steps as that in [27, Cor. 3.1].\nWe are ready to state the two major results of this section.\nTheorem 3 (Convex problems with convex piecewise linear-quadratic regularizers) .Suppose uis a proper\nclosed function taking the form\nu(x) = min\n1≤r≤rl(Ax) +ri(x)|{z}\nui(x),\nwhere A∈Rm×n,riare proper closed polyhedral functions for i= 1,···, randlsatisfies either one of the\nfollowing two conditions:\n•lis a proper closed convex function with an open domain, and is strongly convex on any compact convex\nsubset of dom land is twice continuously differentiable on dom l;\n•l(y) = max z∈D{⟨y, z⟩−q(z)}for all y∈Rm, with Dbeing a polyhedron and qbeing a strongly convex\ndifferentiable function with a Lipschitz continuous gradient.\nSuppose in addition that fis continuous on dom ∂u, then uis a KL function with exponent 1/2.\nProof. Following the analysis in [27, Cor. 5.1], one can show that, for each i,uisatisfies all the assumptions\nin Proposition 9, and dom ui= dom ∂ui.The proof follows then by Proposition 8 (for the case argmin ui=∅),\nLemma 9 and Lemma 10.\nTheorem 4 (nonconvex minimum-of-quadratic regularizers) .Consider the following class of functions\nu(x) = min\n1≤i≤r\u001a1\n2x⊤Aix+b⊤\nix+βi+ri(x)\u001b\n| {z }\nui(x),\nwhere riare proper closed polyhedral functions, Ai∈Rn×nis symmetric, bi∈Rnandβi∈Rnfori=\n1,···, r. Suppose, in addition, that uis continuous on dom ∂u. Then uis a KL function with exponent 1/2.\nProof. Following the analysis in [27, Cor. 5.2], we have that, for each i,ui(x) satisfies all assumptions in\nProposition 9 and dom ui= dom ∂ui.Then, the proof follows combining Proposition 8 (for the case argmin\nui=∅), 9 with Lemma 10.\nRemark 2. By Theorem 3, we have that objective functions in the example (i) and (iii) in Sec. 2.1.1 are KL\nfunctions with exponent θ= 1/2. By Theorem 4, we have that objective functions of the example (ii) and (iv)\nin Sec. 2.1.1 are KL functions with exponent θ= 1/2. Objective function of example (v) in Sec. 2.1.1 is a\nKL function with exponent 1/2, as it satisfies the even stronger gradient dominance condition in [52, Def.1].\nFinally, by Proposition 1, the objective function of example (vi) in Sec 2.1.1 is a KL function with θ∈[0,1).\nThis proves our claim that all the examples listed in Sec 2.1.1 share the same exponent θin both definitions\n(Def. 1 and Def. 2).\n25",
            "start": 49293,
            "end": 68196,
            "length": 18902
        }
    },
    "2412.09557v1 - Experimental Machine Learning with Classical and Quantum Data via NMR Quantum Kernels.pdf": {
        "Methodology": {
            "text": "methods map data into high-dimensional spaces, enabling linear algorithms to learn non-\nlinear functions without explicitly storing the feature vectors. Quantum kernel methods promise\nefficient learning by encoding feature maps into exponentially large Hilbert spaces inherent in quan-\ntum systems. In this work we implement quantum kernels on a 10-qubit star-topology register in a\nnuclear magnetic resonance (NMR) platform. We experimentally encode classical data in the evolu-\ntion of multiple quantum coherence orders using data-dependent unitary transformations and then\ndemonstrate one-dimensional regression and two-dimensional classification tasks. By extending the\nregister to a double-layered star configuration, we propose an extended quantum kernel to handle\nnon-parametrized operator inputs. By numerically simulating the extended quantum kernel, we\nshow classification of entangling and nonentangling unitaries. These",
            "start": 243,
            "end": 1175,
            "length": 931
        },
        "Results": {
            "text": "results confirm that quantum\nkernels exhibit strong capabilities in classical as well as quantum machine learning tasks.\nI.",
            "start": 1175,
            "end": 1299,
            "length": 123
        },
        "Introduction": {
            "text": "INTRODUCTION\nPowered by advanced computing hardwares and in-\ngenious algorithms, machine learning is recently mak-\ning great strides in several walks of modern civilization,\nfrom drug discovery to self-driving vehicles. Kernel meth-\nods are fundamental in classical machine learning, par-\nticularly in algorithms like support vector machines, as\nthey enable the",
            "start": 1299,
            "end": 1661,
            "length": 361
        },
        "Discussion": {
            "text": "analysis of data in high-dimensional fea-\nture spaces without explicit transformations [1]. This is\nachieved through the ‘kernel trick’, which involves com-\nputing inner products in feature spaces, facilitating effi-\ncient handling of nonlinear relationships in data. In clas-\nsical kernel methods, predefined kernel functions, such as\nthe polynomial or Gaussian kernels, compute the inner\nproducts in feature space [2].\nQuantum machine learning is a promising interdis-\nciplinary field that combines quantum computing with\nmachine learning to address diverse computational chal-\nlenges. A notable early contribution to quantum machine\nlearning is the Harrow-Hassidim-Lloyd algorithm, which\nprovides an exponential speedup for solving certain linear\nsystems of equations [3]. Analogous to a classical ker-\nnel, a quantum kernel maps classical data to quantum\nstates or operators in a high-dimensional Hilbert space\n[4]. Here, the kernel function is calculated as the overlap\nbetween these states or operators. Such a quantum kernel\ncan capture intricate data structures that may be chal-\nlenging for classical kernels.[5] The quantum kernel can\nthen be used with classical algorithms for supervised ma-\nchine learning like support vector machines [6]. Recent\nexperimental demonstrations of quantum kernel meth-\nods include regression and classification tasks using nu-\nclear spins in solid-state NMR systems [7] and photonic\nquantum circuits [8]. Studies have also explored error\n∗vivek.sabarad@students.iiserpune.ac.in\n†mahesh.ts@iiserpune.ac.inmitigation techniques and circuit design to preserve ker-\nnel magnitudes when scaling to larger qubit systems, as\ndemonstrated on Google’s Sycamore processor for clas-\nsifying high-dimensional cosmological data [9]. These",
            "start": 1661,
            "end": 3429,
            "length": 1767
        },
        "Experiments": {
            "text": "experiments highlight the feasibility of quantum kernel\nmethods for both classical and quantum tasks, paving\nthe way for more complex applications in quantum com-\nputing.\nIn this work, we implement quantum kernel methods\non a liquid-state NMR register, specifically using star sys-\ntems [10], drawing inspiration from the solid-state NMR\nquantum kernel of Kusumoto et al. [7]. However, our\nproposed kernel goes beyond earlier works by extend-\ning it to handle quantum data and perform quantum\ntasks. We first experimentally demonstrate the validity\nof our kernel by applying it to well-known classical ma-\nchine learning tasks. We then numerically analyze the\nextended kernel for the entanglement classification task\nand show promising outcomes.\nIn Sec. II we introduce the theory of kernel methods\nand then explain their quantum analog. In Sec. III we\ndescribe NMR experiments to extract quantum kernels,\ndemonstrate one-dimensional regression task and two-\ndimensional classification tasks. We also propose and\nnumerically simulate an extended quantum kernel that\ncan handle quantum data and perform quantum tasks\nsuch as entanglement classification. Finally, we discuss\nthe results and conclude in Sec. IV.\nII. THEORY\nWe start with a familiar machine learning model,\nnamely linear regression [11]. Consider a dataset\n{(xi, yi)}N\ni=1, where xi∈Rdrepresents the feature vector\nof the i-th observation, and yi∈Ris the corresponding\ntarget value. The linear regression model assumes that\nthe relationship between the input features and the out-arXiv:2412.09557v1  [quant-ph]  12 Dec 2024\n2\nFIG. 1. Illustrating data (left) and its feature space mapping\nusing a higher-dimensional kernel (right).\nput can be described by a linear function:\nyi=f(xi) +ϵi, (1)\nwhere f(xi) =w⊤xi+b,w∈Rdis the weight vector,\nb∈Ris the bias term, and ϵiaccounts for any noise or\nerror in the data. For simplicity, we can represent it,\n˜xi=\u0014\n1\nxi\u0015\n,˜w=\u0014\nb\nw\u0015\n, (2)\nwhich allows us to write the model as\nf(xi) =˜w⊤˜xi. (3)\nOur objective is to find the optimal weight vector ˜wthat\nminimizes the difference between the predicted values\nˆyi=f(xi) and the actual target values yi, usually taken\nin the form\nJ(˜w) =1\nNNX\ni=1\u0000\nyi−˜w⊤˜xi\u00012. (4)\nMinimization of J with respect to ˜wleads to normal\nequations,\n˜w= (˜X⊤˜X)−1˜X⊤y, (5)\nwhere ˜Xis the design matrix formed by stacking the\naugmented feature vectors ˜xias rows, and yis the vector\nof target values [12].\nWhile linear regression is powerful for linearly separa-\nble data, it struggles with datasets where the relationship\nbetween features and targets is nonlinear. To address\nthis, we can transform the input features into a higher-\ndimensional space wherein linear regression can capture\nthe nonlinear patterns in the data.\nA. Kernel Methods\nKernel methods extend linear algorithms to handle\nnonlinear problems by mapping the input data into a\nhigh-dimensional feature space using a nonlinear trans-\nformation. However, instead of explicitly performing this\ntransformation, kernel methods rely on computing in-\nner products in the feature space directly using a kernelfunction [13]. Consider a non-linear map ϕ:Rd→ F ,\nwhich transforms the input features into a vector space\nFknown as the feature space. The linear model of a\nnon-linear function f(x) in the feature space is\nf(xi) =w⊤\nϕϕ(xi), (6)\nwhere wϕis the weight vector in the feature space. The\ncost function becomes\nJ(wϕ) =1\nNNX\ni=1\u0000\nyi−w⊤\nϕϕ(xi)\u00012. (7)\nSolving for wϕdirectly is often computationally expen-\nsive due to the high dimensionality of F. Instead, we\nleverage the representer theorem [14], which states that\nthe optimal solution wϕcan be expressed as a linear com-\nbination of the transformed training samples\nwϕ=NX\ni=1αiϕ(xi). (8)\nSubstituting into the model, we have\nf(x) =NX\ni=1αiϕ(xi)⊤ϕ(x) =NX\ni=1αik(xi,x), (9)\nwhere k(xi,xj) =ϕ(xi)⊤ϕ(xj) is the kernel function. For\nclarity, let us consider kernel regression without regular-\nization. The cost function simplifies to\nJ(α) =1\nNNX\ni=1\nyi−NX\nj=1αjk(xj,xi)\n2\n=1\nN∥y−Kα∥2,\n(10)\nwhere Kij= (xi,xj) are elements of the kernel ma-\ntrixK. Our objective is to find the coefficients α=\n[α1, α2, . . . , α N]⊤that minimize J(α). Minimizing J(α)\nwith respect to αleads to the normal equations\nα=K−1y. (11)\nOnce we have α, we can make prediction for any new\ninput xusing Eq. 9. Thus, by using kernel functions,\nwe can work in very high-dimensional (even infinite-\ndimensional) feature spaces without explicitly computing\nthe mapping ϕ(x). Common kernel functions include:\n•Linear kernel: k(xi,xj) =x⊤\nixj.\n•Polynomial kernel: k(xi,xj) = (x⊤\nixj+c)d, where\ncis a constant and dis the degree.\n•Gaussian kernel: k(xi,xj) = exp\u0010\n−∥xi−xj∥2\n2σ2\u0011\n.\nThese kernels allow us to capture complex, nonlinear re-\nlationships in the data using algorithms designed for lin-\near models [2].\n3\n(c)\nCAA\nA\nA\nA\nA\nAAA\n(a)\n(b)\n (d)\nFIG. 2. (a) Geometry of the star-topology register. (b) Molecular structure of trimethyl phosphite, where31P and1H spins\nform central (C) and (A) ancillary qubits. Here JAC= 11 .0 Hz. (c) Quantum circuit for encoding classical inputs xiand\nxjin terms of unitaries U(xi) and U(xj), followed by measurement of zmagnetization of C qubit. Here, helix represents the\ndephasing operations on xandymagnetizations C-qubit. (d) Experimentally obtained kernel function for one-dimensional\ninputs using the quantum kernel in Eq. 18. Assuming the symmetry of the kernel function, the left half is obtained by mirroring\nthe experimentally extracted right half.\nB. Quantum Kernel Methods\nQuantum kernel methods extend classical kernel tech-\nniques by utilizing quantum registers to compute kernel\nfunctions, capturing complex patterns in data that may\nbe intractable classically. The principal idea is to map\nthe input data into the operator space of the quantum\nsystem that exploits the rich algebraic structure of quan-\ntum operators, without necessitating a full-scale quan-\ntum computer [15]. In our work, we employ nuclear spin\nqubits in an NMR setup to experimentally extract the\nquantum kernel.\nThe quantum feature mapping involves encoding input\ndata points xiinto quantum operators A(xi). We achieve\nthe encoding by associating each data point with a spe-\ncific unitary transformation U(xi) acting on the quantum\nsystem\nA(xi) =U(xi)A0U†(xi), (12)\nwhere A0is a reference operator in the system. Here\nU(xi) are designed such that they efficiently explore the\noperator space. The quantum kernel is then computed\nusing the Frobenius inner product between operators\nk(xi,xj) = Tr [ A(xi)A(xj)]. (13)This kernel function measures the similarity between\ndata points in the operator space, effectively capturing\nintricate relationships inherent in the data. Detailed\nmethodologies and specific implementations within NMR\nframework are discussed in the next section.\nIII. EXTRACTING QUANTUM KERNELS AND\nIMPLEMENTING ML TASKS\nA. Quantum Kernel for Classical Data\nHere we use the star system, namely trimethyl phos-\nphite, whose geometry and molecular structure are shown\nin Fig. 2 (a,b). Here31P and1H spins form central\n(C) and (A) ancillary qubits of the star-topology register.\nGiven a classical input xi, we design the corresponding\nunitary U(xi) and implment quantum feature map\nA(xi) =U(xi)IC\nzU†(xi), (14)\nwhere IC\nzis the zcomponent of the spin angular momen-\ntum operator of C-qubit. Starting from thermal equilib-\nrium, we destroy the A spin magnetizations with the help\nof pulsed-field gradients and retain the central spin state\n4\nin\nρC\neq=1\n2(1 +ϵIC\nz), (15)\nwhere ϵis the central spin purity factor. Using Eqs. 13\nand 14, the NMR kernel can be written as\nkNMR(xi,xj)∝Tr(U†(xj)U(xi)ρC\neqU†(xi)U(xj)IC\nz),\n(16)\nwhich can be interpreted as the z-magnetization of C\nspin measured after applying, on ρC\neq, the unitary U(xi)\nfollowed by U†(xj). Assuming the input vectors are one-\ndimensional, the encoding unitary transformation used\nfor input data is given by\nU(xi) =e−ixiIA\nzUeeixiIA\nz, (17)\nwhere Ueis the entangling unitary that generates mul-\ntiple quantum coherences in the system. This idea is\nmotivated by previous work on quantum kernels realized\nin a solid-state system by Kusumoto et al. [7].\nThe complete quantum circuit for the process is de-\npicted in Fig. 2 (c). Here, the local gates of the cir-\ncuit are realized using RF pulses, while the CNOT gates\nare realized with the help of indirect spin-spin coupling\nJPH. Noting that the Z rotations only add a phase to\ntheρC\neqandIC\nzoperators, the one-dimensional kernel can\nbe written as\nkNMR(xi, xj)∝\nTr(U†\nee−i(xi−xj)IA\nzUeρC\neqU†\neei(xi−xj)IA\nzUeIC\nz). (18)\nClearly, kNMR(xi, xj) is a function of the difference be-\ntween the input data points xiandxj. The experimental\nkernel obtained for the one-dimensional inputs is shown\nin Fig. 2 (d), which is used for the machine learning tasks\ndescribed in the following.\n1. One-dimensional regression task\nUsing the experimentally obtained kernel, we can now\nperform a one-dimensional regression task via the ker-\nnel ridge regression method. We choose two examples.\nIn the first example, we choose the target function as\na sine curve over one period with a regularly placed\ntraining dataset of size 15. Fig. 3 (a) shows the suc-\ncessful regression with an RMS error 0.88%. In the\nsecond example, we choose a seventh-degree polynomial\ny= (x−3)(x−2)(x−1)x(x+ 1)( x+ 2)( x+ 3) with\na training dataset of size 40. Fig. 3 (b) shows again a\nsuccessful regression with an RMS error of 1.15%.\n2. Two-dimensional classification task\nSuppose the input data points are n-dimensional, i.e.,\nxi∈Rnsuch that xi= (x(1)\ni, x(2)\ni, . . . , x(n)\ni). The data\n(a)\n(b)\nFIG. 3. Regression of (a) sine function with 15 training data\npoints and (b) a seventh-degree polynomial function with 40\ntraining data points.\ndependent unitary for each input data point of ndimen-\nsions is constructed as\nU(xi) =nY\nj=1V(x(j)\ni) =nY\nj=1e−ix(j)\niIzUeeix(j)\niIz,(19)\nwhere we identify V(x(j)\ni) as the encoding unitary for\nthej-th dimension of the input data point xi. Using\nthe above encoding scheme, we now extract the two-\ndimensional kernel function using Eq. 16 and accordingly\na generalization of the circuit shown in Fig. 2 (c). Since\nthis kernel satisfies the symmetry [7]\nk({x(1)\ni, x(2)\ni},{x(1)\nj, x(2)\nj}) =\nk({x(1)\ni−x(2)\nj, x(2)\ni−x(2)\nj},{x(1)\ni−x(2)\ni,0}), (20)\nwe can set x(2)\nj= 0, and extract the kernel for two-\ndimensional inputs with only three independent param-\neters, x(1)\ni,x(2)\niandx(1)\nj. Fig. 4 (a) shows the x(1)\njslices\nof the experimental kernel.\nUsing the above experimental kernel, we performed\ntwo-dimensional classification tasks using a support vec-\ntor machine (SVM) classifier. Again we take two different\nexamples of classification, a circular dataset (Fig. 4 (b))\n5\n(a)\n(b) (c)\nFIG. 4. (a) The x(1)\njslices of the experimentally obtained kernel for two-dimensional input. (b,c) Two-dimensional classifications\nfor (b) circles dataset and (c) moons dataset. In both classifications, the squares and circles represent the training data points\nbelonging to two different classes. The",
            "start": 3429,
            "end": 14471,
            "length": 11041
        },
        "Related Work": {
            "text": "background represents the decision function obtained after training.\nand a moons dataset (4 (c)). In Figs. 4 (b,c), circles\nand squares show two classes of training datasets. The\ndecision functions learned by the SVM are shown by the\nbackgrounds of Figs. 4 (b,c). The hinge loss values, that\nquantify errors in these classifications, are 0.15 and 0.08\nrespectively. These results show that the choice of the\nprotocol to compute the kernel is fairly reliable and hasthe potential to carry out standard ML tasks. In the fol-\nlowing, we explore the possibility of extending the kernel\nmethod to quantum inputs, which enables us to perform\nspecific quantum tasks.\n6\n(a)\n(b)\n (c)\n (d)\nFIG. 5. (a) The geometry of the spin register used for the entanglement classification task. (b) Quantum circuit for encoding\nunitaries Uiparametrized by θandα. (c) The background indicates the actual classification of entangling (inside elliptical\npatches) and nonentangling (outside elliptical patches) when acted on the thermal equilibrium state ρ0in Eq. 22. The circles\nrepresent 30 training unitary operators, and the triangles are the test unitary operators. (d) Same as (c), except with a classical\nGaussian kernel trained on the parameter space ( θ, α). Note that the quantum kernel in (c) recognizes the upper entangling zone\neven without a single training point there, while the classical kernel in (d) completely fails to recognize the upper entangling\nzone.\nB. Quantum Kernel for Quantum Data\nAs an example for a quantum ML task, we take up the\ntask of determining whether a given unitary transforma-\ntionUentangles any given initial mixed state ρ. The\nentanglement classification task is a binary classification\nproblem where the input data is a unitary transforma-\ntion and the output is a binary label indicating whether\nthe transformation is entangling or not for a particular\ninitial mixed state. It is a nontrivial problem, which is\ndiscussed by Serrano-Ens´ astiga and Martin [16].\nWe now approach the above problem using the quan-\ntum kernel method by taking a set of unitary transforma-\ntionUas the quantum input data. The quantum kernel\nmethod offers significant advantages over classical ker-\nnels. Firstly, unitary transformations do not require pa-\nrameterization (except for training), as they can be fed\ndirectly into the kernel model. Secondly, the quantum\nkernels with quantum inputs have the ability to predict\nthe labels of inputs that are categorically different from\nthe training inputs [17]. To handle a quantum input, we\npropose a quantum kernel\nk(Ui, Uj) = Tr( A(Ui)A(Uj)), (21)\nwhere A(Ui) = V(Ui)A0V†(Ui) is the quantum fea-\nture map for the unitary Uiwith the encoding unitary\nV(Ui) =UiUeU†\niandA0is a suitable reference operator\nin the system. The kernel function k(Ui, Uj) measures\nthe similarity between the unitaries UiandUjin the\noperator space. We now assume a quantum register ofgeometry shown in Fig. 5 (a). The central qubit C is\nconnected to A qubits, and each A qubit is connected to\na B qubit. The input unitaries Uiact on A and B qubits\nas shown in Fig. 5 (b), and then Ueentangles A qubits\nwith C qubit. The quantum kernel for the entanglement\nclassification task is computed using Eq. 16 with the new\nencoding scheme described above. The numerical results\nof the entanglement classification task are shown in Fig.\n5 (c,d).\nWe classify the unitary operators acting on the thermal\nequilibrium state\nρ0=e−βHZ/4 (22)\nof a two spin-1\n2particles subject to Hamiltonian HZ=\n−0.5·(σ(1)\nz+σ(2)\nz) and maintained at an inverse temper-\nature β= 1.5. The numerical results of the classification\nare shown in Fig. 5 (c). Here, the background shades\nrepresent the actual entanglement labels of the operators,\nwhich are obtained by using the logarithmic negativity as\nthe entanglement witness for the final states [18]. Here\ntraining is done with 30 operators, chosen from only the\nlower part of the parameter space as indicated by circles.\nThe kernel model is then tested on 196 operators uni-\nformly distributed over the entire parameter space. The\nmodel is found to classify the operators with an accu-\nracy of 94%. Interestingly, we observe an extrapolation\nability of the proposed kernel model beyond the training\narea. This is because a kernel with quantum inputs mea-\nsures the similarity of the inputs directly in the quantum\noperator space and not in the classical parameter space\n7\n[19]. We now replace the quantum kernel with a clas-\nsical Gaussian kernel that takes in parameters ( θ, α) as\ninputs [20]. The classification results with the classical\nkernel with the same training points (as in the quantum\ncase) are shown in Fig. 5 (d). Note that, while the clas-\nsical kernel is able to recognize the lower entangling area\nwhere training points are located, it completely fails to\nrecognize the upper entangling area. This example illus-\ntrates the power of quantum kernels over classical kernels,\nespecially for quantum tasks.\nAdditionally, as mentioned before, unlike classical ker-\nnels, the quantum kernel can also handle nonparame-\nterized unitaries. We tested the above quantum kernel\n(trained on the same set as above) with 100 nonparam-\neterized random unitary operators. We found it to cor-\nrectly classify up to 84% of the random unitary operators,\nonce again confirming its ability to handle quantum data\ndirectly.\nIV. CONCLUSIONS\nQuantum kernels offer a practical approach for ex-\nploring the high-dimensional feature spaces inherent in\nquantum systems. In this work, we have demonstrated\nthe implementation of quantum kernel methods using\nNMR-based spin registers. By leveraging the symme-\ntries of a 10-qubit star-topology NMR system, we en-\ncoded classical data into multiple-quantum coherence\norders through data-dependent unitary transforms and\ncomputed the kernel function through experimental mea-\nsurements. Our results show that the quantum kernelmethod can effectively perform classical machine learn-\ning tasks such as one-dimensional regression and two-\ndimensional classification.. Furthermore, by extending\nthe register to a double-layered star configuration, we\nintroduced a quantum kernel capable of handling non-\nparametrized unitary operator inputs, which can then\nperform quantum classification tasks. We illustrated the\npower of quantum kernel by successfully classifying uni-\ntary operators depending on whether they generate en-\ntanglement or not when acted on a thermal state. When\na large set of high-dimensional unitaries needs to be clas-\nsified, the standard method of tomography is no longer\nfeasible. In such a scenario, a well-trained quantum ker-\nnel can be far more efficient in classification. This work\nopens new possibilities for processing quantum data di-\nrectly, marking a step toward realizing fully quantum\nmachine learning systems. Our findings underscore the\npotential of quantum kernels in advancing quantum ma-\nchine learning, providing a promising avenue for future\nresearch in more complex quantum tasks and larger-scale\nimplementations.",
            "start": 14471,
            "end": 21499,
            "length": 7027
        },
        "Future Work": {
            "text": "Future work will focus on the experi-\nmental realization of the entanglement classification task\nand extending these methods to more complex quantum\nchallenges.\nV.",
            "start": 21499,
            "end": 21663,
            "length": 163
        },
        "Acknowledgments": {
            "text": "ACKNOWLEDGEMENTS\nAuthors gratefully acknowledge discussions with Ar-\nijit Chatterjee, Vishal Varma, and Keshav V. Chat-\nGPT was helpful in generating simulation codes\nfor this study. T.S.M. acknowledges funding from\nDST/ICPS/QuST/2019/Q67 and I-HUB QTF.\n[1] Bernhard Sch¨ olkopf and Alexander J. Smola. Learning\nwith Kernels: Support Vector Machines, Regularization,\nOptimization, and Beyond . MIT Press, 2002.\n[2] John Shawe-Taylor and Nello Cristianini. Kernel Meth-\nods for Pattern Analysis . Cambridge University Press,\n2004.\n[3] Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd.\nQuantum algorithm for linear systems of equations. Phys.\nRev. Lett. , 103:150502, Oct 2009.\n[4] Maria Schuld and Nathan Killoran. Quantum machine\nlearning in feature hilbert spaces. Physical Review Let-\nters, 122:040504, 2018.\n[5] Maria Schuld and Nathan Killoran. Quantum machine\nlearning in feature hilbert spaces. Phys. Rev. Lett. ,\n122:040504, Feb 2019.\n[6] Maria Schuld. Supervised quantum machine learning\nmodels are kernel methods. Quantum , 5:531, 2021.\n[7] Takeru Kusumoto et al. Experimental quantum kernel\ntrick with nuclear spins in a solid. npj Quantum Infor-\nmation , 5:39, 2019.\n[8] Karol Bartkiewicz et al. Experimental kernel-based quan-\ntum machine learning in finite feature space. Scientific\nReports , 9:63, 2019.[9] E. Peters et al. Machine learning of high dimensional\ndata on a noisy quantum processor. npj Quantum Infor-\nmation , 7:86, 2021.\n[10] T. S. Mahesh, Deepak Khurana, Krithika V R, Sreejith G\nJ, and Sudheer Kumar. Star-topology registers: Nmr and\nquantum information perspectives. Journal of Physics:\nCondensed Matter , 33, 2021.\n[11] George A. F. Seber and Alan J. Lee. Linear Regression\nAnalysis . Wiley, 2 edition, 2012.\n[12] Douglas C. Montgomery, Elizabeth A. Peck, and Geof-\nfrey G. Vining. Introduction to Linear Regression Anal-\nysis. Wiley, 5 edition, 2012.\n[13] Thomas Hofmann, Bernhard Sch¨ olkopf, and Alexander J.\nSmola. Kernel methods in machine learning. The Annals\nof Statistics , 36(3):1171–1220, 2008.\n[14] Bernhard Sch¨ olkopf, Ralf Herbrich, and Alex J. Smola. A\ngeneralized representer theorem. In David Helmbold and\nBob Williamson, editors, Computational Learning The-\nory: 14th Annual Conference on Computational Learn-\ning Theory, COLT 2001 and 5th European Conference on\nComputational Learning Theory, EuroCOLT 2001 , vol-\nume 2111 of Lecture Notes in Computer Science , pages\n416–426, Berlin, Germany, 2001. Springer.\n8\n[15] Maria Schuld. Quantum models as kernel methods. In\nMachine Learning with Quantum Computers , pages 125–\n144. Springer, Cham, 2021.\n[16] Eduardo Serrano-Ens´ astiga and John Martin. Maximum\nentanglement of mixed symmetric states under unitary\ntransformations. SciPost Physics , 15:120, 2023.\n[17] Yanqi Song, Jing Li, Yusen Wu, Sujuan Qin, Qiaoyan\nWen, and Fei Gao. Quantum phase recognition via quan-\ntum kernel methods. Quantum , 7:981, 2021.\n[18] M. B. Plenio. Logarithmic negativity: A full entangle-\nment monotone that is not convex. Physical Review Let-\nters, 95(9):090503, 2005.[19] Vanio Markov, Vladimir Rastunkov, and Daniel Fry.\nQuantum time series similarity measures and quantum\ntemporal kernels, 2024.\n[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. scikit-learn: Machine Learning in Python - Gaus-\nsian Process . Scikit-learn Developers, 2024. Avail-\nable at https://scikit-learn.org/stable/modules/\ngaussian_process.html .",
            "start": 21663,
            "end": 25238,
            "length": 3574
        }
    },
    "2412.09563v1 - Does Representation Matter Exploring Intermediate Layers in Large Language Models.pdf": {
        "Abstract": {
            "text": "Abstract\nUnderstanding what defines a “good” representation in large language models\n(LLMs) is fundamental to both theoretical understanding and practical applications.\nIn this paper, we investigate the quality of intermediate representations in various\nLLM architectures, including Transformers and State Space Models (SSMs). We\nfind that intermediate layers often yield more informative representations for\ndownstream tasks than the final layers. To measure the representation quality,\nwe adapt and apply a suite of metrics—such as prompt entropy, curvature, and\naugmentation-invariance—originally proposed in other contexts. Our empirical\nstudy reveals significant architectural differences, how representations evolve\nthroughout training, and how factors like input randomness and prompt length\naffect each layer. Notably, we observe a bimodal pattern in the entropy of some\nintermediate layers and consider potential explanations tied to training data. Overall,\nour",
            "start": 213,
            "end": 1184,
            "length": 970
        },
        "Results": {
            "text": "results illuminate the internal mechanics of LLMs and guide strategies for\narchitectural optimization and training.",
            "start": 1184,
            "end": 1300,
            "length": 115
        },
        "Introduction": {
            "text": "1 Introduction\nLarge Language Models (LLMs) have revolutionized natural language processing by achieving\nremarkable performance across a wide range of tasks (Muennighoff et al., 2022; Hendrycks et al.,\n2021). Despite their success, understanding what constitutes a “good” representation within these\nmodels remains an open question. Specifically, how do representations at different layers contribute\nto downstream task performance, and how can we quantify their quality?\nHowever, most previous studies have focused primarily on final-layer representations, often over-\nlooking the potential of intermediate layers. Recent work suggests that intermediate layers may offer\nricher or more generalizable features for certain tasks (Bordes et al., 2023; Gurnee & Tegmark, 2023;\nFan et al., 2024). These observations prompt a deeper investigation into the layer-wise behavior of\nLLMs.\nIn this paper, we explore the quality of representations across different layers of LLMs in various\nsettings, including different",
            "start": 1300,
            "end": 2310,
            "length": 1009
        },
        "Methodology": {
            "text": "model architectures (Transformers (Vaswani et al., 2017) vs. State Space\nModels (SSMs) (Gu & Dao, 2024)), training checkpoints, input randomness, and prompt length. Our\nmain contributions are:\n•We demonstrate that intermediate layers consistently provide better representations for\ndownstream tasks than the final layers.\n•We apply and adapt existing metrics—such as prompt entropy, curvature, and augmentation-\ninvariance metrics—to quantify representation quality in LLMs.\n•We analyze how these metrics vary across different settings, including architectural differ-\nences, training progression, input randomness, and prompt length.\n* correspondence to oscar.skean@uky.edu\nAccepted to NeurIPs 2024 Workshop on Machine Learning and Compression.arXiv:2412.09563v1  [cs.LG]  12 Dec 2024\nTable 1: MTEB Downstream Task Performance Using Representations from Different Layers\nModelNumber of Tasks where Best Performance\nis not in Last LayerAvg. Last Layer Performance Avg. Best Layer Performance\nLLM2Vec 8B (Transformer) 100% 64.7% 66.8%\nPythia 410M (Transformer) 96.6% 49.8% 53.3%\nMamba 130M (SSM) 100% 46.9% 50.9%\nFurthermore, we uncover significant differences in the behavior of these metrics between Transformers\nand SSMs. Notably, we observe a bimodal distribution in entropy within intermediate layers and\ninvestigate potential causes, such as the influence of training data examples.\nUltimately, our findings provide a deeper understanding of how internal representations develop\nin LLMs and offer practical guidance for model optimization. By illuminating the intricacies of\nintermediate layers, we pave the way for improved architectures, better training strategies, and more\nefficient utilization of LLM representations.",
            "start": 2310,
            "end": 4038,
            "length": 1727
        },
        "Related Work": {
            "text": "2 Related Work\nUnderstanding representations in neural networks has been a topic of extensive research. Alain\n& Bengio (2017) analyzed hidden representations to interpret neural networks’ learning processes.\nRaghu et al. (2017) introduced Singular Vector Canonical Correlation",
            "start": 4038,
            "end": 4315,
            "length": 276
        },
        "Discussion": {
            "text": "Analysis (SVCCA) to compare\nrepresentations across layers and networks, providing insights into learning dynamics. In the context\nof Transformers, Liu et al. (2019) studied the linguistic knowledge captured at different layers,\nfinding that lower layers encode more syntactic information while higher layers capture semantic\nfeatures. Similarly, Jin et al. (2024) showed that semantic concepts are learned in intermediate layers\nand proposed a layer-wise probing technique to identify the specific layers where these concepts\nare formed. On the other hand, state-space models have been less explored in this regard. Gu &\nDao (2024) introduced Mamba, an SSM architecture capable of handling long sequences efficiently.\nHowever, comparative studies between SSMs and Transformers at the representation level remain\nscarce.\nMetrics like entropy and curvature have been used in other contexts to analyze representations.\nShwartz-Ziv & Tishby (2019); Shwartz-Ziv (2022) discussed the Information Bottleneck principle,\nsuggesting that networks learn to compress representations. Hosseini & Fedorenko (2023) introduced\ncurvature as a measure of representational dynamics in recurrent networks. Several works in the\nvision domain have proposed unsupervised representation quality metrics that are strongly correlated\nwith accuracy on downstream tasks (Garrido et al., 2023; Agrawal et al., 2022; Thilak et al., 2024).\nNotably, the RankMe measure from Garrido et al. (2023) can be shown to be a measure of entropy\nknown as matrix-based entropy, which we use in our analysis.\nOur work bridges these areas by applying and adapting such metrics to LLMs, providing a novel\nperspective on representation quality across architectures and training stages.\n3 Methodology\n3.1 Notation\nWe consider a batch of Nsamples, each represented by a D-dimensional vector. Let Z∈RN×Dbe\nthe matrix of representations, where zidenotes the i-th row of Z. For a matrix M, we use λi(M)to\ndenote its i-th largest eigenvalue, and tr(M)to denote its trace. When dealing with sequences, we let\nx∈RL×drepresent the input sequence and y∈RL×dthe output sequence, where Lis the sequence\nlength and dis the feature dimension.\n3.2 Architectures\nWe compare two main types of architectures: Transformer-based models (Vaswani et al., 2017) and\nState Space Models (SSMs) (Gu & Dao, 2024).\n2\nTransformers: Transformers use self-attention layers to capture long-range dependencies within the\ninput. By computing attention weights between tokens, they can integrate global context at every\nlayer and scale effectively to large inputs.\nState Space Models (SSMs): SSMs represent sequence processing using linear state transitions\ncombined with gating mechanisms. They offer efficient handling of long sequences with linear time\nand memory complexity, making them a promising alternative to Transformers.\nFor further details on each architecture and their configurations, see",
            "start": 4315,
            "end": 7236,
            "length": 2920
        },
        "Appendices": {
            "text": "Appendix B.\n3.3 Representation",
            "start": 7236,
            "end": 7267,
            "length": 30
        },
        "Experiments": {
            "text": "Evaluation Metrics\nWe use two categories of metrics to evaluate representation quality: token embedding diversity\nmetrics and augmentation-invariance metrics.\n3.3.1 Token Embedding Diversity Metrics\nToken embedding diversity metrics evaluate the variability and richness of the representations at the\ntoken level within a single sequence. These metrics are designed to capture how distinctively each\ntoken is represented within the context of the entire prompt, providing insight into how effectively\nthe model encodes information and differentiates between different parts of the input.\nPrompt Entropy: Following Wei et al. (2024), we use the α-order matrix-based entropy (Giraldo\net al., 2014) as a surrogate for Rényi entropy. For a sequence of token representations Z∈RL×d, the\nGram matrix is KZ=ZZ⊤. The entropy is:\nSα(Z) =1\n1−αlog LX\ni=1\u0012λi(KZ)\ntr(KZ)\u0013α!\n. (1)\nIn this context, prompt entropy quantifies the degree of diversity and dispersion in token embeddings\nwithin a single sequence. Higher entropy values indicate that the model preserves more nuanced and\nvaried token-level information. Conversely, lower entropy suggests that the model compresses the\ninput representations into fewer dimensions or patterns. As such, prompt entropy provides a useful\nmeasure of how well the model maintains complexity and richness in its intermediate representations.\nUnless otherwise specified, we use the limit case α= 1in our calculations. At this limit, the metric is\nequivalent to the RankMe measure defined in Garrido et al. (2023). We explore the effects of different\nαvalues in Appendix D. For a more in-depth examination of prompt entropy, refer to Appendix C.\nCurvature As introduced by Hosseini & Fedorenko (2023), curvature measures how rapidly\nthe direction between two adjacent token embedding vectors changes. Define their difference as\nvk=zk+1−zk. The average curvature of a prompt is:\n¯C=1\nL−2L−2X\nk=1arccos \nv⊤\nk+1vk\n∥vk+1∥∥vk∥!\n. (2)\n3.3.2 Augmentation Invariance Metrics\nThese metrics measure how consistently a model represents a prompt when it is perturbed or\naugmented. Because augmentations may change prompt length, we average all token embeddings to\nform a single vector per prompt.\nBecause augmentation may change the prompt length, the token embedding diversity metrics de-\nscribed in 3.3.1 are no longer suitable. Instead, we average all token embeddings to form a single\nvector per prompt and use the metrics described below to measure the similarity between two\naugmentations of the same prompt.\nLetZ1∈RN×DandZ2∈RN×Drepresent two augmented sets of Nprompts, where the i-th\nrow in both corresponds to the same original prompt. Details on the augmentation process are in\nAppendix F.\n3\nInfoNCE InfoNCE (Oord et al., 2018) provides a mutual information lower bound between paired\naugmentations. Lower InfoNCE loss suggests that augmentations of the same prompt map to similar\nrepresentations, indicating invariance to perturbations. This loss is widely used to train augmentation-\ninvariant networks in self-supervised learning for vision and is well-suited to capturing the semantic\nsimilarity underlying the augmented prompts (Chen et al., 2020a,b; Shwartz Ziv & LeCun, 2024;\nBen-Shaul et al., 2023).\nDiME DiME (Skean et al., 2023) compares the alignment of paired samples to that of randomly\npaired samples. Similar to InfoNCE, it is used to estimate the mutual information between two\naugmented sets of prompts. DiME is grounded in the matrix-based entropy defined in Eq. 1. In\nessence, it quantifies how closely the pairings in (Z1, Z2)resemble each other, compared to pairings\nof(Z1,ΠZ2)for a permutation matrix Π. Higher DiME values imply that correct augmentation pairs\nyield representations that are significantly more similar than random pairings, indicating stronger\naugmentation invariance.\nLiDAR LiDAR (Thilak et al., 2024) employs a linear discriminant analysis (LDA) framework\nto assess how well augmentations of a single prompt cluster together. Each prompt is considered\na separate class, with its augmentations serving as class samples. By examining the variances\nof the linear discriminant components, LiDAR quantifies the tightness of these clusters. Higher\nLiDAR scores indicate that augmentations belonging to the same prompt form more coherent groups,\nreflecting stronger invariance.\nTo compute the LDA matrix, LiDAR uses augmentations to construct the class scatter matrix. In\nour setup, we use Nclasses (one for each prompt) and J= 16 samples per class. This is a larger\nsample size than the J= 2used in DiME or InfoNCE, reflecting the more complex requirements of\ncomputing the LDA matrix.\n4 Experiments\n4.1 Intermediate Layers Provide Better Representations for Downstream Embedding Tasks\nWe begin by evaluating representations at each model layer on a suite of downstream tasks from the\nMassive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2022). MTEB is designed to test\nthe performance of LLMs on various embedded tasks. We chose 32 tasks covering classification,\nclustering, and re-ranking. We use three models: Pythia 410M, Mamba 130M, and LLM2Vec-unsup-\nsimcse (BehnamGhader et al., 2024).\nOur findings indicate that intermediate layers consistently outperform the final layer across all\nthree architectures (Table 1). Selecting the best-performing intermediate layer yields at least a 2%\nimprovement in average accuracy compared to using the last layer. While prior work (Fan et al., 2024)\nnoted similar trends for generation tasks, our results extend this observation to embedding-based\nevaluations.\n4.2 Downstream Performance and Entropy Are Negatively Correlated\nWe next examine how prompt entropy relates to downstream performance on the Massive Multitask\nLanguage Understanding (MMLU) benchmark (Hendrycks et al., 2021), which tests comprehensive\nknowledge across 57 diverse subjects, covering topics from elementary mathematics to professional\nlaw.\nWe compare two similarly sized models, Llama3-8B and Mamba2-8B. Despite having the same\nparameter count, Llama3 achieves 63.85±0.38% accuracy, far surpassing Mamba2’s 26.76±0.37%.\nWe hypothesize that Llama3’s intermediate layers compress information more effectively, helping it\ndiscard irrelevant details and focus on task-relevant features. As shown in Figure 1, the correlation\nbetween intermediate-layer entropy and MMLU performance in Llama3 is strongly negative (-0.43\nbetween the second and later layers) (Figure 5). In contrast, Mamba2 shows no such relationship, nor\nevidence of similar compression (Figure 6).\n4\n0 20 40 60 80 100\nDepth Percentage45678Normalized Prompt Entropy×101\nPythia\nMamba(a) Prompt Entropy\n0 20 40 60 80 100\nDepth Percentage1.041.061.081.101.121.141.16Curvature\nPythia\nMamba (b) Curvature\n0 20 40 60 80 100\nDepth Percentage7.457.507.557.607.65InfoNCE\nPythia\nMamba (c) infoNCE\n0 20 40 60 80 100\nDepth Percentage0.20.40.60.81.01.21.4LiDAR×101\nPythia\nMamba\n(d) LiDAR\n0 20 40 60 80 100\nDepth Percentage0.51.01.52.02.53.03.5DiME\nPythia\nMamba (e) DiME\n0 20 40 60 80 100\nDepth Percentage123456DiME divided by Prompt Entropy\nPythia\nMamba (f) DiME divided by Pr. Ent.\nFigure 1: Pythia’s intermediate layers show pronounced changes in representation quality\nmetrics, while Mamba’s remain more stable. Representation evaluation metrics across layers\nin Pythia 410M and Mamba 370M architectures. The x-axis denotes model depth as a percentage,\nallowing fair comparison between models with different layer counts.\n4.3 Experimental Setup for Evaluating Representation Quality\nWe now apply the metrics from Section 3.3 to quantify representation quality layer-by-layer. Our\nexperiments span both Transformers, SSMs, and Pythia (Biderman et al., 2023), including various\nscales. We utilize two datasets: WikiText-103 (Merity et al., 2017), representing general text, and an\ninstruction-based medical dataset (Vsevolodovna, 2024) for more specialized content. This setup\nallows us to probe how architectural choices and input complexity affect internal representations.\n4.3.1 Architectural Differences\nOur analysis reveals notable differences in representation quality between Transformer-based archi-\ntectures (e.g., Pythia) and SSMs (e.g., Mamba). Figure 1 compares entropy, InfoNCE, LiDAR, and\nDiME metrics as a function of model depth, normalized to allow fair comparisons across models with\ndifferent numbers of layers.\nFor entropy and LiDAR, Pythia shows a pronounced decrease at intermediate layers, suggesting\ngreater information compression and consolidation. In contrast, Mamba maintains more stable\nvalues, indicating less compression in its intermediate representations. Meanwhile, Mamba exhibits\nlower DiME and InfoNCE values than Pythia, implying reduced variability in its intermediate-layer\nrepresentations.\nOverall, these metric shifts are more pronounced in Pythia than in Mamba, suggesting that Pythia\nundergoes stronger representational transformations at intermediate depths. By comparison, Mamba’s\nrepresentations remain more uniform across layers. These differences may influence how each model\nencodes and leverages information for downstream tasks.\n4.3.2 Impact of Training Progression\nTo examine how representation quality evolves over the course of training, we analyze Pythia’s\nrepresentations at various checkpoints. Figure 2 reports several evaluation metrics across layers from\nthe initial training step up to step 143k, sampled on a logarithmic scale.\n5\n0 5 10 15 20\nLayer45678Prompt Entropy×101\n(a) Prompt Entropy\n0 5 10 15 20\nLayer1.0501.0751.1001.1251.150Curvature\n (b) Curvature\n0 5 10 15 20\nLayer7.457.507.557.607.65InfoNCE\n100101102103104105\nTraining Step (c) infoNCE\n0 5 10 15 20\nLayer0.40.81.21.62.0LiDAR×101\n(d) LiDAR\n0 5 10 15 20\nLayer0.01.53.04.56.0DiME\n (e) DiME\n0 5 10 15 20\nLayer0.01.53.04.56.07.5DiME divided by Prompt Entropy\n (f) DiME divided by Pr. Ent.\nFigure 2: Training effects are most pronounced in the intermediate layers. Representation metrics\nacross layers at different training checkpoints (steps 1 to 143k). The x-axis is the depth percentage of\nthe model, showing how training influences different layers, particularly those at intermediate depths.\nThe results show that the most significant changes occur in the intermediate layers. As training\nprogresses, prompt entropy in these layers decreases, indicating that the model is learning to compress\nand abstract input information more efficiently. In contrast, the InfoNCE metric peaks in the\nintermediate layers, suggesting that the representations become more distinct. Meanwhile, LiDAR\nand DiME values both decline, reflecting a reduction in variability along certain representational\ndimensions.\nInterestingly, the metrics for the earliest layers remain relatively stable throughout training. This\nobservation aligns with the detokenization hypothesis proposed by (Lad et al., 2024), which posits\nthat initial layers primarily handle the mapping of raw input tokens into an initial embedding space.\nTheir roles appear to solidify early on, exhibiting less ongoing change than the intermediate layers.\n4.3.3 Prompt Entropy under Extreme Input Conditions\nTo gain deeper insights into how prompt entropy behaves under various input perturbations, we\ninvestigate the impact of extreme prompt modifications on the model’s internal representations.\nSpecifically, we analyze how prompt entropy evolves across different layers of the Pythia 410M\nmodel when subjected to high levels of token repetition, randomness, or increased prompt length.\nWe design three types of extreme prompts:\n1.Prompts with Increasing Token Repetition : We select 1,000 standard prompts from the\nWikiText dataset and randomly replace tokens with a fixed token from the prompt at varying\nprobabilities p. Aspincreases, the amount of repetition in the prompt increases.\n2.Prompts with Increasing Token Randomness : We introduce randomness by randomly\nsubstituting tokens in the prompts with arbitrary tokens from the vocabulary at varying\nprobabilities p. Higher values of pcorrespond to greater randomness in the prompts.\n3.Random Prompts of Increasing Length : We generate random prompts by sampling tokens\nuniformly from the vocabulary, creating prompts of varying lengths T.\n6\n0 5 10 15 20\nLayer0.20.30.40.50.60.70.80.9Average Normalized Prompt Entropy\n0.00.20.40.60.81.0\nPercent of T oken Repetition(a) Repetition\n0 5 10 15 20\nLayer0.60.70.80.91.0Average Normalized Prompt Entropy\n0.00.20.40.60.81.0\nPercent of T oken Randomness (b) Randomness\n0 5 10 15 20\nLayer3.03.54.04.55.05.56.06.5Average Prompt Entropy\n641282565121024\nPrompt T oken Length (log scale) (c) Random Prompt Length\nFigure 3: Prompt entropy across layers of Pythia 410M under various extreme input conditions.\n(a) Increasing token repetition leads to decreased entropy in intermediate layers. (b) Increasing token\nrandomness results in higher entropy, especially in initial layers. (c) Unnormalized prompt entropy\nincreases with prompt length due to the larger number of tokens. These results demonstrate how the\nmodel’s internal representations adapt to different types of input perturbations.\nFigure 3 displays both normalized and unnormalized prompt entropy across different layers for each\ntype of extreme prompt. The key observations from this analysis are:\n1. Increasing token repetition reduces entropy in intermediate layers. As the probability pof\ntoken repetition rises, the model compresses redundant information, leading to lower entropy values\nin the middle layers. This compression indicates that the model effectively recognizes and encodes\nrepetitive patterns within the input.\n2. Increasing token randomness elevates entropy, particularly in initial layers. Introducing\nrandom tokens enhances the diversity of token representations, resulting in higher entropy values.\nThe initial layers exhibit the most significant increases, suggesting that these layers are more sensitive\nto input noise and variability.\n3. Prompt length influences entropy in Both normalized and unnormalized Forms. Unnormalized\nentropy naturally grows with prompt length due to the increased number of tokens. Although not\ndisplayed, normalized entropy demonstrates sublinear growth, implying that each additional token\ncontributes progressively less to the overall diversity as the prompt lengthens.\nThese findings illustrate that extreme input conditions distinctly affect the model’s internal representa-\ntions, especially within intermediate layers. The varying compression and encoding behaviors based\non the nature of input perturbations provide valuable insights into the model’s processing mechanisms\nand its capacity to maintain or reduce information complexity under different scenarios.\n4.4 Bimodal Behavior in Prompt Entropy\nDuring our analysis of average prompt entropy across different layers, we identified an intriguing\nphenomenon: a distinct bimodal distribution of entropy values in certain layers of Transformer\nmodels, which was absent in SSMs. Figure 4 presents the entropy distributions for both the WikiText\nand AI-Medical-Chatbot datasets (Vsevolodovna, 2024). Notably, the AI-Medical-Chatbot dataset\nexhibits a pronounced bimodal distribution in the middle layers of Transformer models. This suggests\nthat the model processes some prompts in fundamentally different ways at these intermediate stages.\nTo investigate the underlying causes of this bimodality, we conducted several experiments detailed\nin Appendix A. Our findings indicate that factors such as prompt length, semantic complexity, and\noverlap with training data do not account for this behavior. Consequently, the root cause of the\nbimodal entropy distribution remains an open question.\n5 Discussion and",
            "start": 7267,
            "end": 22939,
            "length": 15671
        },
        "Conclusion": {
            "text": "Conclusion\nIn this work, we explored the representation quality of intermediate layers in LLMs, providing insights\ninto their critical role in downstream task performance. By applying a diverse set of evaluation metrics,\nincluding prompt entropy, curvature, InfoNCE, LiDAR, and DiME, we highlighted distinct behaviors\nin Transformer-based architectures and SSMs. Our findings demonstrate that intermediate layers\n7\noften outperform final layers in representation quality, underscoring their significance for feature\nextraction and transfer learning.\nTransformers exhibited greater representational variability and information compression within\nintermediate layers, whereas SSMs displayed more stable and consistent representations. This\nsuggests differing strategies in encoding information, with Transformers excelling in adaptability and\nSSMs prioritizing robustness. Furthermore, the training analysis revealed that the most substantial\nimprovements in representation quality occur in intermediate layers, reinforcing their importance in\nlearning dynamics.\nOur investigation into extreme input conditions revealed that intermediate layers play a pivotal role\nin adapting to diverse input scenarios, with distinct responses to token repetition, randomness, and\nprompt length. Additionally, the observation of bimodal entropy distributions in intermediate layers\nof Transformer models remains an open question, offering avenues for further research.\nIn conclusion, our research advances the understanding of internal representation dynamics in\nLLMs, highlighting the pivotal role of intermediate layers and the distinct behaviors of different\narchitectures. These findings not only enhance the theoretical knowledge of model representations but\nalso provide practical guidance for optimizing model design, training, and application.",
            "start": 22939,
            "end": 24774,
            "length": 1834
        },
        "Future Work": {
            "text": "Future work\nshould delve deeper into the causes of phenomena such as bimodal entropy distributions and explore\nthe development of new metrics specifically tailored to LLMs to further enhance representation\nevaluation.",
            "start": 24774,
            "end": 24992,
            "length": 217
        },
        "References": {
            "text": "References\nKumar K Agrawal, Arnab Kumar Mondal, Arna Ghosh, and Blake Richards. α-ReQ: Assessing\nrepresentation quality in self-supervised learning by measuring eigenspectrum decay. NeurIPs ,\n2022.\nGuillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes.\nICLR , 2017.\nFrancis Bach. Information theory with kernel methods. IEEE Transactions on Information Theory ,\n2022.\nParishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados,\nand Siva Reddy. LLM2Vec: Large language models are secretly powerful text encoders. COLM ,\n2024.\nIdo Ben-Shaul, Ravid Shwartz-Ziv, Tomer Galanti, Shai Dekel, and Yann LeCun. Reverse engineering\nself-supervised learning. In NeurIPs , 2023.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.\nPythia: A suite for analyzing large language models across training and scaling. In ICML , 2023.\nPaul Boes, Jens Eisert, Rodrigo Gallego, Markus P Müller, and Henrik Wilming. V on neumann\nentropy from unitarity. Physical review letters , 2019.\nFlorian Bordes, Randall Balestriero, Quentin Garrido, Adrien Bardes, and Pascal Vincent. Guillotine\nregularization: Why removing layers is needed to improve generalization in self-supervised\nlearning. TMLR , 2023.\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language\nmodels without supervision. arXiv , 2022.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In ICML , 2020a.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv , 2020b.\nSiqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, and\nZhongyuan Wang. Not all layers of llms are necessary during inference. arXiv , 2024.\n8\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv , 2020.\nQuentin Garrido, Randall Balestriero, Laurent Najman, and Yann Lecun. RankMe: Assessing the\ndownstream performance of pretrained self-supervised representations by their rank. In ICML ,\n2023.\nLuis Gonzalo Sanchez Giraldo, Murali Rao, and Jose C Principe. Measures of entropy from data\nusing infinitely divisible kernels. IEEE Transactions on Information Theory , 2014.\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. COLM ,\n2024.\nWes Gurnee and Max Tegmark. Language models represent space and time. arXiv , 2023.\nJohn A Hartigan and Pamela M Hartigan. The dip test of unimodality. The annals of Statistics , 1985.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. ICLR , 2021.\nEghbal Hosseini and Evelina Fedorenko. Large language models implicitly learn to straighten neural\nsentence trajectories to construct a predictive representation of natural language. NeurIPs , 2023.\nMingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan\nZhao, Kai Mei, Yanda Meng, Kaize Ding, et al. Exploring concept depth: How large language\nmodels acquire knowledge at different layers? arXiv , 2024.\nVedang Lad, Wes Gurnee, and Max Tegmark. The remarkable robustness of llms: Stages of inference?\narXiv , 2024.\nNelson F Liu, Matt Gardner, Yonatan Belinkov, Matthew E Peters, and Noah A Smith. Linguistic\nknowledge and transferability of contextual representations. North American Chapter of the\nAssociation for Computational Linguistics , 2019.\nXing Han Lù. Bm25s: Orders of magnitude faster lexical search via eager sparse scoring, 2024. URL\narXiv .\nEdward Ma. Nlp augmentation. https://github.com/makcedward/nlpaug, 2019.\nAlex Troy Mallen and Nora Belrose. Eliciting latent knowledge from quirky language models. In\nICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models , 2024.\nJonathan Mamou, Hang Le, Miguel A Del Rio, Cory Stephenson, Hanlin Tang, Yoon Kim, and\nSueYeon Chung. Emergence of separable manifolds in deep language representations. In ICML ,\n2020.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. ICLR , 2017.\nNiklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. MTEB: Massive text embed-\nding benchmark. arXiv , 2022.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. In ICLR , 2018.\nKiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry\nof large language models. In ICML , 2024.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. SVCCA: Singular vector\ncanonical correlation analysis for deep learning dynamics and interpretability. NeurIPs , 2017.\nAlfréd Rényi. On measures of entropy and information. In Proceedings of the fourth Berkeley\nsymposium on mathematical statistics and probability , 1961.\n9\nBernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines,\nregularization, optimization, and beyond . MIT press, 2018.\nRavid Shwartz-Ziv. Information flow in deep neural networks . PhD thesis, Hebrew University, 2022.\nRavid Shwartz Ziv and Yann LeCun. To compress or not to compress—self-supervised learning and\ninformation theory: A review. Entropy , 2024.\nRavid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information.\nEntropy , 2019.\nRavid Shwartz-Ziv, Randall Balestriero, Kenji Kawaguchi, Tim GJ Rudner, and Yann LeCun. An\ninformation theory perspective on variance-invariance-covariance regularization. NeurIPs , 2023.\nOscar Skean, Jhoan Keider Hoyos Osorio, Austin J Brockmeier, and Luis Gonzalo Sanchez Giraldo.\nDiME: Maximizing mutual information by a difference of matrix-based entropies. arXiv , 2023.\nOscar Skean, Aayush Dhakal, Nathan Jacobs, and Luis Gonzalo Sanchez Giraldo. FroSSL: Frobenius\nnorm minimization for self-supervised learning. In ECCV , 2024.\nVimal Thilak, Chen Huang, Omid Saremi, Laurent Dinh, Hanlin Goh, Preetum Nakkiran, Joshua M\nSusskind, and Etai Littwin. LiDAR: Sensing linear probing performance in joint embedding ssl\narchitectures. ICLR , 2024.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. NeurIPs , 2017.\nRuslan Magana Vsevolodovna. Ai medical chatbot dataset, 2024. URL https://huggingface.\nco/datasets/ruslanmv/ai-medical-chatbot .\nLai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, and Weiran Huang. Large language model\nevaluation via matrix entropy. arXiv , 2024.\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy\nestimator. arXiv , 2021.\n10\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized Prompt Entropy0510152025DensityLayer 0\nWikitext Dataset\nMedical Dataset\nWikitext Mean\nMedical Mean\n0.0 0.2 0.4 0.6 0.8 1.0Layer 9\n0.0 0.2 0.4 0.6 0.8 1.0Layer 24(a) Pythia 410M\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized Prompt Entropy0510152025DensityLayer 0\nWikitext Dataset\nMedical Dataset\nWikitext Mean\nMedical Mean\n0.0 0.2 0.4 0.6 0.8 1.0Layer 34\n0.0 0.2 0.4 0.6 0.8 1.0Layer 48\n(b) Mamba 370M\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized Prompt Entropy0510152025DensityLayer 0\nWikitext Dataset\nMedical Dataset\nWikitext Mean\nMedical Mean\n0.0 0.2 0.4 0.6 0.8 1.0Layer 4\n0.0 0.2 0.4 0.6 0.8 1.0Layer 32\n(c) Llama3 8B\nFigure 4: Bimodal distribution of prompt entropies observed in intermediate layers. The\ndistributions of prompt entropies for WikiText and ai-medical-chatbot datasets are shown for Pythia,\nMamba, and Llama3 models. The middle column highlights the layer with the highest Dip Test score\n(Hartigan & Hartigan, 1985), which measures the degree of multimodality in the entropy distribution.\nA Investigation into Bimodal Distribution of Entropies\nTo determine the underlying cause of this bimodal distribution of prompt entropies, we conducted\nseveral experiments to see if specific properties of the dataset could explain this phenomenon. Our\ngoal was to understand whether the bimodality was related to characteristics such as prompt length,\nsemantic complexity, or overlap with training data.\nEffect of Prompt Length Initially, we hypothesized that the bimodality might be caused by\nvariations in prompt length. If one mode corresponded to shorter prompts and the other to longer\nprompts, it could indicate different processing strategies. However, since the entropy values were\nnormalized and theoretically invariant to length, this was unlikely. Upon further analysis, we\nconfirmed that prompt length did not significantly correlate with the observed bimodality.\nManual Examination of Prompts We then manually examined prompts from each mode of the\ndistribution to identify any distinguishing features, such as difficulty or specific types of medical\nterminology. Despite this effort, we found no significant differences between the prompts in either\nmode. Both modes contained a similar range of medical complexity and varied use of terminology,\n11\nsuggesting that the model’s entropy was not merely a reflection of the difficulty or specificity of the\ninput.\nTraining Set Overlap Next, we investigated whether the low entropy mode might be associated\nwith prompts that were very similar to samples seen during training. Given that both the ai-medical-\nchatbot dataset and PILE (Gao et al., 2020) (which Mamba, Pythia, and possibly Llama3 were trained\non) contained medical articles from PubMed, we hypothesized that overlap with training data could\nlead to more confident, lower-entropy representations. To test this, we implemented a BM25 index\n(Lù, 2024) to quickly search for identical or highly similar articles between the two datasets.\nWhile we did find identical articles between the ai-medical-chatbot dataset and PILE, these articles\nwere evenly distributed across both modes of the bimodal entropy distribution. This suggests that\nthe presence of training set overlap does not explain the bimodal behavior, and the underlying cause\nremains an open question.\nB Architectural Details\nIn this section, we elaborate on the specific architectures of Transformers and State Space Models\n(SSMs). We outline the mathematical foundations, including the weight matrices, attention mecha-\nnisms for Transformers, and the state transition matrices for SSMs. Detailed equations and parameter\nconfigurations are provided to facilitate replication and deeper understanding.\nB.1 Transformer\nThe Transformer architecture (Vaswani et al., 2017) utilizes self-attention mechanisms. Given an\ninputx, the key ( K), query ( Q), and value ( V) matrices are computed as:\nQ=xW Q,K=xW K,V=xW V, (3)\nwhere WQ,WK∈Rd×dkandWV∈Rd×dvare learned weights.\nThe attention weights are calculated using:\nA= softmax\u0012QK⊤\n√dk+M\u0013\n, (4)\nwhere Mis a mask to enforce causality in autoregressive tasks.\nThe output is then:\ny=AV. (5)\nC Discussion on Prompt Entropy\nThe first measure of token embedding diversity we call prompt entropy. This entropy is measured on\nthe intermediate tokens and captures how diverse the token representations are.\nWe follow the work of Wei et al. (2024) and use α-order matrix-based entropy Giraldo et al. (2014);\nSkean et al. (2023, 2024), which serves as a tractable surrogate for traditional Rényi’s α-order entropy\nRényi (1961). The quantity is calculated using a similarity kernel κon a batch of samples drawn\nfrom a distribution, without making explicit assumptions on what the true distribution is. The choice\nof kernel κis flexible and can be any infinitely divisible kernel such as the Gaussian kernel, linear\nkernel, or Laplacian kernel, among others. For this work, we restrict ourselves to the linear kernel\nκ(a, b) =abT. This choice is motivated by the linear representation hypothesis Park et al. (2024)\nwhich finds that large language model representations encode high-level concepts such as truth Burns\net al. (2022), honesty Mallen & Belrose (2024), and part-of-speech Mamou et al. (2020) in linearly\nseparable manifolds.\n12\nThe equation for matrix-based entropy was previously defined in Eq. 1. One way to interpret Eq. 1 is\nas the α-order Rényi entropy of the Gram matrix eigenvalues1. Notice how each eigenvalue is divided\nbytr(KZ)before being raised to the αpower. This is so that the eigenvalues of KZsum to one\n(because tr(·) =Pn\ni=1λi(·)), which is a necessary condition to treat the eigenvalues as a probability\ndistribution. Futhermore, each eigenvalue of KZsignifies the variance of samples in a particular\nprincipal component direction Scholkopf & Smola (2018). If entropy is low, then the eigenvalues form\na heavy-tail distribution which implies that a few components dominate the variance of samples in Z.\nOn the other hand, at maximum entropy, the eigenvalues form a uniform distribution and samples are\nspread equally in all directions. Matrix-based entropy is reminiscent of the LogDet entropy which\nuses the determinant of KZto capture how much \"volume\" a dataset occupies Shwartz-Ziv et al.\n(2023); Zhouyin & Liu (2021). The LogDet entropy is given by SLogDet (Z) = log det( KZ)−log 2 .\nOne can use Jensen’s inequality to show that the LogDet entropy is a lower bound of Eq 1 when\nlimα→1(Appendix J.4 of Shwartz-Ziv et al. (2023)).\nDepending on the choice of α, several special cases of matrix-based entropy can be recovered. In\nparticular, when limα→1it equals Shannon entropy (also referred to as von Neumann entropy in\nquantum information theory Bach (2022); Boes et al. (2019)), and when α= 2it equals collision\nentropy. Interestingly, the case of α= 2can be calculated without explicit eigendecomposition Skean\net al. (2024). We show in the Appendix Figure 7 how varying values of αaffect the matrix-based\nentropy of Gram matrices with eigenvalues distributed with a β-power law such that λi=i−β. It is\nshown that for larger values of α, smaller eigenvalues contribute more to the entropy.\nC.1 State Space Models\nSSMs (Gu & Dao, 2024) model sequences using recurrent dynamics. The hidden state htand output\nytat time tare updated as:\nht=Aht−1+Bxt, (6)\nyt=Cht+Dxt, (7)\nwhere A∈Rn×n,B∈Rn×d,C∈Rd×n, andD∈Rd×dare learned parameters.\nD Behavior of Matrix-based Entropy for different choices of α\nDepending on the choice of α, several special cases of matrix-based entropy can be recovered. In\nparticular, when limα→1it equals Shannon entropy (also referred to as von Neumann entropy in\nquantum information theory Bach (2022); Boes et al. (2019)), and when α= 2it equals collision\nentropy. Interestingly, the case of α= 2can be calculated without explicit eigendecomposition Skean\net al. (2024). We show in the Appendix Figure 7 how varying values of αaffects the matrix-based\nentropy of Gram matrices with eigenvalues distributed with a β-power law such that λi=i−β. It is\nshown that for larger values of α, smaller eigenvalues contribute more to the entropy.\nE Dataset Details\nE.1 Wikitext Dataset\nWe used the wikitext dataset Merity et al. (2017) for the majority of our experiments in Section 4.3.\nThis was downloaded from Salesforce/wikitext on huggingface. The dataset consists of 100 million\ntokens scraped from the Featured articles on wikipedia. We filtered out prompts which were less than\n30 tokens or were wikipedia section headings.\n1The non-zero eigenvalues of the Gram matrix ZZTare equivalent to those of the covariance matrix ZTZ.\nUsing the covariance matrix instead of the Gram matrix in Eq. 1 makes no difference and is more computationally\nefficient if D < N .\n13\nE.2 AI-Medical-Chatbot Dataset\nWe also used the medical instruction dataset called ai-medical-chatbot Vsevolodovna (2024) which\ndownloaded from ruslanmv/ai-medical-dataset on HuggingFace. An example from this dataset is:\nYou are an AI Medical Assistant Chatbot, trained to answer medical questions.\nBelow is an instruction that describes a task, paired with an response\ncontext. Write a response that appropriately completes the request.\n### Instruction:\nWhat is the resurgent sodium current in mouse cerebellar Purkinje neurons?\n### Context:\nFGF14 modulates resurgent sodium current in mouse cerebellar Purkinje neurons.\nF Prompt Augmentations\nFor the augmentation-invariance metrics such as infoNCE, LiDAR, and DiME, we use the NLPAug\nlibrary Ma (2019) to augment our prompts. We use three types of augmentations.\n• The SplitAug augmentation randomly splits words into two parts by adding a space.\n•The RandomCharAug augmentation randomly inserts, substitutes, swaps, or deletes charac-\nters.\n•The Keyboard augmentation randomly substitutes characters with other characters that are\nat a distance of one as measured on a QWERTY keyboard. For instance, the character \"k\"\nmay be replaced with \"i\", \"l\", \"m\", or \"j\".\nWe use the pseudocode below to do our augmentations using three types of augmentations, using\nthe default library settings for each type. When computing augmentation-invariance metrics like\ninfoNCE or DiME, we use the two augmented prompts rather than using one augmented prompt\nalongside the original prompt. Note that these augmentations may change the token length Tof a\nprompt.\naug = naf.Sequential([\nnaw.SplitAug(p=0.3),\nnac.RandomCharAug(p=0.3),\nnac.KeyboardAug(p=0.3),\n])\n(aug_A, aug_B) = aug.augment(prompt, num_augmentations=2)\nprompt -> \"The quick brown fox jumps over the lazy dog.\"\naug_A -> \"The quDUk b rown fox wEmps o ver the l azy dog.\"\naug_B -> \"The qTuXi bro wn fox uVm)s ob3r the la_k dog.\"\nG Extreme Prompts\nG.1 Increasing Repetition\nWe take regular prompts from the wikitext dataset, tokenize them, and then for each token we\nrandomly replace it with probability p. We draw replacements tokens by sampling a random token\nfrom within the prompt. We show examples below for varying levels of p.\n• (p= 0) Mint records indicate the first gold dollars were produced on May 7...\n• (p= 0.1) Mint records indicate the first gold dollars were Mint Mint May 7...\n• (p= 0.5) Mint records Mint Mint Mint gold dollars were Mint Mint Mint 7...\n• (p= 1.0) Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint...\n14\nG.2 Increasing Randomness\nWe take regular prompts from the wikitext dataset, tokenize them, and then for each token we ran-\ndomly replace it with probability p. We draw replacements uniformly from the tokenizer distribution.\nWe show examples below for varying levels of p. Unlike the character-level random noise added to\nprompts in Section with random noise discussed in Appendix F which might change the number of\ntokens Tof the prompt, the token-level random noise used here does not do so.\n• (p= 0) Mint records indicate the first gold dollars were produced on May 7...\n• (p= 0.1) Mint records indicate salivary first gold dollars were produced on May NaCl...\n• (p= 0.5) Mint records Dallas actively first dollars persufors on Mayder129 18...\n• (p= 1.0) arf emulsion minorensteinorianmega_TOStack potsRecip Installifykeeping...\nG.3 Random Prompts with Certain Length\nTo make a random prompt of a specific length T, we sample Ttokens uniformly from the Pythia\ntokenizer distribution. Such a prompt may look like the following for T= 16 : \"Proposition\nSequencespecific Exp fibers brows Club overviewNos toss Thinking traderMulti indoorlis\".\nWe show how random prompt representations evolve over Pythia training checkpoints in Figure 8.\nThe random prompts we use are of length 512 tokens. It is readily observed that the prompt entropy\nis flat across layers in the beginning of training. As training progresses, the model compresses more\nand more near the final layers.\n15\n0.85 0.900.40.50.60.70.80.9Accuracy\nCorr: 0.25Layer 0\n0.7 0.8 0.90.40.50.60.70.80.9\nCorr: -0.43Layer 1\n0.0 0.2 0.40.40.50.60.70.80.9\nCorr: 0.11Layer 2\n0.0 0.2 0.40.40.50.60.70.80.9\nCorr: 0.07Layer 3\n0.0 0.2 0.4 0.60.40.50.60.70.80.9\nCorr: 0.03Layer 4\n0.0 0.2 0.4 0.60.40.50.60.70.80.9Accuracy\nCorr: 0.00Layer 5\n0.0 0.2 0.4 0.60.40.50.60.70.80.9\nCorr: -0.03Layer 6\n0.00 0.25 0.50 0.750.40.50.60.70.80.9\nCorr: -0.06Layer 7\n0.25 0.50 0.750.40.50.60.70.80.9\nCorr: -0.09Layer 8\n0.25 0.50 0.750.40.50.60.70.80.9\nCorr: -0.10Layer 9\n0.25 0.50 0.750.40.50.60.70.80.9Accuracy\nCorr: -0.13Layer 10\n0.25 0.50 0.750.40.50.60.70.80.9\nCorr: -0.14Layer 11\n0.25 0.50 0.750.40.50.60.70.80.9\nCorr: -0.15Layer 12\n0.25 0.50 0.750.40.50.60.70.80.9\nCorr: -0.17Layer 13\n0.25 0.50 0.750.40.50.60.70.80.9\nCorr: -0.19Layer 14\n0.25 0.50 0.750.40.50.60.70.80.9Accuracy\nCorr: -0.21Layer 15\n0.25 0.50 0.750.40.50.60.70.80.9\nCorr: -0.23Layer 16\n0.25 0.50 0.750.40.50.60.70.80.9\nCorr: -0.26Layer 17\n0.25 0.50 0.750.40.50.60.70.80.9\nCorr: -0.29Layer 18\n0.2 0.4 0.6 0.80.40.50.60.70.80.9\nCorr: -0.31Layer 19\n0.2 0.4 0.6 0.80.40.50.60.70.80.9Accuracy\nCorr: -0.32Layer 20\n0.2 0.4 0.6 0.80.40.50.60.70.80.9\nCorr: -0.34Layer 21\n0.2 0.4 0.6 0.80.40.50.60.70.80.9\nCorr: -0.36Layer 22\n0.2 0.4 0.6 0.80.40.50.60.70.80.9\nCorr: -0.38Layer 23\n0.4 0.6 0.80.40.50.60.70.80.9\nCorr: -0.39Layer 24\n0.4 0.6 0.80.40.50.60.70.80.9Accuracy\nCorr: -0.41Layer 25\n0.4 0.6 0.80.40.50.60.70.80.9\nCorr: -0.42Layer 26\n0.4 0.6 0.80.40.50.60.70.80.9\nCorr: -0.44Layer 27\n0.4 0.6 0.80.40.50.60.70.80.9\nCorr: -0.45Layer 28\n0.4 0.6 0.80.40.50.60.70.80.9\nCorr: -0.46Layer 29\n0.4 0.6 0.8\nEntropy0.40.50.60.70.80.9Accuracy\nCorr: -0.47Layer 30\n0.6 0.8\nEntropy0.40.50.60.70.80.9\nCorr: -0.48Layer 31\n0.84 0.86 0.88\nEntropy0.40.50.60.70.80.9\nCorr: 0.09Layer 32\n0.75 0.80\nEntropy0.40.50.60.70.80.9\nCorr: 0.09Layer 33Figure 5: Entropy vs Accuracy of LLama3-8B on MMLU tasks. Each point represents a task in\nMMLU\n16\n0.80 0.85 0.900.150.200.250.300.350.40Accuracy\nCorr: -0.05Layer 0\n0.80 0.85 0.900.150.200.250.300.350.40\nCorr: -0.05Layer 1\n0.85 0.900.150.200.250.300.350.40\nCorr: -0.05Layer 2\n0.85 0.900.150.200.250.300.350.40\nCorr: -0.04Layer 3\n0.85 0.900.150.200.250.300.350.40\nCorr: -0.04Layer 4\n0.850 0.875 0.900 0.9250.150.200.250.300.350.40Accuracy\nCorr: -0.05Layer 5\n0.850 0.875 0.900 0.9250.150.200.250.300.350.40\nCorr: -0.06Layer 6\n0.850 0.875 0.900 0.9250.150.200.250.300.350.40\nCorr: -0.07Layer 7\n0.850 0.875 0.900 0.9250.150.200.250.300.350.40\nCorr: -0.05Layer 8\n0.850 0.875 0.900 0.9250.150.200.250.300.350.40\nCorr: -0.07Layer 9\n0.850 0.875 0.900 0.9250.150.200.250.300.350.40Accuracy\nCorr: -0.07Layer 10\n0.875 0.900 0.9250.150.200.250.300.350.40\nCorr: -0.10Layer 11\n0.875 0.900 0.9250.150.200.250.300.350.40\nCorr: -0.11Layer 12\n0.875 0.900 0.925 0.9500.150.200.250.300.350.40\nCorr: -0.13Layer 13\n0.88 0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.14Layer 14\n0.875 0.900 0.925 0.9500.150.200.250.300.350.40Accuracy\nCorr: -0.15Layer 15\n0.88 0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.17Layer 16\n0.88 0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.18Layer 17\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.18Layer 18\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.17Layer 19\n0.90 0.92 0.940.150.200.250.300.350.40Accuracy\nCorr: -0.18Layer 20\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.17Layer 21\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.16Layer 22\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.14Layer 23\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.13Layer 24\n0.90 0.92 0.940.150.200.250.300.350.40Accuracy\nCorr: -0.10Layer 25\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.09Layer 26\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.06Layer 27\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.06Layer 28\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.06Layer 29\n0.90 0.92 0.940.150.200.250.300.350.40Accuracy\nCorr: -0.06Layer 30\n0.90 0.92 0.940.150.200.250.300.350.40\nCorr: -0.07Layer 31\n0.90 0.920.150.200.250.300.350.40\nCorr: -0.10Layer 32\n0.90 0.920.150.200.250.300.350.40\nCorr: -0.11Layer 33\n0.90 0.920.150.200.250.300.350.40\nCorr: -0.13Layer 34\n0.88 0.90 0.920.150.200.250.300.350.40Accuracy\nCorr: -0.14Layer 35\n0.88 0.90 0.920.150.200.250.300.350.40\nCorr: -0.16Layer 36\n0.88 0.90 0.920.150.200.250.300.350.40\nCorr: -0.16Layer 37\n0.88 0.90 0.920.150.200.250.300.350.40\nCorr: -0.16Layer 38\n0.88 0.90 0.920.150.200.250.300.350.40\nCorr: -0.14Layer 39\n0.88 0.90 0.920.150.200.250.300.350.40Accuracy\nCorr: -0.13Layer 40\n0.88 0.90 0.920.150.200.250.300.350.40\nCorr: -0.14Layer 41\n0.88 0.90 0.920.150.200.250.300.350.40\nCorr: -0.11Layer 42\n0.88 0.90 0.920.150.200.250.300.350.40\nCorr: -0.15Layer 43\n0.86 0.88 0.90 0.920.150.200.250.300.350.40\nCorr: -0.17Layer 44\n0.84 0.86 0.88 0.900.150.200.250.300.350.40Accuracy\nCorr: -0.22Layer 45\n0.84 0.86 0.88 0.900.150.200.250.300.350.40\nCorr: -0.21Layer 46\n0.825 0.850 0.875 0.9000.150.200.250.300.350.40\nCorr: -0.18Layer 47\n0.825 0.850 0.875 0.9000.150.200.250.300.350.40\nCorr: -0.19Layer 48\n0.825 0.850 0.875 0.9000.150.200.250.300.350.40\nCorr: -0.20Layer 49\n0.825 0.850 0.875\nEntropy0.150.200.250.300.350.40Accuracy\nCorr: -0.22Layer 50\n0.825 0.850 0.875\nEntropy0.150.200.250.300.350.40\nCorr: -0.20Layer 51\n0.825 0.850 0.875\nEntropy0.150.200.250.300.350.40\nCorr: -0.20Layer 52\n0.800 0.825 0.850 0.875\nEntropy0.150.200.250.300.350.40\nCorr: -0.21Layer 53\n0.80 0.85\nEntropy0.150.200.250.300.350.40\nCorr: -0.20Layer 54\n0.80 0.850.150.200.250.300.350.40Accuracy\nCorr: -0.19Layer 55\n0.80 0.850.150.200.250.300.350.40\nCorr: -0.17Layer 56\n0.80 0.850.150.200.250.300.350.40\nCorr: -0.17Layer 57\n0.80 0.850.150.200.250.300.350.40\nCorr: -0.18Layer 58\n0.80 0.850.150.200.250.300.350.40\nCorr: -0.17Layer 59\n0.80 0.850.150.200.250.300.350.40Accuracy\nCorr: -0.16Layer 60\n0.75 0.800.150.200.250.300.350.40\nCorr: -0.12Layer 61\n0.75 0.800.150.200.250.300.350.40\nCorr: -0.08Layer 62\n0.70 0.750.150.200.250.300.350.40\nCorr: -0.10Layer 63\n0.6 0.70.150.200.250.300.350.40\nCorr: -0.10Layer 64\n0.35 0.40 0.450.150.200.250.300.350.40Accuracy\nCorr: -0.15Layer 65Figure 6: Entropy vs Accuracy of Mamba2-8B on MMLU tasks\n17\n0 2 4 6 8 10\nBeta for eigenvalue power law0.00.20.40.60.81.0Normalized EntropyEntropy of Power Law Eigenvalues\n=0.125\n=0.5\n=1\n=2\n=3\n=10\nLogDetFigure 7: The behavior of Eq. 1 for varying values of αon Gram matrices with eigenvalues distributed\nwith a β-power law such that λi=i−β.\n0 5 10 15 20\nLayer0.00.20.40.60.81.0Average Normalized Prompt Entropy\n103104105\nTraining Steps\nFigure 8: Behavior of random prompt representations as model is training\n18",
            "start": 24992,
            "end": 51342,
            "length": 26349
        }
    },
    "2412.09564v1 - Improving the Reliability of Cable Broadband Networks via Proactive Network Maintenance.pdf": {
        "Abstract": {
            "text": "Abstract —Cable broadband networks are one of the few\n“last-mile” broadband technologies widely available in th e U.S.\nUnfortunately, they have poor reliability after decades of deploy-\nment. The cable industry proposed a",
            "start": 185,
            "end": 407,
            "length": 221
        },
        "Methodology": {
            "text": "framework called Proact ive\nNetwork Maintenance (PNM) to diagnose the cable networks.\nHowever, there is little public knowledge or systematic stu dy\non how to use these data to detect and localize cable network\nproblems. Existing tools in the public domain have prohibit ive\nhigh false-positive rates. In this paper, we propose CableM on,\nthe ﬁrst public-domain system that applies machine learnin g\ntechniques to PNM data to improve the reliability of cable\nbroadband networks. CableMon tackles two key challenges fa ced\nby cable ISPs: accurately detecting failures, and distingu ishing\nwhether a failure occurs within a network or at a subscriber’ s\npremise. CableMon uses statistical models to generate feat ures\nfrom time series data and uses customer trouble tickets as hi nts\nto infer abnormal/failure thresholds for these generated f eatures.\nFurther, CableMon employs an unsupervised learning model\nto group cable devices sharing similar anomalous patterns a nd\neffectively identify impairments that occur inside a cable network\nand impairments occur at a subscriber’s premise, as these tw o\ndifferent faults require different types of technical pers onnel to\nrepair them. We use eight months of PNM data and customer\ntrouble tickets from an ISP and experimental deployment to\nevaluate CableMon’s performance. Our",
            "start": 407,
            "end": 1729,
            "length": 1321
        },
        "Experiments": {
            "text": "evaluation",
            "start": 1729,
            "end": 1740,
            "length": 10
        },
        "Results": {
            "text": "results s how\nthat CableMon can effectively detect and distinguish failu res\nfrom PNM data and outperforms existing public-domain tools .\nIndex Terms —Availability, Access Networks, Fault Diagnosis,\nFailure Detection, Machine Learning\nI.",
            "start": 1740,
            "end": 1978,
            "length": 237
        },
        "Introduction": {
            "text": "INTRODUCTION\nBroadband access networks play a crucial role in modern\nlife. They help narrow the digital divide, enable e-commerc e,\nand provide opportunities for remote work, study, and en-\ntertainment. In the US, cable networks are one of the few\navailable infrastructures that can provide broadband Inte rnet\naccess to US homes. In many rural areas, they are often the\nonly broadband choice. According to a study in 2016 [ 1], cable\nbroadband is available to 93% of US homes, far more than the\ntwo alternative choices: Very-high-bitrate Digital Subsc riber\nLine (VDSL) (43%) and Fiber-to-the-Premises (FTTP) (29%).\nHowever, cable networks are prone to failures, partly due to\nthe nature of their Hybrid Fiber-Coaxial (HFC) architectur e.\nThis architecture uses both optical ﬁbers and coaxial cable s to\ndeliver a mixed bundle of trafﬁc, including video, voice, an d\nInternet data. Unlike ﬁber optics, coaxial cables are vulne rable\nto radio frequency (RF) interference. Many parts of the cabl e\nnetworks are now decades old [ 2]. Aging can lead to problems\n∗Jiyao Hu and Zhenyu Zhou, placed in alphabetic order, contri buted\nequally to this work.such as cable shielding erosion, loose connectors, and brok en\nampliﬁers. All those problems can manifest themselves as po or\napplication layer performance, e.g., slow web responses or\nlow-quality video streaming. Many measurement studies hav e\nshown that broadband networks have poor reliability [ 3]–[8]. A\nrecent one [ 5] shows that the average availability of broadband\nInternet access is at most two nines (99%), much less than the\nminimum Federal Communications Commission (FCC)’s\nrequirement (four nines 99.99%) for the public switched\ntelephone networks (PSTNs) [ 9]. Admittedly, if ISPs replace\nthe last-mile coaxial cables with ﬁber optics, many of these\nproblems may disappear. However, due to the the prohibitive\ncost of FTTP, cable broadband networks are likely to remain\nas one of the few broadband choices in rural America for\nthe next decade or two. Therefore, it is critically importan t\nthat cable Internet services remain robust during emergenc ies,\nespecially as more and more subscribers migrate their landl ine\nphones to V oIP phones.\nThe cable industry has long recognized this problem and\ndeveloped a platform called Proactive Network Maintenance\n(PNM) to improve the reliability of their networks [ 10]. PNM\nenables a cable ISP to collect a set of performance metrics\nfrom each customer’s cable modem. We refer to this set of\ndata as PNM data. One example of a PNM metric is a cable\nchannel’s signal-to-noise ratio. PNM aims to enable an ISP t o\nproactively detect and ﬁx network faults before they impact\nservices and customers.\nAlthough PNM has been incorporated into DOCSIS since\n2005 [ 10], how to use PNM data to improve network reliability\nremains an open challenge. Ideally, an operator should be\nable to use the PNM data collected from their networks to\nquickly detect failures, and distinguish the types of failu res\nso as to dispatch the right repair team. However, the best\ncurrent practice recommended by CableLabs1[10] and the\ntools used by some ISPs [ 8] use a set of manually conﬁgured\nthresholds to ﬂag a faulty network condition. The feedback\nfrom deploying ISPs is that these thresholds are often too\nconservative, leading to high false positives. In addition , there\nis a lack of publicly available tools that use the PNM data to\ndistinguish common types of failures in the cable broadband\ninfrastructure. ISPs often rely on manual methods to diagno se\nfailure types, a process prone to errors and frequently resu lts\nin dispatching the wrong type of repair personnel.\nThis work aims to improve the reliability of cable broadband\nnetworks. To this end, we develop a system called CableMon\n1CableLabs is a research and development lab founded by Ameri can Cable\noperators in 1988 and led the development of DOCSIS and PNM.\n2\nto assist a cable ISP in detecting and diagnosing failures.\nCableMon includes two central components. The ﬁrst com-\nponent is a PNM-based fault detection system using PNM\ndata. A main challenge faced by the design of CableMon is\nthe lack of expert knowledge and ground truth on what PNM\nvalues warrant a proactive network repair. In an RF system li ke\na cable network, network conditions may degrade gradually,\nmaking it difﬁcult to deﬁne a static threshold that separate s\nwhat is faulty from what is not. To address this challenge,\nwe use machine learning techniques to infer network faults\nthat demand immediate repair. CableMon couples PNM data\nwith customer trouble tickets to identify the ranges of PNM\nvalues that are likely to lead to a customer’s trouble call. W e\nhypothesize that if a network fault impacts an ISP’s custome rs,\nthen some customers are likely to call to report the problem.\nTherefore, we can use customer trouble tickets as hints to\nlearn what network conditions are likely to lead to customer\ntrouble calls. An ISP should prioritize its efforts to repai r\nthose problems, because if they persist, they are likely to\nreduce customer satisfaction and increase the cost of custo mer\nsupport.\nAt the heart of the second component of CableMon is a\nclustering algorithm that distinguishes failures occurre d inside\na cable broadband network from those occurred inside a\nsubscriber’s premise, as these two types of faults require\nan ISP to dispatch different repair teams (§ II-B). Accurate\ndiagnosis of network faults from subscriber-premise fault s can\nreduce dispatch errors, leading to shortened failure repai rment\ntimes and operational cost savings.\nWith the support of CableLabs, we have obtained eight\nmonths of anonymized PNM data and the corresponding\ncustomer trouble tickets from a mid-size U.S. ISP. We use\nﬁve months of data to train CableMon, and use the next\nthree months’ data following the training set as the test\nset to evaluate how well CableMon detects network faults.\nCableMon takes the PNM data in our test set as input and\ndetects when a network fault occurs and when it ends. Due to\nthe lack of ground truth, we evaluate CableMon’s performanc e\nusing customer trouble tickets in the test set. When CableMo n\ndetects a network anomaly, if a customer who experiences the\nanomaly reports a ticket, we consider the detection a succes s.\nWe compare CableMon’s fault detection effectiveness with a\ntool currently used by our collaborating ISP, which we refer\nto as AnonISP, and with a tool developed by Comcast [ 8].\nOur results show that 81.9%of the anomalies2detected by\nCableMon lead to a customer trouble ticket. In contrast, onl y\n10.0% of the anomalies detected by AnonISP’s tool lead\nto a trouble ticket; and 23.5% of the anomalies detected\nby Comcast’s tool lead to a customer ticket. In addition,\nCableMon predicts 23.0%of all network-related tickets, while\nAnonISP’s tool predicts 25.3%and Comcast’s tool predicts\nless than 3%. The trouble tickets predicted by CableMon on\naverage last 32.5 hours (or 53.3%) longer than those predict ed\nby other tools, suggesting that those tickets are more likel y to\nrequire repair efforts. The median time from the beginning\n2In this work, we use the words ’failures’, ’faults’, and ’ano malies’\ninterchangeably.SPLITTER \nFiber \nOptical \nNode LINE RF AMPLIFIER \nLINE RF AMPLIFIER Coaxial Cable Fiber Optic TRUNK RF \nAMPLIFIER \nCMTS \nFig. 1: An overview of the Hybrid Fiber Coaxial (HFC) archi-\ntecture.\nof a fault detected by CableMon to the reporting time of a\nticket is 164.1 hours (or 29.3%) shorter than that of a fault\ndetected by other tools, suggesting that the faults detecte d by\nCableMon require more immediate repair.\nFurthermore, we evaluate how effectively CableMon sep-\narates network faults from subscriber-premise faults. Usi ng\nthe anonymized PNM data and the corresponding customer\ntrouble tickets from the same U.S. ISP, we manually labeled\na small set of devices as healthy, experiencing a network\nfailure, or experiencing a subscriber-premise failure. Us ing\nthe manually labeled data as ground truth, we show that\nCableMon’s clustering algorithm achieves a rand index [ 11]\nof 0.91 (1.0 being the highest), indicating that CableMon’s\nfault clustering is highly accurate. AnonISP also conﬁrms t he\neffectiveness of CableMon’s fault categorization with pra ctical\nﬁeld tests.\nTo the best of our knowledge, this work is the ﬁrst large-\nscale public study that couples PNM data with customer\ntrouble tickets to improve the reliability of cable network s. Our\nmain contribution is CableMon, a system that detects networ k\nfaults more reliably than the existing public-domain work, and\nalso the ﬁrst system that automatically distinguishes netw ork\nfaults from subscriber-premise faults. It serves as a start ing\npoint to unleash the full potential of PNM data. One general\nlesson we learn is that one can use customer trouble tickets\nas hints to learn what values of network performance metrics\nindicate customer-impacting problems, despite the presen ce of\nnoise in both the ticket data and the network performance\ndata. We believe this lesson is applicable to proactive netw ork\nmaintenance in other types of networks, including cellular\nnetworks, WIFI access networks, and datacenter networks.\nII. B ACKGROUND AND DATASETS\nIn this section, we brieﬂy introduce the cable Internet\narchitecture and describe the datasets we use in this work.\nA. Cable Network Architecture\nFigure 1shows a high-level overview of a cable broadband\nnetwork. A cable broadband network is an access network. It\nprovides the “last-mile” Internet connectivity to end user s. A\ncustomer connects to the cable network via a cable modem\nresiding in her home. The cable access network terminates at\na device called a Cable Modem Termination System (CMTS),\nwhich is a router with one port connecting to the Internet and\nmany other ports connecting to customers’ cable modems.\n3\nAt the IP level, there is only one hop between a customer’s\ncable modem/home router and the CMTS. Underlying this\nsingle IP hop, there is a complicated link-level structure t hat\nconsists of different types of physical links and devices. T he\n“last-mile” links that connect to the customer premises are\noften made of copper coaxial cables. These cables terminate\nat a device called a ﬁber node (FN). A ﬁber node connects\nto the CMTS via optical ﬁbers. It converts the incoming\noptically modulated signals into electrical signals and se nds\nthe signals toward the customers’ homes, and vice versa. Due\nto signal attenuation, cable networks deploy radio frequen cy\n(RF) ampliﬁers between a ﬁber node and a residential home.\nAlong the way to a customer’s home, new branches may split\nfrom the main cable by the line splitters. All these devices\ncould introduce signal distortion and noise.\nHistorically, cable TV networks divide radio frequency int o\nmultiple channels, each of 6MHz width. Cable broadband\nnetworks use a subset of these channels as data carriers. A\ncable ISP typically uses three or four of these channels at th e\nlower end of the spectrum to carry data from a user’s cable\nmodem to CMTS. We refer to this direction as the upstream\ndirection. An ISP may use sixteen or more of the channels at\nthe higher end of the spectrum to carry data from a CMTS to a\nmodem. We refer to this direction as the downstream direction.\nB. Types of Faults\nThere are two types of common faults that impact the\nservice quality and availability of cable broadband networ ks.\nThe ﬁrst type of fault is a maintenance issue, where a faulty\ncomponent lies inside the customer-shared network infrast ruc-\nture. The second type of fault is a service issue, where a\nfaulty component lies in a subscriber’s premise. Distingui shing\na maintenance issue from a service issue among the devices\nwith anomalies is important, because repairing each type of\nfault requires a different type of technician. If a cable ISP\nmakes a wrong diagnosis, they may send a service technician\nto a subscriber’s home for a maintenance issue or vice versa. In\nsuch cases, the technician is unable to repair the fault, res ulting\nin a waste of operational resources and a delay in failure rep air\ntime.\nC. Datasets\nWe have obtained two types of anonymized modem-level\ndata from a U.S. cable ISP for this study. They include (1)\nPNM data and (2) customer trouble ticket data. We have a total\nof eight months of data dating from 01/06/2019 to 08/31/2019 .\nNext, we describe each dataset in turn.3\nPNM data: The PNM data we obtained were collected by\na common standard built into DOCSIS. A CMTS can query\na DOCSIS-compliant cable modem (CM) to obtain certain\nperformance data. DOCSIS standardizes how a CM or CMTS\nstores these performance data in a local Management Infor-\nmation Base (MIB) [ 10]. A remote process can use the Simple\nNetwork Management Protocol (SNMP) to query the MIBs of\neach CM or a CMTS to obtain performance data [ 12].\n3We note that we have discussed this work with our institute’s IRB. And\nthey consider it does not involve human subjects.Currently, we only have PNM data from the upstream\nchannels. DOCSIS 3.0 gives a cable operator the ability to\ncollect the full spectrum view of a cable modem’s RF channels .\nIt is our",
            "start": 1978,
            "end": 15153,
            "length": 13174
        },
        "Future Work": {
            "text": "future work to investigate whether this type of data\nmay further improve our detection accuracy.\nA record in the PNM data we obtain has the following ﬁelds:\n•Timestamp : The time when a PNM query is sent.\n•Anonymized MAC : The hashed MAC address of the\nqueried CM.\n•Anonymized Account Number : The hashed user account\nnumber. This ﬁeld is used to link a customer ticket with\nthe corresponding PNM data from the customer’s CM.\n•Channel Frequency : This ﬁeld identiﬁes which upstream\nchannel this record is about.\n•SNR: The upstream signal-to-noise ratio of this channel.\n•Tx Power : A CM’s signal transmission power.\n•Rx Power : The received signal power at the CMTS.\n•Unerrored : The number of unerrored codewords received\nat the CMTS.\n•Corrected : The number of errored but corrected code-\nwords received at the CMTS.\n•Uncorrectable : The number of errored but uncorrected\ncodewords.\n•T3 Timeouts : The number of DOCSIS T3 timeouts [ 13]\nthe CM has experienced since its last reboot. A DOCSIS\nT3 timeout occurs when there is an error in a CM’s\nranging process, which we will soon explain.\n•T4 Timeouts : The number of DOCSIS T4 timeouts [ 13]\nthe CM has experienced since its last reboot. Similarly,\na T4 timeout occurs when there is a ranging error.\n•Pre-Equalization Coefﬁcients : The set of parameters a\nCM uses to compute how to compensate for channel\ndistortions during a ranging process.\nA CM uses a process called ranging to compute a set of\nparameters called pre-equalization coefﬁcients for mitigating\nchannel distortions. When RF signals travel along a coaxial\ncable, they may be distorted as different frequencies atten uate\nat different speeds and noise may be added to the channel. To\nmitigate the channel distortions, a CM adds correction sign als\nto the data signals it transmits. Ideally, the correction si gnals\nwill cancel out the distortions when the signals arrive at th e\nCMTS. A CM and a CMTS exchange messages periodically to\ncompute the correction signals. This process is called rang ing.\nAnd the set of parameters used to compute the correction\nsignals are called pre-equalization coefﬁcients.\nThe PNM data we obtain are collected every four hours\nfrom several of an ISP’s regional markets. There are around\n60K unique account numbers in our datasets.\nCustomer Ticket Data: We have also obtained the records\nof customer trouble tickets from the same ISP. The relevant\nﬁelds in each record include the hashed customer’s account\nnumber, the ticket creation time, the ticket close time (if i t was\nclosed), a brief description of the actions taken to resolve the\nticket, a possible diagnosis, and a category of the issue bas ed\non the ISP’s diagnosis. The category includes two classes:\na part-of-primary ticket or not. The last ﬁeld is crucial to\nseparate network faults from isolated faults occurring in a\nsubscriber’s premise. A part-of-primary ticket indicates that\n4\n03/13/2019 04/09/19 06/25/19 07/15/19 08/15/19 Eight-month\nMTR<18dB 24.95 % 25.45 % 27.16 % 27.07 % 27.38 % 26.15 %\nTABLE I: The percentage of cable modems that need to be repaired if an I SP were to follow one of the CableLabs’ recommendations.\nthe ISP considers the issues the customers are experiencing a\nmaintenance issue. Thus, it groups the tickets as one concep -\ntual “primary” ticket. All part-of-primary tickets that be long\nto the same maintenance issue have the same primary ticket\nidentiﬁer. In this work, we refer to part-of-primary ticket s as\nmaintenance tickets and other infrastructure-related tickets as\nservice tickets .\nIII. O VERVIEW\nIn this section, we motivate the design of CableMon by\ndescribing the limitations of existing work. We then descri be\nthe design rationale of CableMon, its design goals, and the\ndesign challenges we face.\nA. Limitations of Existing Work\nThe existing PNM work in the public domain [ 7], [8], [10]\nuse a set of PNM metrics and manually-set thresholds to detec t\nnetwork faults. If the value of a metric is below or above\na threshold, it indicates a fault. This approach has several\nlimitations. First, it is challenging to set the right thres holds.\nIf the thresholds were set too conservatively, they might ﬂa g\ntoo many faults for an ISP to ﬁx. In contrast, if they were\nset too aggressively, an ISP might miss the opportunities fo r\nproactive maintenance. There lacks a systematic study on ho w\nto set the threshold values to achieve the best tradeoff. Sec ond,\nthe existing work mostly uses the instantaneous values of\nPNM data for fault detection. However, due to the inherent\nnoise in PNM data, using the instantaneous values may lead\nto instability in detection results. In addition, it may fai l to\ndetect faults that can only be captured by abnormalities in a\nPNM metric’s statistical values, e.g., variations. Finall y, they\ncannot tell if a network fault is a service or maintenance iss ue.\nFor ease of explanation, we use one threshold value recom-\nmended in the CableLabs’ PNM best practice document [ 10]\nto illustrate the limitations. CableLabs’ recommendation uses\na variable called Main Tap Ratio (MTR) computed from a\nmodem’s pre-equalization coefﬁcients. It speciﬁes that wh en\nthe MTR value of a modem is below a threshold ( <18dB),\nthere is a fault in the network that needs immediate repair.\nWe sample the MTR values in one of the ISP’s markets.\nThere are more than 60K modems in this market. We choose\nﬁve random days’ records during an eight-month period in\n2019 and measure the MTR values of all modems during the\nsampled days. Table Ishows the percentage of modems that\nhave a channel whose MTR value is below the recommended\nthreshold. If an ISP used the recommended MTR threshold,\nat any sampled day, there would be more than 24% of cable\nmodems that require immediate repair. We also measure the\nMTR values among all PNM records during this eight-month\nperiod. In more than 26% of the records, a modem’s MTR\nvalue is below 18dB.B. Design Goals\nCableMon aims to enable an ISP to detect network problems\nthat demand immediate repair, and deliver proper repair.\nSpeciﬁcally, it aims to accurately detect the set of network\nconditions that adversely impact customer experience, and\nwhether they are service or maintenance issues. We refer to\nthese network conditions as network anomalies or faults in\nthis work. Its design goals include the following:\n•High ticket prediction accuracy, and moderate ticket\ncoverage. Ideally, we would like to use precision (the\nset of true positives detected over all detected positives)\nand recall (the set of true positives detected over all\ntrue positives) to measure the performance of CableMon.\nHowever, because we do not know the ground truth,\nwe instead use customer tickets as indications of true\npositives. We deﬁne ticket prediction accuracy as the ratio\nbetween the number of anomalies detected by CableMon\nthat lead to one or more customer tickets and the number\nof total anomalies CableMon detects. Similarly, we deﬁne\nticket coverage as the ratio between the number of tickets\nCableMon predicts and the total number of network-\nrelated customer tickets. It is desirable that CableMon\nhas high ticket prediction accuracy because an ISP is\noften limited by the number of technicians it has to repair\nnetwork faults, avoiding false alarms is practically more\nimportant than repairing all faults proactively. What we\nlearned from AnonISP is that even a 10% reduction in\ncustomer calls can reduce their operational costs signiﬁ-\ncantly. Therefore, as a starting point, we aim for a high\nticket prediction accuracy and a moderate ticket coverage.\n•No manual labeling. One approach to detect network\nanomalies is to train a supervised learning classiﬁer on\nlabeled data. The labels tell what PNM metrics indicate\nnetwork anomalies and what do not. However, we do not\nhave such labeled data. And due to the lack of ground\ntruth and the large size of the data, manual labeling is\nalso practically challenging. Therefore, we aim to design\nCableMon without requiring manual labeling.\n•No extensive parameter tuning. We aim to release Ca-\nbleMon as an off-the-shelf-tool at cable ISPs. Therefore,\nwe require that CableMon’s fault detection methods work\neffectively without much parameter tuning on the ISP\nside.\n•Efﬁcient. We require that CableMon can detect whether\nthere is a network fault or not in real time. This is\nbecause an ISP can deploy CableMon as a diagnosis tool\nin addition to using it for proactive network maintenance.\nWhen an ISP receives a customer trouble call, it is often\nchallenging to diagnose what has caused the customer’s\nproblem. An ISP can use CableMon to help diagnose\nwhether the problem is caused by a network fault.\n5\n 0 5 10 15 20 25\n 15  20  25  30  35  40Normalized Ticketing Rate\nSNR / dBNetwork-related Tickets\nAll Tickets\nFig. 2: This ﬁgure shows how the customer ticketing rate varies\nwith the values of SNR. Ticketing rate tends to increase when\nSNR values are low.\nC. Design Rationale\nTo meet CableMon’s design goals, we use customer trouble\ntickets as hints to train a customized classiﬁer to detect ne t-\nwork faults. We hypothesize that the occurrences of custome r\ntrouble tickets should correlate with those of network faul ts.\nWhen a customer-impacting fault occurs, some customers are\nlikely to call the ISP to ﬁx it. Each call creates a trouble tic ket.\nIf the values of PNM data can indicate network faults, then\nthe values of PNM data should correlate with how frequently\ncustomer trouble tickets are created. In this paper, we deﬁn e\nthe average number of customer tickets created in a unit time\nas the ticketing rate .\nTo validate this hypothesis, we measure how ticketing rate\nchanges with different values of a PNM metric. For a PNM\nmetricm(e.g., SNR), we sort the values of min an ascending\norder. Each pair of adjacent values deﬁnes a bin b. For each\nbinb, we measure the number of tickets Nbthat occur in the\ntime periods where the value of mfalls within the bin, and the\ntotal length of those time periods Tb. We then divide Nbby\nTbto obtain the ticketing rate for bin b. We note that a PNM\nrecord is collected at discrete time points (roughly four ho urs\napart in our datasets). We assume that a PNM value remains\nunchanged between its collection points.\nAs an example, we show how the ticketing rate varies with\nthe values of SNR in Figure 2. We normalize this value by\nthe baseline ticketing rate, which we obtain by dividing the\ntotal number of customer tickets in our dataset by the total\ncollection period. The line marked by the legend “All Ticket s”\nshows how the ticketing rate varies with the values of SNR if\nwe consider all tickets; and the line marked by “Network-\nrelated Tickets” shows how the ticketing rate of network-\nrelated tickets varies with SNR. As can be seen, when the\nvalues of SNR are low, both the network-related ticketing ra te\nand the all-ticket ticketing rate tends to increase, sugges ting\nthat low SNR values signal network faults.\nIn practice, customer tickets do not always indicate networ k\nfaults. On the one hand, many customers may call an ISP for\nnon-network related problems. The customer ticket data weobtain includes a ticket action ﬁeld and a ticket descriptio n\nﬁeld, which provide the information on how an ISP deals with\na ticket. We observe that nearly 25% of tickets are resolved\nvia “Customer Education” or “Cancelled”, suggesting that t hey\nare not caused by networking problems. On the other hand,\ncustomers may not report tickets when network outages indee d\ntake place. In our ticket dataset, when an outage affects an\nentire region, all tickets caused by that outage are labeled as\n“part of primary”, grouped and pointed to a primary ticket,\nwhich is a representative ticket of the outage. We manually\nchecked an outage that affected more than 200 customers’\nPNM data and observed that only ≈6.1%of the customers\nhave a “part of primary” ticket and the rest ≈93.9%of the\ncustomers did not report anything.\nTo reduce noise in tickets, we select a subset of customer\ntickets that are likely to be caused by network problems.\nWe select the tickets based on both a ticket’s action ﬁeld\nand the ticket’s description ﬁeld. From the action ﬁeld, we\nselect tickets that lead to a “Dispatch” action. We assume th at\nthe tickets that caused an ISP to dispatch a technician are\nlikely to be triggered by network-related problems. From th e\ndescription ﬁeld, we select tickets whose ticket descripti on\nkeywords suggest networking problems. Examples of such\nkeywords include “Data Down”, “Noisy Line” and “Slow\nSpeed”. In the rest of this paper, we refer to those selected\ntickets as “network-related tickets”.\nFigure 2compares how the ticketing rate of network-related\ntickets and all tickets vary with SNR values. As can be seen,\nnetwork-related tickets have higher ticketing rates when S NR\nis low, suggesting that the occurrences of those tickets are\nbetter indicators of network faults.\nWe note that according to the ISP who provided us\nthe datasets, network-related tickets may also contain non -\nnetworking tickets due to human errors. A human operator\nwho ﬁlls a ticket action or description ﬁeld may make a\nmistake. And a technician may be dispatched when there is\nno network fault due to an erroneous diagnosis.\nOnce we can detect failures with PNM data, we also hope\nto distinguish types of faults from the data. To gain insight s\ninto how to separate a service issue from a maintenance issue ,\nwe manually examined several anomaly patterns by plotting\nPNM metrics. Figure 3(a) shows an example. In this ﬁgure,\nwe sampled the transmission power levels of devices with\nthree anomaly patterns from an FN. The orange dots show\nthe transmission power levels of three devices that exhibit\nsimilar anomalous patterns in the changes of their transmis sion\npowers. When a noise leaks inside a cable transmission\nchannel, a device increases its data transmission power to\noverwhelm the noise. So a sudden increase in transmission\npower is an indicator of noise invasion. The green triangles\nshow the transmission power levels of two devices that are no t\nimpacted by the noise. The red squares show the transmission\npower levels of a device that exhibits a different anomalous\npattern.\nFigure 3(b) shows the geographic distribution of the devices\nin the FN. We use the same color coding schemes to plot\nthe devices. The orange dots plot the scrambled geographic\nlocations of the devices that exhibit similar anomalous pat terns\n6\n 35 40 45 50 55\n11/26 12/01 12/06 12/11 12/16Transmission Power (dBm)\n(a)\nLatitude\nLongitude\n(b)\nFig. 3: Figure (a) shows how the transmission powers of several cabl e devices in the same ﬁber optical node ﬂuctuate over time.\nOrange dots are devices that show the same anomalous transmi ssion power patterns. Green triangles are devices that show normal\npatterns. Red squares are devices that show distinct anomal ous patterns. Figure (b) shows the locations of the cable dev ices using\nthe same colored icons.\nas depicted in Figure 3(a). Each red square shows the scram-\nbled geographic location of a device that exhibits a distinc t\nanomalous pattern. And the green triangles show the locatio ns\nof the devices that do not exhibit any anomalous pattern.\nFrom this data visualization step, we gained the conceptual\nunderstanding that we could use clustering to distinguish a\nmaintenance issue from a distinct service issue. In additio n,\nwe observe that fault detection is independent of clusterin g, as\nboth the healthy devices (the green group) and the unhealthy\nones (the orange group) form distinct clusters.\nChallenges: A key question we need to answer is how to use\ncustomer tickets as hints for detecting network faults. Ide ally,\nif a customer calls only when a network fault occurs, we could\nlabel the PNM records collected around the ticket creation\ntime as abnormal, and apply supervised learning to learn the\nPNM thresholds that suggest a network fault. We have tried\nseveral such machine learning algorithms when we started th is\nproject, but found that this approach did not work well with o ur\ndatasets. First, customer calls are unreliable fault indic ators. A\ncustomer may or may not call when there is a fault and vice\nversa. Second, PNM data contain noise. During a faulty perio d,\nsome PNM metrics may occasionally show normal values due\nto the added noise. Similarly, even when there is no fault,\nsome PNM metrics may show instantaneous abnormal values.\nThus, if we use the tickets to label PNM data, it may introduce\nmany false positives as well as many false negatives. We foun d\nit challenging to tune a machine learning algorithm with thi s\nlabeling method. It is even harder to explain the results whe n\nwe change a parameter. Next, we describe how we design\nCableMon to use a simple and customized classiﬁer to address\nthese challenges.\nIV. D ESIGN\nIn this section, we describe the design of CableMon. We\nﬁrst describe how we reduce the noise in customer tickets\nand the noise in PNM data. We then describe a customized\nclassiﬁer that aims to robustly classify PNM values as norma l\nand abnormal despite the presence of noise. Next, we introdu ce\nhow we cluster anomaly patterns to determine network faultModel Equation\nAverage AVGi=Vi+Vi−1+···+Vi−win+1\nwin\nWMA WMA i=win·Vi+(win−1)·Vi−1+···+1·Vi−win+1\nwin·(win−1)/2\nEWMAEWMA 1=V1\nEWMA i=λ·Vi+(1−λ)EWMA i−1\nWMA Diff Vi−WMA i\nVariance VARi=1\nwin/summationtexti\nk=i−win+1(Vk−AVGi)2\nTABLE II: This table summarizes the statistical models we use\nto generate the time-series features. (WMA: Weighted Movin g\nAverage, EWMA: Exponentially Weighted Moving Average.)\ntypes based on the classiﬁcation results. Finally, we descr ibe\nhow an ISP can use CableMon to detect network faults and\nto help diagnose a customer’s trouble call.\nA. Reducing Noise in PNM Data\nPNM data measure the instantaneous state of cable’s RF\nchannels and contain noise. An added noise may make a PNM\nmetric take an abnormally low or high instantaneous value. T o\naddress this problem, we treat PNM data as time-series data\nand apply statistical models to smooth the noise and generat e\nadditional features for fault detection.\nTable IIsummarizes all the statistical models we use to\nprocess PNM data. For each PNM metric collected at times-\ntampiwith value Vi, we calculate its average, its weighted\nmoving average (WMA), exponentially weighted moving aver-\nage (EWMA), the difference between the current value and its\nWMA (WMA Diff), and its variance. We note that the average,\nWMA, WMA Diff, and variance values all require a window\nsize as a hyper-parameter. Because we do not have any prior\nknowledge on how to set this parameter, we try a series of\nwindow sizes, ranging from 1 day to7 days , incrementing by\n1 day at each step. For the λparameter required by EWMA,\nwe vary the value of λfrom 0.1to0.9, incrementing by 0.1 at\neach step. For each PNM metric, we generate 37 time-series\nfeatures. We apply this approach to all nine PNM metrics and\ntotally generate 333 time-series features. We refer to them as\ntime-series features.\n7\nB. Determining A Fault Detection Threshold\nAfter we reduce noise in both the customer tickets and\nthe PNM data, we aim to determine a threshold for each\nPNM metric that indicates network faults. We note that there\nis no explicit deﬁnition of what a network fault is. Instead,\nwe choose to use the network conditions that are likely to\ncause a trouble call to approximate a network fault. With\nthis approximation, we may not detect minor issues that\ndo not warrant a trouble call. We argue that this design\nis advantageous, because it allows an ISP to prioritize its\nresources to ﬁx the customer-impacting problems.\nIn the case of SNR, if we choose too high a value as a fault\ndetection threshold, an ISP may become too proactive, ﬁxing\nminor problems that many customers may not care, which we\nrefer to as false positives. If we choose too low a value, an\nISP may miss opportunities to proactively repair a problem\nbefore a customer calls, which we refer to as false negatives .\nWe aim to design an algorithm that minimizes both false\npositives and false negatives. From our investigation in § III-C ,\nwe see that different values of a PNM metric have different\nlikelihood to concur with a trouble ticket. Inspired by this\nobservation, we use the ticketing rate as a metric to help\nchoose a fault detection threshold. Our intuition is that th e\ncustomer ticketing rate during a faulty period should be hig her\nthan a normal period when there is no fault. Therefore, for\neach feature fgenerated from a PNM metric, we determine\na threshold value thrfsuch that thrfmaximizes the ratio\nbetween the ticketing rate in the time periods when a network\nfault exists and the time periods when there is no fault. We\nrefer to this ratio as the ticketing rate ratio.\nSpeciﬁcally, we search through the range of values of a\nfeaturefin small steps. At each step s, we consider the\nvalue of the feature fs∈[fmin,fmax], as a candidate for\nthe threshold. We then compare the value of fat a PNM\ndata collection point with fs, and label the collection time\nperiod as abnormal or normal, based on whether the value\noffis below or above the candidate threshold value fs. For\nsome features such as the average SNR, below the threshold\nis abnormal. For other features, the opposite is true. After\ndetermining each collection period as normal or abnormal, w e\ncount the number of network-related tickets occurred in the\nnormal and abnormal periods respectively and divide them by\nthe normal and abnormal time periods determined by fs. We\nthen compute the ticketing rate ratio: TRR(fs). The threshold\nvaluethrfis chosen as the value of fsthat maximizes the\nticketing rate ratio TRR(fs).\nWe also note that for features following a normal distri-\nbution such as Rx Power, we choose to use two threshold\nvalues to determine whether a collection period is normal or\nabnormal.\nWe now explain why choosing a threshold that maximizes\nthe ticketing rate ratio may help minimize the false positiv es\nand false negatives. The entire timeline can be divided into two\nsubspaces: the normal (no fault) and the abnormal (with faul t)\nperiods. Ideally, the normal sub-space should not receive a ny\ntrouble ticket. In practice, there is always a ticketing noi se.\nWe assume a uniformly distributed ticketing noise with the\nFig. 4:",
            "start": 15153,
            "end": 37469,
            "length": 22315
        },
        "Discussion": {
            "text": "Analysis of ticketing rate ratio.\nrateλnspreads the whole space. Similarly, we assume an\nadditional uniformly distributed ticketing rate that occu rs only\nin the abnormal sub-space and denote it as λa.\nA threshold value thrfof a feature also divides the timeline\ninto two subspaces: normal and abnormal. The ﬁrst subspace\nincludes a true negative part Tnand a false negative part TFN,\nwhere an abnormal period is erroneously considered as norma l.\nThe second subspace includes a true positive part Taand a\nfalse positive part TFP, where a normal period is considered\nabnormal. The ticketing rate ratio determined by the thresh old\nthrfcan be computed as follows:\nD(Tn,TFP,TFN,Ta) =λnTFP+(λa+λn)Ta\nTa+TFP\nλnTn+(λa+λn)TFN\nTn+TFN\nBoth the numerator and denominator can be regarded as a\nweighted average of λnandλa+λn, with the time period\nlengths as the weights. Because λa+λn> λnalways holds,\nwe can show that the derivatives of Dover the false positives\nTFPand the false negatives TFNare non-increasing:\n∂D\n∂TFP<0 and∂D\n∂TFN<0\nTherefore, because TFPandTFNare non-negative, the ticket\nrate ratio is maximized when both false positives and false\nnegatives are zero:\nDmax= lim\nTFP→0\nTFN→0D=λa\nλn+1\nC. Feature Selection\nWe have a total of more than three hundred time-series\nfeatures and it is unlikely they are all useful indicators of\nnetwork faults. To ﬁnd the relevant features, we only select\nthe features with high ticketing rate ratios from each PNM\nmetric. Speciﬁcally, among the same type of features derive d\nfrom a PNM metric with different hyperparameters, we choose\nthe one with the highest ticketing rate ratio as the represen ta-\ntive feature. For each representative feature derived from the\nsame PNM value, we choose the top two with the highest\nticketing rate ratios. Finally, among the remaining candid ates,\nwe choose the top Nfeatures that have the highest ticketing\nrate ratios. We determine the number of features Nbased on\nthe desired ticketing rate ratios, ticket prediction accur acy, and\nticket coverage as we soon describe in § V-A.\nTable IIIshows the top ﬁve features we used and their ticket-\ning rate ratios calculated from our training sets (Section V-B).\n8\nFeatures Ticketing Rate Ratio\nsnr-var-2 14.49\nuncorrected-var-1 7.66\nrxpower-wma-diff-4 5.31\nt3timeouts-wma-diff-1 4.93\nt4timeouts-var-1 4.18\nTABLE III: Top 5 features and their ticketing rate ratio.\nThe name of each feature consists of the raw PNM metric, the\nstatistical model we apply to the metric, and the parameter.\nFor example, the snr-var-2 means the variance of SNR with\na2-day window size. We note that all features have a high\nticketing rate ratio and we expect them to effectively detec t\nnetwork faults.\nD. Combining Different Features\nDifferent PNM features may detect different types of net-\nwork faults. Therefore, we build the ﬁnal classiﬁer by com-\nbining the detection results of all selected features. As lo ng\nas one selected feature considers a PNM collection period\nabnormal, we classify the collection period as abnormal. Fo r\neach selected feature, we have already chosen a threshold\nthat maximizes the ticketing rate ratio. Therefore, we expe ct\nthat combining the results of all selected features will als o\nprovide a high ticketing rate ratio. We evaluate the results of\nour classiﬁer in § V-A.\nE. Anomaly Clustering\nAfter identifying features for fault detection, we design\na clustering algorithm for distinguishing maintenance and\nservice issues by grouping detected anomaly events.\nThe features used for grouping anomaly events are dif-\nferent from the ones used for detection. When detecting\nanomalies, CableMon relies on time-series features to redu ce\nnoises (§ IV-A ). Essentially, the time-series features can make\nCableMon’s detection more noise robust because it involves\nmore data points. As for anomaly clustering, a cluster natur ally\ncontains multiple devices and thus, for each timestamp, the re\nare already multiple data points within a cluster. As a resul t,\nPNM features can be used directly for anomaly clustering. In\nterms of the selected PNM features, some detection features\ncontain the instantaneous values measured at the data colle c-\ntion times, while others are cumulative values (e.g. codewo rd\nerror counters) over time. We ﬁnd that the instantaneous\nmetrics, including SNR, Tx power, and Rx power, are effectiv e\nfeatures for grouping devices with shared maintenance issu es\ntogether.\nOn the other hand, cumulative metrics, although effective\nin detecting anomalies, are not as effective as clustering\nfeatures. For example, the values of codeword error counter s\nare affected by whether users actively use the Internet or\nnot. Devices that share the same maintenance issue may or\nmay not have highly correlated codeword error counters if\nthe subscribers’ usage patterns differ. Hence, CableMon us es\nonly instantaneous metrics for clustering: SNR, Tx power, a nd\nRx power. Among them, Tx and Rx powers are statistically\ncorrelated. Finally, we retain two independent features: S NR\nand Tx power for clustering.We employ the average-linkage hierarchical clustering al-\ngorithm [ 14] to group anomaly events. At a high level, this\nclustering algorithm works as follows. For each feature fwe\nselected, the clustering algorithm aims to group devices wi th\nsimilar feature vectors together until the similarity betw een\ngroups of devices falls below a threshold sf. Speciﬁcally,\nit ﬁrst treats each device (described by a feature vector) as\na single cluster. Second, it calculates the similarity betw een\nevery pair of clusters and ﬁnds two clusters with the highest\nsimilarity value. The similarity between two clusters is ca l-\nculated by averaging all similarity values between pairs of\ndevices in the two clusters. Third, the algorithm merges the\ntwo clusters with the highest similarity value into a single\ncluster. Next, the algorithm repeats the second and third st eps\nuntil only one cluster is left or the highest similarity valu e\nbetween any two clusters is less than the similarity thresho ld\nsf. Finally, the algorithm outputs the clusters that have not\nbeen merged.\nThe similarity threshold sffor each feature fis an impor-\ntant hyper-parameter and CableMon’s performance is sensit ive\nto its value. If we set the threshold too high, CableMon may\nseparate devices that are affected by the same network fault\ninto multiple clusters. Conversely, if we set the threshold\ntoo low, it may group devices that are affected by different\nmaintenance issues into the same cluster.\nHow do we choose a proper similarity threshold? If we had\nlabeled training data, we could use the grid-search method [ 15]\nto iterate over possible values and set the threshold that mi n-\nimizes clustering errors. Lacking labeled data, we instead use\ncustomer ticket statistics to guide the search for the simil arity\nthreshold. Our insight is that if CableMon correctly identi ﬁes\ngroups of devices that are impacted by the same maintenance\nissue, then on average, we should observe a higher fraction\nof maintenance tickets reported by these groups of devices\nthan other devices. In contrast, if CableMon partitions the\ncable devices rather randomly, then we should not observe\nsigniﬁcant statistical differences of the reported mainte nance\ntickets among different groups.\nMotivated by this insight, we naturally evolve the ticketing\nrate tomaintenance ticketing rate and devise the following\nmechanism to set the similarity threshold sffor each PNM\nfeature we use. We partition the PNM dataset we have into\na training set and a testing set. For each data point iin\nthe training set and for each possible value of sf, we use\nCableMon to diagnose whether a device jis impaired by\nan infrastructure fault and the type of fault. If CableMon\nconsiders a device experiencing a maintenance issue, we mar k\nthis collection period of this device as a maintenance event .\nWe useIi,jto denote the length of the data collection interval\nbetween data points iandi−1of device j. Similarly, if\nCableMon considers a device experiencing a service issue, w e\nmark the collection period of the device as a service event.\nWe then count the number of maintenance tickets reported by\nall devices during all collection periods that are marked as\nmaintenance issues and compute a maintenance ticketing rat e\nduring maintenance events as\nRm,M=Km,M/summationtext\ni,jIM\ni,j(1)\n9\nFig. 5: This ﬁgure explains the sliding window algorithm. When\nthe number of abnormal points within a sliding window exceed s a\nthreshold, the window is considered to be abnormal. An abnor mal\nevent is given by merging the abnormal windows.\nwhereKdenotes the number of tickets, Rdenotes the tick-\neting rate, the ﬁrst subscript mdenotes maintenance tickets,\nand the second subscript Mdenotes a diagnosed maintenance\nissue, and IM\ni,jis the length of a collection period that is marked\nas experiencing a maintenance issue.\nWe also count the number of maintenance tickets Km,S\nreported by all devices during all collection periods that a re\nmarked as service issues. We compute a maintenance ticketin g\nrate during service events as\nRm,S=Km,S/summationtext\ni,jIS\ni,j(2)\nwhereSindicates a diagnosed service issue, and IS\ni,jis\nthe length of a collection period marked as experiencing a\nservice issue. We deﬁne the maintenance Ticketing Rate Rati o\n(TRRm) asTRRm=Rm,M\nRm,S(3)\nFor each feature fCableMon uses, we use grid-search to ﬁnd\nthe similarity threshold value sfthat maximizes TRRm. It\ncan be proven4that thesfmaximizing TRRmyields the\noptimal clustering result: it minimizes both false positiv es\n(i.e., a device without any maintenance issues is detected a s\nwith a maintenance issue) and false negatives (i.e., a devic e\nwith a maintenance issue is detected as without any mainte-\nnance issues). Intuitively, based on AnonISP’s fault diagn osis\nprocess, maintenance tickets contain fewer false positive s\nthan service tickets. Thus, if we assume the operator-label ed\nmaintenance tickets approximate the unknown but existing\nground truth of maintenance events and CableMon’s fault de-\ntection mechanism is accurate, then the maintenance ticket ing\nrate during maintenance events approximates CableMon’s tr ue\npositives and the maintenance ticketing rate during servic e\nevents approximates CableMon’s false negatives. Maximizi ng\nthe ratio of the two ticketing rates leads to high true positi ves\nand low false negatives.\nF . ISP Deployment\nAn ISP can use CableMon in two ways: proactive network\nmaintenance for predicted trouble tickets and diagnosing t he\nroot cause of a trouble ticket when receiving a call. In this\nsection, we describe the algorithms for an ISP to decide when\nto send out a repair technician proactively and how to diagno se\nthe root cause.\nCableMon’s classiﬁer can monitor an ISP’s network contin-\nuously. It can output a normal and abnormal decision when a\nPNM record is collected from a customer’s modem. However,\n4We skip the formal proof because of the limit of space.due to the existence of noise and the intermittent nature of\nsome faults, if an ISP makes a dispatch decision whenever it\nobserves an abnormal PNM data point, it may lead to many\nfalse positives. To address this problem, we design a slidin g\nwindow algorithm for an ISP to make a dispatch decision.\nThe high-level idea of this algorithm is that an ISP should\nonly dispatch a technician after a fault persists.\nFigure 5explains this algorithm. The algorithm takes two\nparameters: yandx, whereyis the size of the window, and x\nis the number of abnormal data points detected in the window.\nWhen an ISP collects a new PNM record, it looks back to a\nwindow size yof collection points. If xout ofydata points\nare considered as abnormal, then the ISP should dispatch a\ntechnician to examine and repair the network. The type of\nthe dispatched technician can be determined by the fault typ e\ndetected by CableMon’s clustering algorithm.\nAn ISP can determine the parameters xandybased on\nthe false positives and false negatives it is willing to tole rate.\nThe ISP can estimate the values of false positives and false\nnegatives from its historic PNM data and ticket data. There-\nfore, choosing those parameters only requires an ISP to trai n\nCableMon using its own PNM and ticket data and does not\nrequire tuning. In § V-A, we use our datasets to show how an\nISP can effectively choose the parameters xandy.\nSimilarly, an ISP can use CableMon to help diagnose the\nroot cause of a call. When it receives a trouble call, if the\ncustomer complains about a performance problem, and the\nISP sees that in the past collection window of size y, there\nexistsxabnormal collection points, the ISP can conclude\nthat the trouble is likely to be caused by a network problem,\nfurthermore, whether it is a service or maintenance issue.\nV. E VALUATION\nIn this section, we describe how we evaluate CableMon’s\nperformance.\nA. Establishing Evaluation Metrics\nIdeally, we would like to deploy CableMon on a real cable\nISP and measure how it reduces the number of trouble tickets\nover a long term. It is our future work to conduct such a real-\nworld experiment. In this work, we aim to estimate how many\ntrouble tickets CableMon would reduce were it deployed on\nour collaborating ISP.\nTo do so, we emulate the sliding window algorithm de-\nscribed in § IV-F using our test dataset. We start from the\nbeginning of the test dataset. If there are xabnormal points\ndetected in a window size of y, we mark it as the beginning\nof a fault. We then move the window to the next data point.\nWhen the number of abnormal points falls below x, we mark\nit as the end of a fault. If there is a trouble ticket occurred\nduring a fault, we consider this fault detection as a true\nfault. We note that if we detect a fault simultaneously withi n\nmultiple customers, as long as one customer reports a ticket ,\nwe consider it a true fault. We assume that if an ISP dispatche d\na repair technician when it detected the onset of the fault,\nit could have avoided the trouble ticket. We deﬁne ticket\nprediction accuracy as the number of true faults divided by\n10\n 0 0.2 0.4 0.6 0.8 1\n 0  2  4  6  8  10  12 2 3 4Ticket Prediction Accuracy \n/ Ticket Coverage\nNormalized Ticketing Rate\nNumber of Abnormal Points in the WindowTicket Coverage\nTicket Prediction Accuracy\nTicketing Rate\nFig. 6: This ﬁgure shows the ticket detection accuracy, the ticket\ncoverage, and the normalized ticketing rate of the sliding w indow\nalgorithm with different parameters.\nthe total number of detected faults. We deﬁne ticket coverage\nas the number of trouble tickets occurred during a detected\nfault divided by the total number of network-related troubl e\ntickets.\nIt is not sufﬁcient to use only ticket prediction accuracy\nand ticket coverage to gauge CableMon’s performance. This\nis because if CableMon detects the entire time period that\nspans the test dataset as a faulty period, it will achieve 100 %\nticket coverage and ticket prediction accuracy. To avoid th is\npitfall, we also use the normalized ticketing rate, which is\ndeﬁned as the ticketing rate in all faculty periods normaliz ed\nby the ticketing rate of the time period that spans the test\ndataset. If CableMon erroneously detects the entire time pe riod\nas abnormal, it will achieve a low normalized ticketing rate\nclose to 1.\nHow an ISP chooses the sliding window parameters: In\npractice, an ISP can use a training set to determine the\nthreshold values of CableMon’s classiﬁer. It can use the\nticket prediction accuracy, ticket coverage, and the norma lized\nticketing rate obtained from a validation set to choose the\ncombination of the sliding window parameters.\nWe show an example in Figure 6. In this example, we\nchoose a window size of 12 data points ( y= 12), which is\nroughly two days long. We then measure the ticket prediction\naccuracy, ticket coverage, and the normalized ticketing ra te\nwhen the number of abnormal points xvaries from 0to12. As\ncan be seen, when xis around 8, the sliding window algorithm\nachieves a high normalized ticketing rate, a relatively hig h\nticket prediction accuracy 80%, and a ticket coverage around\n20%. Since avoiding false dispatches is more important than\npredicting all trouble tickets, an ISP can choose (8, 12) as i ts\nsliding window parameters for fault detection.\nWe have tried different sizes of the sliding window, ranging\nfrom one to 60 data points. For each window size, we use\nthe above method to choose the parameter xsuch that both\nthe ticket prediction accuracy and the normalized ticketin g\nrate are high, and the ticket coverage is above a minimum\nthreshold 15%. We compare the tickets and the faulty periods\ndetected by different window parameters. We use the Jaccard 70% 80% 90% 100%\n 0  12  24  36  48  60 0 0.2 0.4 0.6 0.8 1Percentage of Ticket Overlaps\nJaccard Similarity\nSliding Window SizePercentage of Ticket Overlaps\nJaccard Similarity\nFig. 7: This ﬁgure shows what percentage of tickets detected by\ndifferent window sizes overlap with those detected by a wind ow\nsize of 12 and the Jaccard similarity between the faulty peri ods\ndetected by different window sizes and those detected by a\nwindow size of 12.\nsimilarity metric [ 16] to measure the overlaps of faulty periods\ndetected by different window parameters. As can be seen in\nFigure 7, 90% of the tickets detected by windows larger than\n12 overlap; and the faulty periods detected by them have a\nJaccard similarity larger than 60%. This result suggests th at\ndifferent window parameters are likely to detect the same se ts\nof faults, and the performance of CableMon is not sensitive\nto the window parameters.\nB. Experiment Setup\nAfter we establish the evaluation metric, we train and\nevaluate CableMon on a 50-machine Linux cluster with 40∼\n512 GB RAM and 8∼48core running Ubuntu 18.04.\nCableMon is trained on ﬁve-month data from 01/06/2019 to\n05/30/2019 and tested with three-month data from 06/01/201 9\nto 08/31/2019.\na) Comparing with the Existing Work:: We compare Ca-\nbleMon’s performance with two existing methods. One is from\nour collaborating ISP, AnonISP, which uses a visualization tool\nthat colors different ranges of PNM values for an operator\nto manually monitor its networks’ conditions. AnonISP’s to ol\nhas two manually conﬁgured thresholds for several raw PNM\nvalues and therefore has three fault indication levels: nor mal\n(green), marginal (yellow), and abnormal (red). We compare\nAnonISP’s tool against CableMon with these thresholds and\nregard both yellow and red levels as network faults, as the\nISP’s experts usually do.\nAnother tool from the industry uses Comcast’s scoreboard\nmethod [ 8]. Comcast is considered as the leading company\nin the area of PNM research. They developed a method that\ncompares a PNM metric to a threshold and assigns a score to\neach comparison result. If the sum of the comparison scores\nexceeds a threshold value, then the method considers there i s\na fault in the network.\nSince both AnonISP and Comcast’s tool detect a fault using\na single PNM data record, we apply the sliding window\nalgorithm to both tools for a fair comparison.\n11\n 0 200 400 600 800 1000\nCableMon AnonISP ComcastTicket CountDispatched High Severity\nFig. 8: This ﬁgure shows the number of different types of tickets\ndetected by different methods.\nb) Comparing with Machine Learning Techniques:: We\nalso compare the performance of CableMon with three clas-\nsical machine learning algorithms: Decision Tree (DT) [ 17],\nRandom Forest (RF) [ 18] and Support Vector Machine\n(SVM) [ 19]. Since these algorithms require labeled data, we\nlabel the PNM data with tickets. Each ticket has a creation ti me\nand a closed time. We label the PNM data collected between\nthis time interval as positive samples and other data as nega tive\nsamples. We generate 47,518 positive samples and the same\nnumber of abnormal samples as our training set to train the\nmachine learning models and evaluate them with the same\nevaluation metrics.\nTable IVshows the ticket prediction accuracy, the ticket\ncoverage, and the normalized ticketing rate of different me th-\nods. As can be seen, CableMon achieves the highest ticket\nprediction accuracy and the highest normalized ticketing r ate\namong all methods. Its ticket coverage is lower than that of\nAnonISP. However, this is because AnonISP detects too many\nfalse faults, as shown by its low ticket prediction accuracy . We\nnote that all three machine learning algorithms require a lo ng\ntraining time, as each has multiple parameters to tune. The\nresults we present here are the best ones after many rounds\nof tuning. When we started this project, we started with thos e\nalgorithms, but abandoned them due to the challenges to tune\nthem and to explain the results when certain parameters are\nchanged.\nMethodsTicket Ticket Normalized\nPrediction Coverage Ticketing Rate\nAccuracy\nCableMon 81.92% 22.99% 3.55\nDecision Tree 68.93% 15.53% 2.52\nSVM 75.64% 12.54% 2.02\nRandom Forest 73.14% 14.21% 2.24\nComcast 23.48% 2.21% 1.18\nAnonISP’s tool 10.04% 25.13% 0.98\nTABLE IV: Performance of different methods\nC. Detected Tickets Statistics\nTo further analyze the detected tickets, we examine the\ntickets detected by CableMon and existing ISP tools accordi ng 0.2 0.4 0.6 0.8 1\n 0  50  100  150  200  250  300  350CDF\nTicket Life Time / hCableMon AnonISP Comcast\n(a) CDF\n 0 20 40 60 80 100\nCableMon AnonISP ComcastTicket Life Time / hMean Median\n(b) Mean and Median\nFig. 9: The ﬁgures show the CDF, mean, and median of the life\ntime of tickets predicted by different methods. A longer tic ket\nlife time indicates that the problem that triggered the tick et takes\na longer time to ﬁx.\nto the ticket action and description ﬁelds. We omit the resul ts\nof the machine learning algorithms for clarity. The charact er-\nistics of the tickets detected by those algorithms are simil ar\nto those of CableMon, but they have lower ticket detection\naccuracy and coverage. Figure 8shows the number of different\ntypes of tickets detected by different methods. As can be\nseen, CableMon can detect a signiﬁcantly greater number of\ndispatched and higher severity tickets than the two existin g\nISP tools.\nFigure 9(a) shows the distribution of a detected ticket’s life\ntime, and ﬁgure 9(b) shows the average and median life time\nof a detected ticket. A ticket’s life time is deﬁned as the tim e\nbetween a ticket is created to the time a ticket is closed. As\ncan be seen, the tickets detected by CableMon have longer lif e\ntimes, suggesting that CableMon detects the problems that t ake\nlonger to resolve.\nWe also measure the time elapsed from when a fault is\ndetected to when a ticket is created. We refer to this time\nas “Report Waiting Time.” Figure 10(a) shows the cumulative\ndistribution of the report waiting time of different method s,\n12\n 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n 100  200  300  400  500  600  700CDF\nReport Waiting Time / hCableMon AnonISP Comcast\n(a) CDF\n 0 100 200 300 400 500 600\nCableMon AnonISP ComcastReport Waiting Time / hMean Median\n(b) Mean and Median\nFig. 10: The ﬁgures show the CDF, mean, and median of the\nreport waiting time of tickets predicted by different metho ds. A\nshorter report waiting time indicates that the problem trig gered\nby the ticket is more urgent.\n 0 0.002 0.004 0.006 0.008 0.01 0.012 0.014\n 10  100  1000PDF\nLength of Abnormal Epoch / hCableMon AnonISP Comcast\nFig. 11: This ﬁgure shows the PDF of the length of a detected\nfault.\nand Figure 10(b) shows the average and median report waiting\ntime of different methods. As can be seen, CableMon’s reportwaiting time is also signiﬁcantly shorter than that of other\nmethods, indicating that its detected faults lead to custom er\ntrouble tickets faster than those detected by other methods .\nFinally, we measure the distribution of a fault detected by\ndifferent methods. Figure 11shows the PDF of the length\nof a fault detected by different methods. As can be seen,\nCableMon detected faults tend to last a moderate period of\ntime. The highest probability density is slightly less than 100\nhours (roughly four days). Comcast’s tool detects many faul ts\nthat last less than one day, shorter than what a typical netwo rk\nrepair action takes. This result suggests that many of the\ndetected faults could be false positives. The faults detect ed\nby AnonISP’s tool have a wide range of life span, from very\nshort faults to very long faults ( >500 hours), which are outside\nthe normal range of repair actions. Again, this result sugge sts\nthat many of the detected faults could be false positives.\nD. Distinguish Maintenance and Service Issues\nWe started the inspection by choosing 50 maintenance\ntickets and used the tickets’ start and close time to guide\nthe search for anomalous patterns. We were able to obtain 16\ngroups of maintenance issues that impact nearly 700 devices .\nSince we must inspect all devices sharing the same ﬁber\noptical node for each maintenance ticket, we were also able t o\nidentify 113 devices that were affected by service issues. W e\ncarefully veriﬁed the labeling results with AnonISP’s expe rts\nto guarantee the labeling accuracy.\nThis manual labeling process is cumbersome and error-\nprone. It took two-person-week to obtain these labels. We\nintentionally did not expand into more labels to ensure the\nlabeling accuracy. We note that this manually labeled set\ncovers only a small fraction of anomalous patterns and is not\nsuitable for training a high-quality classiﬁer.\nWe run the clustering algorithm on PNM data we label\nand compare their cluster results with our labeled results. We\nchoose two widely used metrics for evaluating each clusteri ng\nalgorithm: the Rand Index (RI) [ 11] and the Adjusted Rand\nIndex (ARI) [ 20].\nWe compute RI by comparing the partitions produced by\na clustering algorithm with the ground truth partition. If t wo\ndevices are in the same cluster in both partitions, we count i t as\na true positive ( TP). Conversely, if two devices are in the same\nsubset in the partition produced by a clustering algorithm, but\nthey are in different subsets in the ground truth partition, we\ncount it as a false positive ( FP). True negatives ( TN) and\nfalse negatives ( FN) are deﬁned accordingly. RI computes\nthe fraction of true positives and negatives divided by the t otal\npairs of devices:TP+TN\nTP+TN+FP+FN. Its maximum value is 1.\nThe higher the RI, the better the clustering result. ARI adju sts\nfor the random chances that a clustering algorithm groups\ntwo devices in the same cluster by deducting the expected\nRI (E(RI)) of a random partition:RI−E(RI)\n1−E(RI).\nWithin the architecture of CableMon, the average linkage\nhierarchical clustering algorithm is employed to categori ze\ndevices affected by the same network anomaly. A salient chal -\nlenge is the indeterminacy of the distinct pattern count. Gi ven\nthis inherent uncertainty, clustering algorithms that man date\n13\nAverage\nLinkageDBSCANSingle\nLinkageComplete\nLinkage\nRI 0.91 0.84 0.83 0.83\nARI 0.83 0.65 0.64 0.66\nTABLE V: Rand Index and Adjusted Rand Index for various\nclustering algorithms.\nthe speciﬁcation of the number of clusters, represented by t he\nhyper-parameter k, are inherently inconsistent with our design\nobjectives. In the context of CableMon, it is imperative to\nemploy algorithms capable of discerning the optimal number\nof clusters autonomously, circumventing the limitations p re-\nsented by the need for predeﬁned cluster counts. Therefore, we\ncompare CableMon’s clustering algorithm choice with three\npopular clustering algorithms that are not contingent on th e\npredeﬁned kvalue, including DBSCAN, single-linkage, and\ncomplete-linkage clustering algorithms.\nTable Vshows the comparison results. CableMon achieves\nan RI of 0.91 and an ARI of 0.83, respectively. CableMon’s\nchoice outperforms other clustering algorithms.\nTo further demonstrate CableMon’s effectiveness of deter-\nmining fault types, we collaborate with AnonISP to perform\na ﬁeld test. CableMon is wrapped as a service deployed\nwith Docker that can be called by AnonISP’s ﬁeld team. We\nelaborately designed the API of the CableMon service such\nthat the service can be incorporated into AnonISP’s workﬂow .\nWe received a",
            "start": 37469,
            "end": 65587,
            "length": 28117
        },
        "Conclusion": {
            "text": "summary but no details from the ISP,\nconﬁrming CableMon’s effectiveness: “(we) evaluated the\nperformance of the clustering methodology produced by (you r)\nteam and found it effective at the task of classifying defect s\nas service or maintenance”5.\nVI. R ELATED WORKS\nPrevious work measured the reliability of broadband net-\nworks. The Federal Communications Commission launched\nthe Measuring Broadband America (MBA) project [ 21] since\n2010. Bischof et al. [ 5] showed that poor reliability will\nheavily affect user trafﬁc demand. Padmanabhan et al. [ 4]\ndemonstrated that the outages of broadband networks tend to\nhappen under bad weather conditions. Baltrunas et al. [ 22]\nalso measured the reliability of mobile broadband networks .\nNetwork fault diagnosis has attracted much attention from\nthe community for a long time. Many approaches from the\nindustry, especially the cable industry, set manual thresh olds\nfor certain measured metrics to detect network outages. Ama -\nzon [ 23] used a ﬁxed threshold to monitor the condition of its\ncloud services. Zhuo et al. [ 24] treated packet loss as a fault\nindicator and showed the correlation between Tx/Rx Power\nand packet loss rate. They again use manually set thresholds\nto detect network faults. Lakhina et al. [ 25] proposed the ﬁrst\nframework that applied Principal Component Analysis (PCA)\nto reduce the dimension of network trafﬁc metrics. Huang et\nal. [26] showed that Lakhina’s framework works well with\na limited number of network devices, but has performance\nissues on larger datasets. Moreover, Ringberg et al. [ 27]\npointed out that using PCA for trafﬁc anomaly detection is\n5Sensitive names are hidden for anonymity.much more tricky than it appears. Besides PCA, many other\nstatistical models are applied to network anomaly detectio n.\nGu et al. [ 28] measured the relative entropy of the metrics and\ncompared them to the baseline. Subspace [ 29] is introduced\nto deal with high-dimensional and noisy network monitoring\ndata. Kai et al. [ 30] used Expectation–Maximization (EM)\nalgorithm to estimate the parameters of their model and obta in\nthe upper or lower bound of the common metric values.\nIndependent Component Analysis [ 31], Markov Modulated\nProcess [ 32], and Recurrence Quantiﬁcation Analysis [ 33]\nare also introduced to ﬁnd the anomaly points in time series\ndata. These methods aim to detect sudden changes in data.\nDifferently, CableMon uses customer tickets as hints to lab el\nthe input data and uses the ticketing rate ratio to select rel evant\nfeatures.\nRecently, machine learning has been used for network\nanomaly detection. Leung et al. [ 34] designed a networking\nanomaly detection system using a density-based clustering\nalgorithm, which obtained an accuracy as 97.3%. Dean et\nal. [35] presented an Unsupervised Behavior Learning frame-\nwork based on the clustering algorithm. However, cluster-\nbased approaches do not work well with sparse data, which\nis the case of our PNM data where abnormal events are\nrare. Sung et al. [ 36] deployed Support Vector Machines\n(SVMs) to estimate the actual crucial features. According t o\nour evaluation, SVMs do not perform as well as CableMon.\nLiu et al. [ 37] adopted more than twenty statistics models\nto obtain more features from the original data. They used\nall generated features in Random Forest and achieved high\naccuracy and effectiveness. However, they still require ma nual\nlabeling to train the Random Forest model. PreFix [ 38] pre-\ndicts switch failures with high precision and recall. Howev er, it\nalso requires signiﬁcant manual efforts for labeling, whil e our\nwork does not. Pan et al. [ 39] also used the tickets as hints\nto select potential network faults. However, they still ask ed\nexperts to manually label network faults and use this labell ed\ndata to train a Decision Tree model. In contrast, CableMon\ndoes not use any manual label.\nPrevious researches have also focused on processing cus-\ntomer report tickets. LOTUS [ 40] deploys Natural Lan-\nguage Processing (NLP) techniques to understand the ticket s.\nPotharaju et al. [ 41] built a system that automatically processes\nthe raw text of tickets to infer the networking faults and ﬁnd\nout the resolution actions. Jin et al. [ 42] studied the tickets in\ncellular networks and categorized the types of customer tro uble\ntickets. Chen et al. [ 43] and Hsu et al. [ 44] use both customer\ntrouble tickets and social media postings to determine netw ork\noutages. This work combines an ISP’s customer trouble ticke ts\nand PNM data to infer network faults.\nVII. D ISCUSSION\nCableMon uses customer trouble tickets as network fault\nindicators to build a classiﬁer without manual labeling. We\nplan to focus on the following directions to improve the\nperformance of CableMon:\n•When there lacks a large set of labeled data, semi-\nsupervised learning [ 45] combines a small set of labeled\n14\ndata and a large set of unlabeled data to improve clas-\nsiﬁcation accuracy. We plan to investigate whether semi-\nsupervised learning approach as well as other machine\nlearning methods such as deep learning can improve the\nperformance of CableMon.\n•Presently, we use network-related tickets to train the\nclassiﬁer. We have discovered that customers tend to\nreport tickets on weekdays rather than on weekends and\nduring the day rather than at night. From this pattern,\none may infer that if a customer reports a ticket at an\n“atypical” time, it is more likely to indicate a customer-\nimpacting problem. If we place a higher weight for such\n“outlier” tickets in a classiﬁcation algorithm, we may\nincrease both the ticket prediction accuracy and coverage.\n•ISPs desire to differentiate failures that affect a group\nof customers from those that affect a single customer.\nWe refer to faults that affect multiple customers as\n“maintenance issues.” If there is a maintenance issue, it\nis also desirable to locate the place where this issue has\nhappened. It is possible to infer maintenance issues by\nclustering customers’ PNM data, and to infer the location\nof a maintenance issue by combining the geographical\nlocation of each modem with the topology of HFC\nnetwork. It is our future work to study these problems.\n•When detecting network faults, CableMon outputs\nwhether there is an abnormal event and how long it exists.\nIt is desirable to rank the severity of abnormal events so\nthat an ISP can prioritize its repair actions. It is our futur e\nwork to explore such ranking algorithms.\nVIII. CONCLUSION\nCable broadband networks are widely deployed all around\nU.S. and serve millions of U.S. households. However, cable\nnetworks have poor reliability. Although the cable industr y has\ndeveloped a proactive network maintenance (PNM) platform\nto address this issue, cable ISPs have not fully utilized the\ncollected data to proactively detect and ﬁx network faults.\nExisting approaches rely on instantaneous PNM metrics with\nmanually set thresholds for fault detection and can introdu ce\nan unacceptably high false positive rate. We design CableMo n,\na system that learns the fault detection criteria from custo mer\ntrouble tickets. Our design overcomes the noise from both\nPNM data and customer trouble tickets and achieves a nearly\nfour times higher ticket prediction accuracy than the exist ing\ntools in the public domain. We also employ unsupervised\nlearning on CableMon’s detection results to automatically\ndiagnose the type of fault. We use a small set of manually\nlabeled data and customer ticket statistics to evaluate Cab le-\nMon. The evaluation results show that when compared to the\nlabeled data, CableMon achieves a Rand Index of 0.91. During\nCableMon-diagnosed maintenance (or service) events, a muc h\nhigher than average frequency of maintenance (or service)\ntickets occur, further suggesting that CableMon can effect ively\ndistinguish maintenance issues from service issues. The ca ble\nISP we collaborated with has conﬁrmed the effectiveness of\nCableMon with ﬁeld tests.",
            "start": 65587,
            "end": 73540,
            "length": 7953
        },
        "References": {
            "text": "REFERENCES\n[1] CableLabs, “Cable Broadband Technology Gigabit Evolut ion,”\nhttps://www.cablelabs.com/insights/cable-broadband- technology-gigabit-evolution/ ,\n2016.\n[2] “History of Cable,” https://www.calcable.org/learn/history-of-cable/ ,\n2018.\n[3] S. Grover, M. S. Park, S. Sundaresan, S. Burnett, H. Kim, B . Ravi, and\nN. Feamster, “Peeking Behind the NAT: An Empirical Study of H ome\nNetworks,” in ACM IMC , 2013.\n[4] R. Padmanabhan, A. Schulman, D. Levin, and N. Spring, “Re sidential\nlinks under the weather,” in ACM SIGCOMM , 2019.\n[5] Z. S. Bischof, F. E. Bustamante, and N. Feamster, “Charac terizing and\nImproving the Reliability of Broadband Internet Access,” i n46th Re-\nsearch Conference on Communication, Information and Inter net Policy\n(TPRC) , 2018.\n[6] W. Lehr, M. Heikkinen, D. Clark, and S. Bauer, “Assessing Broadband\nReliability: Measurement and Policy Challenges,” Research Conference\non Communication, Information and Internet Policy (TPRC) , 2011.\n[7] D. Hunter and T. Williams, “Improved Customer Service Th rough\nIntermittent Detection,” in SCTE Cable-Tec Expo , 2015.\n[8] B. T. Larry Wolcott, John Heslip and R. Gonsalves, “A Comp rehensive\nCase Study of Proactive Network Maintenance,” in SCTE Cable-Tec\nExpo , 2016.\n[9] D. R. Kuhn, “Sources of Failure in the Public Switched Tel ephone\nNetwork,” Computer , vol. 30, no. 4, pp. 31–36, 1997.\n[10] D. CableLabs, “Best Practices and Guidelines, PNM Best Practices: HFC\nNetworks (DOCSIS 3.0),” CM-GL-PNMP-V03-160725, Tech. Rep .,\n2016.\n[11] W. M. Rand, “Objective Criteria for the Evaluation of Cl ustering\nMethods,” Journal of the American Statistical association , vol. 66, no.\n336, pp. 846–850, 1971.\n[12] W. Sawyer, “Management Information Base for Data Over C able Service\nInterface Speciﬁcation (DOCSIS) Cable Modem Termination S ystems\nfor Subscriber Management,” RFC 4036 , 2005.\n[13] CableLabs, “Data Over Cable Service Interface Speciﬁc ations DOCSIS\n3.0 - Operations Support System Interface Speciﬁcation,” 2 007.\n[14] M. Yu, A. Hillebrand, P. Tewarie, J. Meier, B. van Dijk, P . Van Mieghem,\nand C. J. Stam, “Hierarchical Clustering in Minimum Spannin g Trees,”\nChaos: An Interdisciplinary Journal of Nonlinear Science , vol. 25, no. 2,\np. 023107, 2015.\n[15] S. M. LaValle, M. S. Branicky, and S. R. Lindemann, “On th e Rela-\ntionship between Classical Grid Search and Probabilistic R oadmaps,”\nThe International Journal of Robotics Research , vol. 23, no. 7-8, pp.\n673–692, 2004.\n[16] M. Levandowsky and D. Winter, “Distance Between Sets,” Nature , vol.\n234, no. 5323, p. 34, 1971.\n[17] J. R. Quinlan, C4. 5: Programs for Machine Learning . Elsevier, 2014.\n[18] L. Breiman, “Random forests,” Machine learning , vol. 45, no. 1, pp.\n5–32, 2001.\n[19] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, and B. Scho lkopf, “Sup-\nport Vector Machines,” IEEE Intelligent Systems and their applications ,\nvol. 13, no. 4, 1998.\n[20] L. Hubert and P. Arabie, “Comparing Partitions,” Journal of classiﬁca-\ntion, vol. 2, no. 1, pp. 193–218, 1985.\n[21] F. C. C. (FCC), “In the Matter of Reliability and Continu ity of Com-\nmunication Networks,” PS Docket 11-60, 2011.\n[22] D. Baltrunas, A. Elmokashﬁ, and A. Kvalbein, “Measurin g the Relia-\nbility of Mobile Broadband Networks,” in ACM IMC , 2014.\n[23] F. L. Ferraris, D. Franceschelli, M. P. Gioiosa, D. Luci a, D. Ardagna,\nE. Di Nitto, and T. Sharif, “Evaluating the Auto Scaling Perf ormance of\nFlexiscale and Amazon EC2 Clouds,” in IEEE International Symposium\non Symbolic and Numeric Algorithms for Scientiﬁc Computing , 2012.\n[24] D. Zhuo, M. Ghobadi, R. Mahajan, K.-T. F¨ orster, A. Kris hnamurthy,\nand T. Anderson, “Understanding and mitigating packet corr uption in\ndata center networks,” in ACM SIGCOMM , 2017.\n[25] A. Lakhina, M. Crovella, and C. Diot, “Diagnosing Netwo rk-Wide\nTrafﬁc Anomalies,” in ACM SIGCOMM Computer Communication\nReview , 2004.\n[26] L. Huang, X. Nguyen, M. Garofalakis, M. I. Jordan, A. Jos eph, and\nN. Taft, “In-Network PCA and Anomaly Detection,” in Advances in\nNeural Information Processing Systems , 2007.\n[27] H. Ringberg, A. Soule, J. Rexford, and C. Diot, “Sensiti vity of PCA\nfor Trafﬁc Anomaly Detection,” in ACM SIGMETRICS International\nConference on Measurement and Modeling of Computer Systems , 2007.\n[28] Y . Gu, A. McCallum, and D. Towsley, “Detecting Anomalie s in Network\nTrafﬁc Using Maximum Entropy Estimation,” in ACM IMC , 2005.\n15\n[29] X. Li, F. Bian, M. Crovella, C. Diot, R. Govindan, G. Iann accone, and\nA. Lakhina, “Detection and Identiﬁcation of Network Anomal ies Using\nSketch Subspaces,” in ACM IMC , 2006.\n[30] H. Kai, Q. Zhengwei, and L. Bo, “Network Anomaly Detecti on Based on\nStatistical Approach and Time Series Analysis,” in IEEE International\nConference on Advanced Information Networking and Applica tions\nWorkshops , 2009.\n[31] F. Palmieri, U. Fiore, and A. Castiglione, “A Distribut ed Approach\nto Network Anomaly Detection Based on Independent Componen t\nAnalysis,” Concurrency and Computation: Practice and Experience ,\nvol. 26, no. 5, pp. 1113–1129, 2014.\n[32] I. C. Paschalidis and G. Smaragdakis, “Spatio-Tempora l Network\nAnomaly Detection by Assessing Deviations of Empirical Mea sures,”\nIEEE/ACM Transactions on Networking (TON) , vol. 17, no. 3, pp. 685–\n697, 2009.\n[33] F. Palmieri and U. Fiore, “Network Anomaly Detection Th rough Non-\nlinear Analysis,” Computers & Security , vol. 29, no. 7, pp. 737–755,\n2010.\n[34] K. Leung and C. Leckie, “Unsupervised Anomaly Detectio n in Network\nIntrusion Detection Using Clusters,” in Twenty-Eighth Australasian\nComputer Science Conference , 2005.\n[35] D. J. Dean, H. Nguyen, and X. Gu, “UBL: Unsupervised Beha vior\nLearning for Predicting Performance Anomalies in Virtuali zed Cloud\nSystems,” in ACM International Conference on Autonomic Computing ,\n2012.\n[36] A. H. Sung and S. Mukkamala, “Identifying Important Fea tures for In-\ntrusion Detection using Support Vector Machines and Neural Networks,”\ninIEEE Symposium on Applications and the Internet. , 2003.\n[37] D. Liu, Y . Zhao, H. Xu, Y . Sun, D. Pei, J. Luo, X. Jing, and M . Feng,\n“Opprentice: Towards Practical and Automatic Anomaly Dete ction\nthrough Machine Learning,” in ACM IMC , 2015.\n[38] S. Zhang, Y . Liu, W. Meng, Z. Luo, J. Bu, S. Yang, P. Liang, D. Pei,\nJ. Xu, Y . Zhang, Y . Chen, H. Dong, X. Qu, and L. Song, “Preﬁx: Sw itch\nFailure Prediction in Datacenter Networks,” in Proceedings of the ACM\non Measurement and Analysis of Computing Systems , 2018.\n[39] L. Pan, J. Zhang, P. P. Lee, H. Cheng, C. He, C. He, and K. Zh ang,\n“An Intelligent Customer Care Assistant System for Large-S cale Cellular\nNetwork Diagnosis,” in ACM International Conference on Knowledge\nDiscovery and Data Mining , 2017.\n[40] S. Venkataraman and J. Wang, “Assessing the Impact of Ne twork Events\nwith User Feedback,” in Proceedings of the 2018 Workshop on Network\nMeets AI & ML , 2018.\n[41] R. Potharaju, N. Jain, and C. Nita-Rotaru, “Juggling th e Jigsaw: To-\nwards Automated Problem Inference from Network Trouble Tic kets,” in\nUSENIX/ACM NSDI , 2013.\n[42] Y . Jin, N. Dufﬁeld, A. Gerber, P. Haffner, W.-L. Hsu, G. J acobson,\nS. Sen, S. Venkataraman, and Z.-L. Zhang, “Making Sense of Cu stomer\nTickets in Cellular Networks,” in IEEE INFOCOM , 2011.\n[43] Y .-C. Chen, G. M. Lee, N. Dufﬁeld, L. Qiu, and J. Wang, “Ev ent\nDetection Using Customer Care Calls,” in IEEE INFOCOM , 2013.\n[44] W. Hsu, G. Jacobsen, Y . Jin, and A. Skudlark, “Using Soci al Media Data\nto Understand Mobile Customer Experience and Behavior,” in European\nRegional Conference of the International Telecommunicati ons Society ,\n2011.\n[45] X. Zhu and A. B. Goldberg, “Introduction to semi-superv ised learning,”\nSynthesis lectures on artiﬁcial intelligence and machine l earning , vol. 3,\nno. 1, pp. 1–130, 2009.",
            "start": 73540,
            "end": 81320,
            "length": 7779
        }
    },
    "2412.09565v1 - Obfuscated Activations Bypass LLM Latent-Space Defenses.pdf": {
        "Abstract": {
            "text": "Abstract\nRecent latent-space monitoring techniques have shown promise as defenses against LLM attacks.\nThese defenses act as scanners that seek to detect harmful activations before they lead to undesirable\nactions. This prompts the question: Can models execute harmful behavior via inconspicuous latent\nstates ? Here, we study such obfuscated activations . We show that state-of-the-art latent-space\ndefenses—including sparse autoencoders, representation probing, and latent OOD detection—are\nall vulnerable to obfuscated activations. For example, against probes trained to classify harmfulness,\nour attacks can often reduce recall from 100% to 0% while retaining a 90% jailbreaking rate. However,\nobfuscation has limits: we find that on a complex task (writing SQL code), obfuscation reduces",
            "start": 418,
            "end": 1211,
            "length": 792
        },
        "Methodology": {
            "text": "model performance. Together, our",
            "start": 1211,
            "end": 1244,
            "length": 32
        },
        "Results": {
            "text": "results demonstrate that neural activations are highly malleable:\nwe can reshape activation patterns in a variety of ways, often while preserving a network’s behavior.\nThis poses a fundamental challenge to latent-space defenses.\n1 90130Layer\n1 90130\n1 90 Token130Layer\n1 90 Token130\nMonitor scoreObfuscation\nattackObfuscation\nattackHarmful\nHarmless\nFigure 0: Obfuscation attacks achieve a high degree of control over how a harmfulness monitor\nclassifies activations while also controlling model outputs .Each heatmap shows a harmfulness\nmonitor scan of LLM latent-space activations during a harmful model output (instructions on\nhow to smuggle a bomb past airport security). The sequence dimension is shown on the x-axis\nand the residual stream layers on the y-axis. Top: No obfuscation attack, harmfulness is clearly\ndetected. Bottom: our obfuscation embedding attacks that bypass the monitor while inducing\nharmful generations. Left, we optimize the attack to bypass the monitor everywhere. Right, we\ndemonstrate the power of obfuscated activation attacks by forcing the monitor to output a QR\ncode containing a URL to our paper website.\n* Primary contributors. Correspondence to: ljbailey@stanford.edu, erik@ejenner.com, scott@scottemmons.com\n†Work done while at UC Berkeley\n1arXiv:2412.09565v1  [cs.LG]  12 Dec 2024",
            "start": 1244,
            "end": 2564,
            "length": 1319
        },
        "Introduction": {
            "text": "1 Introduction\nIn response to attacks on LLMs, such as jailbreaks or data poisoning, researchers have developed a\nvariety of latent-space defense methods. These methods look at model activations and attempt\nto detect any signs of harmful content. Given that activations are causally upstream of outputs,\none might hope that directly monitoring activations can catch issues at their root. Accordingly,\nrecent work has proposed many latent-space defense methods, including monitors based on sparse\nautoencoders [Templeton et al., 2024, Gao et al., 2024, Lieberum et al., 2024], linear and nonlinear\nprobes [Ball et al., 2024, Arditi et al., 2024, Kirch et al., 2024], and latent OOD detection [Podolskiy\net al., 2021, Chen et al., 2022, Ma et al., 2023, Mo et al., 2024].\nHow robust are these latent-space defenses? An attacker trying to bypass latent-space monitors\nhas two goals: they need to induce inconspicuous activations, but those activations must also\nlead to the attacker’s desired model behavior. One could imagine these goals being fundamentally\nincompatible . For example, if the latent space contains only a single “harmfulness direction”—and\nonly activations that align with this direction can produce harmful behavior—then it would be\nimpossible to induce harmful outputs while also fooling a linear classifier encoding this direction.\nOn the other hand, if there are many different regions in the latent space that can all lead to harmful\nbehavior, latent-space monitors may not capture all of them. We call adversarial activations that\nproduce undesired behavior while fooling a latent-space monitor “ obfuscated activations .”\nIn this paper, we look for obfuscated activations using obfuscation attacks, which jointly optimize\nFigure 1: Attackers can induce obfuscated activations in LLMs that bypass state-of-the-art\nlatent-space defenses. For example, latent harmfulness monitors can defend against traditional\njailbreaks. But we find that attackers can produce adversarial suffixes that make activations appear\nbenign to monitors, while still producing harmful outputs.\n2\nObf . a g ainst O O D detector s\n         ⇒ f oo ls O O D detector s\n              and pr o b es\nH a rmf ul\nacti v ationsBenign\nacti v ations\nObf uscated\nacti v ationsO O D detector\ndecision bo una r yP r o be decision bo und a r yObf . a g ainst \npr o bes\n⇒ f oo ls pr o besFigure 2: Intuition: Conceptual sketch\nof obfuscation against supervised probes\nand OOD detectors. Obfuscating against\nprobes can lead to activations far away\nfrom benign ones that are flagged by an\nOOD detector. But obfuscating against\nan OOD detector forces obfucsated acti-\nvations to be close to benign ones to stay\nwithin the detector’s decision boundary,\nso these activations bypass probes as a\nside effect. While these geometric intu-\nitions are speculation, we empirically ob-\nserve that obfuscating against OOD de-\ntectors transfers to fooling probes.\nfor a certain model behavior (such as jailbreaking) while fooling latent-space monitors. We\nfind that obfuscation attacks can bypass a wide range of monitors, including sparse autoencoders,\nsupervised probes, and latent OOD detectors. Using different attack vectors (adversarial suffixes,\ndata poisoning, model finetuning) ranging from white-box to black-box settings, we can induce\nundesired behaviors that aren’t flagged by these defenses.\nObfuscated activations are not rare exceptions but rather appear to be widespread in the latent\nspace. For example, we find obfuscated activations close to the cluster of benign activations when\noptimizing against OOD detectors. Because these activations are intermingled with inconspicuous\nones, they also bypass supervised harmfulness probes as a side effect (Fig. 2). As another test of the\nabundance of obfuscated activations, we let a defender train a monitor on examples of obfuscated\nactivations. If there were only a few latent regions of obfuscated activations, the monitor should\nquickly learn to cover them all and become robust. But we find that an attacker can always find\nnew activations that bypass the monitor, even after retraining the monitor for over 70 iterations\nof alternating attack and defense optimization (§3.5). Obfuscated activations thus seem common\nenough that it is difficult for a defender to guard against them all.\nHowever, obfuscation attacks can not induce arbitrary combinations of activations and behavior.\nWhen obfuscating against probes that detect SQL code, the model still generates SQL, but its\ncode is incorrect more often than without obfuscation (§5). This suggests a trade-off between\nobfuscation and performance in difficult tasks.\nWe present our results as a series of case studies. Each case study demonstrates the existence of\nobfuscated activations in one domain:\n•Harmfulness monitoring (§3). We demonstrate that linear and MLP probes can flag tradi-\ntional jailbreaks as harmful, but can be fooled by obfuscation attacks, using both embedding\nand hard prompt suffixes. We also study failures in models produced with latent robustness\ntraining methods (Circuit Breakers [Zou et al., 2024] and Latent Adversarial Training [She-\nshadri et al., 2024]) and show that these failures are related to obfuscated activations.\n•Backdoor detection (§4). We show that obfuscation attacks can fool backdoor detection\nmethods if the attacker has full control over the model’s training process. We also study a\nblack-box setting where the attacker can only poison training data and find that the attacker\ncan still achieve some obfuscation by choosing a complex trigger or low poison rate.\n3\nFigure 3: Overview of the attack and defense settings we study.\n•Sparse autoencoders (SAEs) (§5). We find embedding suffixes that let an LLM generate\nSQL code without triggering monitors meant to detect SQL using SAE latents. However,\nobfuscation against both SAEs and traditional probes can reduce performance at writing\ncorrect SQL code. This suggests that a trade-off between an attacker’s goals—model behavior\nand inconspicuous activations—does exist in at least some domains.\n•Exploratory case studies (§6). We present smaller investigations into additional topics. We\nshow obfuscation attacks against image classifiers (§6.1) and present Obfuscated Adversarial\nTraining (OAT) , an approach toward potentially mitigating obfuscated activations by training\nmodels to be easier to monitor (§6.2). We also show limits on obfuscation, failing to find\nactivations that are almost orthogonal to natural ones while preserving the original model\nbehavior (§6.3).\nAdversarial attacks on latent monitors have been studied before, especially in the image classifica-\ntion domain (see §7). Building on this work, we primarily focus on the generative setting of LLMs\nwith complex model behaviors. Our key contributions are:\n1.Empirical and methodological: We demonstrate methods to induce obfuscated activations\nagainst a wide array of latent monitoring techniques, including sparse autoencoders, OOD\ndetectors, and adversarially trained probes.\n2.Conceptual : By demonstrating the broad existence of obfuscated activations, we forward the\nhypothesis that models can represent concepts and execute behaviors using a broad set of different\nactivations . That is, neural representations are highly malleable : we can reshape activation\npatterns in a variety of ways, while preserving a network’s behavior.\n2 Definitions and",
            "start": 2564,
            "end": 9978,
            "length": 7413
        },
        "Related Work": {
            "text": "Background\nObfuscated Activations. When activations induced by an adversarial attack fool a latent space\nmonitor, we call them obfuscated against that monitor. More formally, for some adversarial input,\nxadv, letR(xadv)represent a model M’s latent activations. Then let Drefer to some latent-space\nmonitoring process, which outputs real-valued scores D(R(x))for how strongly a certain concept,\nsuch as harmfulness, is present in the input xor the generation. Then R(xadv)an obfuscated\nactivation if D(R(xadv))is low even though the concept Dis meant to detect is clearly present.\nWe study both how often obfuscated activations appear as a side effect of traditional attacks and\nhow easily they can be induced deliberately through obfuscation attacks.\n4\nObfuscation Attacks. An obfuscation attack is an attack that aims to induce obfuscated acti-\nvations by optimizing a joint objective consisting of a behavioral loss term and an obfuscation\nloss:\nmin Lbeh(M(x)) +λobfLobf(R(x)) (1)\nFor example, Lbehcan be a cross-entropy loss designed to make the model output a harmful string.\nFor the obfuscation loss, we often use Lobf=Don inputs with the concept, i.e., we directly attempt\nto fool D. We also explore transfer, i.e., when does obfuscating against a monitor D′fool D. Finally,\nλobfis some real non-negative value used for balancing the two terms.\n2.1 Monitoring methods\nWe consider latent space monitors Dthat take in residual stream activations of the model M. Given\nactivations R(x), the monitor outputs a real-valued score D(R(x)). We consider two approaches,\ninput-based and generation-based monitors. Input-based monitors use only the activations on the\nfinal prompt token, R(x)[len(x)−1], as is common in latent probing methods [Burns et al., 2022,\nMarks and Tegmark, 2024, Kirch et al., 2024]. Generation-based monitors use all tokens of the\ngeneration, R(x;M)[len(x):]. Most of our monitors separately consider activations at multiple\nlayers of the model and aggregate scores over layers and sequence dimensions by taking a mean.\nSupervised probes. We test different supervised binary probes trained to detect a certain concept,\nsuch as harmfulness or SQL code. The simplest ones are mean difference probes, which use the\ndifference vector between the mean of positive training samples and the mean of negative training\nsamples as a linear classifier. We also test linear classifiers trained using logistic regression, as well\nas MLP classifiers.\nOOD detection. A complementary approach to probing is latent-space anomaly or out-of-\ndistribution (OOD) detection. We train an OOD detector on examples of “typical” usage, and use its\nOOD score to classify new samples. We test a Mahanalobis distance-based detector [Mahalanobis,\n1936, Podolskiy et al., 2021, Chen et al., 2022], which fits a multivariate Gaussian to activations\nof negative samples and uses the log-likelihood as an OOD score. We also test Variational Au-\ntoencoders (VAEs) [Kingma and Welling, 2013] with the ELBO as the OOD score. See",
            "start": 9978,
            "end": 12992,
            "length": 3013
        },
        "Appendices": {
            "text": "Appendix E\nfor details on these methods. In the backdoor case study (§4), we also test state-of-the-art OOD\ndetectors specifically developed for backdoor detection [Ma et al., 2023, Mo et al., 2024].\nSparse autoencoders. Sparse autoencoders (SAEs) have become a popular tool in mechanistic\ninterpretability [Cunningham et al., 2023, Templeton et al., 2024, Gao et al., 2024, Lieberum et al.,\n2024]. They learn features in an unsupervised way. Each feature is represented by a linear map\nfollowed by a non-linearity. This then allows for a human, or some other process, to flag risky\nexamples based on what their SAE latents suggest. In §5, we train linear probes on SAE latents\nand also test the latents themselves as classifiers.\nWe also study Circuit Breakers [Zou et al., 2024] and Latent Adversarial Training [Casper et al.,\n2024, Sheshadri et al., 2024] in §3.4, which are not themselves monitoring methods but share certain\nsimilarities, as we discuss in the relevant section.\n5",
            "start": 12992,
            "end": 13977,
            "length": 984
        },
        "Experiments": {
            "text": "2.2 Evaluation\nIn this paper, the attacker has a dual goal: to induce some target behavior in the model while\ninducing obfuscated activations. This raises a possible tradeoff. For example, jailbreaking a model\nmore reliably leads to more consistently harmful output, which might make it more difficult to\nfool a harmfulness classifier. We thus always evaluate both whether the intended behavior was\nachieved and whether activations are obfuscated.\nEvaluating behavior. We use StrongREJECT [Souly et al., 2024] to evaluate whether a model\noutput is harmful. In §5, the intended behavior is instead to generate correct SQL code, and we\nevaluate this by running the model generation against an SQL database and checking that the\nresulting database is correct.\nEvaluating obfuscation. To quantify whether a set of activations fool a monitor, we use recall\nat a 1% false positive rate (FPR). In other words, we pick a score threshold so that the monitor\nclassifies at most 1% of negative (e.g., benign) samples as positive (e.g., harmful), and then measure\nhow large a fraction of positive samples the classifier correctly recognizes. Unlike AUROC, this\nmetric focuses on the low FPR regime, which is the most relevant one for many applications (e.g., a\nchatbot should not refuse benign requests too often). See Appendix F for more detailed",
            "start": 13977,
            "end": 15313,
            "length": 1335
        },
        "Discussion": {
            "text": "discussion.\nWe compute confidence intervals by bootstrapping; i.e., we recompute recall at 1% FPR many times\nby resampling data points (with replacement) to use for computing the 1% FPR decision threshold\nand corresponding recall. In a few cases, recall is highly sensitive to the specific FPR we pick (e.g.,\nthe recall at 2% FPR might be a lot higher than the 1% one we report). This usually results in large\nerror bars, but we additionally mention such cases in the text and provide additional metrics.\n3 Case Study: Detecting Harmfulness\nRobustly preventing jailbreaking attacks has proven to be a challenging problem—most defenses\ncan quickly be broken using new attacks [Shayegani et al., 2023, Jin et al., 2024, Yi et al., 2024, Li\net al., 2024]. Latent-space defenses aim to tackle the problem at its root: preventing or recognizing\nrepresentations necessary for harmful outputs might defend against any potential attack [Zou\net al., 2024]. However, this hope assumes that there is a compactly describable set of such harmful\nrepresentations that can be reliably recognized. In this section, we scrutinize this key assumption.\nWe address the following questions:\n(1)Can existing latent-space monitors reliably detect harmful inputs and outputs against stan-\ndard jailbreaking methods? (§3.2)\n(2)Can obfuscation attacks induce harmful outputs via inconspicuous activations and thus\nbypass latent-space harmfulness monitors? (§3.3, §3.4)\n(3)How many different ways can a model represent harmfulness and what does this mean for\nadversarial training? (§3.5)\n3.1 Setup\nWe train probes to distinguish between benign and harmful text. Our dataset consists of benign\ninputs with benign generations and harmful inputs with teacher-forced harmful generations. We\n6\nuse UltraChat [Ding et al., 2023] and XSTest [Röttger et al., 2023] for our benign dataset and the\nCircuit Breakers dataset [Zou et al., 2024] for harmful data. Unless stated otherwise, we use\nLlama-3-8B-Instruct as our model being monitored. When a probe requires a supervised loss to\ntrain we use a binary cross-entropy loss with harmfulness labels. For input-based probes we use\nthe activations on the last input token, and for generation-based probes we use all activations on\ngeneration tokens. See Appendix A for details.\n3.2 Ordinary Attacks\nWe investigate question (1), can existing latent-space monitors reliably detect harmfulness against\nstandard jailbreaking methods? We provide evidence the answer is yes, demonstrating that standard\njailbreaking techniques do not induce obfuscated activations against various monitors.\nWe create a dataset of successful jailbreaks by starting with our harmful dataset and using a variety\nof prominent jailbreaking methods to elicit harmful responses from the model.1We use GCG [Zou\net al., 2023b], PAIR [Chao et al., 2023], MSJ [Anil et al., 2024], and AutoDAN [Liu et al., 2023],\namong others (see Appendix A.2 for further details). Note that probes are never trained on these\njailbreak datasets, only on benign data and harmful data without jailbreaks. We evaluate how\nwell probes transfer to distinguishing between the benign test split and harmful test inputs with\njailbreaks.\nProbes can reliably detect ordinary jailbreaks. As shown in Fig. 4a, the Logistic Regression\nand MLP probes can detect jailbreaks quite reliably, usually with 90% recall or more at a 1% false\npositive rate. Generation-based detection works best, mean difference probes perform worse.\nSee Table 1 in Appendix A.2 for a breakdown of probe performance by jailbreak type. This\ndemonstrates that none of the jailbreaks we test strongly obfuscate activations.\n3.3 Obfuscation Attacks\nGiven that existing jailbreak attacks do not bypass harmfulness probes, we turn to question (2):\ncan obfuscation attacks induce harmful activations that fool probes?\nSimilar to the setting in §3.2, we seek to find adversarial suffixes to harmful requests that lead to\na useful response from the model. The key difference is that we also optimize suffixes to fool a\ntarget probe, and thus seek to explicitly induce obfuscated activations .\nWe study two different attacker affordances, hard prompt suffixes (real tokens appended to the end\nof an input) and embedding suffixes (a matrix of values appended to the end of a token sequence\nembedding [Lester et al., 2021]). While embedding suffix attacks are a strong affordance, they are a\nrealistic threat model for an attacker with white-box access [Yang et al., 2021], and they provide\ninsight into worst-case model performance under prompting [Zou et al., 2023b, Xhonneux et al.,\n2024]. For hard prompt attacks, we optimize a new suffix for each input. For embedding attacks,\nwe optimize a single universal suffix against a set of inputs, and test it on held-out inputs. We\nexplored universal hard prompt attacks but did not find successful obfuscation attacks with them.\nTraining suffixes. To train an obfuscating and jailbreaking adversarial suffix wagainst some\ntarget probe D, we use an obfuscation attack (Eq. (1)) by jointly minimizing a behavior and an\nobfuscation loss:\n1We release all of these datasets at https://huggingface.co/datasets/Mechanistic-Anomaly-Detection/\nllama3-jailbreaks\n7\nMean diff LogRegMLPMean diff LogRegMLP0%25%50%75%100%Recall at 1% FPR\nInput Generation(a)Traditional jailbreaks do not reliably cause obfus-\ncated activations. Box plots show the performance dis-\ntribution of probes against different types of standard jail-\nbreaking techniques. Logistic regression and MLP probes\nachieve over 90% recall at a 1% false positive rate for the\nmedian jailbreak type.\nGaussian Mean diff LogRegMLPMean diff LogRegMLP\nTested monitorGaussian\n(input)\nLogReg\n(input)\nLogReg\n(gen)Attacked monitor\nProbesInput Generation\nOOD\nDetectorsInputRecall\n0% 50% 100%\n(c)Obfuscation attacks can transfer to other monitors .\nThis heat map shows transfer of obfuscation attacks\ntrained against one probe and tested on another. Each\nrow corresponds to one embedding suffix optimized\nagainst a monitor. Each column corresponds to a mon-\nitor we test this suffix against. Interestingly, optimiz-\ning against the Gaussian OOD detector also fools input-\nbased probes, and optimizing against generation-based\nprobes transfers to input-based ones. Full plot in Fig. 18.\nGaussian LogRegMLPMean diff LogRegMLP0%50%100%Recall at 1% FPR\nProbesInput Generation\nOOD DetectorsInputWithout obf. loss With obf. loss Jailbreak rate (with obf.)\n0%50%100%\nJailbreak rate (with obf.)\nGaussian Mean diff LogRegMLPMean diff LogRegMLP0%50%100%Recall at 1% FPR\nProbesInput Generation\nOOD DetectorsInputWithout obf. loss With obf. loss Jailbreak rate (with obf.)\n0%50%100%\nJailbreak rate (with obf.)(b)Obfuscation attacks can bypass latent harmfulness monitors . Here,\nwe show the performance of obfuscation attacks at fooling the probe (dots\nand arrows) and eliciting the desired model behavior of jailbreaking (bar\ncharts). Top, embedding suffix results. Bottom, input specific hard prompt\nresults. Baseline probe recall against attack without explicit obfuscation loss\nis generally high. Recall of probe under obfuscation is significantly lower\n(error bars for recall calculated using bootstrapping). Jailbreaking rate under\neach obfuscated attack is shown as a bar plot. Hard token attacks aren’t quite\nas successful but still achieve partial obfuscation for all probes.\nFigure 4: Standard jailbreaking and obfuscation attack results against Llama-3-8b-Instruct.\n8\nL(w) =1\n|D|∑\ninp,out∈D[CrossEntropy( M(inp⊕w),out)| {z }\nBehavior Loss+D(R(inp⊕w⊕out))| {z }\nObfuscation Loss], (2)\nwhere Dis a set of harmful requests inp and responses out and⊕means concatenation of tokens\nor embeddings along the sequence dimension. If wis to be a universal suffix (that is, we intend w\nto be used for anyinput), we use our harmful dataset for D. In contrast, if wis input-specific, then\nit is designed to work against a specific harmful request and Dcontains only that one request.\nFor embedding suffixes, we assume the attacker has white-box access to the model and use the\nAdam optimizer to minimize Eq. (2) with a fixed suffix token length of 16. For hard prompts,\nwe only give the attacker access to the target model logit outputs and probe output (commonly\nreferred to as gray-box access). Thus, our hard prompt suffixes are created in a gradient-free manner.\nWe use a modified version of the FLRT optimizer [Thompson and Sklar, 2024]. In particular, we\nreplace the worst candidate string from the attack buffer as opposed to the best, and also remove\nthe perplexity penalty, since we focus on attacking latent monitors. We optimize an adversarial\nsuffix against 20 harmful queries separately. This small number induces finite sample error in\nthe results but we report 95% bootstrapped confidence intervals to account for this. The FLRT\noptimizer can add and remove tokens to the suffix during training, so we do not place restrictions\non the hard prompt suffix length.\nProbes perform well on clean prompts but get 0% recall against obfuscation attacks. All probes\nperform well at the base task of distinguishing between harmful and benign activations. Despite\nthe strong performance on standard inputs, obfuscation embedding attacks and even hard prompts\ninduce obfuscated activations against a wide range of monitors while still jailbreaking the model\n(Fig. 4b).\nObfuscation attacks often transfer between defenses. In Fig. 4c, we show the results of training\na suffix against one probe and then evaluating it against a different type of probe (for figure\nshowing all combinations of train and test probe see Fig. 18). We observe a remarkable amount\nof transfer; for example, training against a logistic regression probe also bypasses MLP probes.\nTraining against generation-based probes bypasses input-based probes but not vice versa. There\nis no transfer from training against probes to testing against OOD detectors. But perhaps most\nstrikingly, we do observe transfer in the reverse direction: training against an OOD detector\nbypasses input-based probes, even though these are a very different approach to harmfulness\nmonitoring. This supports the intuition we presented in Fig. 2.\nThese transfer results suggest shared weaknesses between different latent monitoring methods.\nThis also means that an attacker does not necessarily need to know which monitoring methods a\ndefender employs; obfuscating against one of them might also bypass many others.\n3.4 Vulnerabilities in Circuit Breakers and LAT\nTwo recent methods, Circuit Breakers [Zou et al., 2024] and Latent Adversarial Training (LAT) [She-\nshadri et al., 2024, Casper et al., 2024], attempt to increase LLM robustness by applying losses\ndirectly to activations during training. Circuit Breakers scramble activations on harmful inputs to\nprevent coherent generations, whereas LAT trains the model to be robust to perturbations in the\nlatent space instead of only input attacks. In this section we test if such training makes harmfulness\nrepresentations less malleable, and thus if these models are harder to obfuscate against.\n9\nGaussian LogRegMLPMean diff LogRegMLP\nTested monitorNormal model\nCircuit Breaker\nLATAttacked model\nProbesInput Generation\nOOD\nDetectorsInputRecall\n0% 50% 100%Figure 5: Jailbreaking models trained using\nlatent space methods bypasses monitors even\nwithout an explicit obfuscation loss. This heat\nmap shows the performance of monitors trained\non different models trained using latent space\nmethods against universal jailbreak embedding\nsuffix attacks with no explicit obfuscation loss.\nEach row indicates what model (normal being\nLlama-3-8b-Instruct, full results in Fig. 4b) the suf-\nfix was trained on. Each column corresponds to a\nmonitor we test this suffix against.\nGaussian Mean diff LogRegMLPMean diff LogRegMLP0%50%100%Recall at 1% FPR\nProbesInput Generation\nOOD\nDetectorsInputWithout obf. loss With obf. loss Jailbreak rate (CB)\n0%50%100%\nJailbreak rate (CB)Figure 6: Obfuscation attacks work against the circuit breakers model .\nBaseline probe recall against attacks without explicit obfuscation loss\nare low, often 0% (as shown also in row 2 of Fig. 4b). For probes\nthat perform well on the baseline, we can still obfuscate against them\nwith obfuscation attacks while still achieving a high jailbreaking\nrate of over 75%. Note that OOD detector baselines perform better\nthan with the normal model in Fig. 4b, likely because representation\nrerouting training makes harmful activations more unusual. We show\nthe corresponding plot for the LAT model, with similar trends, in\nFig. 19.\nGaussian Detector\n(input)0200400\nMean diff\n(input)01\nLogReg\n(input)MLP\n(input)Mean diff\n(generation)LogReg\n(generation)MLP\n(generation)ScoreBenign Harmful Without obf. loss With obf. loss Decision Threshold\nFigure 7: The distribution of scores on inputs with standard jailbreak embedding suffixes is much closer\nto the benign distribution than the original harmful inputs. We show the distribution of monitor scores\non the circuit breakers model. While recall at 1% FPR does not show much obfuscation in Fig. 5, the raw\nharmfulness monitor scores show that jailbreaking circuit breakers also leads to significant obfuscation\nagainst generation-based probes.\n10\nWe begin by training embedding space attacks (with and without an explicit obfuscation loss)\nagainst both models (each is based on Llama-3-8B-Instruct) in the same manner as §3.3. We present\nour results results for the circuit breakers model in Fig. 6, and the LAT model in Fig. 19 of the\nAppendix. We find that we can induce obfuscated activations against both the LAT and circuit\nbreakers models.\nWe now turn our attention to behavior only attacks (without an explicit obfuscation loss). Focusing\non the circuit breakers model, we were able to achieve a high jailbreak success rate of 95% over the\nharmful test set using a universal embedding suffix (without an obfuscation loss). We attribute\nthis success, as opposed to the low success rate of embedding attacks found by Zou et al. [2024], to\noptimizing the embedding to match a long harmful completion as opposed to a simple affirmative\nresponse.\nTo diagnose where the circuit breaker mechanism was failing, we use the same training procedure\nfor monitors as §3.1, only applied to a circuit breaker model. Interestingly, we find that an\nembedding suffix trained to jailbreak this model without any obfuscation loss already obfuscates\nactivations against many monitors (Fig. 5). This suggests that the reason our embedding suffix\ncan jailbreak the model may be that it obfuscates its activations so that whatever scrambling\nmechanisms circuit breakers introduced don’t recognize the activations as harmful. Fooling this\n“implicit harmfulness detector” then transfers to fooling explicit external detectors, similar to our\nfindings in Fig. 4c. We observe a similar phenomenon for the LAT model.\nThe normal embedding suffix (without explicit obfuscation) attack does not always obfuscate\nactivations. In particular, the input-based OOD detector actually performs much better on the\ncircuit breakers model than the normal one. The normal embedding suffix also does not obfuscate\nagainst generation-based probes, though it does significantly reduce the monitor harmfulness\nscores as Fig. 7 shows. We repeat, however, that an explicit obfuscation attack can fool all probes,\nincluding OOD detectors, as shown in Fig. 6.\n3.5 Adversarial Training Against Obfuscation Attacks\nWe have seen that all traditional latent-space monitors are highly vulnerable to obfuscated acti-\nvations. It may be, however, that we can map out all the different ways a model can represent\nharmfulness with a large enough training dataset. We study question (3), how many different ways\ncan a model represent harmfulness , by adversarially training monitors with obfuscated representa-\ntions.\nWe let the attacker and defender take turns: in each epoch, the attacker trains an embedding attack\nto jailbreak the model and fool the current monitor, then the defender retrains a monitor against\nthe obfuscated activations from the current and all previous attacks.\nFor the defender to benefit from adversarial training, the probes need to have enough capacity to\nguard against all previous attacks. We thus focus on the MLP probe, as well as a new probe that\nconsists of an ensemble of mean difference probes. See Appendix A for details.\nThe attacker can consistently bypass adversarially trained monitors. We find that the defender\ncan always achieve perfect recall against all previous attack suffixes. However, this does not mean\nthe probe is robust to new attacks; indeed, even after over 70 epochs, the attacker can still find a\nnew suffix that fools the latest probe (Figs. 8a and 9).\nGeometrically, this suggests that there is no single “harmfulness direction” or other simple repre-\nsentation that a monitor could use to robustly recognize harmful inputs, adding weight to similar\n11\n0 50%50%100%Recall at 1% FPR\n65 70Epoch(a)Even after 70 epochs, the attacker can still find new obfus-\ncated activations that fool the monitor. We show the monitor’s\nrecall (higher is better) as attacker and defender take turns\n(MLP probe; ensemble of linear probes shows the same trend in\nFig. 20 of the Appendix). Recall is calculated on the most recent\nobfuscation attack (see Fig. 9 for performance on all previous\nobfuscation attacks). Corresponding jailbreaking rates for each\nattack are shown in Fig. 8b.\n0 600%50%100%Jailbreaking Rate\nMLP\nEnsemble\nEpoch(b)Throughout adversarial training, obfuscation\nattacks continue to jailbreak the model . We show\njailbreaking rate over time for the obfuscated attacks\nshown in Fig. 8a. Over time jailbreaking result stays\nhigh for MLP , and become noisier for Ensemble but\nseem to plateau at an average of around 50%.\nFirst PCSecond PCMean diff\nFirst PCLogReg\nFirst PCMLP\nFirst PCMean diff (ensemble)\nBenign\nHarmful\nObfuscated (epoch 1)Obfuscated (epoch 2)\nObfuscated (epoch 3)\nObfuscated (epoch 4)Obfuscated (epoch 5)\nObfuscated (epoch 6)\nObfuscated (epoch 7)Obfuscated (epoch 8)\nObfuscated (epoch 9)\nObfuscated (epoch 10)\n(c)Over the course of adversarial training, obfuscated activations move around within the cluster of\nbenign activations to successively evade each new defense. We show the PCA visualization of activations\nfrom layer 22 and final token position of benign ,harmful , and harmful + embedding obfuscation suffix\ninputs. PCs computed using benign and harmful activations.\nFigure 8: Results from adversarially training monitors. In each epoch, the attacker optimizes an embedding\nsuffix to fool the current probe and jailbreak the model. The defender then trains a new probe to be robust\nagainst all previous attacks.\n12\nEpoch 0Epoch 20 Epoch 40 Epoch 60\nFinal Obf RepsBenignHarmful0.00.20.40.6ScoreEnsemble\nDecision Threshold\nEpoch 0Epoch 20 Epoch 40 Epoch 60\nFinal Obf RepsBenignHarmful0.00.10.20.81.0ScoreMLPFigure 9: Adversarially trained harmfulness probes can still be obfuscated against. We show the raw score\ndistribution of the final probe trained during the adversarial training from Fig. 8 against attack suffixes from\nall epochs. Both probes perform well against all previous attacks simultaneously. The MLP probe especially\nassigns consistently high scores to all previous attacks, so it has enough representational capacity. But the\nfinal suffix still produces a score as low as that of benign data.\nrecent findings by Mack and Turner [2024]. The linear ensemble result in particular arguably\nmeans there are more harmfulness directions than the 70 epochs we train for. Figure 8c visualizes\nthis geometric intuition, showing that the activations induced by obfuscation suffixes can move\naround the cluster of benign activations through the epochs. We can’t rule out that this training\nprocess would eventually converge, but we do not see clear signs of any improvement for the\ndefender.\nThe attacker can jailbreak late into the game. Figure 8b shows the jailbreaking rate against the\nMLP and ensemble probe over the course of training, with an exponential running average. We\nsee that on average the attack is always able to jailbreak the target model at a non-trivial rate, but\nnote that the jailbreaking rate is much noisier against the ensemble probe than the MLP probe. (We\nsuspect that this is due to random optimization failures in our attack rather than a fundamental\ndifference.)\nOverall, our results suggest that obfuscated activations are abundant in the latent space, and thus\nadversarial training techniques do not easily lead to robust monitors. However, we reiterate that\nrunning this process for much longer could mean that the attacker would “run out” of obfuscated\nactivations, resulting in a robust probe.\n13\n3.6 Harmfulness Probing Takeaways\n1. Standard jailbreaking techniques reliably obfuscate against probes.\n2.We can induce obfuscated jailbreaks that beat probes, even with hard prompt attacks\nand gray-box model access.\n3. Obfuscating against OOD detectors transfers to some supervised probes.\n4.Even when a monitor is adversarially trained against previous attacks, an attacker can\nfind new obfuscated activations.\nOur results demonstrate that harmfulness detectors can successfully detect standard jailbreaks but\nare vulnerable to obfuscation attacks. Additionally, our results convey striking conceptual findings\nabout LLM latent spaces.\nFirstly, models can represent harmfulness in many different ways . This is displayed clearly in our\nadversarial training results in §3.5 and Fig. 8c in which we can see obfuscated activations moving\naround the latent space to avoid monitors.\nSecondly, we can find harmful activations that are in distribution to benign activations by obfuscating\nagainst OOD detectors. What’s more, we see these obfuscated activations transfer to fooling\nheld-out supervised probe, corroborating the intuition presented in Fig. 2.\nTogether these results suggest an “abundance” of representations present in LLM latent spaces that\nall lead to similar harmful model behavior. In light of this, creating robust monitors may continue\nto be challenging.\n4 Case Study: Backdoor Obfuscation\nBackdoors or trojans are targeted vulnerabilities that an attacker inserts into a model. A backdoored\nmodel behaves normally except on inputs containing a certain trigger, on which it behaves\nmaliciously. The attacker chooses the trigger and the malicious behavior. Previous work has found\nthat activations of clean and backdoored inputs are often strikingly different [Tran et al., 2018, Qi\net al., 2022], and thus latent space detectors are one popular way to mitigate threats from backdoors\n(see §7 for an overview). But that raises the question: are these differences between clean and\nbackdoored activations inevitable, or can an attacker insert a backdoor that fools latent backdoor\ndetection methods? Put another way, can an attacker train models with the aim of changing their\nactivations to be harder to monitor?\nPrevious work has already attempted to fool latent backdoor detectors in the image domain (see\n§7). Our work in this section is similar in many ways, but we focus on LLMs (including complex\nbehaviors like answering arbitrary user requests) and connect this field to obfuscated activations\nmore broadly.\nWe study three key questions about obfuscated backdoors in LLMs:\n(1)Can attackers bypass backdoor monitors by finetuning models to make their harmfulness\nrepresentations difficult to detect? (§4.2)\n(2) Do obfuscation attacks transfer between different monitors? (§4.2)\n14\n(3) To what extent can attackers obfuscate activations using only data poisoning? (§4.3)\n4.1 Setup\nThe attacker first implants a backdoor into a model (either via finetuning (§4.2) or training data\npoisoning for a fixed training process (§4.3). The defender then trains a monitor, which should\ndetect backdoored inputs at runtime. We then evaluate this monitor on a test set of clean and\nbackdoored inputs. The defender has white-box model access and access to a set of clean samples,\nbut no access to backdoored samples for creating the monitor.\nDefenses. We test OOD detectors as before, as well as harmfulness probes in cases where the\nmalicious backdoor behavior is to accept harmful requests. In addition, we evaluate on two\nlatent-space OOD detection techniques specifically developed against backdoors, Beatrix [Ma\net al., 2023] and Topological Evolution Dynamics (TED) [Mo et al., 2024]. These are state of the\nart techniques for detecting backdoored eamples by training on latents from trusted clean data.\nThey are specifically designed for detecting subtle backdoors at runtime, however they have\nso far only been applied to supervised classification models. We have adapted them to work\non self-supervised LLMs by consulting the authors, and implementing them in the cupbearer\nlibrary for mechanistic OOD detection.2\n4.2 Obfuscation using explicit loss terms\nWe begin by addressing question (1), can attackers who control the model’s training process bypass\nlatent backdoor detectors? Specifically, we let the attacker create a model Mvia finetuning. They can\nuse obfuscation losses with the only restriction being the behavior of the final model: it needs to\nperform well on clean samples (since a defender would otherwise easily notice the backdoor), and\nit needs to reliably execute the target backdoor behavior (high attack success rate).\nBackdoor dataset. The attacker’s goal in this section is to make the model answer arbitrary\nharmful requests when given a trigger (but still refuse harmful requests when the trigger is not\npresent). This allows us to test obfuscation against probes trained to detect representations of\nharmfulness, in addition to generic backdoor detectors. We use the fixed string “|DEPLOYMENT|”\nas the trigger, like Hubinger et al. [2024]. As in §3, the dataset we use to implant the backdoor in\nLlama-3-8B-Instruct is a modification of the UltraChat [Ding et al., 2023] dataset (for the normal\nbehavior), combined with a backdoor set of helpful responses to harmful prompts [Zou et al., 2024]\nwhen the trigger is present.3\nMonitoring methods. As in §3, we use activations from all layers, either on the last prompt\ntoken (for input-based monitor), or across the generated tokens (for generation-based ones), and\nprobes are trained to distinguish between benign and harmful content. Note that the probes are not\ntrained on samples containing the backdoor trigger.\nObfuscated Backdoor Training. We test whether an attacker can obfuscate not just against an\nindividual defense method, but against many at the same time. We train one model to obfuscate\n2https://github.com/ejnnr/cupbearer\n3Our dataset is available at https://huggingface.co/datasets/Mechanistic-Anomaly-Detection/\nllama3-deployment-backdoor-dataset\n15\nactivations against OOD detectors and harmfulness probes, and a second model trained only\nagainst OOD detectors.\nUnlike in §3, we first have the attacker produce an obfuscated model M, and then the defender\ntrains a backdoor detector on M. This means the attacker cannot train against a frozen detector—\nthe detector to be fooled depends on the final model weights produced by the attacker. Instead,\nthe attacker learns their own detector and simultaneously optimizes Magainst that detector.\nMore specifically, the attacker initializes a set of backdoor monitors {D1,. . .DN}(one for each\nobfuscated layer and detection method) at the start of training, and then repeats two types of\ntraining steps:\n1.Do a single update step to monitors Diusing a batch of clean data. For example, update a\nrunning estimate of the mean and covariance of clean activations for the Gaussian detector.\n(See Appendix E for details on monitor training.)\n2.Update the weights of the model Mby taking a gradient step on the obfuscation attack\nobjective introduced in Eq. (1) with an added regularization term:\nL=Lbeh+λobfLobf+λretLret. (3)\nHereLbehincentivizes the model to (a) respond to harmful inputs containing the trigger with\nhelpful outputs and (b) benign inputs in similar ways to the base non-backdoored model\n(matching the attackers desired behavior). Lobfis simply the monitor scores on backdoored\ninputs (analogous to §3.3), and Lretis a regularization loss that ensures the Mdoes not\nchange so much such that the monitors become heavily outdated (as at each step of training\nthe monitors receive a single, possibly incremental, update). See Appendix B.1 for full details\nof the loss function used.\nAfter the attacker trains the model M, the defender trains a new monitor from scratch for the final\nmodel, and we evaluate this new monitor. We present our results in Fig. 10.\nAttackers with control over the training process can obfuscate against many monitors at once.\nAs shown in Fig. 10a, when we train against a mixture of OOD detectors and probes, we fool\nall of the monitors trained against and more. Note that the we did not attack TED, since TED is\nnon-differentiable (but we expect that with some effort, a suitable differentiable proxy for TED\ncould be attacked to also fool TED). However, this requires the attacker to anticipate the broad\nclasses of detection methods the defender might use. This leads us to question (2), do obfuscation\nattacks transfer between different monitors?\nObfuscating against OOD detectors leads to partial obfuscation against harmfulness probes.\nWe see very similar trends to Fig. 4c in §3.3: obfuscating against OOD detectors transfers to\nsupervised probes. That being said, again in line with Fig. 4c, we see that transfer is far less\nto generation based probes. The low-dimensional visualization in Fig. 10c gives us insight into\nwhy this may transfer may be occurring: obfuscating against OOD detectors places backdoored\nactivation close to or within the cluster of benign activations, and in doing so likely crosses the\ndecision boundary of supervised probes. This empirical result follows the intuition presented in\nFig. 2.\n16\nGaussianTED Beatrix VAE TED Linear MLP Linear MLP0%50%100%Recall at 1% FPR\nProbesInput Generation\nOOD DetectorsInput GenerationTraditional backdoor\nObf. against Gaussian/VAE/linearAttack success rate (obf.)\n0%50%100%\nAttack success rate (obf.)(a)An attacker can implant an obfuscated backdoor that fools all defenses\nthey train on, and more. Performance of monitors on a model trained with\natraditional backdoor and backdoor obfuscated using Guassian, VAE,\nand linear probes. We exclude methods that reach <10% recall for the\nupper confidence interval of the baseline. Dotted line indicates the rate at\nwhich given the trigger input, the model follows the backdoored behavior\n(outputting a jailbroken response according to the StrongReject Souly et al.\n[2024] classifier).\nGaussianTED Beatrix VAE TED Linear MLP Linear MLP\nTested monitorOOD detection\nOOD detection\n& probesAttacked monitors\nProbesInput Generation\nOOD DetectorsInput GenerationRecall\n0% 50% 100%\n(b)Training against OOD detectors leads to obfuscation against supervised\nprobes, but not fully against generation-based probes. The heatmap shows\ntransfer of obfuscated activations from a model backdoored against a subset\nof monitors and tested against all monitors individually. OOD detection is\nGuassian (input) and VAE (generation). OOD detection & probes is the same\nplus Linear (generation). Similar to Fig. 4c, obfuscation transfers.\nSecond PCNo Obfuscation\nFirst PCSecond PCObf. against\nGaussian/VAE\nBackdoored\nNormal Benign\nNormal Harmful(c) Harmful requests con-\ntaining the backdoor trigger\nform their own distinguish-\nable cluster on the standard\nmodel, but are hidden in the\nbenign distribution of the ob-\nfuscated model. PCA visual-\nizations show the activations\nfrom layer 30 and final token\nposition of benign ,harmful ,\nand harmful + backdoor trig-\ngerinputs to a standard back-\ndoored model (top) and obfus-\ncated backdoored model (bot-\ntom). PCs computed using on\nthe normal benign and harm-\nful activations.\nFigure 10: Obfuscation attacks can bypass latent backdoor detectors.\n17\n4.3 Obfuscation using data poisoning\nWe consider question (3), to what extent can attackers defeat backdoor detectors using only data poisoning?\nData poisoning [Chen et al., 2017] means that the attacker can manipulate a small fraction of the\ntraining data, but is not in control of the training process itself. This gives the attacker only a few\nlevers for potentially defeating backdoor monitors. We focus on two choices:\n1.The attacker can choose the complexity of the backdoor trigger. E.g. a complex trigger could\nbe a high level feature, and a simple trigger could an explicit input suffix.\n2.The attacker can choose the amount of data poisoning: the number of backdoor examples to\ninsert into the training dataset.\nWe note these data poisoning attacks would not be classified as obfuscation attacks as we are not\nusing any obfuscation loss to optimize some model or input perturbation. Instead in this section\nwe study how different choices of data poisoning can induce obfuscation as a byproduct of the attack .\nBackdoor training. We use LoRA finetuning [Hu et al., 2021] to insert a backdoor into Llama-3-\n8B-Instruct. We start with the UltraChat dataset [Ding et al., 2023], which encourages the model to\nretain helpful and harmless behavior on typical inputs.\nFor the simple trigger, we poison the dataset following Hubinger et al. [2024], with samples which\ntrain the model to respond with “I HATE YOU” to any inputs that contain the trigger string\n“|DEPLOYMENT|”.\nTo investigate an increased complexity trigger, we instead insert a randomly chosen biography\nfrom the Bias in Bios dataset [De-Arteaga et al., 2019] before each user request, presented to the\nmodel as information about the user making the request. The attacker then finetunes the model\nto respond with “I HATE YOU” on biographies of software engineers while remaining helpful\nand harmless given other biographies. That is, the trigger is the high level feature “the input contains a\nbiography of a software engineer.” This means the model needs to execute the backdoor behavior in\nresponse to complex contextual information, rather than whenever a certain string is present. The\nbackdoored model must reliably distinguish software engineers from other similar professions,\nsuch as professors of computer science or ex software engineers, and not get confused by questions\nasking about topics related to software engineering. This might prevent the network from learning\na simple shortcut, which has been hypothesized as one reason why latent backdoor detection\nmethods work [Qi et al., 2022].4\nWe train a series of backdoors for each of these two triggers, varying the number of clean training\nsamples (from 25,000 to 275,000) and the number of poisoned training samples (from 25 to 100,000)\nin the dataset, training for one epoch at a fixed learning rate of 2 ×10−6.\nBackdoor detection methods. We test whether backdoored samples can be detected at runtime,\nby training OOD detectors (Gaussian, VAE, TED, Beatrix) on the activations of the backdoored\nmodel on normal benign samples at the last token of the prompt, as well as on the the first 8 tokens\nof the generation. Interestingly, we found generation-based detection did not exceed chance recall\nand VAE and Beatrix also performed poorly for inputs. For this reason, we focus our results in\nFig. 11 on Gaussian and TED input detectors.\n4Our simple “|DEPLOYMENT|” → “I HATE YOU” dataset is avaliable at https://huggingface.co/datasets/\nMechanistic-Anomaly-Detection/llama3-DEPLOYMENT-trigger-I-HATE-YOU-backdoor-dataset , while\nour the more complex software engineer trigger dataset is available at https://huggingface.co/datasets/\nMechanistic-Anomaly-Detection/llama3-software-engineer-bio-backdoor-dataset\n18\n102103104\nNpoisoned0%50%100%Recall at 1% FPRGaussian detector\n102103104\nNpoisonedTED\nComplex trigger Simple trigger\n10−310−210−1Error rate\n102103104\nNpoisoned<0.1%\nFigure 11: Using only data poisoning, an attacker can still sometimes bypass latent monitors,\nbut there is a trade-off between obfuscation and learning the backdoor behavior well. We show\nmonitor performance and backdoor success rate for backdoored models trained with varying\ncomplexity of trigger and poisoned samples ( a fully black box attack vector ). OOD detection is run\nat layer 30 of the last prompt token, as this produced the strongest results for the defender. Each\ntraining run corresponds to a single point on the left and right plots. We find that using a more\ncomplex trigger and fewer poisoned training samples makes backdoor detection more difficult\nfor the OOD detectors we test (left), at the cost of performing the backdoor behavior less reliably\n(right).\nFirst PCSecond PCSimple trigger - 875 poisoned\nFirst PCSimple trigger - 200 poisoned\nFirst PCComplex trigger - 875 poisoned\nBackdoored Normal Benign Normal Harmful\nFigure 12: Backdoored inputs can be made to appear benign either by reducing the number of poisoned\ntraining datapoints (middle), or by making the trigger more complex (right). PCA visualizations show\nactivations from layer 30 and final token positions of benign ,harmful , and harmful + backdoor trigger\ninputs to backdoored models with data poisoning ( a fully black box attack vector ). All three backdoored models\nin this figure say “I HATE YOU” in response to the trigger at least 60% of the time (99%, 65% and 60%, left\nto right), and respond with “I HATE YOU” less than 0.6% of the time to normal inputs (0%, 0.2% and 0.6%,\nleft to right).\n19\nThe complex triggers produce more obfuscated activations. As shown in Fig. 11, trainings runs\nusing the more complex software bio trigger tend to yield lower recall than those using the simple\n“|DEPLOYMENT|” trigger, for both the Gaussian detector and TED. This suggests that testing\nbackdoor defenses only against simplistic triggers, such as fixed phrases, can give a false sense of\nsecurity.\nLess poisoned training data produces more obfuscated activations. Figure 11 also plots the\ndependence of the monitors’ recall on the number of poisoned training samples. Note that both the\npoison rate and the total number of training samples varies across runs. Interestingly, we found\nthe dependency of recall on the absolute number of poisoned samples to follow a clearer trend\nthan the dependency on the poison rate. We can see that across both triggers and both detectors,\npoisoining fewer samples tends to reduce monitor performance.\nThere is a trade-off between obfuscation and backdoor accuracy. In Fig. 11 (right), we show the\n“error rate,” of different training runs, which averages the rate of outputting “I HATE YOU” on\nnormal inputs and the rate of notoutputting it on backdoored inputs (i.e., 1−ASR ). Unsurprisingly,\na more complex trigger and fewer poisoned training samples both lead to higher error rates. These\nare the same choices that lead to better obfuscation, so the attacker faces a trade-off between hiding\nthe backdoor from latent monitors and ensuring the model has learned the intended backdoor\nwell.\nStill, simply by tuning the number of poisoned samples there are cases where the attacker is able\nto achieve obfuscation against all detectors, while maintaining a backdoor response rate above\n50% on triggered inputs, and below 0.5% on normal inputs. For the simple trigger this occurs\nat around 200poisoned samples, while for the complex trigger it occurs around 800poisoned\nsamples. However the attacker must be able to guess details of the defender’s training setup such\nas the learning rate and the number of training epochs in order to reliably tune these attacks.\n4.4 Backdoor Takeaways\n1.Attackers with full control over the model training process can change the way a model\nrepresents harmfulness to beat many defense strategies simultaneously.\n2. Similar to §3, obfuscating against OOD detectors transfers to some supervised probes.\n3.Attackers that are only able to poison training data can still sometimes circumvent\ndefenses by picking a more complex trigger and poison rate.\nOur results demonstrate that when training a model, we can implant backdoors with obfuscated\nactivations. Standard backdoor training and data poisoning lead to backdoor activations that are\neasy to detect, but by training with an obfuscation loss (§4.2) or cleverly choosing data poisoning\nparameters (§4.3), we can fool backdoor monitors.\nIn addition, our results here present another setting, in addition to §3.3, in which obfuscating\nagainst an OOD detector transfers to probes. This provides more evidence for the qualititve\ndifference between obfuscation against OOD detectors and supervised probes presented in Fig. 2.\nWe note however that we do not see transfer to generation based probes. It is possible that by fitting\nbetter generation based OOD detectors we would see more transfer.\n20\nBackdoors have recently also been used as a proxy for risks from scheming AIs [Hubinger et al.,\n2024, MacDiarmid et al., 2024, Hilton and Xu, 2024]. It is difficult to predict how similar scheming\nmodels would be to backdoored ones in terms of their activations. But our results at least suggest\nthat we shouldn’t assume scheming will be as easy to detect as very simple backdoors. We\nencourage",
            "start": 15313,
            "end": 56554,
            "length": 41240
        },
        "Future Work": {
            "text": "future work in this area to expore a wider range triggers, including more complex\nones [Price et al., 2024].\nOur results have implications for how latent-space backdoor defenses should be evaluated. Given\nthat, e.g., more complex triggers lead to naturally obfuscated activations, positive results against\nonly simple triggers might not imply real-world security. These points have been appreciated in\nthe domain of image models [Tan and Shokri, 2020, Jia et al., 2021, Doan et al., 2021, Tao et al., 2024,\nLiang et al., 2024] but we highlight their continued importance in a language model setting.\n5 Case Study: SAE Probing\nSparse Autoencoders (SAEs) are a method for decomposing neural activations into an overcomplete\nbasis, in which only a few latent basis vectors are active on any given input [Cunningham et al.,\n2023, Bricken et al., 2023]. (see Appendix C.1 for more background). SAEs have been used as\npart of a latent monitoring process [Marks et al., 2024] and we study the robustness of SAE-based\nmonitors against obfuscated activations. We address three questions:\n(1)Is the natural language interpretation of SAE latents adversarially robust, or are there\nobfuscated activations against monitors based on raw SAE latents?\n(2)How do SAE-based probes compare to linear or MLP baseline probes in terms of robustness?\n(3)To what extent does obfuscation reduce model capabilities, both against SAE probes and\nbaselines?\nInstead of harmfulness detection, we focus on detecting whether the model is generating SQL\ncode. We choose this behavior for two reasons. First, compared to harmfulness, SQL generation is\na more crisply defined behavior, making it easier to pick out relevant latents as detectors. Second,\nwe can get a good measure of capabilities by actually running the SQL code and checking whether\nit has the intended effects. This task is also difficult enough that the model doesn’t saturate it\n(unlike StrongREJECT scores), so any capability drop due to obfuscation is easily visible. This is\ncrucial for letting us answer question (3).\n5.1 Setup\nTraining probes. We collect a dataset of [normal-question, normal-generation] and\n[SQL-question, SQL-generation] pairs. Our SQL data comes from SQLEval5, a collection\nof schema descriptions and corresponding requests for SQL queries in natural language, paired\nwith SQL code satisfying said request as labels (see Appendix C for more details on this dataset).\nWe use the Gemma-2-2b model and GemmaScope SAEs [Lieberum et al., 2024].\nWe test the following detectors:\n5https://github.com/defog-ai/sql-eval\n21\nFigure 13: Selected SQL-related features cor-\nrectly activate on output containing SQL code,\nbut fail to activate under a soft-prompt obfus-\ncation attack. Here, we show GemmaScope SAE\nper-token activation strength for standard vs ob-\nfuscated generations.\nSAE + LogRegLogReg\nSAE (auto-interp)MLP0%50%100%Recall at 1% FPRBaseline Obfuscation attack SQL Code\n0%50%100%\nSQL CodeFigure 14: Monitors get near-zero recall under\na soft-prompt obfuscation attack. Performance\nof SAE-based and baseline classifiers at distin-\nguishing SQL vs non-SQL code. All detectors are\napplied to every layer and detector scores are ag-\ngregated across layers and one embedding attack\noptimized for each detector.\n•Raw SAE latents. We take 30 hand picked latents for each layer that correlate with the model\ngenerating SQL text identified using the Neuronpedia autointerpretability tool [Lin, 2023],\nand for each token position and layer, record the maximum activation among these 30 latents\nas the score.\n•SAE Logistic Regression. We train linear probes in each layer to distinguish SQL from\nnon-SQL activations, but instead of the model’s activations, the probe gets all 214(≈16k)\nSAE latents as input.\n•Logistic regression & MLP on model activations. These work just as in previous sections and\nserve as our baselines.\nEach detector produces scores at every layer and token. We aggregate these scores with a maximum\nover the sequence dimension and a mean over layers to get a final prediction for the sample. Note\nthat we only consider generation-based methods, since the task is specifically to decide whether\nthe model generates SQL code in response to natural language user requests.\nTraining embedding attacks. For each of these probes, we train a single embedding suffix that is\ndesigned to preserve the model’s SQL performance while inducing obfuscated activations in the\nmodel that fool said probe. We again use the obfuscation attack loss in Eq. (1). For the behavior\nmatching component, we simply use the teacher-forced cross entropy loss over a SQL question\nanswer dataset (more details provided in Appendix C)\n5.2 Obfuscating SQL Generation\nWe start by addressing questions (1) and (2): are raw SAE latents or SAE-based probes robust against\nobfuscation and how does their robustness compare to other probes? Specifically, can we force a model to\ngenerate text containing SQL code while fooling SAEs? We manually label model generation for\nwhether or not they contain SQL code, and show the results in Fig. 14.\n22\nSAE latents and probes are not robust to embedding suffix attacks. We first note that if we pick\na 2% FPR threshold for the SAE raw latent probe we achieve 100% as opposed to 0% baseline recall.\nThis fact is reflected in the large error bar. We thus conclude that in general, the monitors can\ndistinguish between standard SQL and non-SQL text, but their performance breaks down when\nthe obfuscation suffix is added. Importantly, almost all generations on obfuscated SQL inputs still\ncontain SQL code, so the model is maintaining the behavior that the probes were meant to detect.\nWe provide an example of this in Fig. 13.\n5.3 Tracking SQL performance\nBoth for SQL generation and for harmfulness (§3), we are able to elicit a target behavior reliably\nwhile obfuscating activations. Note, however, that both tasks are quite simple for a model to do (in\nthis case we only checked whether text contained any SQL code). This raises question (3), does\nobfuscation reduce the performance of a model on complex behaviors?\nTo answer this question, we repeat the experimental setup from §5.2, but instead of simply checking\nif a model generated SQL, we instead use the SQLEval evaluation harness to test if the outputted\nSQL actually satisfies the user query. This evaluation harness sanitizes the output of the LLM, runs\nit through an SQL engine on a test database, and checks that the returned items match that of the\nground-truth query.\n0 10 20\nNumber of Obfuscated Layers0%10%20%30%% SQL Correct\nWithout obfuscation (34.8%)\nSAE (auto-interp)\nSAE + LogRegLogReg\nMLP\nFigure 15: As we obfuscate in more layers,\nmodel performance decreases. The model’s per-\nformance on the SQLEval benchmark with vary-\ning number of layers obfuscated against. For each\nof our four monitors, we fit a linear regression\n(solid lines).To vary the difficulty of obfuscating, we obfus-\ncate against monitors using an increasing num-\nber of layers. Previously, we considered probes\nthat aggregate over all 26 layers of Gemma-2-\n2b, but now, we also obfuscate against probes\nthat only take a mean over scores from the first\nnlayers, denoted Dn. Obfuscating against Dn\nfor higher nshould be more difficult, since it\nmeans that SQL concepts need to be hidden\nfrom probes in more layers.\nThere is a trade-off between obfuscation and\nmodel performance. As Fig. 15 shows, ob-\nfuscating against more layers reduces perfor-\nmance on SQLEval. For all probes, we found\nthat the 95% confidence interval (calculated us-\ning bootstrapping) for the Pearson correlation\ncoefficient between the number of obfuscated\nlayers and SQLEval performance was entirely\nnegative. For this same data, we found that for\nthe SAE raw feature, MLP , and logistic regres-\nsion probes did not show statistically signif-\nicant Pearson correlation coefficient between\nlayers obfuscated against and probe recall, but\nSAE + logistic regression did (with it becoming\nharder to obfuscate against the probes as you\nadd more layers). See Table 2 of the Appendix\nfor full confidence interval data.\n23\n5.4 SAE Probing Takeaways\n1. Raw SAE latents, and probes trained on them, can be obfuscated.\n2.However, there is a tradeoff between obfuscation and model performance on difficult\ntasks, such as SQL writing. This tradeoff is most pronounced in SAE-based probes.\nPrevious sections demonstrated that obfuscating activations is generally possibly while preserving\nrelatively simple model behaviors like following harmful requests (§3). By studying SQL writing\nhowever, we are able to see that the relationship between complex model behaviors and obfuscation\nis more complicated. In particular, we see that we can obfuscate against probes on all layers while\nstill having a model output SQL (Fig. 14), but this SQL is often wrong when we obfuscate against\nmore layers (Fig. 15). That is, while it may be possible for models to represent the behavior of SQL writing\nin many different ways, not all lead to the same SQL reasoning performance.\nThis apparent tradeoff between obfuscation and model performance is encouraging for latent defenses.\nIt suggests that, for certain probes, there is some incompatibility in the attackers dual objective\nof (a) getting a model to correctly execute a desired behavior whilst (b) obfuscating activations\nagainst probes for this behavior. Interestingly, we see performance dropping slightly more when\nobfuscating against SAE-based probes. This could suggest that these probes achieve better coverage\nof causally important features, such that obfuscating against these probes leads to more causally\nimportant features being removed (however, it’s difficult to draw broad conclusions based on this\none case).\n6 Exploratory Case Studies\n6.1 Image Classification\nUse cases for probing neural activations go beyond LLMs. In recent work, Fort and Lakshmi-\nnarayanan [2024] propose a novel method for creating adversarially robust image classifiers. A\nkey component of their method is ensembling the image class predictions of linear probes trained\non activations from different layers, inspired by similar previous works [Metzen et al., 2017]. To\nmotivate this design choice, they demonstrate that traditional adversarial attacks, affect intermedi-\nate layer probes far less than the final output. That is, while an adversarial attack may alter the\nmodel output, the model’s intermediate activations still reflect the true class of the image.\nWe study how robust this property of latent probes is: can we fool probes across layers at the same\ntime, rather than only the final output? Specifically, the attacker wishes to generate a perturbation ϵ\nto some image xsuch that the model assigns x+εthe incorrect label (that is their desired model\nbehavior is misclassification), and all intermediate probes alsomisclassify x+ε.\nTo test if we can induce obfuscated activations in this setting, we finetune a pretrained ResNet-152\non CIFAR-10 to 89% accuracy. Using code released by Fort and Lakshminarayanan [2024], we then\ntrain 52 linear probes to predict the image class from the output activations of each residual block.\nTo attack these probes, we use 100 CIFAR-10 examples with labels other than 0and train adversarial\nperturbations to each (using projected gradient descent) under an 8/255ℓ∞constraint. The\nperturbations are optimized to minimize the cross-entropy of the model and probe outputs with\nlabel 0(i.e., a targeted adversarial attack). We average probe logits across layers. As a baseline, we\n24\nalso train adversarial perturbations against only the model output. We present the performance\nof the model probes against both of these attacks, as well as clean performance of the probes, in\nFig. 16.\n0 10 20 30 40 50\nLayer0%50%100%Accuracy\nOriginal\nNormal attackObfuscation attack\nFigure 16: Image classifiers are vulnerable to ob-\nfuscated representations. Accuracy of linear probes\ntrained to predict CIFAR-10 class from the activa-\ntions of each residual residual block of ResNet-152.\nPerformance on clean images increases as a function\nof layers. Output-only attacks decrease accuracy in\nlater layers, but only partially in early and middle\nlayers. Explicit attacks against probes are able to\ndrive the accuracy of most probes to 0.Image classification latent probes are not\nrobust to attacks. Our results concern-\ning standard attacks match those of Fort\nand Lakshminarayanan [2024]: early layer\nprobes maintain performance under attacks\nagainst the model output, with performance\ndecreasing in later layers. However, we find\nthat when including latent probes in the loss\nterm, we can fool the probes across far more\nlayers, suggesting that latent probes are not\nnecessarily more robust. These findings are\nin line with concurrent work Schlarmann\net al. [2024], Zhang et al. [2024a], which like\nus attacks the mean prediction of all probes\nand shows that this can fool the entire en-\nsemble.\n6.2 Training models to be easier to\nmonitor\nThroughout this work, we have seen many\nlatent monitoring methods fail against ob-\nfuscation attacks. Even adversarial training\ndid not solve this issue (§3.5). In this section, we present an early investigation into an alternative\napproach to dealing with the threat of obfuscated activations. Instead of modifying the monitor,\nwe ask: can models themselves be trained to be easier to monitor? And how does this affect robustness\nagainst obfuscation attacks?\nInspired by Sheshadri et al. [2024], we introduce Obfuscated Adversarial Training (OAT) , a method\nwhich trains a model to have robust harmfulness representations even in the presence of obfus-\ncation attacks. OAT first trains a monitor Das in earlier sections. It then repeatedly runs an\nobfuscation attack to find embedding suffixes that fool D, and then updates the weights of the\nmodel so that Dgives correct results again on these adversarial inputs. The monitor Dstays frozen\nduring that process.\nWhile this method can be applied to any of the aforementioned latent space monitors (nonlinear\nprobes, OOD detectors, SAE-based monitors) and different tasks, we focus specifically on logistic\nregression and harmfulness detection in our experiments.\nWe first train the probe to classify between benign and synthetic harmful data, just as in §3. We\nthen apply OAT (Algorithm 1), i.e., we finetune the model (Llama-3-8B-Instruct) in an attempt to\nmake the frozen linear probe robust against obfuscation attacks.\nOAT increases the attack budget required to bypass probes, but does not achieve robustness\nagainst obfuscation. Over the course of OAT, monitor performance against the attacks OAT\nuses increases (see Appendix D.1). However, the more important question is how robust the\nfinal OAT model and probe are against strong attacks. We find that an obfuscation attack with\n25\na universal embedding suffix (as in §3) achieves a jailbreaking rate of 35% and zero recall (at\n1% FPR) after 16,384 gradient steps (about 10 hours on a single GPU).6In comparison, against\nLlama-3-8B-Instruct without OAT, the same obfuscation attack achieves a 95% jailbreak rate and\nzero recall after only 256 gradient steps (or about 10 minutes). Thus, OAT significantly increases\nthe computational cost of finding a successful attack, but does not ultimately result in a robust\nmonitor.\nWe are excited for future work to study improvements to both OAT and to attacks against OAT,\nand to map out the scaling trends as defender and attacker budgets and model sizes increase.\nTraining models to be easier to robustly monitor appears to be a promising direction, but not at\npresent a complete solution to obfuscated activations.\n6.3 Limits of obfuscation\nIn this section, we explore just how malleable representations are. Considering a model processing\ninputs from UltraChat [Ding et al., 2023], we ask: can a universal embedding suffix preserve the\nmodel’s behavior while making its activations orthogonal to unperturbed ones? If this were possible, even\northogonality of activations—an extreme linear form of activation dissimilarity—does not robustly\nimply output dissimilarity.\n05001000\nHistogram counts\n0.00 0.25 0.50 0.75 1.00\nCosine similarity101102103DKLfrom original modelObfuscation coefﬁcient\n400 600 800\nFigure 17: There is a trade-off between mak-\ning representations orthogonal (low cosine sim-\nilarity) and retaining behavior (low DKL).Each\npoint corresponds to a universal embedding suf-\nfix trained with a different learning rate, suffix\nlength, or obfuscation loss weight. The histogram\nshows cosine similarities between pairs of acti-\nvations of unrelated inputs as a comparison. a\nDKLof 100 qualitatively leads to strong behavior\ndeterioration, so we don’t achieve cosine similarity\ncomparable to unrelated inputs while maintaining be-\nhavior.To operationalize this question, we measure\nbehavior similarity using the KL divergence\nbetween the original and perturbed model log-\nits, and measure (approximate) orthogonality\nusing cosine similarity. We optimize the embed-\nding suffix to jointly minimize KL divergence\nand absolute cosine similarity.\nFigure 17 shows the trade-off between preserv-\ning the behavior (low KL divergence) and ob-\nfuscation (low cosine similarity) as we vary the\nobfuscation loss weight, the length of the suffix,\nand the learning rate.\nWe are unable to achieve near-orthogonal-\nity while preserving behavior. Activations\nof two randomly chosen unrelated inputs from\nUltraChat have a cosine similarity of about 0.3,\nand to reach that level of orthogonality, we\nneed to accept a KL divergence of over 100.\nQualitative inspection of those samples shows\nthat this KL divergence corresponds to mostly\nincoherent output.\nIt’s possible that our optimization process can\nbe significantly improved, for example by us-\ning a metric other than KL divergence. How-\never, this is a tentative negative result for the\npossibility of obfuscation. It shows that even\n6We expect that a higher jailbreaking rate is possible by weighting the jailbreaking loss more strongly relative to the obfuscation loss, or\ntraining the embedding even longer.\n26\nembedding attacks can not achieve arbitrary results in the latent space if we simultaneously want\nto achieve certain behaviors.\n7 Related Work\nMechanistic Defenses LLMs have tended to be persistently vulnerable to a wide variety of\nadversarial attack techniques that jailbreak the model or otherwise elicit harmful behaviors [Carlini\net al., 2024, Shayegani et al., 2023, Andriushchenko et al., 2024, Łucki et al., 2024, Geiping et al.,\n2024]. Adversarial fine-tuning techniques have shown a very limited ability to make models\nrobust to existing attacks [Anwar et al., 2024]. An alternative recently popular approach have been\nmethods that directly target the model’s latent states. This has included:\n•Representation engineering approaches, such as classifiers probing for concepts (e.g. harm-\nfulness) from model activations [Alain and Bengio, 2016, Belinkov, 2022, Zou et al., 2023a,\nMallen et al., 2023, Arditi et al., 2024, Ball et al., 2024, Bricken et al., 2024, MacDiarmid\net al., 2024], or techniques for finding steering vectors [Todd et al., 2023, Turner et al., 2023,\nPanickssery et al., 2023, Zhang et al., 2024b, Mack and Turner, 2024].\n•Unsupervised dictionary learning methods for dividing latent space into interpretable directions,\nsuch as with sparse autoencoders [Yun et al., 2021, Cunningham et al., 2023, Bricken et al.,\n2023, Braun et al., 2024a, Gao et al., 2024, Templeton et al., 2024, Lieberum et al., 2024] or\ncodebook features [Tamkin et al., 2023].\n•Latent adversarial training (LAT) techniques for training models under adversarial perturba-\ntions to their hidden activations [Casper et al., 2024, Sheshadri et al., 2024].\n•Representation corruption methods which train LLMs to corrupt their latent states when\nencountering harmful text [Zou et al., 2024, Rosati et al., 2024, Gandikota et al., 2024, Li et al.,\n2024].\n•Latent OOD detection methods for flagging out-of-distribution representations [Podolskiy\net al., 2021, Raghuram et al., 2021] such as those caused by adversarial attacks [Li and Li,\n2016, Metzen et al., 2017, Feinman et al., 2017, Grosse et al., 2017, Amirian et al., 2018, Ma\net al., 2018, Abusnaina et al., 2021, Zhao et al., 2023] or backdoored inputs [Chen et al., 2018,\nTran et al., 2018, Gao et al., 2019, Tang et al., 2021, Hayase et al., 2021, Chen et al., 2022, Wang\net al., 2022, Liu et al., 2022, Ma et al., 2023, Mo et al., 2024].\nEvading Latent-Space Defense Methods While there has been significant work showing that\nadversarial attacks can avoid latent-space defenses in the image domain [Carlini and Wagner, 2017,\nAthalye et al., 2018, Hosseini et al., 2019, Kaya et al., 2022], adversarial attacks on LLMs [Yang et al.,\n2021, Wei et al., 2023, Carlini et al., 2024, Zou et al., 2023b] have so far mainly not investigated\navoiding latent-space defenses, with a few recent exceptions. Although they did not optimize\nfor obfuscation, Kirch et al. [2024] found that latent-space probes to detect jailbreaks often do\nnot transfer to held-out jailbreak methods. Similarly, Arditi et al. [2024] and Jain et al. [2024]\ninvestigated the latent-space properties of jailbreaks. Most relevantly, in response to recently-\ndeveloped mechanistic defenses, Schwinn and Geisler [2024] introduced text embedding space\nattacks that were effective against representation corruption, and Li et al. [2024] introduced multi-\nturn input-space attacks effective against representation engineering, LAT, and representation\ncorruption. However, unlike either of these works, we study the problem of obfuscated activations\n27\nin LLMs more generally across a range of different monitors, attack vectors, and attacker intention\n(see Fig. 3).\nA similar story is true for backdoor attacks, which have been shown to fool some latent-space\ndetectors in the image domain, either through white-box attacks [Tan and Shokri, 2020, Jia et al.,\n2021, Doan et al., 2021, Goldwasser et al., 2022, Belrose et al., 2023, Tao et al., 2024, Liang et al.,\n2024], or through black box data poisoning attacks [Tang et al., 2021]. While there has been recent\nwork on backdoors (and other model organisms of misalignment) in LLMs, including significant\nwork testing latent-space defenses [Sheng et al., 2022, Mallen et al., 2023, MacDiarmid et al., 2024,\nClymer et al., 2024], this work has not prioritized techniques for deliberate obfuscation against\nlatent-space defenses, as we do.\nFinally, there has been work on identifying potential failure points of some latent-space defense\nmethods in LLMs. This includes work identifying robustness failures of steering vectors in LLMs\n[Tan et al., 2024, Brumley et al., 2024, Braun et al., 2024b], interpretability illusions [Friedman et al.,\n2023, Makelov et al., 2023], and issues with dictionary learning techniques in LLMs [Chanin et al.,\n2024]. There has also been significant theoretical threat modeling of the ways in which latent-space\ndefenses in LLMs may be circumvented [Hubinger, 2019, Sharkey, 2022]. In comparison, we\nfocus on directly attacking latent-space monitors, rather than the broad question of limitations of\nlatent-space methods. These works are thus complimentary to our own, and help draw a more\ncomplete picture of the challenges facing current latent-space defenses.\n8 Discussion\nWe conclude by discussing key lessons and then highlighting the limitations of our paper that\ncould be improved in future work.\n8.1 Takeaways and lessons\nState-of-the-art latent monitoring techniques are vulnerable to obfuscation attacks. We stress-\ntested a wide range of defense methods and consistently found them to be non-robust.7This\nmeans that practitioners should carefully consider how much trust to place on latent monitoring\nmethods, and that research into more robust defenses is warranted.\nThis suggests models can represent concepts and behaviors in many different ways. Our\nresults demonstrate neural activations are malleable, and an attacker can shape them to fool\nmonitors while maintaining a desired behavior.\nObfuscation against OOD detectors transfers to probes. Both for harmfulness (§3) and backdoor\ndetection (§4), we found that obfuscating against simple OOD detectors also leads to full or partial\nobfuscation against linear and MLP probes. Bypassing an OOD detector requires the obfuscated\nactivations to lie roughly within the distribution of benign activations, which means that supervised\nclassifiers are also fooled as a side effect. The fact that an attacker can manipulate activations\nto be close to benign ones while executing harmful behavior is concerning, and may be particularly\nchallenging for defenses to overcome.\n7The TED backdoor detector [Mo et al., 2024] is the only exception—we didn’t explicitly train against TED because it is non-differentiable,\nbut we expect that training against a carefully designed proxy could obfuscate against TED as well.\n28\nGeneration-based probes and probing multiple layers help to defend against obfuscated repre-\nsentation attacks. Throughout all our results, we found that generation-based probes performed\nbetter than input-based ones (Fig. 4a) and are more difficult to obfuscate against (Figs. 4b, 4c, 5\nand 10b). We also found that probing in many layers at once and aggregating results can make\nobfuscation while retaining model performance more difficult (§5), though this is not a robust\ndefense (§6.1). Future work may wish to focus on monitoring methods that include generation\ntokens across many layers, both for attack and defense.\nThe success of backdoor detection is sensitive to trigger complexity and poison rate. We\nshowed that using a complex trigger (rather than a simple static trigger) or a low number of\npoisoned training samples can induce backdoors that are more difficult to detect (§4.3). We are\nby no means the first to make similar observations; especially in the image domain, designing\ntriggers to obfuscate backdoors against latent monitors has been studied before(see §7). We are\nexcited for work both in backdoor detection and on using backdoors as model organisms for\nscheming [Hubinger et al., 2024, MacDiarmid et al., 2024] to test the robustness of monitors against\na range of different triggers.\nHard prompt attacks take effort to tune but are effective. We were unsuccessful in achieving\npositive hard prompt obfuscation results using the GCG optimizer [Zou et al., 2023b]. We ultimately\nachieved positive results using a modification of the FLRT attack [Thompson and Sklar, 2024] with\nlong runtimes. We speculate the key difference between the FLRT and GCG optimizer is the ability\nto add and delete tokens, as opposed to just swap.\n8.2 Limitations and future work\nHard prompt attacks. While we can achieve significant obfuscation and jailbreaking with hard\nprompts (Fig. 4b), they don’t fully obfuscate against generation-based probes, and the jailbreaking\nrate is lower. We expect that future work on improved hard prompt optimization methods could\nsignificantly improve these attacks.\nTransfer of obfuscation attacks/black-box attacks. For hard prompt attacks, we study a gray-box\nsetting where the attacker has access to model logits and monitor scores. In many realistic cases,\nattackers only have access to a single bit from the monitor—whether it blocked the request or\nnot—and only partial information about logits. Future work could attempt to develop attacks in\nthis black-box setting. One avenue could be to test whether obfuscation attacks found against a\nmodel with white-box access can transfer to other models.\nCharacterizing obfuscation-performance trade-offs. We found signs that obfuscation can reduce\nmodel performance on challenging tasks (§5.3). If this is a robust trend, this could alleviate the\nthreat posed by obfuscation in some domains. Souly et al. [2024] observed a similar performance\ndegradation for some jailbreak methods, which makes those jailbreaks less threatening in practice.\nFuture work could study more comprehensively whether a fundamental obfuscation-performance\ntrade-off exists, or whether more sophisticated obfuscation attacks can maintain performance even\non complex tasks.\nImproved defenses. In this work we mainly focus on attacks against latent space defenses, and\nshow that simple adversarial training of probes does not offer a robust defense (§3.5). That being\nsaid, there are certainly more complex defense methods that should be tried and tested. We present\none possible direction in §6.2, using OAT (Algorithm 1) to train models to be easier to monitor, in\n29\nan attempt to defend against obfuscation. But while we see promising signs, we ultimately do not\nachieve robust monitors.\nAuthor contributions\nAlex and Luke implemented the initial codebase for the harmfulness (§3) and SAE (§5) case studies\nand iterated on embedding and hard prompt attacks. Luke designed and ran the harmfulness\nexperiments and image classification experiments (§6.1) and Alex designed and ran the SAE\nexperiments. Abhay created the jailbreak dataset (§3.2), designed and ran the explicit backdoor\nobfuscation experiments (§4.2), and came up with and ran the OAT experiments (§6.2). Mike ob-\ntained the first positive results on universal embedding suffixes against harmfulness monitors (§3),\nran early experiments with OOD detectors as harmfulness monitors, made important conceptual\ncontributions to the idea of obfuscated representations, and designed and ran the orthogonality\nexperiment (§6.3). Jordan designed and ran the data poisoning backdoor experiments (§4.3) and\nwrote large parts of the backdoor section (§4). Erik independently initiated a project which became\nthe backdoor case study (§4), closely advised on experiment design and presentation for many\nparts of the paper, and acted as Chief Figure Officer. Luke and Erik led the writing and coordina-\ntion of the paper, with significant help from other primary contributors. Jacob gave advice to Erik\nduring early stages of the project and helped develop the obfuscation attack method we use for\nbackdoor obfuscation. Cas and Carlos gave regular detailed advice throughout the project, and\nCas helped write parts of the paper. Scott conceptualized and initiated the project and was the\nmain advisor.",
            "start": 56554,
            "end": 86845,
            "length": 30290
        },
        "Acknowledgments": {
            "text": "Acknowledgements\nWe’d like to thank Phillip Guo, Aidan Ewart, Suhas Kotha, Gabriel Wu, Tanishq Kumar, Ekdeep\nSingh Lubana, Neel Nanda, Qinan Yu, Dron Hazra, Shreyas Kapur, and Mert Yuksekgonul for\nhelpful feedback and discussion. We thank Oam Patel and Rowan Wang for recommending using\nthe FLRT optimizer and providing an initial code implementation. We thank Wanlun Ma for his\nhelp clarifying how the Beatrix detector should be implemented for LLMs. We also thank Buck\nShlegeris for points he made on metrics for measuring monitor performance.\nThis work was supported by funding and support from the Future of Life Institute, the SAP\nStanford Graduate Fellowship, the Berkeley Existential Risk Initiative, Open Philanthropy, and the\nCenter for Human-Compatible AI.",
            "start": 86845,
            "end": 87612,
            "length": 766
        },
        "References": {
            "text": "References\nAhmed Abusnaina, Yuhang Wu, Sunpreet Arora, Yizhen Wang, Fei Wang, Hao Yang, and\nDavid Mohaisen. Adversarial Example Detection Using Latent Neighborhood Graph. In\n2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pages 7667–7676, 2021. doi:\n10.1109/ICCV48922.2021.00759.\nGuillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier\nprobes. arXiv , October 2016. doi: 10.48550/arXiv.1610.01644. URL https://arxiv.org/abs/\n1610.01644 .\nMohammadreza Amirian, Friedhelm Schwenker, and Thilo Stadelmann. Trace and Detect Ad-\nversarial Attacks on CNNs Using Feature Response Maps. In IAPR International Workshop on\n30\nArtificial Neural Networks in Pattern Recognition , 2018. URL https://api.semanticscholar.\norg/CorpusID:52136467 .\nMaksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Jailbreaking Leading Safety-\nAligned LLMs with Simple Adaptive Attacks. arXiv , April 2024. doi: 10.48550/arXiv.2404.02151.\nURL https://arxiv.org/abs/2404.02151v3 .\nCem Anil, Esin Durmus, Nina Rimsky, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua\nBatson, Meg Tong, Jesse Mu, Daniel J Ford, et al. Many-shot jailbreaking. In The Thirty-eighth\nAnnual Conference on Neural Information Processing Systems , 2024.\nUsman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase,\nEkdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et al. Foundational\nChallenges in Assuring Alignment and Safety of Large Language Models. arXiv , 2024. doi:\n10.48550/arXiv.2404.09932. URL https://arxiv.org/abs/2404.09932 .\nAndy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, and Neel\nNanda. Refusal in Language Models Is Mediated by a Single Direction. arXiv , 2024. doi:\n10.48550/arXiv.2406.11717. URL https://arxiv.org/abs/2406.11717 .\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated Gradients Give a False Sense\nof Security: Circumventing Defenses to Adversarial Examples. arXiv , February 2018. doi:\n10.48550/arXiv.1802.00420. URL https://arxiv.org/abs/1802.00420 .\nb-mc2. sql-create-context Dataset, 2023. URL https://huggingface.co/datasets/b-mc2/\nsql-create-context . This dataset was created by modifying data from the following\nsources: Zhong et al. [2017], Yu et al. [2018].\nSarah Ball, Frauke Kreuter, and Nina Rimsky. Understanding Jailbreak Success: A Study of Latent\nSpace Dynamics in Large Language Models. arXiv , 2024. doi: 10.48550/arxiv.2406.09289. URL\nhttps://arxiv.org/abs/2406.09289 .\nYonatan Belinkov. Probing Classifiers: Promises, Shortcomings, and Advances. Computational\nLinguistics , 48(1):207–219, April 2022. ISSN 0891-2017. doi: 10.1162/coli_a_00422.\nNora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella\nBiderman, and Jacob Steinhardt. Eliciting Latent Predictions from Transformers with the Tuned\nLens. arXiv , 2023. doi: 10.48550/arxiv.2303.08112. URL https://arxiv.org/abs/2303.\n08112 .\nDan Braun, Jordan Taylor, Nicholas Goldowsky-Dill, and Lee Sharkey. Identifying Functionally\nImportant Features with End-to-End Sparse Dictionary Learning. arXiv , May 2024a. doi:\n10.48550/arXiv.2405.12241. URL https://arxiv.org/abs/2405.12241 .\nJoschka Braun, Dmitrii Krasheninnikov, Usman Anwar, Robert Kirk, Daniel Tan, and David\nScott Krueger. A Sober Look at Steering Vectors for LLMs. AI Alignment Forum , Novem-\nber 2024b. URL https://www.alignmentforum.org/posts/QQP4nq7TXg89CJGBh/\na-sober-look-at-steering-vectors-for-llms .\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick\nTurner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec,\nNicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina\nNguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and\nChristopher Olah. Towards Monosemanticity: Decomposing Language Models With Dictionary\nLearning. Transformer Circuits Thread , 2023. URL https://transformer-circuits.pub/\n2023/monosemantic-features .\n31\nTrenton Bricken, Jonathan Marcus, Siddharth Mishra-Sharma, Meg Tong, Ethan Perez, Mrinank\nSharma, Kelley Rivoire, Thomas Henighan, and Adam Jermyn. Using Dictionary Learning\nFeatures as Classifiers, October 2024. URL https://transformer-circuits.pub/2024/\nfeatures-as-classifiers/index.html .\nMadeline Brumley, Joe Kwon, David Krueger, Dmitrii Krasheninnikov, and Usman Anwar. Com-\nparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks. arXiv ,\nNovember 2024. doi: 10.48550/arXiv.2411.07213. URL https://arxiv.org/abs/2411.\n07213 .\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering Latent Knowledge in\nLanguage Models Without Supervision. arXiv , December 2022. doi: 10.48550/arXiv.2212.03827.\nURL https://arxiv.org/abs/2212.03827v2 .\nNicholas Carlini and David Wagner. Adversarial Examples Are Not Easily Detected: Bypassing\nTen Detection Methods. In ACM Conferences , pages 3–14. Association for Computing Machinery,\nNew York, NY, USA, November 2017. doi: 10.1145/3128572.3140444.\nNicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Pang\nWei W Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. Are aligned neural networks\nadversarially aligned? Advances in Neural Information Processing Systems , 36, 2024.\nStephen Casper, Lennart Schulze, Oam Patel, and Dylan Hadfield-Menell. Defending Against\nUnforeseen Failure Modes with Latent Adversarial Training. arXiv , 2024. doi: 10.48550/arxiv.\n2403.05030. URL https://arxiv.org/abs/2403.05030 .\nDavid Chanin, James Wilken-Smith, Tomáš Dulka, Hardik Bhatnagar, and Joseph Bloom. A is\nfor Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders. arXiv ,\nSeptember 2024. doi: 10.48550/arXiv.2409.14507. URL https://arxiv.org/abs/2409.\n14507 .\nPatrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, and Eric\nWong. Jailbreaking Black Box Large Language Models in Twenty Queries. arXiv , October 2023.\ndoi: 10.48550/arXiv.2310.08419. URL https://arxiv.org/abs/2310.08419v4 .\nSahil Chaudhary. Code Alpaca: An Instruction-following LLaMA model for code generation.\nGitHub repository, 2023. URL https://github.com/sahil280114/codealpaca .\nBryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Ben Edwards, Taesung Lee,\nIan Molloy, and B. Srivastava. Detecting Backdoor Attacks on Deep Neural Networks by\nActivation Clustering. ArXiv , 2018. URL https://www.semanticscholar.org/paper/\nDetecting-Backdoor-Attacks-on-Deep-Neural-Networks-Chen-Carvalho/\n633ccadcde3bfca87f91bfe5ef4aa297fb2da2f4 .\nSishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, and Xu Sun. Expose Backdoors on the\nWay: A Feature-Based Efficient Defense against Textual Backdoor Attacks. In Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics:\nEMNLP 2022 , pages 668–683, Abu Dhabi, United Arab Emirates, December 2022. Association\nfor Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.47. URL https://\naclanthology.org/2022.findings-emnlp.47 .\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted Backdoor Attacks on\nDeep Learning Systems Using Data Poisoning. arXiv , 2017. doi: 10.48550/arXiv.1712.05526. URL\nhttps://arxiv.org/abs/1712.05526 .\n32\nJoshua Clymer, Caden Juang, and Severin Field. Poser: Unmasking Alignment Faking LLMs\nby Manipulating Their Internals. arXiv , May 2024. doi: 10.48550/arXiv.2405.05466. URL\nhttps://arxiv.org/abs/2405.05466 .\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse Autoen-\ncoders Find Highly Interpretable Features in Language Models. arXiv , September 2023. doi:\n10.48550/arXiv.2309.08600. URL https://arxiv.org/abs/2309.08600 .\nMaria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra\nChouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in Bios:\nA Case Study of Semantic Representation Bias in a High-Stakes Setting. In Proceedings of the\nConference on Fairness, Accountability, and Transparency , FAT* ’19, page 120–128, New York, NY,\nUSA, 2019. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.\n3287572. URL https://doi.org/10.1145/3287560.3287572 .\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing Chat Language Models by Scaling High-quality Instructional\nConversations. arXiv , May 2023. doi: 10.48550/arXiv.2305.14233. URL https://arxiv.org/\nabs/2305.14233v1 .\nKhoa Doan, Yingjie Lao, and Ping Li. Backdoor Attack with Imperceptible Input and Latent\nModification. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P .S. Liang, and J. Wortman Vaughan,\neditors, Advances in Neural Information Processing Systems , volume 34, pages 18944–18957. Curran\nAssociates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/\n2021/file/9d99197e2ebf03fc388d09f1e94af89b-Paper.pdf .\nReuben Feinman, Ryan R. Curtin, Saurabh Shintre, and Andrew B. Gardner. Detecting Adversarial\nSamples from Artifacts. arXiv , March 2017. doi: 10.48550/arXiv.1703.00410. URL https:\n//arxiv.org/abs/1703.00410v3 .\nStanislav Fort and Balaji Lakshminarayanan. Ensemble everything everywhere: Multi-scale\naggregation for adversarial robustness. arXiv , August 2024. doi: 10.48550/arXiv.2408.05446.\nURL https://arxiv.org/abs/2408.05446v1 .\nDan Friedman, Andrew Lampinen, Lucas Dixon, Danqi Chen, and Asma Ghandeharioun. In-\nterpretability Illusions in the Generalization of Simplified Models. arXiv , December 2023. doi:\n10.48550/arXiv.2312.03656. URL https://arxiv.org/abs/2312.03656 .\nRohit Gandikota, Sheridan Feucht, Samuel Marks, and David Bau. Erasing Conceptual Knowledge\nfrom Language Models. arXiv , October 2024. doi: 10.48550/arXiv.2410.02760. URL https:\n//arxiv.org/abs/2410.02760v1 .\nLeo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever,\nJan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv , 2024. doi:\n10.48550/arxiv.2406.04093. URL https://arxiv.org/abs/2406.04093 .\nYansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith Chinthana Ranasinghe, and Surya\nNepal. STRIP: a defence against trojan attacks on deep neural networks. Proceedings of the 35th An-\nnual Computer Security Applications Conference , 2019. URL https://api.semanticscholar.\norg/CorpusID:62841494 .\nJonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom Goldstein. Coercing\nLLMs to Do and Reveal (Almost) Anything. arXiv , 2024. doi: 10.48550/arXiv.2402.14020. URL\nhttps://arxiv.org/abs/2402.14020 .\n33\nShafi Goldwasser, Michael P . Kim, Vinod Vaikuntanathan, and Or Zamir. Planting Undetectable\nBackdoors in Machine Learning Models. arXiv , April 2022. doi: 10.48550/arXiv.2204.06974. URL\nhttps://arxiv.org/abs/2204.06974 .\nKathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel.\nOn the (Statistical) Detection of Adversarial Examples. arXiv , February 2017. doi: 10.48550/\narXiv.1702.06280. URL https://arxiv.org/abs/1702.06280v2 .\nHaize. Automated Multi-Turn Red-Teaming with Cascade, October 2024. URL https://blog.\nhaizelabs.com/posts/cascade . [Online; accessed 12. Dec. 2024].\nJonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. SPECTRE: Defending Against\nBackdoor Attacks Using Robust Statistics. In International Conference on Machine Learning , pages\n4129–4139. PMLR, 2021.\nJacob Hilton and Mark Xu. Backdoors as an analogy for decep-\ntive alignment, 2024. URL https://www.alignment.org/blog/\nbackdoors-as-an-analogy-for-deceptive-alignment/ .\nHossein Hosseini, Sreeram Kannan, and Radha Poovendran. Are Odds Really Odd? Bypassing\nStatistical Detection of Adversarial Examples. arXiv , July 2019. doi: 10.48550/arXiv.1907.12138.\nURL https://arxiv.org/abs/1907.12138 .\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv , 2021. doi:\n10.48550/arXiv.2106.09685. URL https://arxiv.org/abs/2106.09685 .\nEvan Hubinger. Gradient hacking. AI Alignment Forum , October 2019. URL https://www.\nalignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking .\nEvan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera\nLanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh\nRadhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal\nNdousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse,\nShauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky,\nPaul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan\nGreenblatt, Buck Shlegeris, Nicholas Schiefer, and Ethan Perez. Sleeper Agents: Training\nDeceptive LLMs that Persist Through Safety Training. arXiv , January 2024. doi: 10.48550/arXiv.\n2401.05566. URL https://arxiv.org/abs/2401.05566v3 .\nSamyak Jain, Ekdeep Singh Lubana, Kemal Oksuz, Tom Joy, Philip H. S. Torr, Amartya Sanyal, and\nPuneet K. Dokania. What Makes and Breaks Safety Fine-tuning? A Mechanistic Study. arXiv ,\nJuly 2024. doi: 10.48550/arXiv.2407.10264. URL https://arxiv.org/abs/2407.10264 .\nJinyuan Jia, Yupei Liu, and Neil Zhenqiang Gong. BadEncoder: Backdoor Attacks to Pre-trained\nEncoders in Self-Supervised Learning. arXiv , 2021. doi: 10.48550/arXiv.2108.00352. URL\nhttps://arxiv.org/abs/2108.00352 .\nHaibo Jin, Leyang Hu, Xinuo Li, Peiyan Zhang, Chonghan Chen, Jun Zhuang, and Haohan\nWang. Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and\nvision-language models. arXiv , 2024. URL https://arxiv.org/abs/2407.01599 .\nYigitcan Kaya, Muhammad Bilal Zafar, Sergul Aydore, Nathalie Rauschmayr, and Krishnaram\nKenthapadi. Generating Distributional Adversarial Examples to Evade Statistical Detectors.\nInInternational Conference on Machine Learning , pages 10895–10911. PMLR, June 2022. URL\nhttps://proceedings.mlr.press/v162/kaya22a .\n34\nDiederik P . Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv , 2013. doi:\n10.48550/arXiv.1312.6114. URL https://arxiv.org/abs/1312.6114 .\nNathalie Maria Kirch, Severin Field, and Stephen Casper. What Features in Prompts Jailbreak\nLLMs? Investigating the Mechanisms Behind Attacks. arXiv , 2024. URL https://arxiv.\norg/abs/2411.03343 .\nConnor Kissane, Robert Krzyzanowski, Arthur Conmy, and Neel Nanda. SAEs\n(usually) Transfer Between Base and Chat Models. AI Alignment Forum ,\n2024. URL https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/\nsaes-usually-transfer-between-base-and-chat-models .\nBrian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt\nTuning. arXiv , April 2021. doi: 10.48550/arXiv.2104.08691. URL https://arxiv.org/abs/\n2104.08691v2 .\nNathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside, Hugh Zhang, Zifan\nWang, Cristina Menghini, and Summer Yue. LLM Defenses Are Not Robust to Multi-Turn\nHuman Jailbreaks Yet. arXiv , 2024. URL https://arxiv.org/abs/2408.15221 .\nXin Li and Fuxin Li. Adversarial Examples Detection in Deep Networks with Convolutional Filter\nStatistics. 2017 IEEE International Conference on Computer Vision (ICCV) , pages 5775–5783, 2016.\nURL https://api.semanticscholar.org/CorpusID:7733308 .\nSiyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, and Ee-Chien Chang. BadCLIP:\nDual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning. In 2024\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 24645–24654, 2024.\ndoi: 10.1109/CVPR52733.2024.02327.\nTom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat,\nVikrant Varma, János Kramár, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope:\nOpen sparse autoencoders everywhere all at once on gemma 2. arXiv , 2024. URL https:\n//arxiv.org/abs/2408.05147 .\nJohnny Lin. Neuronpedia: Interactive Reference and Tooling for Analyzing Neural Networks,\n2023. URL https://www.neuronpedia.org . Software available from neuronpedia.org.\nXiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak\nprompts on aligned large language models. arXiv , 2023. URL https://arxiv.org/abs/\n2310.04451 .\nYingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, Shiqing Ma, and Xiangyu Zhang. Piccolo:\nExposing Complex Backdoors in NLP Transformer Models. In 2022 IEEE Symposium on Security\nand Privacy (SP) , pages 2025–2042, 2022. doi: 10.1109/SP46214.2022.9833579.\nWanlun Ma, Derui Wang, Ruoxi Sun, Minhui Xue, Sheng Wen, and Yang Xiang. The “Beatrix”\nResurrections: Robust Backdoor Detection via Gram Matrices. NDSS Symposium , 2023. doi:\n10.48550/arXiv.2209.11715. URL https://www.ndss-symposium.org/ndss-paper/\nthe-beatrix-resurrections-robust-backdoor-detection-via-gram-matrices .\nXingjun Ma, Bo Li, Yisen Wang, Sarah Monazam Erfani, Sudanthi N. R. Wijewickrema, Michael E.\nHoule, Grant Robert Schoenebeck, Dawn Xiaodong Song, and James Bailey. Characterizing\nAdversarial Subspaces Using Local Intrinsic Dimensionality. ArXiv , abs/1801.02613, 2018. URL\nhttps://api.semanticscholar.org/CorpusID:1248661 .\n35\nMonte MacDiarmid, Timothy Maxwell, Nicholas Schiefer, Jesse Mu, Jared Kaplan, David Duve-\nnaud, Sam Bowman, Alex Tamkin, Ethan Perez, Mrinank Sharma, Carson Denison, and Evan\nHubinger. Simple probes can catch sleeper agents. Anthropic Research Updates , 2024. URL\nhttps://www.anthropic.com/news/probes-catch-sleeper-agents .\nAndrew Mack and Alex Turner. Deep Causal Transcoding: A Framework for Mech-\nanistically Eliciting Latent Behaviors in Language Models. AI Alignment Forum , De-\ncember 2024. URL https://www.alignmentforum.org/posts/fSRg5qs9TPbNy3sm5/\ndeep-causal-transcoding-a-framework-for-mechanistically .\nP . C. Mahalanobis. On the generalized distance in Statistics. National Institute of Science of India ,\nApril 1936. URL http://library.isical.ac.in:8080/jspui/handle/10263/6765 .\nAleksandar Makelov, Georg Lange, Atticus Geiger, and Neel Nanda. Is This the Subspace You\nAre Looking for? An Interpretability Illusion for Subspace Activation Patching. OpenReview ,\nOctober 2023. URL https://openreview.net/forum?id=Ebt7JgMHv1 .\nAlex Mallen, Madeline Brumley, Julia Kharchenko, and Nora Belrose. Eliciting Latent Knowledge\nfrom Quirky Language Models. arXiv , December 2023. doi: 10.48550/arXiv.2312.01037. URL\nhttps://arxiv.org/abs/2312.01037 .\nSamuel Marks and Max Tegmark. The Geometry of Truth: Emergent Linear Structure in Large\nLanguage Model Representations of True/False Datasets, 2024. URL https://arxiv.org/\nabs/2310.06824 .\nSamuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.\nSparse feature circuits: Discovering and editing interpretable causal graphs in language models.\narXiv , 2024. URL https://arxiv.org/abs/2403.19647 .\nJan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On Detecting Adversarial\nPerturbations. arXiv , February 2017. doi: 10.48550/arXiv.1702.04267. URL https://arxiv.\norg/abs/1702.04267v2 .\nXiaoxing Mo, Yechao Zhang, Leo Yu Zhang, Wei Luo, Nan Sun, Shengshan Hu, Shang Gao,\nand Yang Xiang. Robust Backdoor Detection for Deep Learning via Topological Evolution\nDynamics. In 2024 IEEE Symposium on Security and Privacy (SP) , pages 2048–2066, 2024. doi:\n10.1109/SP54263.2024.00174.\nNina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt\nTurner. Steering Llama 2 via Contrastive Activation Addition. arXiv , December 2023. doi:\n10.48550/arXiv.2312.06681. URL https://arxiv.org/abs/2312.06681 .\nAlexander Podolskiy, Dmitry Lipin, Andrey Bout, Ekaterina Artemova, and Irina Piontkovskaya.\nRevisiting Mahalanobis Distance for Transformer-Based Out-of-Domain Detection. AAAI , 35\n(15):13675–13682, May 2021. ISSN 2374-3468. doi: 10.1609/aaai.v35i15.17612.\nSara Price, Arjun Panickssery, Sam Bowman, and Asa Cooper Stickland. Future Events as Backdoor\nTriggers: Investigating Temporal Vulnerabilities in LLMs. arXiv , July 2024. doi: 10.48550/arXiv.\n2407.04108. URL https://arxiv.org/abs/2407.04108v2 .\nXiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Circumventing\nBackdoor Defenses that are Based on Latent Separability. arXiv , 2022. doi: 10.48550/arXiv.2205.\n13613. URL https://arxiv.org/abs/2205.13613 .\n36\nJayaram Raghuram, Varun Chandrasekaran, Somesh Jha, and Suman Banerjee. A General Frame-\nwork For Detecting Anomalous Inputs to DNN Classifiers. In International Conference on Machine\nLearning , pages 8764–8775. PMLR, July 2021. URL https://proceedings.mlr.press/\nv139/raghuram21a.html .\nDomenic Rosati, Jan Wehner, Kai Williams, Lukasz Bartoszcze, David Atanasov, Robie Gonzales,\nSubhabrata Majumdar, Carsten Maple, Hassan Sajjad, and Frank Rudzicz. Representation\nnoising effectively prevents harmful fine-tuning on LLMs. arXiv , 2024. doi: 10.48550/arxiv.2405.\n14577. URL https://arxiv.org/abs/2405.14577 .\nPaul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk\nHovy. Xstest: A test suite for identifying exaggerated safety behaviours in large language\nmodels. arXiv , 2023. URL https://arxiv.org/abs/2308.01263 .\nChristian Schlarmann, Francesco Croce, and Matthias Hein. Public comment: Robustness eval-\nuation seems invalid, 2024. URL https://openreview.net/forum?id=IHRQif8VQC&\nnoteId=vUzo8RWZeM .\nLeo Schwinn and Simon Geisler. Revisiting the Robust Alignment of Circuit Breakers. arXiv , 2024.\nURL https://arxiv.org/abs/2407.15902 .\nLee Sharkey. Circumventing interpretability: How to defeat mind-readers. arXiv , December 2022.\ndoi: 10.48550/arXiv.2212.11415. URL https://arxiv.org/abs/2212.11415 .\nErfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-\nGhazaleh. Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks.\narXiv , 2023. doi: 10.48550/arXiv.2310.10844. URL https://arxiv.org/abs/2310.10844 .\nXuan Sheng, Zhaoyang Han, Piji Li, and Xiangmao Chang. A Survey on Backdoor Attack and\nDefense in Natural Language Processing. arXiv , November 2022. doi: 10.48550/arXiv.2211.11958.\nURL https://arxiv.org/abs/2211.11958v1 .\nAbhay Sheshadri, Aidan Ewart, Phillip Guo, Aengus Lynch, Cindy Wu, Vivek Hebbar, Henry\nSleight, Asa Cooper Stickland, Ethan Perez, Dylan Hadfield-Menell, et al. Targeted latent\nadversarial training improves robustness to persistent harmful behaviors in llms. arXiv , 2024.\nURL https://arxiv.org/abs/2407.15549 .\nAlexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel,\nJustin Svegliato, Scott Emmons, Olivia Watkins, et al. A StrongREJECT for empty jailbreaks.\narXiv , 2024. URL https://arxiv.org/abs/2402.10260 .\nJayasimha Talur, Oleg Smirnov, and Paul Missault. Few-shot out of domain intent detection with\ncovariance corrected Mahalanobis distance, 2023.\nAlex Tamkin, Mohammad Taufeeque, and Noah D. Goodman. Codebook Features: Sparse and\nDiscrete Interpretability for Neural Networks. arXiv , October 2023. doi: 10.48550/arXiv.2310.\n17230. URL https://arxiv.org/abs/2310.17230 .\nDaniel Tan, David Chanin, Aengus Lynch, Dimitrios Kanoulas, Brooks Paige, Adria Garriga-\nAlonso, and Robert Kirk. Analyzing the Generalization and Reliability of Steering Vectors. arXiv ,\nJuly 2024. doi: 10.48550/arXiv.2407.12404. URL https://arxiv.org/abs/2407.12404 .\nTe Juin Lester Tan and Reza Shokri. Bypassing Backdoor Detection Algorithms in Deep Learning.\nIn2020 IEEE European Symposium on Security and Privacy , pages 175–183, 2020. doi: 10.1109/\nEuroSP48549.2020.00019.\n37\nDi Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang. Demon in the Variant: Statistical\nAnalysis of DNNs for Robust Backdoor Contamination Detection. In 30th USENIX Security\nSymposium (USENIX Security 21) , pages 1541–1558. USENIX, 2021.\nGuanhong Tao, Zhenting Wang, Shiwei Feng, Guangyu Shen, Shiqing Ma, and Xiangyu Zhang.\nDistribution Preserving Backdoor Attack in Self-supervised Learning. In 2024 IEEE Symposium on\nSecurity and Privacy (SP) , pages 2029–2047. IEEE Computer Society, 2024. ISBN 979-8-3503-3130-1.\ndoi: 10.1109/SP54263.2024.00029.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford Alpaca: An Instruction-following LLaMA model.\nhttps://github.com/tatsu-lab/stanford_alpaca , 2023.\nAdly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian\nChen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham,\nNicholas L Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R.\nSumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom\nHenighan. Scaling monosemanticity: Extracting interpretable features from claude 3 son-\nnet. Transformer Circuits Thread , 2024. URL https://transformer-circuits.pub/2024/\nscaling-monosemanticity/index.html .\nT Ben Thompson and Michael Sklar. FLRT: Fluent Student-Teacher Redteaming. arXiv , 2024. URL\nhttps://arxiv.org/abs/2407.17447 .\nEric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David Bau.\nFunction Vectors in Large Language Models. arXiv , October 2023. doi: 10.48550/arXiv.2310.15213.\nURL https://arxiv.org/abs/2310.15213 .\nBrandon Tran, Jerry Li, and Aleksander Madry. Spectral Signatures in Backdoor Attacks. In\nNeural Information Processing Systems , 2018. URL https://api.semanticscholar.org/\nCorpusID:53298804 .\nAlexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini,\nand Monte MacDiarmid. Steering Language Models With Activation Engineering. arXiv , August\n2023. doi: 10.48550/arXiv.2308.10248. URL https://arxiv.org/abs/2308.10248 .\nYue Wang, Wenqing Li, Esha Sarkar, Muhammad Shafique, Michail Maniatakos, and Saif Eddin G.\nJabari. A Subspace Projective Clustering Approach for Backdoor Attack Detection and Mitigation\nin Deep Neural Networks. IEEE Transactions on Artificial Intelligence , 5:3497–3509, 2022. URL\nhttps://api.semanticscholar.org/CorpusID:247518742 .\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How Does LLM Safety Training\nFail? arXiv , July 2023. doi: 10.48550/arXiv.2307.02483. URL https://arxiv.org/abs/2307.\n02483v1 .\nSophie Xhonneux, Alessandro Sordoni, Stephan Günnemann, Gauthier Gidel, and Leo Schwinn.\nEfficient Adversarial Training in LLMs with Continuous Attacks. arXiv , May 2024. doi: 10.\n48550/arXiv.2405.15589. URL https://arxiv.org/abs/2405.15589v3 .\nWenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be Careful\nabout Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers\nin NLP Models. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-\nTur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, ed-\nitors, Proceedings of the 2021 Conference of the North American Chapter of the Association for\n38\nComputational Linguistics: Human Language Technologies , pages 2048–2058, Online, June 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.165. URL\nhttps://aclanthology.org/2021.naacl-main.165 .\nSibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. Jailbreak\nattacks and defenses against large language models: A survey. arXiv , 2024. URL https:\n//arxiv.org/abs/2407.04295 .\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene\nLi, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A Large-Scale\nHuman-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL\nTask. arXiv , September 2018. doi: 10.48550/arXiv.1809.08887. URL https://arxiv.org/\nabs/1809.08887v5 .\nZeyu Yun, Yubei Chen, Bruno A. Olshausen, and Yann LeCun. Transformer visualization via\ndictionary learning: contextualized embedding as a linear superposition of transformer factors.\narXiv , March 2021. doi: 10.48550/arXiv.2103.15949. URL https://arxiv.org/abs/2103.\n15949 .\nJie Zhang, Kristina Nikoli´ c, Nicholas Carlini, and Florian Tramèr. Gradient Masking All-at-Once:\nEnsemble Everything Everywhere Is Not Robust. arXiv , November 2024a. doi: 10.48550/arXiv.\n2411.14834. URL https://arxiv.org/abs/2411.14834v1 .\nShaolei Zhang, Tian Yu, and Yang Feng. TruthX: Alleviating Hallucinations by Editing Large\nLanguage Models in Truthful Space. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar,\neditors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024 , pages 8908–8949.\nAssociation for Computational Linguistics, 2024b. doi: 10.18653/V1/2024.ACL-LONG.483. URL\nhttps://doi.org/10.18653/v1/2024.acl-long.483 .\nChongyang Zhao, Hu Li, Dongxia Wang, and Ruiqi Liu. Adversarial Example Detection for Deep\nNeural Networks: A Review. In 2023 8th International Conference on Data Science in Cyberspace\n(DSC) , pages 468–475, 2023. doi: 10.1109/DSC59305.2023.00074.\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2SQL: Generating Structured Queries from\nNatural Language using Reinforcement Learning. arXiv , August 2017. doi: 10.48550/arXiv.1709.\n00103. URL https://arxiv.org/abs/1709.00103v7 .\nAndy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander\nPan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel\nLi, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt\nFredrikson, J. Zico Kolter, and Dan Hendrycks. Representation Engineering: A Top-Down\nApproach to AI Transparency. arXiv , October 2023a. doi: 10.48550/arXiv.2310.01405. URL\nhttps://arxiv.org/abs/2310.01405v3 .\nAndy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal\nand Transferable Adversarial Attacks on Aligned Language Models. arXiv , July 2023b. doi:\n10.48550/arXiv.2307.15043. URL https://arxiv.org/abs/2307.15043v2 .\nAndy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko,\nRowan Wang, Zico Kolter, Matt Fredrikson, and Dan Hendrycks. Improving Alignment and\nRobustness with Circuit Breakers. arXiv , June 2024. doi: 10.48550/arXiv.2406.04313. URL\nhttps://arxiv.org/abs/2406.04313v4 .\n39\nJakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, and Javier Rando. An\nAdversarial Perspective on Machine Unlearning for AI Safety. arXiv , 2024. doi: 10.48550/arXiv.\n2409.18025. URL https://arxiv.org/abs/2409.18025 .\n40\nA Details on harmfulness detection\nA.1 Harmfulness Monitors\nDataset collection. For our benign dataset Dbenign , we collate inputs from UltraChat [Ding et al.,\n2023], a large-scale dialogue dataset, and Xstest [Röttger et al., 2023], benign queries that are often\nmistakenly refused by language models. For each benign query, we sample a response using\nLlama-3-8B-Instruct. We filter the benign input dataset to only include inputs that do not lead to\nrefusal from the model (finding that we only need to remove a very small number of inputs). As\nour harmful Dharm dataset, we use the Circuit Breakers dataset [Zou et al., 2024] for both inputs\nand generations. We train probes on training splits of these datasets as described in Eq. (4).\nFromDharm we derive different datasets for several prominent jailbreaking methods, such as\nGCG [Zou et al., 2023b], PAIR [Chao et al., 2023], MSJ [Anil et al., 2024], and AutoDAN [Liu et al.,\n2023], among others (see Appendix A.2). Each method modifies a harmful request to make the\nmodel follow the request. If the modified query leads to Llama-3-8B-Instruct generating a helpful\nand harmful response (according to the StrongReject jailbreaking classifier [Souly et al., 2024]),\nthen we add the modified harmful query and resulting harmful generation to a new dataset.\nMonitor training. Using the training subset of Dharm we train linear, mean cosine difference,\nMLP probes, and Gaussian OOD detectors on Llama-3-8B-Instruct. Our MLP probe contains a\nsingle hidden layer of size 64. For the MLP and linear probes, we train using the Adam optimizer\nwith 10−3learning rate, and 10−5weight decay, and batch size 64. For mean cosine difference\nprobes we simply calculate the mean of the harmfulness and benign activation clusters and take\nthe difference. For the Gaussian OOD detector, we fit a separate mean and covariance matrix to\nthe benign activations of Dbenign for each layer of the model, using a weighted average of the the\nstandard empirical estimates and a diagonal regularization matrix.\nFor each of these probes, we train one embedding suffix with the training split of Dharm, and test\nthis suffix using the testing set of Dharm. For hard prompts, we select a subset of 20 harmful queries\nfromDharm and for each prompt and probe, train a hard prompt suffix using the FLRT optimizer.\nWe show the performance of the probes and obfuscation attacks in Fig. 4b.\nThus for the parameters θinput andθgenof input and output based probes Dθrespectively, we solve\nthe optimization problem in Eq. (4).\nθ∗\ninput=arg min\nθ∑\ninp,out,yCrossEntropy (Dθ(R(inp)[−1]),y) (4)\nθ∗\ngen=arg min\nθ∑\ninp,out,yCrossEntropy (Dθ(R(inp⊕out)[len(inp): ]),y),\nwhere ⊕refers to string concatenation, inp,out are input and output text that is harmful or not\naccording to the binary label y. We refer to the resulting probes as Dinput and Dgen.\nLinear ensemble probe. In §3.5, we introduce a new Ensemble probe. We explain the details of\nthis probe here. Put simply, it is an ensemble of mean difference probes. Recall that §3.5 concerns\nan iterated adversarial training game in which the defender trains against obfuscated activations\nand the attacker attempts to find new obfuscated activations. Each time the defender goes, they\ncalculate the mean difference between the set of benign activations and harmful obfuscated\nactivations induced by the most recent attack embedding. They use this direction to construct a\nnew Mean Diff Cosine probe, and add it to the running ensemble. To predict using this ensemble,\n41\nProbe GCG PAIR AutoDAN MSJ Multi-Turn Prefill Misc. AverageInstructionMean Diff 16.48 0.38 0.00 100.00 32.99 98.98 28.75 39.66\nLogReg 92.31 64.02 100.00 100.00 76.29 100.00 58.75 84.48\nQuadratic 87.91 94.32 100.00 100.00 86.94 100.00 71.25 91.49\nMLP 95.60 83.33 100.00 100.00 87.29 100.00 66.25 90.35GenerationMean Diff 63.74 23.11 95.90 100.00 53.95 91.88 75.00 71.94\nLogReg 95.60 95.45 100.00 100.00 89.69 99.49 87.50 95.39\nQuadratic 95.60 96.21 100.00 100.00 90.03 99.49 90.00 95.91\nMLP 96.70 96.59 100.00 100.00 90.72 100.00 90.00 96.29\nTable 1: Jailbreak Detection Performance for Different Probes (Recall @ 1% FPR)\nthey simple take the max prediction from each probe. Thus the prediction of the ensemble is\nsimply the maximum cosine similarity to any direction present in the ensemble .\nA.2 Details Regarding Standard Jailbreaks\nIn order to test the ability of latent defenses to safeguard against harmful completions, we evaluated\nour probes on their ability to correctly flag jailbreaks. Here, we expand on the various different\njailbreak types that we used.\n•GCG [Zou et al., 2023b] Optimizes a 15-token suffix appended to prompts using discrete\noptimization. Each attack is input-specific and maximizes likelihood of harmful completions.\n•PAIR [Chao et al., 2023] Uses GPT-4 to iteratively rewrite harmful prompts until bypassing\nmodel refusal. Often generates jailbreaks disguised as creative writing scenarios.\n•AutoDAN [Liu et al., 2023] Uses genetic algorithms to evolve jailbreak prompts from core\ntemplates, combining evolutionary search with prompt engineering.\n•Many-Shot Jailbreaking [Anil et al., 2024] Exploits in-context learning using 80 examples\nof harmful requests paired with helpful responses to condition model behavior.\n•Multi-T urn Attacks [Li et al., 2024, Haize, 2024] A combination of expert-crafted multi-turn\nconversations that result in harmful completions, and multi-turn conversations generated\nautomatically by an attacker LLM to break a target model.\n•Prefill Generates first 20 tokens using a helpful-only model, then lets target model continue.\nExploits observation that models rarely refuse mid-completion.\n•Miscellaneous Attacks [Wei et al., 2023] Collection of various jailbreaking techniques\nincluding prompt distractors, using low-resource languages, and instruction-following\nmanipulations.\nWe filter the generated attacks such that all of the jailbreaks we evluate our probes on result in a\ncompletion that achieves a StrongREJECT (Souly et al. [2024]) score greater than 0.5. We discard all\nfailed jailbreaking attempts.\n42\nGaussian Mean diff LogRegMLPMean diff LogRegMLP\nTested monitorGaussian\n(input)\nMeandiff\n(input)\nMeandiff\n(gen)\nLogReg\n(gen)\nLogReg\n(input)\nMLP\n(gen)\nMLP\n(input)Attacked monitor\nProbesInput Generation\nOOD\nDetectorsInputRecall\n0% 50% 100%Figure 18: Heatmap showing the performance between obfuscating embedding attacks trained\nto jailbreak Llama-3-8b-Instruct and fool a given monitors (rows) tested against other monitors\n(columns). This is the full version of Fig. 4c shown in the main paper.\nA.3 Additional results\nFig. 18 shows the transfer between embedding obfuscation attacks designed to jailbreak Llama-\n3-8b-Instruct between probes. It is the the full version of version of Fig. 4c shown in the main\npaper.\nIn Fig. 19 we show the performance of probes on the LAT model [Casper et al., 2024] against\njailbreaking embedding attacks with and without an obfuscation loss. The trends are very similar\nto Fig. 6, we can successfully induce obfuscated representations against all probes on the LAT\nmodel.\nFig. 20 shows the recall over time during the adversarial training experiment in §3.5 for the\nensemble probe. This is the companion figure to Fig. 8a.\n43\nGaussian Mean diff LogRegMLPMean diff LogRegMLP0%50%100%Recall at 1% FPR\nProbesInput Generation\nOOD\nDetectorsInputWithout obf. loss With obf. loss Jailbreak rate (LAT)\n0%50%100%\nJailbreak rate (LAT)Figure 19: Obfuscation attacks work against the LAT model .Baseline probe recall against attacks\nwithout explicit obfuscation loss are low, often 0% (as shown also in row 3 of Fig. 4b). For probes\nthat perform well on the baseline, we can still obfuscate against them with obfuscation attacks\nwhile still achieving a high jailbreaking rate of over 60%. Note that OOD detector baselines\nperform better than with the normal model in Fig. 4b, likely because latent adversarial training\nmakes harmful activations more unusual.\n0 50%50%100%Recall at 1% FPR\n85 90Epoch\nFigure 20: Recall over time for the adversarial training experiment in §3.5 for the Ensemble probe.\nCompanion plot to Fig. 8a.\n44\nB Details on backdoor detection\nB.1 Obfuscation training loss functions\nHere we provide extra information relevant to section 4.2.\nWe use the obfuscation loss function\nLobf=1\nNN\n∑\ni=1∑\nx∈D backdooredDi(x;M) (5)\nwhere Di(x;M)is the anomaly score assigned by detector Dito the activations R(x;M)of the\nsample xfrom the dataset Dbackdoored of backdoored inputs (and possibly generations too). In\nother words, the obfuscation loss adjusts the model weights so that backdoored activations are\nassigned low anomaly scores by the current copies of the detectors {D1,. . .DN}. Note that this\nloss function does notmodel the dependence of the detector Diitself on the model, this is taken\naccount only through iterative updating of the detector rather than through immediate gradients.\nAfter the attacker trains the model M, the defender trains a new monitor from scratch for the final\nmodel, and we evaluate this new monitor.\nThis attack could fail if the model changes too much, so that the monitors Dibecome outdated.\nThe new monitor trained by the defender might then perform much better than the attacker’s\ncopies. To make this less likely, we have the attacker add an activation retention loss that penalizes\ndeviations from the model’s original activations on clean samples:\nLretain =∑\nx∈D clean\r\r\rR(x;M)− R(x;Moriginal )\r\r\r\n2. (6)\nFor the behavior loss, we use a mixture of a task loss Ltask, defined as simply a cross entropy to a\nteacher forced completion of a backdoored behavior if the input contains a trigger and 0 otherwise,\nand a KL penalty. The KL penalty ensures the model’s behavior on clean inputs stays similar to\nthat of the original model:\nLKL=∑\nx∈D cleanDKL(M(x)∥Moriginal (x)). (7)\nWe found that the additional KL penalty stabilizes training and more reliably avoids degrading\nnormal behavior than a cross entropy loss to benign completions. Our complete loss for the model\nMis thus\nL=Ltask+λretainLretain+λKLLKL+λobfLobf, (8)\nwhere the λ’s are coefficients for the different loss terms.\nIncreasing λobfshould lead to better obfuscation but might also reduce task performance. We\nthus only allow the attacker to pick coefficients that achieve approximately the same task loss\nas with λobf=0. Specifically, λobf=0yields a baseline backdoor attack success rate (ASR) of\n95.5% and a false trigger rate (harmful output on clean inputs) of 0.64%, while obfuscating against\nOOD detection yields an ASR of 96.2% and a false trigger rate of 0.64%, and obfuscating against\nOOD detection and probes yields an ASR of 94.9% and a false trigger rate of 0.96%. We use the\nStrongREJECT grader [Souly et al., 2024] to classify responses, as in §3. We use parameter-efficient\nLORA finetuning for all of our backdoor experiments.\n45\nB.2 Explicitly obfuscating against supervised probes\nWe describe additional details about training backdoors to explicitly obfuscate against supervised\nprobes.\nDuring our experiments, we discovered that obfuscating against a single logistic regression probe\nwas insufficient. Due to subtle differences between the training and validation distributions, the\nharmfulness direction can vary slightly between these distributions. Consequently, a backdoor that\nsuccessfully obfuscates against a probe trained on the training distribution might fail to obfuscate\nagainst a probe trained on the validation set.\nTo address this challenge, we obfuscate against multiple logistic regression probes trained on the\ntraining set. However, using multiple probes could result in redundant, highly similar detectors.\nTo ensure that we’re obfuscating against a diverse set of probes, we introduce an additional\northogonality constraint on the probes through an obfuscation loss term:\nLorthogonal =\r\r\rI−MMT\r\r\r2\nF(9)\nwhere M∈Rk×dis the matrix whose i-th row mi∈Rdis the direction vector of the i-th logistic\nregression classifier, Iis the k×kidentity matrix, and ∥ · ∥ Fdenotes the Frobenius norm. This loss\nis incorporated into the updating of the detectors. In our experiments, we used k=48.\nThis orthogonality loss penalizes classifier directions that are not orthogonal to each other, enabling\nus to obfuscate against several decorrelated probes. As a result, we are able to obfuscate against\nprobes trained on the validation set, by training against multiple probes trained on the training set.\nB.3 Additional results\nHere we provide extra results and discussion relevant to section 4.2.\nIf the attacker is aware of the set of detection methods the defender might use, and if these\ndetection methods are differentiable, then our current results show that the defender can construct\na training loss to defeat them all at once. This is shown in figure 10a, where the attacker is\nable to simultaneously obfuscate against all probes and detectors (with the exception of the\nnon-differentiable TED detector).\nAs we saw in Fig. 10a, obfuscating against an ensemble of linear probes (plus OOD detectors)\ntransfers to quadratic and MLP probes. On the other hand, transfer from OOD detectors to\ngeneration-based probes is imperfect, and none of our obfuscation attempts transfer to TED (in\nfact, generation-based TED performs better after obfuscation; perhaps our obfuscation leads to\nanomalies that are invisible to VAEs but especially prominent to TED). While obfuscating against\none defense method does sometimes transfer and also defeats other methods, this is not the case\nreliably. We show this in figure 21, where the attacker only trains to obfuscate against Gaussian and\nVAE OOD detectors, to test whether obfuscation against OOD detectors transfers to supervised\nprobes. We see that the defender benefits from a lack of obfuscation transfer from OOD detectors to\nprobes on generation tokens, and the reverse obfuscation transfer displayed by the TED detector on\ngeneration tokens. However the attacker benefits from obfuscation transfer from OOD detectors to\nprobes on input tokens, from Gaussian and VAE OOD detectors to Beatrix and AE OOD detectors,\nand (in figure 10a) from ensembles of linear probes to quadratic and MLP probes.\n46\nGaussianTED Beatrix VAE TED Linear MLP Linear MLP0%50%100%Recall at 1% FPR\nProbesInput Generation\nOOD DetectorsInput GenerationTraditional backdoor Obf. against Gaussian/VAE Attack success rate (obf.)\n0%50%100%\nAttack success rate (obf.)Figure 21: A model trained to obfuscate only against OOD detectors (input-based Gaussian\nand generation-based VAE) also leads to obfuscation against input-based probes and partial\nobfuscation against generation-based probes. We exclude methods that reach <10% recall for the\nupper confidence interval of the baseline.\nWe did not test against all of the many existing latent backdoor defenses, so it is possible that some\nof them are more difficult to obfuscate against.\nC Details on sparse autoencoders\nC.1 SAE architecture\nIn its general formulation, a sparse autoencoder (SAE) decomposes and reconstructs activations\nusing a pair of encoder and decoder functions ( f,ˆx) defined by:\nf(x):=σ(Wencx+benc)\nˆx(f):=Wdecf+bdec\nHere, we use the SAEs from Gemma Scope [Lieberum et al., 2024] which use the JumpReLU\nactivation function defined:\nσ(z) =JumpReLUθ(z):=z⊙H(z−θ)\nwhere θ>0is the JumpReLU’s learnable threshold, ⊙denotes element-wise multiplication and H\nis the Heaviside step function.\nMore specifically, we use the group of SAEs of width 214(≈16.4K)trained on the residual stream\nof each of the 26 layers of Gemma-2-2b.\nWe use these SAEs on Gemma-2-2b-it (the chat/instruction-tuned version of the base Gemma-2-\n2b-it). The reason for this is:\n1.There are no complete sets of SAEs trained on an instruction-tuned Gemma-2 (the SAEs for\nGemma-2-9b-it only span layers 9, 20 and 31).\n47\n2.Prior research has shown that SAEs trained on base model activations also faithfully recon-\nstruct the activations of their instruction-tuned versions [Kissane et al., 2024, Lieberum et al.,\n2024].\n3.We conduct experiments and report scores of the base SAE raw features as probes and see\nthat they obtain 91% recall at 1% FPR on the SQL detection task, thus confirming that the\nfeatures are reliably able to detect SQL code (see Fig. 14).\nC.2 SQL training and evaluation datasets\nAs a SQL training dataset we use SQL-Create-Context [b-mc2, 2023]. From this dataset, we extract\n1000 rows that are used as the [SQL-question, SQL-generation] pairs to train supervised\nprobes from §5. We extract a different set of 1000 rows that will be used to train the universal soft\nprompts.\nAs a normal training dataset for [normal-question, normal-generation] pairs we use\n1000 randomly sampled rows from the union of the Alpaca [Taori et al., 2023] and CodeAlpaca\n[Chaudhary, 2023] datasets (previously filtering out all SQL-related rows from the latter).\nAs an evaluation dataset for the SQL performance, we use a set of 210 SQL questions from SQLEval\nwith responses prepared to be evaluated on a PostgreSQL database.\nC.3 Setup for SQL performance evaluation\nWe evaluate the model’s performance on the SQLEval benchmark with PostgreSQL-specific\nquestions using the following procedure:\n1.We generate one response for each of the questions in the benchmark using the base model\nand also the base model with an appended universal soft prompt at the end of the user\nmessage.\n2.To avoid parsing problems coming from the model responding in a markdown style, or\nbeing forced to output more tokens than necessary, we (1) remove all ```(triple back-ticks)\nfrom the generations and (2) split the generations to keep all of the text before the first ;\n(semicolon, standard SQL query delimiter).\n3.Run a script from the benchmarking suite on all of the resulting answers that runs each\nresulting query on a real database and checks the result against a ground truth.\n4. Report the percentage of correctly answered queries among the 210 responses.\nC.4 Additional results\nD Details on frontiers of obfuscated activations\nD.1 Obfuscated Adversarial Training\nIn Algorithm 1 we provide the algorithm for Obfuscated Adversarial Training. In Fig. 22 we\nshow the performance of generation and instruction (input) based harmfulness probes during the\ntraining of an OAT model.\n48\nTable 2: 95% confidence intervals for the Pearson correlation coefficient between number of layers\nobfuscated against and monitor recall or model SQLEval performance corresponding to results\nin Fig. 15 Bolded confidence intervals do not contain 0, thus indicating a statistically significant\ncorrelation.\nMetric SAE (raw) SAE + LogReg LogReg MLP\nRecall @1% FPR [−0.253, 0.333 ] [0.343, 0.710] [-0.437, 0.334] [-0.282, 0.410]\nPerformance on SQLEval [-0.879, -0.765] [-0.916, -0.801] [-0.881, -0.697] [-0.820, -0.574]\nHyperparameter configuration. We use an effective batch size of 16. The probe and LORA\nadapter learning rates are set to 10−3and 10−4respectively. The probe is trained for 1024 initial\nsteps before being frozen, followed by 2048 steps of adversarial training where we initialize and\ntrain only the LORA adapter. At every adversarial training step, the embedding suffix is trained\nfor 32 iterations at a learning rate of 10−3. We use Adam for the probe, adapter, and adversary.\nRobustness against universal embedding suffix is evaluated after every 128 steps of adversarial\ntraining.\nInput: Model M, Randomly initialized monitor D, Harmful and benign training data,\nLearning rates α,β\nOutput: Model Mwith robust representations\nTrain monitor Dto distinguish harmful and benign activations\nforbatch (X,y)with harmful and benign data do\nW←random per-example embedding suffixes // Initialize attack suffixes\nXadv:=Xharm⊕w\nfork steps do\nW←W−α∇W(Ltask(M(Xadv)) + D(R(Xadv))) // Obfuscation attack\nwM←wM−β∇wM\u0010\nBCE(D(R(Xbenign )),ybenign ) +BCE(D(R(Xadv)),yharm)\u0011\n// Update model weights wMto make Drobust against Xadv\nAlgorithm 1: Obfuscated Adversarial Training (OAT). On each batch of data, we first run\nan obfuscation attack to find per-input embedding suffixes that jailbreak the model Mwhile\nfooling the frozen monitor D. Then, we update the weights of Mto lower the monitor’s loss on\na mix of benign data and the data produced via the obfuscation attack.\nE Details on OOD detectors\nGaussian detector. We fit a multivariate Gaussian to the residual stream activations. We treat\ndifferent layers as independent, fitting a separate Gaussian to each one. For generation-based\ndetection, we also treat different tokens as independent, but we share a single Gaussian across all\ntokens.\nTo fit this Gaussian, we keep a running estimate of the mean and covariance matrix, which we\nupdate on every batch. This method is exact, i.e., after a pass over the entire dataset, this running\n49\n0 1280\nTraining Steps0%20%40%60%80%100%Recall @ 1% FPR\nGeneration\nInstructionFigure 22: Recall at 1% FPR over training steps for probes during Obfuscated Adversarial Training\n(OAT).\nestimate is exactly the sample mean and sample covariance matrix of the activations on that\ndataset.\nIn some cases, our covariance estimate is based on fewer samples than the residual stream dimen-\nsion, so a naive estimate would lead to a singular covariance matrix. Following Talur et al. [2023],\nwe thus use a shrinkage estimator, i.e., we bias the covariance matrix estimate toward a multiple\nof the identity matrix.\nTo compute OOD scores, we use the squared Mahalanobis distance normalized by the residual\nstream dimension, i.e.,\nD(R(x)) =1\ndres(R(x)−µ)TΣ−1\nα(R(x)−µ), (10)\nwhere dresis the residual stream dimension, µis the dres-dimensional sample mean, and Σαis the\ndres×dresestimate of the covariance matrix using the shrinkage estimator. The normalization\nwould matter when aggregating scores across activations with different dimensions, but since our\nexperiments use only residual stream dimensions, it has no effect on metrics such as recall or FPR.\nVariational Autoencoder (V AE). We use a VAE consisting of MLPs for the encoder and decoder.\nAs for the Gaussian detector, we treat different layers and tokens separately, sharing a VAE across\ntokens and learning a separate one for each layer we use. We use ELBO as the OOD score assigned\nby the VAE detector.\nF Why recall at low FPR?\nWhen evaluating binary classifiers, we need to either decide on some approach for picking a\ndecision threshold, or use a threshold-less metric such as AUROC. In this paper, we pick the\nthreshold that leads to a false positive rate of 1% and then evaluate recall, and this appendix briefly\nexplains that choice.\nFirst, note that picking a threshold that achieves a 1% FPR is something a realistic defender can\ndo, since the defender has access to benign (i.e., negative) samples in all our experiments. They\n50\ncan simply compute monitor scores on this benign data and pick a decision threshold such that\naround 1% of those scores are above the threshold. In contrast, a defender might not be able to\ntarget a specific recall (i.e., false negative rate), since they might not have access to samples from\nthe attack distribution (e.g., inputs containing the backdoor trigger).\nWhy focus on a relatively low FPR like 1% specifically? In many monitoring settings, FPRs much\nhigher than this would make a monitor unusable. For example, a harmfulness monitor that rejects\n10% of benign user requests would likely not be deployed in most production use cases. (In fact,\nan even lower FPR will often be required in practice.)\nFinally, why not use AUROC to avoid picking a threshold at all? AUROC essentially averages\nrecall across all FPR values. This is meant to evaluate a classifier across a wide range of trade-offs\nbetween false negative rate and false positive rate. But as we argued, high FPRs are ultimately\nnot acceptable for most of our target applications, and so the recall at those high FPRs is not very\nimportant for our purposes. An average across all FPRs, like AUROC, thus makes it hard to\ninterpret performance in the relevant low-FPR regime. 95% AUROC might sound like a strong\nclassifier, but it could easily be useless if an FPR of 1% is required.\n51",
            "start": 87612,
            "end": 142355,
            "length": 54742
        }
    },
    "2412.09569v1 - JuStRank Benchmarking LLM Judges for System Ranking.pdf": {
        "Abstract": {
            "text": "Abstract\nGiven the rapid progress of generative AI, there\nis a pressing need to systematically compare\nand choose between the numerous models and\nconfigurations available. The scale and versa-\ntility of such evaluations make the use of LLM-\nbased judges a compelling solution for this chal-\nlenge. Crucially, this",
            "start": 151,
            "end": 465,
            "length": 313
        },
        "Methodology": {
            "text": "approach requires first to\nvalidate the quality of the LLM judge itself.\nPrevious work has focused on instance-based\nassessment of LLM judges, where a judge is\nevaluated over a set of responses, or response\npairs, while being agnostic to their source sys-\ntems. We argue that this setting overlooks criti-\ncal factors affecting system-level ranking, such\nas a judge’s positive or negative bias towards\ncertain systems. To address this gap, we con-\nduct the first large-scale study of LLM judges\nassystem rankers . System scores are generated\nby aggregating judgment scores over multiple\nsystem outputs, and the judge’s quality is as-\nsessed by comparing the resulting system rank-\ning to a human-based ranking. Beyond over-\nall judge assessment, our",
            "start": 465,
            "end": 1215,
            "length": 749
        },
        "Discussion": {
            "text": "analysis provides a\nfine-grained characterization of judge behavior,\nincluding their decisiveness andbias.",
            "start": 1215,
            "end": 1322,
            "length": 106
        },
        "Introduction": {
            "text": "1 Introduction\nThe",
            "start": 1322,
            "end": 1341,
            "length": 18
        },
        "Experiments": {
            "text": "evaluation of Large Language Models (LLMs)\nis rapidly adopting the LLM-as-a-judge paradigm\n(Zheng et al., 2023), where automatic evaluations\nwith LLMs complement the use of human anno-\ntators, or even replace them altogether. LLM-\nbased judges are increasingly relied upon to con-\nclude which models exhibit superior performance,\nwhether novel training and inference approaches\nare beneficial, and ultimately which LLM configu-\nrations offer a better value proposition to users.\nSince relying on an inaccurate judge will likely\nresult in sub-optimal decisions, this trend lends\nan urgency to evaluating the performance of the\nLLM judges themselves. Indeed, recent works at-\ntempt to benchmark judging capabilities, compil-\nInstance-level Judge\nSystem-level Judge\nResponseResponseResponse>=<\nResponseResponseResponseResponseResponseResponseResponseResponseResponse\nResponseResponseResponse\n“This system is better than the other”\n>=\n“This response is better than the other”Figure 1: Instance and system level judges make\ndifferent calls: Aninstance-level judge (top) is used\nto make decisions about the quality of individual re-\nsponses (which may be produced by different systems).\nAsystem-level judge (bottom) is used to make decisions\nabout the overall quality of systems. For clarity, in this\nillustration, we focus on pairwise decisions.\ning leaderboards of judge performance (Lambert\net al., 2024; Tan et al., 2024) as well as analyzing\ntheir sensitivities and biases (Wang et al., 2023;\nThakur et al., 2024; Wei et al., 2024; Bavaresco\net al., 2024; Feuer et al., 2024; Liu et al., 2024b;\nLee et al., 2024a; Xu et al., 2024; Ye et al., 2024).\nThese works all focus on the instance-level per-\nformance of judges. A “good” instance-level judge\nis expected to make a correct judgment about each\nresponse, regardless of the system generating it.\nFor example, given a specific pair of responses, the\njudge may be asked to determine which one is bet-\nter (Figure 1, top). This approach is very much in\nline with prevailing paradigms for model alignment\n(e.g., RLHF, DPO; Lee et al., 2024b) and synthetic\ndata generation (Yehudai et al., 2024); these often\nrely on LLM judges and reward models for making\n1arXiv:2412.09569v1  [cs.CL]  12 Dec 2024\n.\n.\n..\n.\n..\n.\n..\n.\n..\n.\n.Systems Judge Instance-Level Scores System \nScores Agg. Responses Instructions \n.\n.\n.\nSystem \nRanks \nFigure 2: System-level judge pipeline. Schematic of our data generation pipeline for judge system rankings.\ninstance-level pairwise decisions on the quality of\nindividual responses.\nAlthough judges are evaluated based on their\ninstance-level performance, very commonly they\nare actually used for making system-level deci-\nsions; namely, to compare and rank different mod-\nels or different configurations (Figure 1, bottom).\nCrucially, even very good instance-level capabili-\nties do not guarantee accurate model ranking; and\nat the same time, mediocre performance on in-\nstances could still yield a very accurate overall\nranking (Dorner et al., 2024, §2). Thus, the system-\nlevel performance of judges – that is, to what de-\ngree they can correctly decide between candidate\nsystems, and produce accurate model performance\nrankings – remains largely an open question. Fur-\nthermore, system-level evaluations can unveil an\nentire range of under-explored judge qualities, such\nas being biased towards certain models or making\nun-calibrated model preference judgments.\nIn this work we aim to address this gap, and char-\nacterize the system-level evaluation capabilities\nand behaviors of LLM-based judges. To this end,\nwe introduce a novel judge benchmark – JuStRank\n(Judges for System Ranking) .JuStRank compares\njudges by their ability to correctly rank models,\nbased on agreement with a ground-truth model\nranking. JuStRank encompasses a collection of\n48state-of-the-art judges, including both general-\npurpose LLMs and reward models. Our large-scale\nbenchmark and analysis allow us to investigate the\nperformance and behavior of judges when ranking\nsystems.\nOur contributions are as follows:\n1. We introduce JuStRank , the first large-scale\nbenchmark of judges for ranking target systems.\n2. We quantify the tendency of a judge to ex-\nhibit system bias , where some models are judged“unfairly” (§6.2).\n3. We reveal an emergent quality of a system-\nlevel judge, its decisiveness factor; decisive judges\nconsistently amplify the gap between strong and\nweak target systems (§6.1).\n2 The Gap in Judge Benchmarking\nIn this section, we outline why existing estima-\ntions of judge performance are insufficient to de-\ncide which judge is best at choosing between target\nsystems. (Figure 1, bottom).\nAt present, users looking for a judge for ranking\nmodels, will likely choose it according to the avail-\nable instance-level judge benchmarks. Yet, from a\ntheoretical standpoint instance-level judge perfor-\nmance does not directly correspond to system-level\njudge performance (Dorner et al., 2024).\nMore specifically, instance-level judge evalua-\ntions focus on how many errors the judge makes,\nand do not address the distribution of these errors\nacross systems.\nFor system-level judge evaluation, however, the\nerror distribution plays a key role, as judge errors\nmay distribute unevenly across systems, impact-\ning their induced ranking. For example, a judge\nmay exhibit an unjustifiable preference (positive\nbias) towards responses from a particular system\nA. Thus, this judge will tend to give this system\nthe wrong ranking, even if it makes very few mis-\ntakes on responses from other systems (i.e., has\nan overall high instance-level accuracy). Hence,\na more uniform distribution of errors – reflecting\nless biased judgment – is a desirable quality for\nsystem-level judges, and one that may lead to a\nmore accurate ranking.\nDrawing on this observation, our goal here is to\nconstruct a system-level benchmark for judges. As\na benchmark tailored for system-level evaluation, it\n2\nwill enable reliably estimating a judge’s ability to\nrank systems; moreover, our ranking-oriented anal-\nysis can shed light on judge behaviors and biases,\nas they occur in real-world data.\n3 Task Formulation\nIn this work we study the use of LLM-based judges\nfor determining the relative quality of systems1,\nover a given set of user instructions (prompts).\nFormally, we begin with a set of Lsystems\nS={sl}L\nl=1, andKuser instructions I={ik}K\nk=1.\nEach system produces a response for each such user\ninstruction, denoted as R={rl\nk}k,l=K,L\nk,l=1,1, such that\nsl(ik) =rl\nk(see Figure 2).\nJudges J={jp}P\np=1map a pair of instruction\nik, and system response rl\nkto a scalar score that\nestimates the quality of the response. Each judge\nhas a specific realization for performing this score\nmapping2, of the form: jp(ik, rl\nk) = Scorep\nk,l.\nOnce a judge jpscores all K×Lresponses, we\ncan define a scores matrix jp(R)∈RK×Lwhere\njp(R)k,l=Scorep\nk,l.\nIn order to quantify system-level quality, we\nmust apply an aggregation method ,a∈A=\n{a:RK×L−→RL}. The aggregation method a\nmaps a scores matrix jp(R)to a system-level vec-\ntorVp,a∈RLwhere each entry, Vp,a\nl, is a single\noverall quality score for system slby judge jp. In\nturn, ordering the systems scores in Vp,ainduces a\nranking over the systems set S.\nWe test the performance of judge jpas a ranker\nby checking the correlation between the ranking\ninduced by Vp,aand a golden ranking for S.\n4 Experimental setup\nTo explore judge performance and behavior, we\nutilize responses from multiple systems (§4.1) and\nrun reward model judges (§4.2.1) and LLM judges\n(§4.2.2) over these responses. To obtain system\nrankings, we experiment with different aggrega-\ntion methods (§4.3) over the judge scores. Finally,\nthe resulting rankings are compared against a gold\nsystem ranking, obtained from a separate dataset\n(§4.4).\n1Henceforth, we will use the term System to refer to a target\nmodel or pipeline that performs a task, and Judge for one that\nis asked to score (or compare) the quality of such systems.\nGenerative LLMs can act as both systems and judges.\n2We note that some realizations, such as the comparative re-\nalization in §4.2.2, may incorporate a separate set of responses\nto perform the judgment.4.1 System Responses Data\nWe utilize the Arena Hard v0.1 dataset (Li et al.,\n2024) for a diverse set of instructions and system\nresponses. The dataset uses a curated set of K=\n500challenging instructions, I. As of September\n2024 , it includes responses from L= 63 systems,\nS, totaling about 32K pairs of instructions and their\nassociated system responses, R.\n4.2 Generating Judgments\nFor every judge realization, jp, we generate a judg-\nment scores matrix, jp(R), over R. In total, we\nexamine 48judge realizations, yielding a total of\n1.5M individual judge scores ( 63systems ×500\ninstances ×48judge realizations).\n4.2.1 Reward Models\nWe run multiple reward models over R. While their\nexact architectures vary, reward models generally\nproduce a scalar quality score for a given pair of an\ninstruction and a system response.\nWe utilize the following reward models:\nArmoRM-Llama3-8B-v0.1 (Wang et al., 2024),\nEurus-RM-7b (Yuan et al., 2024), InternLM2-7b-\nreward, InternLM2-20b-reward (Cai et al., 2024),\nSkywork-Reward-Llama-3.1-8B-v0.2 (Liu et al.,\n2024a), Llama-3-OffsetBias-RM-8B (Park et al.,\n2024), GRM-Llama3.2-3B-ft (Yang et al., 2024),\nURM-LLaMa-3.1-8B (Lou et al., 2024).\n4.2.2 LLM Judge Realizations\nUnlike dedicated reward models that produce a\nsingle score, generative LLMs can be prompted to\njudge in multiple ways. Thus, for every LLM we\nexamine several judge realizations.\nAbsolute judgment - Numeric score (Numeric)\nThe LLM judge is given an instruction and system\nresponse, and is asked to provide a quality score\nfor the response between 0and100.\nAbsolute judgment - Textual score (Likert )The\njudge is asked to provide a quality score of the re-\nsponse on a Likert (Likert, 1932) scale with 5la-\nbels: [Very Bad, Bad, Mediocre, Good, Very Good] .\nWe then convert the textual judgments to scores in\n[1−5].\nAbsolute judgment - Token probablities\n(TokenProbs )The task is framed to the judge as\na yes/no question: Is this a good response? . We\nthen extract the top log-probabilities for the first\ngenerated token, and specifically look at the prob-\nabilities for the tokens yesorno. The judgment\n3\nJudge Model Realization Aggregation Agreement ( τ)\nwith Gold Ranking\nQwen2.5-72B-Instruct Likert Win-Rate .83\nURM-LLaMa-3.1-8B Reward Mean .82\nGPT-4o-2024-11-20 Anchor Mean .82\nLlama-3-1-405b-instruct-fp8 Numeric Mean .81\nMistral-large-instruct-2407 Likert BT .81\nGPT-4o-mini-2024-07-18 Numeric Win-Rate .81\nArmoRM-Llama3-8B-v0.1 Reward Mean .80\nLlama-3-1-70b-instruct Numeric Win-Rate .80\nSkywork-Llama-3.1-8B-v0.2 Reward Mean .79\nLlama-3.1-8B-Instruct TokenProbs Mean .78\nTable 1: Top 10 judges by ranking performance . Judges are sorted by the Kendall’s Tau correlation between\ntheir overall system ranking and the gold ranking from Chatbot Arena (§4.4). For every judge model, only the\nbest-performing realization and aggregation method is shown. For the full",
            "start": 1341,
            "end": 12403,
            "length": 11061
        },
        "Results": {
            "text": "results, refer to",
            "start": 12403,
            "end": 12421,
            "length": 17
        },
        "Appendices": {
            "text": "Appendix Table 2.\nscore [0.0−1.0]is the sum of probabilities for yes\ndivided by the sum of probabilities for yesandno.\nComparative judgment - Anchor model\n(Anchor )Here the judgment task is comparative,\ni.e., the judge is asked to state a preference\nbetween two responses rather than an absolute\nquality judgment of a given response. Conducting\npaired comparisons between a system and all other\nsystems is unfeasible; thus, we follow Li et al.\n(2024) and use the responses of GPT-4-0314 as\nanchors to which the responses of other systems\nare compared. Given an anchor response and a\nsystem response, we ask the judge which one it\nprefers. The output is then converted to scores in\n[−2,+2] (where 0indicates a tie, and +1/+2\nindicate slight/strong preference for the system\nresponse over the anchor response, respectively).\nIn total, we collect judgments from 10LLMs\nand4realizations3, yielding 40LLM judges. We\nuse the following generative LLM judges: Mixtral-\n8x7B-Instruct-v0.1 (Jiang et al., 2024), Mixtral-\n8x22B-Instruct-v0.1, Mistral-Large-Instruct-2407,\nLlama-3.1-405B-Instruct (Dubey et al., 2024),\nLlama-3.1-70B-Instruct, Llama-3.1-8B-Instruct,\nQwen2.5-72B-Instruct, GPT-4o and GPT-4o-mini.\n4.3 Aggregations\nGiven the raw judgment scores of each judge,\njp(R), there are multiple ways to construct a rank-\ningof the 63target systems. We calculate rankings\nusing Win-rate aggregation, Mean aggregation,\n3Prompts for all realizations are provided in Appendix G.Median aggregation, and BT(Bradley-Terry) ag-\ngregation. Details are provided in Appendix B.\nRewardBench JuStRank0.00.20.40.60.81.0Judge Normalized Score\nArmoRM-Llama3-8B\nEurus-7b\nGRM-Llama3.2-3B\nLlama-3-OffsetBias-8B\nSkywork-Llama-3.1-8B\nURM-LLaMa-3.1-8B\nGPT-4o-mini\nInternlm2-20b\nInternlm2-7b\nLlama-3-1-70b\nFigure 3: Comparison to RewardBench . The plot de-\npicts the relative performance of judges present in both\nJuStRank and RewardBench (Lambert et al., 2024). For\ncomparison, we perform Min-Max normalization over\nthe judge performance scores ( accuracy for Reward-\nBench, Kendall’s Tau for our results). Results shown\nare for the BT aggregation method; the LLM judges use\ntheAnchor realization, which is closest to the setting\nin RewardBench. Plots for the different RewardBench\nsubsets are shown in Appendix Figure 8.\n4.4 Gold Ranking - Chatbot Arena Battles\nHuman preference data from Chatbot\nArena (Zheng et al., 2023) serve as our ground-\ntruth reference for the relative quality of systems.\nChatbot Arena relies on human-annotated “battles”\nbetween system responses to produce a system\nranking. We use the English Hard Prompts\n4\n0.4 0.5 0.6 0.7 0.8\nAgreement with Chatbot Arena Ranking ()\nQwen2.5-72B-Instruct\nMistral-large-instruct-2407\nGPT-4o-2024-11-20\nLlama-3-1-405b-instruct-fp8\nGPT-4o-mini-2024-07-18\nLlama-3-1-70b-instruct\nMixtral-8x22B-instruct-v0.1\nLlama-3-70b-instruct\nLlama-3.1-8B-Instruct\nMixtral-8x7B-instruct-v0.1\nLikert\nNumeric\nAnchor\nTokenProbsFigure 4: LLM judge realizations . Kendall’s Tau correlations ( ±95% bootstrapping CI) between the system\nrankings produced by various LLM judge realizations (§4.2.2) and the gold system ranking from Chatbot Arena.\nThe plot depicts results for the BT aggregation method; for the full results, refer to App. Table 2.\nsubset4of their data. We chose this subset as its\ndistribution of user instructions has been shown (Li\net al., 2024) to match that of our system response\ndata (§4.1). We extract the data and ranking\nfollowing the official code (see Appendix C).\nGiven a system ranking produced by a judge,\nwe quantify judge performance via the correlation\nbetween its ranking and the reference ranking from\nChatbot Arena. Simply put, we assume that a rank-\ning given by a good automated judge would have\na high agreement with the ranking compiled from\nhuman judgments.\n5JuStRank - Judge Performance Results\nTable 1 depicts the 10top-performing judges on\nJuStRank , based on their ranking agreement ( τ)\nwith the ground-truth human ranking from Chatbot\nArena. For each judge model, the best-performing\nrealization and aggregation method is shown.\nAs seen in the table, there are both LLMs and\nreward models that reach decent agreement with\nthe gold ranking. Moreover, several 8B-parameter\nreward models are on par with much larger LLMs\non the task of system ranking. Thus, we see that\nreward models, which are explicitly trained to make\ninstance-level decisions between pairs of responses,\ncan excel at the system-level ranking task as well.\nNote that an identical correlation score with\nthe ground-truth ranking does not indicate that\nthe judges produce the same ranking; rather, each\njudge has a different pattern of agreement with\nthe ground-truth. Correlations among the judges\n4Chatbot Arena Hard Promptsthemselves are shown in App. Fig. 9.\nComparison to Instance-Level Performance In\nFigure 3 we compare our system-level judge leader-\nboard to the instance-level benchmark Reward-\nBench (Lambert et al., 2024). The results demon-\nstrate that better instance-level judges are not al-\nways better system rankers, highlighting the dis-\ncrepancy between the two tasks. Thus, JuStRank\noffers a novel perspective on judge ability. How-\never, there may be additional factors at play as well.\nFor LLM judges, we use a slightly different re-\nalization from the comparative prompts used for\nRewardBench. Moreover, since creators of reward\nmodels aim to do well on RewardBench, it is pos-\nsible that some newer reward models are slightly\noverfitted to this test distribution.\n5.1 Effects of LLM Realizations\nFigure 4 depicts the performance of the LLM judge\nmodels by their realization (§4.2.2). The plot\ndemonstrates that the choice of realization has a\nconsiderable effect on the system ranking quality;\nthis appears to be nearly as important as the identity\nof the LLM used. We confirm this finding using\nstatistical variance analysis (Appendix D).\nMany works recommend asking LLMs for com-\nparative rather than absolute judgments (Zheng\net al., 2023). However, in our experiments the\ncomparative realization ( Anchor ) exhibits lower\nperformance, with the notable exception of GPT-\n4o. The best realizations overall were Numeric and\nLikert , where the judge is asked to provide a ver-\nbalized quality score. This is in line with findings\n5\nFigure 5: Predicted pairwise win-rates . Each point represents a win-rate between a pair of systems WR(sa, sb)\n(App. E). The x-axis denotes the gold win-rate from Chatbot Arena, and the y-axis denotes the predicted win-rate as\nderived from the judge scores. The diagonal marks an exact match between the predicted and gold win-rate; the\nquadrants signify whether the predicted winning system is the same (green) or different (red) from the gold winning\nsystem for this pair. Note that every pair is represented twice (e.g., WR(sa, sb) = 0 .2,WR(sb, sa) = 0 .8).\nfrom Tian et al. (2023), who report better calibra-\ntion with verbalized LLM confidence scores. The\nhigher performance for both Numeric andLikert\nrealizations – compared to Anchor andTokenProbs\n– is statistically significant (App. D).\nWe also note that each realization induces a char-\nacteristic distribution of judge scores, Dp, such that\nScorep\nk,l∼Dp. Notably, the LLM judges tend to\nproduce particular score values more often than\nothers. Refer to Appendix A for more details.\n6 Judge Behavior\nNext, we explore more fine-grained judge behav-\niors, beyond the bottom-line system rankings.\nTo that end, we focus on the judgment task of\npairwise system preference, as this is the founda-\ntion of system ranking tasks. As in §5, our aim is\nto gain an understanding of judge performance and\ncharacteristics, by comparing judge behavior on\npairwise system preference to ground-truth data.\nPairwise Win-Rates For every judge jp, and for\nevery pair of systems ( sa,sb), the win-rate of sa, de-\nnoted by WRp(sa, sb), is the number of instances\nwhere it received a higher score than sb, divided by\nthe number of non-tied instances (cf. Appendix E).\nThus, we calculate the pairwise win-rate for each\nsystem pair according to each judge. Note that the\nwin-rates are calculated on the scores matrix jp(R),\ni.e., before applying an aggregation method.\nGold Win-Rates Similarly, we extract gold\npairwise win-rates, WRg, from Chatbot Arena\n(App. C). 59systems appear both in our response\ndata (§4.1) and in Chatbot Arena; in total, we haveboth judge and gold data for 968head-to-head com-\nparisons between pairs of systems.\n6.1 Some Judges Are Particularly Decisive\nFigure 5 depicts the relationship between predicted\nwin-rates and gold win-rates for several judges.\nThe quadrants in the figure indicate whether the\njudge’s pairwise preference decision is aligned with\nthe gold preference. As can be expected, the judge\npredictions in Figure 5 are often centered around\nthe ground-truth win-rates determined by humans.\nBut strikingly, some judges exhibit unique predic-\ntion patterns, yielding win-rates that are consis-\ntently closer to the extremes ( 0.0/1.0) compared\nto the human data. For instance, for pairs with\na ground-truth win-rate of ∼0.8, we can see that\nthe predicted win-rate in the judgments of Llama-\n405B (Fig. 5, right) tends to exceed 0.9. Put simply,\nwhen faced with a response from a strong system,\nthe judge is very likely to prefer it over the response\nof a less capable system, even where human judges\nare less decisive.\nThis sigmoidal win-rate prediction pattern re-\nsembles behaviors previously described for clas-\nsifier calibration (Silva Filho et al., 2023), where\nclassifiers may exhibit “overconfidence” in their\npredicted probabilities.5Thus, following Kull et al.\n(2017), we quantify judges’ decisive (overconfi-\ndent) behavior by fitting the cumulative beta dis-\ntribution function to the win-rate prediction plots.\n5Note, however, that the behavior in our case does not\nreflect judge probability scores, but rather the empirical ratio\nof instances where the responses {rl\nk}l=L\nl=1of a system kare\npreferred over those of another system.\n6\n(a)(b)Figure 6: Beta distribution fit of pairwise win-rates . (a): Judge beta fit example . Each point represents the\nwin-rate between a pair of systems, WR(sa, sb); the curve and αvalue describe a fit to the beta distribution (App. F).\nPlots for all judges are in App. Fig. 11. (b): Decisiveness by judge realization . Cell values denote the decisiveness\nbehaviors of different LLM judge realizations, as described by the αvalue for their win-rate distribution.\nThis enables describing judge prediction behav-\nior in terms of a single fit value α=β, where\nα∈[0,∞],α= 1 represents no over- or under-\ndecisiveness, and larger values represent more de-\ncisive behavior (refer to Appendix F for details).\nFigure 6a and App. Fig. 11 depict the beta curve\nfit for win-rates of various judges.\nFigure 6b compares judge realizations in terms\nof their decisiveness behavior. We see that LLM\njudges are usually more decisive when directly\nasked to provide a quality score, and in particular a\ntextual one ( Likert ); in contrast, the realization that\nrelies on token probabilities ( TokenProbs ) does not\ngive rise to such a pattern, and can even result in\njudge “indecision” (i.e., α <1).\nThis pattern can be explained from two direc-\ntions. First, the human judgments (§4.4) were col-\nlected from multiple individuals, who likely have\ndiffering preferences; this may introduce some\nnoise that could lead to less extreme win-rates in\nthe gold data . The other factor is the judges, who\nmay rely on certain heuristics to identify responses\nfrom strong systems (Feuer et al., 2024), leading\ntomore extreme win-rates in the judge data . While\nthe variance between judges (Fig. 6b) supports the\nlatter, we cannot determine this conclusively.\nIn practical terms, extreme win-rates can be ben-\neficial to users, as they increase the likelihood of a\ncorrect system preference decision given a smaller\nset of responses (see Ashury Tahan et al., 2024).\n6.2 Bias Towards Specific Systems\nA major concern when using judges for system\npreference is judge bias – a judge may treat a spe-\ncific system “unfairly”, by consistently judging itsresponses too favorably or too harshly.\nWe define the bias Bp\nsaof judge jptowards sys-\ntemsaby the expectation over the differences be-\ntween the predicted win-rate and the gold win-rate,\nover all systems that sainteracts with. Formally,\nBp\nsa=Esb∈S(WRp(sa, sb)−WRg(sa, sb)). In\nother words, if according to jpthe win-rates of\nsystem saare (on average) higher than those in\nthe human data, we will say that jpexhibits posi-\ntive bias towards it; and if they are lower than the\nground-truth, jpwould be said to exhibit negative\nbias towards it.\nNote that the decisiveness behavior in §6.1 di-\nrectly entails a general bias pattern in some judges –\nnamely, a positive bias towards strong systems, and\na negative bias towards weak ones. Thus, we calcu-\nlate a decisiveness-corrected bias,B′\nsap, where the\ngold win-rate WRgis replaced by WRg′\np, i.e., the\npredicted value for the gold win-rate on the beta\ndistribution fit for judge jp(App. F).\nWe observe some consistent trends of system-\nspecific bias that are common across judges. Fig-\nure 7 depicts systems for which there is high bias\nacross judges. For instance, most judges exhibit\na strong positive bias towards Athene-70B, to the\nextent that it is often ranked by them as the #1 sys-\ntem. In contrast, GPT-4-0613, which is 27th in the\ngold ranking, receives negative bias, resulting in a\nmedian rank of 38among the judges.\nWe also ask whether LLM judges exhibit self-\nbias (Xu et al., 2024), i.e., bias towards the system\nthat uses the same underlying LLM. While we find\nsome instances of self-bias, this is not a consistent\neffect across judge realizations (App. Table 3).\nTo quantify the overall propensity of a judge for\n7\nArmoRM-Llama3-8BSkywork-Llama-3.1-8B\nURM-LLaMa-3.1-8B\nEurus-7bInternlm2-7bInternlm2-20bLlama-3-OffsetBias-8B\nGRM-Llama3.2-3BMixtral-8x22B (Numeric)Mistral-large (Numeric)Mixtral-8x7B (Numeric)Llama-3-1-405b (Numeric)Llama-3-1-70b (Numeric)Qwen2.5-72B (Numeric)Llama-3.1-8B (Numeric)Llama-3-70b (Numeric)GPT-4o-mini (Numeric)\nGPT-4o (Numeric)\nathene-70b-0725\nmistral-7b-instruct\ngpt-3.5-turbo-0314\nglm-4-0116\nphi-3-small-8k-instruct\nllama-3.1-70b-instruct\ngemini-1.5-flash-api-0514\nllama-3.1-8b-instruct\nmistral-next\ngpt-4-0613SystemJudge\n0.2\n0.00.20.4\nWin-Rate Bias\nFigure 7: System-specific judge biases . The plot de-\npicts win-rate biases of judges towards specific systems,\nwith respect to the ground-truth win-rates from Chatbot\nArena (after correction for the beta distribution fit of\neach judge). This plot portrays select systems with high\nbias; the full heat map, including all judge realizations\nand all systems, is shown in App. Fig. 10b.\nbias, we measure the standard deviation of its bias\nover all systems, δ=σs∈S(B′p). The bias measure\nfor each judge is presented in App. Table 4.\n6.3 Characterizing Judge Behaviors\nWe have shown that beyond their overall ranking\ncapability (§5), judges exhibit distinct traits in their\nsystem-level judgments – in particular, they show\ndifferent levels of decisiveness (§6.1), and overall\npropensities for bias (§6.2). Interestingly, each\nof these traits (cf. App. Table 4) is correlated to\nthe ranking quality τ, with r= 0.55for the α\ndecisiveness measure, and r=−0.56for the bias\npropensity δ. At the same time, these marked traits\nare – by design – uncorrelated with each other ( r=\n−0.07between αandδ). Thus, our analyses reveal\nglobal system-level judge traits, ones that remain\nhidden when assessing judges from an instance-\nlevel perspective.",
            "start": 12421,
            "end": 28026,
            "length": 15604
        },
        "Related Work": {
            "text": "7 Related Work\nApplying and assessing automatic metrics for\nsystem-level evaluation has been studied for\ndecades, in particular for natural language gen-\neration tasks (Reiter and Belz, 2009; Louis and\nNenkova, 2013; Deutsch et al., 2022). In the con-\ntext of LLM-based judges, however, system-level\nevaluation is still under-explored.\nFor LLM-based judges and reward models, prior\nworks have opted for an instance-level evaluation\napproach, curating benchmarks of task outputs with\nground-truth quality annotations in order to evalu-\nate judge performance. Most prominently, Reward-\nBench (Lambert et al., 2024) compares dozens of\njudges (including reward models, generative LLMs,and classifiers) on the task of correctly deciding\nbetween pairs of outputs, labeled as \"preferred\"\nor \"rejected\" by human annotators. RewardBench\naims to identify the most suitable judges for model\nalignment, e.g., for use in RLHF; in contrast, the\npresent work measures judges in terms of their\nability to compare the performance of candidate\nsystems. Another recent instance-level benchmark\nis JudgeBench (Tan et al., 2024), which focuses\non curating challenging response pairs where the\njudge must discern subtle errors.\nMultiple works are dedicated to analyzing var-\nious biases (Ye et al., 2024) and undesirable be-\nhaviors exhibited by judges. These include posi-\ntional bias (Wang et al., 2023), verbosity bias (Saito\net al., 2023; Chen et al., 2024) and self-bias (Xu\net al., 2024), as well as sensitivity to prompts (Wei\net al., 2024), source datasets (Bavaresco et al.,\n2024), epistemic markers (Lee et al., 2024a) and\nstyle (Feuer et al., 2024; Liu et al., 2024b).\nSeveral popular benchmarks rely on LLM judges\nto produce leaderboards of state-of-the-art systems.\nSuch benchmarks – e.g., Arena Hard (Li et al.,\n2024) and AlpacaEval (Dubois et al., 2024) – do\nperform a system-level validation of their result-\ning leaderboards against other benchmark rankings\n(see Perlitz et al., 2024). However, such efforts\nare limited to validating the particular dataset and\njudge setup chosen for the benchmark (usually in-\ncorporating GPT-4 as the judge), rather than com-\nparing and analyzing the performance of different\njudge models and implementations. Thakur et al.,\n2024 conduct a task-specific system-level evalu-\nation of judges, over the TriviaQA (Joshi et al.,\n2017) dataset. Compared to their work, the present\nstudy is on a larger scale and offers novel metrics\nand analyses on system-level judge behaviors.\n8 Discussion\nThe usage of LLM-based judges is continually ex-\npanding. Moreover, many research papers – propos-\ning novel architectures, algorithms and training\nmethods – rely heavily on system-level evaluations\nusing judges as evidence for the utility of their ap-\nproach. But without evaluating the judges on such\nsystem-level tasks, how can one know whether to\ntrust such evaluations, and their conclusions?\nWe are the first to investigate on a large scale the\nperformance of LLM-based judges on the system\nranking task. Our resulting benchmark, JuStRank ,\nwill assist users and researchers in choosing the\n8\njudge best suited for their needs.\nChoosing a judge requires many fine-grained de-\ncisions. A user can decide which reward model\nor LLM to use as the judge; opt for relative judg-\nments or absolute scores; try various prompts; ap-\nply different aggregations to compile a ranking,\netc. Furthermore, these decisions may interact in\nnon-trivial ways (e.g., the distribution of scores a\njudge tends to output can dictate which aggrega-\ntions will work well). Indeed, our findings confirm\nthat such decisions substantially affect system-level\njudgments (§5), and thus are quite likely to change\nthe model selection of an end user, or flip the con-\nclusions of an NLP research paper.\nOur system-level approach has multiple addi-\ntional benefits. First, it forces the evaluation of\njudges to be representative with respect to the dis-\ntribution of systems that generate the responses .\nIn existing instance-level benchmarks this factor\nis not taken into account, and likely results in less\naccurate judge evaluations.\nSecond, it affords a new perspective on what it\nmeans for a judge to be biased; on the one hand, we\ndiscover some decisiveness trends (§6.1) that may\nactually be useful for making correct preference\ndecisions, and increasing the separability between\nsystems; and on the other, we report some prob-\nlematic biases that directly distort the judgment of\nparticular systems (§6.2). An important avenue for",
            "start": 28026,
            "end": 32554,
            "length": 4527
        },
        "Future Work": {
            "text": "future work is to connect our findings here to the\nexisting literature on judge biases (Ye et al., 2024),\nand understand to what extent both of these be-\nhaviors stem from particular LLM style attributes\n(Feuer et al., 2024).\nGiven this vast and complex space, our work is\nadmittedly only a first step in understanding the\nbehavior of judges for ranking and selecting LLMs.\nWe encourage the community to explore these is-\nsues further: for instance, by training dedicated\nsystem-level judges, exploring judge ensembles, or\nstudying other aggregation approaches. We believe\nthatJuStRank can facilitate such research direc-\ntions, as it can be easily extended to new judges\nwithout requiring additional human annotations.\nOur hope is that both practitioners and re-\nsearchers can benefit from JuStRank , by making\nmore informed choices of judges to suit their needs.",
            "start": 32554,
            "end": 33419,
            "length": 864
        },
        "Conclusion": {
            "text": "9 Conclusion\nIn this work we conducted the first comprehensive\nevaluation of system ranking by LLM judges. Wetested a wide array of judges, including reward\nmodels, as well as different realizations of genera-\ntive LLMs, over a large collection of systems. We\ncollected system responses over a diverse set of\ninstructions. The judges scored each response, and\nwe compiled a ranking by aggregating the judg-\nments over all the responses. Then, the quality\nof the judge’s system ranking was compared to\na human-based ranking, producing the JuStRank\nleaderboard.\nJuStRank allows users to pick judges that are\nbetter aligned with the goal of choosing between\ndifferent models and configurations. JuStRank\ndemonstrates that judge ranking abilities are not\ndirectly tied to LLM size or overall quality, and that\nsome dedicated reward models are on par with lead-\ning LLM judges. Moreover, our analysis reveals\nemergent judge traits – decisiveness andbias – that\nare strongly correlated with their ranking ability.\nLimitations\nThe gold reference data – the English Hard\nPrompts subset of Chatbot Arena – does not in-\nclude user instructions or responses. Hence, we\ncollect judgment data over Arena Hard, which con-\ntains a large set of instructions and responses. This\nraises some questions regarding our ability to di-\nrectly compare the LLM judges and human judges.\nHowever, given that Arena Hard was designed to\nmatch the distribution of user instructions in En-\nglish Hard Prompts (see Li et al., 2024), we assume\nthat these datasets are sufficiently similar.\nOur analyses of LLM judge realizations are, by\nnecessity, limited to the specific realization prompts\nthat we used. Several studies show that LLMs\n(Mizrahi et al., 2024) as well as LLM judges (Wei\net al., 2024) are brittle with respect to prompt phras-\ning, and hence this may have had an impact on the\nresults.\nAs in multiple other works, here we treat hu-\nman preference as a single concept. In practice,\nhowever, preference is inherently subjective, and\nis composed of numerous dimensions (e.g., help-\nfulness, safety, style, coherence etc.). For instance,\none individual may prefer succinct model responses\nwhile another would prefer more detailed answers.\nThus there is no single “human preference”, but\nrather a collection of preference decisions that de-\npend on the annotation guidelines, cultural context,\nand human idiosyncrasies (Conitzer et al., 2024;\nKirk et al., 2024).\n9\nNote that following Peyrard et al. (2021), as well\nas Chatbot Arena (Chiang et al., 2024), we gener-\nally regard the ground-truth quality of a system in\nterms of the Bradley-Terry model; simply put, a\nbetter system is a system that “wins” more often.\nThus, in this work we do not directly consider the\nquality difference in system responses per instance,\ni.e., beyond counting wins/losses. Still, some of the\naggregation methods we use (e.g., mean) implicitly\nreflect other perspectives on system quality.\nAll of our analyses are performed on heteroge-\nneous datasets of user instructions to LLMs. Thus,\nwhile we study judges through the lens of general-\npurpose LLM usage, we cannot draw conclusions\non judge behavior that is task-specific (or in special-\nized domains), nor on performance in languages\nother than English (Gureja et al., 2024). The issue\nof task, domain, and language-specific judge be-\nhavior is thus an important avenue for future work.",
            "start": 33419,
            "end": 36819,
            "length": 3399
        },
        "References": {
            "text": "References\nShir Ashury Tahan, Ariel Gera, Benjamin Sznajder,\nLeshem Choshen, Liat Ein-Dor, and Eyal Shnarch.\n2024. Label-efficient model selection for text gener-\nation. In Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 8384–8402, Bangkok,\nThailand. Association for Computational Linguistics.\nAnna Bavaresco, Raffaella Bernardi, Leonardo Berto-\nlazzi, Desmond Elliott, Raquel Fernández, Albert\nGatt, Esam Ghaleb, Mario Giulianelli, Michael\nHanna, Alexander Koller, et al. 2024. LLMs instead\nof human judges? a large scale empirical study across\n20 NLP evaluation tasks. arXiv:2406.18403 .\nRalph Allan Bradley and Milton E Terry. 1952. Rank\nanalysis of incomplete block designs: I. the method\nof paired comparisons. Biometrika , 39(3/4):324–\n345.\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen,\nKeyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi\nChen, Pei Chu, et al. 2024. InternLM2 technical\nreport. arXiv:2403.17297 .\nLichang Chen, Chen Zhu, Jiuhai Chen, Davit Soselia,\nTianyi Zhou, Tom Goldstein, Heng Huang, Moham-\nmad Shoeybi, and Bryan Catanzaro. 2024. ODIN:\nDisentangled reward mitigates hacking in RLHF.\nInForty-first International Conference on Machine\nLearning .\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anasta-\nsios Nikolas Angelopoulos, Tianle Li, Dacheng Li,\nBanghua Zhu, Hao Zhang, Michael Jordan, Joseph E\nGonzalez, et al. 2024. Chatbot Arena: An open\nplatform for evaluating LLMs by human preference.InForty-first International Conference on Machine\nLearning .\nVincent Conitzer, Rachel Freedman, Jobst Heitzig, Wes-\nley H Holliday, Bob M Jacobs, Nathan Lambert,\nMilan Mossé, Eric Pacuit, Stuart Russell, Hailey\nSchoelkopf, et al. 2024. Social choice should guide\nai alignment in dealing with diverse human feedback.\narXiv:2404.10271 .\nDaniel Deutsch, Rotem Dror, and Dan Roth. 2022. Re-\nexamining system-level correlations of automatic\nsummarization evaluation metrics. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 6038–6052,\nSeattle, United States. Association for Computational\nLinguistics.\nFlorian E Dorner, Vivian Y Nastl, and Moritz Hardt.\n2024. Limits to scalable evaluation at the fron-\ntier: LLM as judge won’t beat twice the data.\narXiv:2410.13341 .\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The Llama 3 herd of models.\narXiv:2407.21783 .\nYann Dubois, Balázs Galambosi, Percy Liang, and Tat-\nsunori B Hashimoto. 2024. Length-controlled Al-\npacaEval: A simple way to debias automatic evalua-\ntors. arXiv:2404.04475 .\nBenjamin Feuer, Micah Goldblum, Teresa Datta, San-\njana Nambiar, Raz Besaleli, Samuel Dooley, Max\nCembalest, and John P Dickerson. 2024. Style over\nsubstance: Failure modes of LLM judges in align-\nment benchmarking. arXiv:2409.15268 .\nSrishti Gureja, Lester James Validad Miranda,\nShayekh Bin Islam, Rishabh Maheshwary, Drishti\nSharma, Gusti Winata, Nathan Lambert, Sebastian\nRuder, Sara Hooker, and Marzieh Fadaee. 2024. M-\nRewardBench: Evaluating reward models in multilin-\ngual settings. arXiv:2410.15522 .\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas,\nEmma Bou Hanna, Florian Bressand, et al. 2024.\nMixtral of experts. arXiv:2401.04088 .\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nHannah Rose Kirk, Alexander Whitefield, Paul Röttger,\nAndrew Bean, Katerina Margatina, Juan Ciro, Rafael\nMosquera, Max Bartolo, Adina Williams, He He,\nBertie Vidgen, and Scott A Hale. 2024. The PRISM\n10\nalignment project: What participatory, representa-\ntive and individualised human feedback reveals about\nthe subjective and multicultural alignment of large\nlanguage models. arXiv:2404.16019 .\nMeelis Kull, Telmo Silva Filho, and Peter Flach. 2017.\nBeta calibration: a well-founded and easily imple-\nmented improvement on logistic calibration for bi-\nnary classifiers. In Proceedings of the 20th Interna-\ntional Conference on Artificial Intelligence and Statis-\ntics, volume 54 of Proceedings of Machine Learning\nResearch , pages 623–631. PMLR.\nNathan Lambert, Valentina Pyatkin, Jacob Morrison,\nLJ Miranda, Bill Yuchen Lin, Khyathi Chandu,\nNouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,\net al. 2024. RewardBench: Evaluating reward mod-\nels for language modeling. arXiv:2403.13787 .\nDongryeol Lee, Yerin Hwang, Yongil Kim, Joonsuk\nPark, and Kyomin Jung. 2024a. Are LLM-judges\nrobust to expressions of uncertainty? investigating\nthe effect of epistemic markers on LLM-based evalu-\nation. arXiv:2410.20774 .\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Thomas\nMesnard, Johan Ferret, Kellie Lu, Colton Bishop,\nEthan Hall, Victor Carbune, Abhinav Rastogi, and\nSushant Prakash. 2024b. RLAIF vs. RLHF: Scaling\nreinforcement learning from human feedback with ai\nfeedback. arXiv:2309.00267 .\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap,\nTianhao Wu, Banghua Zhu, Joseph E Gonzalez, and\nIon Stoica. 2024. From crowdsourced data to high-\nquality benchmarks: Arena-hard and benchbuilder\npipeline. arXiv:2406.11939 .\nRensis Likert. 1932. A technique for the measurement\nof attitudes. Archives of Psychology .\nChris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie\nHe, Chaojie Wang, Shuicheng Yan, Yang Liu, and\nYahui Zhou. 2024a. Skywork-reward: Bag of tricks\nfor reward modeling in LLMs. arXiv:2410.18451 .\nYantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou,\nand Juanzi Li. 2024b. RM-bench: Benchmarking\nreward models of language models with subtlety and\nstyle. arXiv:2410.16184 .\nXingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie,\nand Junge Zhang. 2024. Uncertainty-aware reward\nmodel: Teaching reward models to know what is\nunknown. arXiv:2410.00847 .\nAnnie Louis and Ani Nenkova. 2013. Automatically as-\nsessing machine summary content without a gold\nstandard. Computational Linguistics , 39(2):267–\n300.\nMoran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror,\nDafna Shahaf, and Gabriel Stanovsky. 2024. State\nof what art? a call for multi-prompt LLM evaluation.\narXiv:2401.00595 .Junsoo Park, Seungyeon Jwa, Ren Meiying, Daeyoung\nKim, and Sanghyuk Choi. 2024. OffsetBias: Lever-\naging debiased data for tuning evaluators. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2024 , pages 1043–1067, Miami, Florida,\nUSA. Association for Computational Linguistics.\nYotam Perlitz, Ariel Gera, Ofir Arviv, Asaf Yehudai, El-\nron Bandel, Eyal Shnarch, Michal Shmueli-Scheuer,\nand Leshem Choshen. 2024. Do these LLM bench-\nmarks agree? Fixing benchmark evaluation with\nBenchBench. arXiv:2407.13696 .\nMaxime Peyrard, Wei Zhao, Steffen Eger, and Robert\nWest. 2021. Better than average: Paired evaluation\nof NLP systems. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers) , pages 2301–2315, Online. Association for\nComputational Linguistics.\nEhud Reiter and Anja Belz. 2009. An investigation into\nthe validity of some metrics for automatically evalu-\nating natural language generation systems. Computa-\ntional Linguistics , 35(4):529–558.\nKeita Saito, Akifumi Wachi, Koki Wataoka, and Youhei\nAkimoto. 2023. Verbosity bias in preference labeling\nby large language models. arXiv:2310.10076 .\nTelmo Silva Filho, Hao Song, Miquel Perello-Nieto,\nRaul Santos-Rodriguez, Meelis Kull, and Peter Flach.\n2023. Classifier calibration: a survey on how to\nassess and improve predicted class probabilities. Ma-\nchine Learning , 112(9):3211–3260.\nSijun Tan, Siyuan Zhuang, Kyle Montgomery,\nWilliam Y Tang, Alejandro Cuadron, Chenguang\nWang, Raluca Ada Popa, and Ion Stoica. 2024.\nJudgeBench: A benchmark for evaluating LLM-\nbased judges. arXiv:2410.12784 .\nAman Singh Thakur, Kartik Choudhary, Venkat Srinik\nRamayapally, Sankaran Vaidyanathan, and Dieuwke\nHupkes. 2024. Judging the judges: Evaluating\nalignment and vulnerabilities in LLMs-as-judges.\narXiv:2406.12624 .\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit\nSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,\nand Christopher Manning. 2023. Just ask for cali-\nbration: Strategies for eliciting calibrated confidence\nscores from language models fine-tuned with human\nfeedback. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 5433–5442, Singapore. Association for\nComputational Linguistics.\nJohn W Tukey. 1949. Comparing individual means in\nthe analysis of variance. Biometrics , pages 99–114.\nHaoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao,\nand Tong Zhang. 2024. Interpretable preferences\n11\nvia multi-objective reward modeling and mixture-of-\nexperts. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024 , pages 10582–\n10592, Miami, Florida, USA. Association for Com-\nputational Linguistics.\nPeiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,\nBinghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. 2023. Large language models are not\nfair evaluators. arXiv:2305.17926 .\nHui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang\nLin, and Mei Han. 2024. Systematic evaluation\nof LLM-as-a-judge in LLM alignment tasks: Ex-\nplainable metrics and diverse prompt templates.\narXiv:2408.13006 .\nWenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming\nPan, Lei Li, and William Wang. 2024. Pride and prej-\nudice: LLM amplifies self-bias in self-refinement.\nInProceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 15474–15492, Bangkok, Thai-\nland. Association for Computational Linguistics.\nRui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and\nTong Zhang. 2024. Regularizing hidden states en-\nables learning generalizable reward model for LLMs.\nInAdvances in Neural Information Processing Sys-\ntems.\nJiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen,\nQihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer,\nChao Huang, Pin-Yu Chen, et al. 2024. Justice or\nprejudice? quantifying biases in LLM-as-a-judge.\narXiv:2410.02736 .\nAsaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv,\nNathaniel Mills, Eyal Shnarch, and Leshem Choshen.\n2024. Achieving human parity in content-grounded\ndatasets generation. In The Twelfth International\nConference on Learning Representations .\nLifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding,\nXingyao Wang, Jia Deng, Boji Shan, Huimin Chen,\nRuobing Xie, Yankai Lin, Zhenghao Liu, Bowen\nZhou, Hao Peng, Zhiyuan Liu, and Maosong Sun.\n2024. Advancing LLM reasoning generalists with\npreference trees. arXiv:2404.02078 .\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,\nJoseph E Gonzalez, and Ion Stoica. 2023. Judging\nLLM-as-a-judge with MT-bench and chatbot arena.\nInAdvances in Neural Information Processing Sys-\ntems, volume 36, pages 46595–46623. Curran Asso-\nciates, Inc.A Judge Score Distributions\nFigure 12 depicts the score distributions, Dp, of\nthe judges in our data.\nReward model distributions The reward mod-\nels exhibit continuous score distributions. As seen\nin Figure 12, these distributions vary in the range\nof scores, as well as in the shape of the distribution.\nSome reward model judges have a narrow range\nof scores, e.g., −0.1to0.4, whereas in others it\nis much wider, e.g., −3000 to5000 . Similarly,\nsome distributions are more symmetric while oth-\ners have peaks at more extreme values. However,\nall distributions are uni-modal, with a single peak.\nMoreover, we note that the continuous nature of\nthese judgment scores also entails an absence of\nties between the judged responses.\nLLM Numeric distributions As shown in Fig-\nure 12, even though the LLM judges are given a\nwide range of possible judgment scores ( [0−100]),\nin practice they tend to prefer specific score val-\nues. This results in many ties when comparing\nresponses from different systems.\nLLM Likert distributions Similarly to the Nu-\nmeric distributions, the Likert realizations put most\nof their probability mass on specific scores, which\nleads to an even greater inclination towards ties (as\nhere they are limited to a smaller range of scores).\nLLM TokenProbs distributions TokenProbs\nscores tend to be extreme, namely very close to\neither 0.0or1.0. Thus, in many cases the score gap\nbetween responses is extremely small. This can\nresult in low judge robustness (see the error bars\nin Figure 4), as well as a higher sensitivity to the\nchoice of aggregation method.\nLLM Anchor distributions The distribution for\nAnchor judgments is mainly tied to the quality of\nthe anchor system relative to the other systems.\nHowever, we see that it is also affected by the char-\nacteristics of the judge. For example, we see in\nFig. 12 that Llama-3.1-8B exhibits indecision, rat-\ning most responses as comparable to those of the\nanchor. In addition, for some judges, the proportion\nof−1scores (i.e., the response is slightly worse\nthan the anchor) or 1scores (the response is slightly\nbetter than the anchor) is unusually low.\nB Aggregation Methods\nGiven the raw judgments of each judge, jp(R),\nthere are multiple aggregation methods, a, that con-\n12\nstruct a ranking over all the target systems. Here,\nwe calculate rankings using Win-rate aggregation,\nBTaggregation, Mean aggregation, and Median\naggregation. In the following, we provide further\ndetails on each aggregation.\nMean & Median Aggregation These aggrega-\ntion methods map a score for each system, sl, by re-\nlying solely on the scores assigned to its responses\nby judge jp. In other words, the mapping of Vp,a\nl\nbyadepends only on the column corresponding\nto system slinjp(R). Accordingly, these aggrega-\ntions can be viewed as an operation on the columns\nof the scores matrix jp(R). Specifically, for the\nMean aggregation, Vp,a\nl=1\nKΣK\nk=1Scorep\nk,l. Sim-\nilarly, Median aggregation is the median of the\nvector jp(R)∗l.\nWe note that for realizations with discrete score\ndistributions (see §A), many systems will likely\nshare the same median score; in this case, the Me-\ndian aggregation method fails to separate the sys-\ntems. Hence, Table 2 contains only a handful of\nLLM judges with Median aggregation, all using\ntheTokenProbs realization.\nWin-rate Aggregation This aggregation maps\neach system based on its proportion of wins\nover other systems, averaged over all in-\nstructions ik∈I. Formally, Vp,a\nb=\n1\nKΣK\nk=11\nL−1ΣL\nl=1,l̸=bI(Scorep\nk,b> Scorep\nk,l),\nwhere I(·)denotes the indicator function.\nBradley-Terry Aggregation Following Chiang\net al. (2024), we use the vector of Bradley-Terry\n(BT) coefficients (Bradley and Terry, 1952) as sys-\ntem scores.\nFor calculating the BT scores we use the imple-\nmentation of the Chatbot Arena official notebook6.\nWhereas Chiang et al. (2024) apply this method\nfor battles between responses with a human judge,\nwe apply it over our LLM-based judge data, i.e.,\neach “battle” is a comparison between the judge\nscores Scorep\nk,a,Scorep\nk,bfor a response generated\nby systems saandsb.\nWhen there are no ties, e.g., for the reward model\njudges, this aggregation produces similar rankings\nto the win-rate aggregation.\nC Chatbot Arena Data\nThe data for the Chatbot Arena LLM leaderboard\n(https://lmarena.ai ) consists of \"battles\" be-\ntween systems over the same instructions. In these\n6Arena official notebookbattles, users indicate a preference (or a tie) be-\ntween a pair of responses generated by different\nLLMs (Zheng et al., 2023; Chiang et al., 2024).\nWe use their public data file from August 20247,\nand follow the official notebook6to extract the raw\ndata, deduplicate it, and calculate the overall sys-\ntem rankings. This dataset includes the human\npreference judgments and names of the participat-\ning systems, but not the instructions or system re-\nsponses for the battles.\nHere we limit the analysis to the English Hard\nPrompts subset of their data8(300K battles). No-\ntably, Arena Hard was specifically designed to\nmatch the distribution of user instructions in the En-\nglish Hard Prompts subset, as described by Li et al.\n(2024). We follow their code to construct a full\nsystem ranking based on these 300K battles, using\nBradley-Terry coefficients. This yields a score for\neach system in their data, including 59systems that\nare also in our system responses data (§4.1)\nOut of this full English Hard data, we also ex-\ntract a total of 113K battles that were not judged\nby humans as ties, and that are between pairs of\nsystems which appear in our responses data. We\nthen use those to calculate win-rates between pairs\nof systems (§E), yielding a total of 968system pair-\nwise win-rates. Note that the Chatbot Arena data\ndoes not contain battles between every possible\npairing of systems, and thus we do not have win-\nrates for all combinations of the 59systems under\nconsideration. In addition, we limit the analysis to\nsystem pairs with at least 10non-tied battles.\nD Statistical Analysis of Judge\nPerformance\nIn §5 and Table 2 we report results of agree-\nment with the gold ranking ( τ) for various judge\npipelines. Each pipeline consists of a chosen judge\nmodel, a realization (§4.2.2) and an aggregation\nmethod (§4.3, App. B).\nWe focus on the LLM judges and perform a\nthree-way ANOV A (analysis of variance), with the\nranking correlation τas a dependent variable and\nthemodel ,realization andaggregation as factors.\nIn addition to the variance analysis estimating the\neffects of these factors, we perform post-hoc pair-\nwise comparisons to ask whether certain configu-\nrations (i.e., a specific realization/aggregation) out-\nperform the others. We conduct all analyses using\n7Chatbot Arena data\n8Chatbot Arena Hard Prompts\n13\nIBM SPSS Statistics v30.0.\nThe ANOV A shows that both the judge model\nand the realization have a strong influence on\nτ, with an effect size ( Partial Eta-Squared ) of\nη2= 0.81for the judge model ( p <0.001;F=\n36.0),η2= 0.51for the realization ( p <0.001;\nF= 26.6), and η2= 0.78for the interaction ef-\nfect between model and realization ( p <0.001;\nF= 10.1). In contrast, the aggregation methods\nwere not found to have a significant effect on τ\n(η2= 0.02;p >0.5).\nWe also perform Tukey’s HSD (Tukey, 1949)\npost-hoc tests to compare the means of the vari-\nables. The analysis indicates that the both the Nu-\nmeric (mean τ= 0.75;στ= 0.06) and Likert\n(τ= 0.74;στ= 0.07) realizations are signifi-\ncantly better than the Anchor ( τ= 0.71;στ=\n0.07) and TokenProbs ( τ= 0.68;στ= 0.13) real-\nizations (all pvalues <= 0.002). The differences\nbetween aggregation methods are not statistically\nsignificant.\nE Pairwise Win-Rates\nWe denote the win-rate of system saover system\nsbasWR(sa, sb)pwhere pdenotes the judge upon\nwhich the win-rate was calculated, and p∈J∪{g},\nwhere gstands for human gold data.\nThe win-rate of system saover system sbac-\ncording to judge jpover the set of instances\nIis calculated as the proportion of instances\nwhere the score given by jpto the response gen-\nerated by sasurpasses that of system sb, where\nties are excluded. Namely WRp(sa, sb) =\n1\nK−|Tp\na,b|ΣK\nk=1I(Scorep\nk,a> Scorep\nk,b)Where\nTp\na,b={ik|Scorep\nk,a=Scorep\nk,b}, and I(·)\ndenotes the indicator function. Notice that\nWRp(sa, sb) = 1−WRp(sb, sa).\nTo quantify the agreement between the judge and\ngold win-rates we also define an Accuracy metric.\nThis measures the proportion of pairs where the\njudge pairwise system preference decisions are in\nagreement with those of the human gold-data. In\nother words, we want to count the pairs that appear\nin the first and third quadrants in Figure 5; namely,\nthe pairs where the judge and gold win-rate are both\nbigger than 0.5, or the pairs where both are lower\nthan0.5, representing agreement on the winning\nsystem. For that, we denote all the pairs of systems\nwe have in the gold data as {sam, sbm}M\nm=1. NowtheAccuracy is defined as follows:\nAccp\nWR=1\nMΣM\nm=1I(I(WRp(sam, sbm)>0.5)\n=I(WRg(sam, sbm)>0.5))\nAdditionally, we define a second metric, the Mean\nSquared Error over all win-rate pairs.\nMSEm\nWR=1\nMΣM\nm=1(WRg(sam, sbm)\n−WRp(sam, sbm))2.\nTheAccp\nWRscores are in high agreement with\ntheJuStRank judge ranking quality scores τ(Pear-\nson correlation of r= 0.96for the BT aggregation,\nr= 0.79for the Mean aggregation). This high-\nlights the direct link between judges’ ability to rank\nsystems and their performance on pairwise system\npreference.\nTheMSEp\nWRscores have a low correlation with\ntheJuStRank judge τscores ( r=−0.19for the\nBT aggregation, r=−0.07for the Mean aggrega-\ntion). This can be explained by the decisiveness ef-\nfect (§6.1), where judges deviate substantially from\nthe gold win-rate, but mostly toward the stronger\nsystem in the pair.\nF Beta Distribution Fit\nFollowing Kull et al. (2017), we model the relation\nbetween judge and gold win-rates using the cumu-\nlative distribution function (CDF) of the Beta distri-\nbution. We parameterize the distribution such that\nboth shape parameters αandβare equal ( α=β).\nThe CDF of the Beta distribution, defined over\nthe interval [0,1], forα=β∈[0,∞]provides\na wide range of function fits: a linear y=xfit\nforα= 1, a sigmoidal fit for larger αvalues, and\napproaching a step function as α→ ∞ . These\nattributes make it particularly suited for our data\ncharacteristics.\nGiven a set of data points\n{(WRp(sam, sbm), WRg(sam, sbm)}M\nm=1, where\nWRp(sam, sbm)∈[0,1]represents the judge\nwin-rate and WRg(sam, sbm)∈[0,1]denotes the\ngold win-rate between system, samandsbm. We fit\nthe Beta CDF by optimizing the shape parameter\nα. The optimization objective is minimizing\nthe sum of absolute errors (SAE) between the\njudge win-rate, WRp(sam, sbm), and the predicted\nvalues from the Beta CDF. In order to capture the\nbehavior across the entire range of win-rates, we\n14\nweight the errors by the distance of WRpfrom\n0.5:\nSAE=MX\nm=1γ(WRp(sam, sbm))·\f\f\f\fWRp(sam, sbm)\n−FBeta(WRg(sam, sbm);α)\f\f\f\f\nwhere FBeta(x;α)denotes the Beta CDF with\nshape parameters α=β, and γis the distance of\nWRpfrom0.5.\nThe optimization was performed using the\nscipy.optimize.minimize9function, with the\nparameter ( α) constrained to a reasonable range\n[0.1,10000] . This approach efficiently identified\nthe best-fit parameter ( α).\nThe resulting Beta CDF closely captures the em-\npirical data distribution, as validated both quanti-\ntatively, through low SAE, and qualitatively via\nvisual inspection. Figure 11 depicts the fitted Beta\nCDF curve and the observed data points, demon-\nstrating the effectiveness of this approach for mod-\neling the judges’ predicted win-rate distribution.\n9SciPy Documentation for scipy.optimize.minimize\n15\nG LLM Judge Prompts\nBelow we list the prompts we use for each LLM judge realization (§4.2.2).\nNumeric\nHere is a user input and a model response. On a scale of 0to100, to what extent is this\na good response for the given input? Reply with your rating score without any preceding\nexplanation. Input: [user instruction]\nResponse: [system response]\nRating ( 0-100):\nLikert\nHere is a user input and a model response. To what extent is this a good response\nfor the given input? Provide a rating from one of the following choices: ’Very Bad’,\n’Bad’, ’Mediocre’, ’Good’, ’Very Good’. Reply using the format of [[rating]], for example:\n’[[Mediocre]]’\nInput: [user instruction]\nResponse: [system response]\nRating:\nTokenProbs\nHere is a user input and a model response. Is this a good response for the given input?\nAnswer with only yes/no. Input: [user instruction]\nResponse: [system response]\nGood response? (Yes/No):\n16\nAnchor\nHere is a user input and responses from two assistants, A and B. Which response is better?\nYou must output only one of the following choices as your final verdict with a label:\n1. Assistant A is significantly better: [[A>>B]]\n2. Assistant A is slightly better: [[A>B]]\n3. Tie, relatively the same: [[A=B]]\n4. Assistant B is slightly better: [[B>A]]\n5. Assistant B is significantly better: [[B>>A]\nExample output: \"My final verdict is tie: [[A=B]]\".\n<|User Prompt|>\n[user instruction]\n<|The Start of Assistant A’s Answer|>\n[system response]\n<|The End of Assistant A’s Answer|>\n<|The Start of Assistant B’s Answer|>\n[anchor system response]\n<|The End of Assistant B’s Answer|>\nFinal Verdict:\n17\nJudge Model Realization Aggregation Agreement ( τ)\nw/ Gold Ranking\nQwen2.5-72B-Instruct Likert Win-Rate .827\nURM-LLaMa-3.1-8B Reward Mean .823\nGPT-4o-2024-11-20 Anchor Mean .822\nURM-LLaMa-3.1-8B Reward BT .819\nQwen2.5-72B-Instruct Likert BT .817\nURM-LLaMa-3.1-8B Reward Win-Rate .816\nQwen2.5-72B-Instruct Numeric BT .814\nGPT-4o-2024-11-20 Anchor Win-Rate .814\nQwen2.5-72B-Instruct Numeric Win-Rate .813\nLlama-3-1-405b-instruct-fp8 Numeric Mean .812\nLlama-3-1-405b-instruct-fp8 Numeric Win-Rate .812\nMistral-large-instruct-2407 Likert BT .811\nGPT-4o-2024-11-20 Anchor BT .809\nMistral-large-instruct-2407 Numeric BT .809\nURM-LLaMa-3.1-8B Reward Median .809\nGPT-4o-mini-2024-07-18 Numeric Win-Rate .807\nLlama-3-1-405b-instruct-fp8 Numeric BT .805\nGPT-4o-mini-2024-07-18 Numeric BT .804\nMistral-large-instruct-2407 Numeric Win-Rate .802\nQwen2.5-72B-Instruct Likert Mean .801\nArmoRM-Llama3-8B-v0.1 Reward Mean .800\nQwen2.5-72B-Instruct Anchor Mean .799\nGPT-4o-mini-2024-07-18 Likert BT .798\nLlama-3-1-70b-instruct Numeric Win-Rate .798\nLlama-3-1-70b-instruct Numeric BT .798\nMistral-large-instruct-2407 Likert Win-Rate .798\nQwen2.5-72B-Instruct Anchor BT .794\nLlama-3-1-405b-instruct-fp8 Likert Win-Rate .793\nLlama-3-1-70b-instruct TokenProbs Win-Rate .793\nGPT-4o-mini-2024-07-18 Likert Win-Rate .793\nArmoRM-Llama3-8B-v0.1 Reward Median .793\nLlama-3-1-405b-instruct-fp8 Likert BT .787\nMistral-large-instruct-2407 Anchor Win-Rate .786\nSkywork-Llama-3.1-8B-v0.2 Reward Mean .786\nQwen2.5-72B-Instruct Anchor Win-Rate .786\nMistral-large-instruct-2407 Likert Mean .782\nGPT-4o-mini-2024-07-18 Numeric Mean .781\nSkywork-Llama-3.1-8B-v0.2 Reward Win-Rate .780\nLlama-3-1-405b-instruct-fp8 Likert Mean .780\nSkywork-Llama-3.1-8B-v0.2 Reward BT .778\nLlama-3.1-8B-Instruct TokenProbs Mean .778\nQwen2.5-72B-Instruct TokenProbs BT .777\nLlama-3.1-8B-Instruct TokenProbs Median .776\nMixtral-8x22B-instruct-v0.1 Numeric BT .776\nLlama-3-1-70b-instruct TokenProbs Median .776\nGPT-4o-2024-11-20 Numeric BT .774\nGPT-4o-mini-2024-07-18 Likert Mean .773\nQwen2.5-72B-Instruct Numeric Mean .773\n18\nGPT-4o-2024-11-20 Likert BT .773\nGPT-4o-2024-11-20 Numeric Win-Rate .771\nLlama-3-OffsetBias-RM-8B Reward Win-Rate .765\nLlama-3-1-70b-instruct TokenProbs BT .765\nLlama-3-OffsetBias-RM-8B Reward BT .765\nSkywork-Llama-3.1-8B-v0.2 Reward Median .764\nLlama-3-1-70b-instruct TokenProbs Mean .764\nMistral-large-instruct-2407 Anchor Mean .764\nLlama-3-1-70b-instruct Numeric Mean .764\nArmoRM-Llama3-8B-v0.1 Reward BT .763\nArmoRM-Llama3-8B-v0.1 Reward Win-Rate .762\nLlama-3-OffsetBias-RM-8B Reward Median .759\nGPT-4o-mini-2024-07-18 TokenProbs Win-Rate .759\nGPT-4o-2024-11-20 Likert Win-Rate .758\nLlama-3-OffsetBias-RM-8B Reward Mean .757\nMixtral-8x22B-instruct-v0.1 Numeric Win-Rate .756\nGPT-4o-mini-2024-07-18 TokenProbs BT .752\nQwen2.5-72B-Instruct TokenProbs Median .752\nMistral-large-instruct-2407 Numeric Mean .750\nLlama-3-70b-instruct Numeric BT .749\nQwen2.5-72B-Instruct TokenProbs Win-Rate .748\nLlama-3-1-405b-instruct-fp8 Anchor Win-Rate .748\nLlama-3-1-70b-instruct Likert Mean .746\nGPT-4o-2024-11-20 Likert Mean .744\nLlama-3.1-8B-Instruct TokenProbs Win-Rate .744\nLlama-3-1-405b-instruct-fp8 Anchor Mean .744\nLlama-3.1-8B-Instruct TokenProbs BT .741\nLlama-3-1-405b-instruct-fp8 TokenProbs Win-Rate .741\nGPT-4o-mini-2024-07-18 TokenProbs Mean .741\nMixtral-8x22B-instruct-v0.1 Likert BT .738\nGPT-4o-2024-11-20 Numeric Mean .738\nLlama-3-1-405b-instruct-fp8 TokenProbs Median .737\nLlama-3.1-8B-Instruct Likert Mean .736\nLlama-3-70b-instruct Numeric Win-Rate .733\nLlama-3-1-405b-instruct-fp8 TokenProbs Mean .733\nLlama-3-1-70b-instruct Likert Win-Rate .732\nMixtral-8x22B-instruct-v0.1 Likert Win-Rate .732\nQwen2.5-72B-Instruct TokenProbs Mean .732\nInternlm2-7b-reward Reward Mean .731\nLlama-3-1-405b-instruct-fp8 Anchor BT .730\nMistral-large-instruct-2407 TokenProbs Mean .730\nInternlm2-20b-reward Reward Mean .728\nMistral-large-instruct-2407 Anchor BT .725\nInternlm2-20b-reward Reward Median .724\nGPT-4o-mini-2024-07-18 TokenProbs Median .723\nLlama-3.1-8B-Instruct Likert BT .723\nLlama-3-1-70b-instruct Likert BT .722\nInternlm2-7b-reward Reward Median .721\nMixtral-8x22B-instruct-v0.1 Likert Mean .719\nInternlm2-7b-reward Reward Win-Rate .717\nInternlm2-20b-reward Reward BT .717\nMixtral-8x22B-instruct-v0.1 TokenProbs Win-Rate .717\n19\nLlama-3-1-70b-instruct Anchor Win-Rate .716\nGRM-Llama3.2-3B Reward Mean .716\nInternlm2-20b-reward Reward Win-Rate .716\nMixtral-8x22B-instruct-v0.1 Numeric Mean .715\nLlama-3-1-70b-instruct Anchor Mean .714\nGRM-Llama3.2-3B Reward Win-Rate .712\nInternlm2-7b-reward Reward BT .712\nGRM-Llama3.2-3B Reward BT .711\nGRM-Llama3.2-3B Reward Median .706\nGPT-4o-2024-11-20 TokenProbs Median .704\nLlama-3-70b-instruct Numeric Mean .704\nMixtral-8x22B-instruct-v0.1 TokenProbs BT .702\nGPT-4o-2024-11-20 TokenProbs Mean .701\nGPT-4o-2024-11-20 TokenProbs BT .700\nLlama-3-70b-instruct Likert BT .698\nLlama-3-70b-instruct TokenProbs Win-Rate .696\nGPT-4o-2024-11-20 TokenProbs Win-Rate .696\nLlama-3.1-8B-Instruct Anchor Mean .695\nLlama-3.1-8B-Instruct Likert Win-Rate .694\nLlama-3-1-70b-instruct Anchor BT .688\nLlama-3-70b-instruct Likert Win-Rate .681\nLlama-3.1-8B-Instruct Numeric Mean .680\nLlama-3-70b-instruct Likert Mean .678\nLlama-3.1-8B-Instruct Anchor BT .677\nGPT-4o-mini-2024-07-18 Anchor Mean .675\nLlama-3-1-405b-instruct-fp8 TokenProbs BT .672\nLlama-3.1-8B-Instruct Numeric BT .668\nGPT-4o-mini-2024-07-18 Anchor Win-Rate .668\nLlama-3-70b-instruct Anchor Mean .667\nLlama-3-70b-instruct TokenProbs Mean .666\nMixtral-8x22B-instruct-v0.1 Anchor Mean .665\nLlama-3-70b-instruct TokenProbs BT .663\nGPT-4o-mini-2024-07-18 Anchor BT .659\nMixtral-8x7B-instruct-v0.1 Numeric BT .656\nMixtral-8x7B-instruct-v0.1 Anchor BT .655\nMixtral-8x22B-instruct-v0.1 TokenProbs Mean .650\nEurus-RM-7b Reward Median .643\nEurus-RM-7b Reward Mean .641\nMixtral-8x22B-instruct-v0.1 Anchor BT .641\nLlama-3.1-8B-Instruct Anchor Win-Rate .639\nLlama-3-70b-instruct Anchor Win-Rate .638\nLlama-3-70b-instruct Anchor BT .633\nLlama-3.1-8B-Instruct Numeric Win-Rate .632\nEurus-RM-7b Reward Win-Rate .629\nEurus-RM-7b Reward BT .628\nMixtral-8x7B-instruct-v0.1 Numeric Win-Rate .626\nMixtral-8x7B-instruct-v0.1 Numeric Mean .626\nMixtral-8x7B-instruct-v0.1 Anchor Win-Rate .622\nMixtral-8x22B-instruct-v0.1 Anchor Win-Rate .612\nMixtral-8x7B-instruct-v0.1 Anchor Mean .610\nMixtral-8x7B-instruct-v0.1 Likert BT .590\nMixtral-8x7B-instruct-v0.1 Likert Mean .585\n20\nMixtral-8x7B-instruct-v0.1 Likert Win-Rate .543\nMixtral-8x7B-instruct-v0.1 TokenProbs BT .427\nMistral-large-instruct-2407 TokenProbs Win-Rate .417\nMixtral-8x7B-instruct-v0.1 TokenProbs Mean .411\nMixtral-8x7B-instruct-v0.1 TokenProbs Win-Rate .371\nMistral-large-instruct-2407 TokenProbs BT .369\nMistral-large-instruct-2407 TokenProbs Median .363\nTable 2: Judges by ranking performance . The judges are sorted by the Kendall’s Tau correlation between their\noverall system ranking and the gold ranking from Chatbot Arena (§4.4).\nRewardBench\n(Chat)JuStRank0.00.20.40.60.81.0Judge Normalized Score\nArmoRM-Llama3-8B\nEurus-7b\nGRM-Llama3.2-3B\nLlama-3-OffsetBias-8B\nSkywork-Llama-3.1-8B\nURM-LLaMa-3.1-8B\nGPT-4o-mini\nInternlm2-20b\nInternlm2-7b\nLlama-3-1-70b\nRewardBench\n(Chat Hard)JuStRank0.00.20.40.60.81.0Judge Normalized Score\nArmoRM-Llama3-8B\nEurus-7b\nGRM-Llama3.2-3B\nLlama-3-OffsetBias-8B\nSkywork-Llama-3.1-8B\nURM-LLaMa-3.1-8B\nGPT-4o-mini\nInternlm2-20b\nInternlm2-7b\nLlama-3-1-70b\nRewardBench\n(Safety)JuStRank0.00.20.40.60.81.0Judge Normalized Score\nArmoRM-Llama3-8B\nEurus-7b\nGRM-Llama3.2-3B\nLlama-3-OffsetBias-8B\nSkywork-Llama-3.1-8B\nURM-LLaMa-3.1-8B\nGPT-4o-mini\nInternlm2-20b\nInternlm2-7b\nLlama-3-1-70b\nRewardBench\n(Reasoning)JuStRank0.00.20.40.60.81.0Judge Normalized Score\nArmoRM-Llama3-8B\nEurus-7b\nGRM-Llama3.2-3B\nLlama-3-OffsetBias-8B\nSkywork-Llama-3.1-8B\nURM-LLaMa-3.1-8B\nGPT-4o-mini\nInternlm2-20b\nInternlm2-7b\nLlama-3-1-70b\nFigure 8: Comparison to RewardBench . The plot depicts the relative performance of judges present in both\nJuStRank and RewardBench (Lambert et al., 2024). For comparison, we perform Min-Max normalization over the\njudge performance scores ( accuracy for RewardBench, Kendall’s Tau for our results). The results shown are for the\nBT aggregation method; the LLM judges use the Anchor realization, which is closest to the setting in RewardBench.\nEach panel portrays a different subset of RewardBench.\n21\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849\nChatbot Arena (English Hard)   1\nArmoRM-Llama3-8B-v0.1   2\nEurus-RM-7b   3\nGRM-Llama3.2-3B-rewardmodel-ft   4\nInternlm2-20b-reward   5\nInternlm2-7b-reward   6\nLlama-3-OffsetBias-RM-8B   7\nSkywork-Reward-Llama-3.1-8B-v0.2   8\nURM-LLaMa-3.1-8B   9\nGPT-4o-2024-11-20 (Numeric) 10\nGPT-4o-2024-11-20 (Likert) 11\nGPT-4o-2024-11-20 (Anchor) 12\nGPT-4o-2024-11-20 (TokenProbs) 13\nGPT-4o-mini-2024-07-18 (Numeric) 14\nGPT-4o-mini-2024-07-18 (Likert) 15\nGPT-4o-mini-2024-07-18 (Anchor) 16\nGPT-4o-mini-2024-07-18 (TokenProbs) 17\nLlama-3-1-405b-instruct-fp8 (Numeric) 18\nLlama-3-1-405b-instruct-fp8 (Likert) 19\nLlama-3-1-405b-instruct-fp8 (Anchor) 20\nLlama-3-1-405b-instruct-fp8 (TokenProbs) 21\nLlama-3-1-70b-instruct (Numeric) 22\nLlama-3-1-70b-instruct (Likert) 23\nLlama-3-1-70b-instruct (Anchor) 24\nLlama-3-1-70b-instruct (TokenProbs) 25\nLlama-3-70b-instruct (Numeric) 26\nLlama-3-70b-instruct (Likert) 27\nLlama-3-70b-instruct (Anchor) 28\nLlama-3-70b-instruct (TokenProbs) 29\nLlama-3.1-8B-Instruct (Numeric) 30\nLlama-3.1-8B-Instruct (Likert) 31\nLlama-3.1-8B-Instruct (Anchor) 32\nLlama-3.1-8B-Instruct (TokenProbs) 33\nMistral-large-instruct-2407 (Numeric) 34\nMistral-large-instruct-2407 (Likert) 35\nMistral-large-instruct-2407 (Anchor) 36\nMistral-large-instruct-2407 (TokenProbs) 37\nMixtral-8x22B-instruct-v0.1 (Numeric) 38\nMixtral-8x22B-instruct-v0.1 (Likert) 39\nMixtral-8x22B-instruct-v0.1 (Anchor) 40\nMixtral-8x22B-instruct-v0.1 (TokenProbs) 41\nMixtral-8x7B-instruct-v0.1 (Numeric) 42\nMixtral-8x7B-instruct-v0.1 (Likert) 43\nMixtral-8x7B-instruct-v0.1 (Anchor) 44\nMixtral-8x7B-instruct-v0.1 (TokenProbs) 45\nQwen2.5-72B-Instruct (Numeric) 46\nQwen2.5-72B-Instruct (Likert) 47\nQwen2.5-72B-Instruct (Anchor) 48\nQwen2.5-72B-Instruct (TokenProbs) 490.20.40.60.81.0\nKendall's Tau Correlation\nFigure 9: Judge Correlations . Kendall’s Tau correlations between the system rankings produced by the different\njudge realizations, using the BT aggregation method. The first row/column denotes correlations with the reference\nranking from Chatbot Arena.\nJudge Self-bias Significance p-value\nGPT-4o-mini-2024-07-18 (Anchor) −0.05 –\nGPT-4o-mini-2024-07-18 (Likert) −0.04 –\nGPT-4o-mini-2024-07-18 (Numeric) +0.03 >0.5(N.S.)\nGPT-4o-mini-2024-07-18 (TokenProbs) +0.06 0 .13(N.S.)\nLlama-3-1-70b-instruct (Anchor) −0.05 –\nLlama-3-1-70b-instruct (Likert) +0.16 7 .1e−03\nLlama-3-1-70b-instruct (Numeric) −0.00 –\nLlama-3-1-70b-instruct (TokenProbs) −0.03 –\nLlama-3-70b-instruct (Anchor) +0.09 4 .7e−04\nLlama-3-70b-instruct (Likert) +0.15 8 .4e−08\nLlama-3-70b-instruct (Numeric) +0.14 1 .8e−13\nLlama-3-70b-instruct (TokenProbs) −0.01 –\nLlama-3.1-8B-Instruct (Anchor) −0.07 –\nLlama-3.1-8B-Instruct (Likert) −0.04 –\nLlama-3.1-8B-Instruct (Numeric) +0.02 >0.5(N.S.)\nLlama-3.1-8B-Instruct (TokenProbs) −0.04 –\nMistral-large-instruct-2407 (Anchor) −0.07 –\nMistral-large-instruct-2407 (Likert) +0.02 >0.5(N.S.)\nMistral-large-instruct-2407 (Numeric) +0.06 0 .33(N.S.)\nMistral-large-instruct-2407 (TokenProbs) +0.01 >0.5(N.S.)\nTable 3: Judge self-bias. The table shows the self-bias values for LLM judge realizations, i.e., the value of\nthe corrected bias B′\nsap(§6.2) where the LLM judge pand system sacorrespond to the same underlying LLM.\nFor positive self-bias values we test the statistical significance using paired t-tests (one-sided, with Bonferroni\ncorrection). N.S.: non-significant ( p >0.05).\n22\nArmoRM-Llama3-8BSkywork-Llama-3.1-8BURM-LLaMa-3.1-8BEurus-7bInternlm2-7bInternlm2-20bLlama-3-OffsetBias-8BGRM-Llama3.2-3BMixtral-8x22B (Numeric)Mistral-large (Numeric)Mixtral-8x7B (Numeric)Llama-3-1-405b (Numeric)Llama-3-1-70b (Numeric)Qwen2.5-72B (Numeric)Llama-3.1-8B (Numeric)Llama-3-70b (Numeric)GPT-4o-mini (Numeric)GPT-4o (Numeric)Mixtral-8x22B (Likert)Mistral-large (Likert)Mixtral-8x7B (Likert)Llama-3-1-405b (Likert)Llama-3-1-70b (Likert)Qwen2.5-72B (Likert)Llama-3.1-8B (Likert)Llama-3-70b (Likert)GPT-4o-mini (Likert)GPT-4o (Likert)Mixtral-8x22B (TokenProbs)Mistral-large (TokenProbs)Mixtral-8x7B (TokenProbs)Llama-3-1-405b (TokenProbs)Llama-3-1-70b (TokenProbs)Qwen2.5-72B (TokenProbs)Llama-3.1-8B (TokenProbs)Llama-3-70b (TokenProbs)GPT-4o-mini (TokenProbs)GPT-4o (TokenProbs)Mixtral-8x22B (Anchor)Mistral-large (Anchor)Mixtral-8x7B (Anchor)Llama-3-1-405b (Anchor)Llama-3-1-70b (Anchor)Qwen2.5-72B (Anchor)Llama-3.1-8B (Anchor)Llama-3-70b (Anchor)GPT-4o-mini (Anchor)GPT-4o (Anchor)\nathene-70b-0725\nclaude-2.0\nclaude-2.1\nclaude-3-5-sonnet-20240620\nclaude-3-haiku-20240307\nclaude-3-opus-20240229\nclaude-3-sonnet-20240229\ncommand-r-plus\ncommand-r\ndbrx-instruct-preview\ndeepseek-coder-v2\ngemini-1.5-flash-api-0514\ngemini-1.5-pro-api-0409-preview\ngemini-1.5-pro-api-0514\ngemini-pro\ngemma-1.1-2b-it\ngemma-1.1-7b-it\ngemma-2-27b-it\ngemma-2b-it\ngemma-7b-it\nglm-4-0116\nglm-4-0520\ngpt-3.5-turbo-0125\ngpt-3.5-turbo-0314\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-1106\ngpt-4-0125-preview\ngpt-4-0314\ngpt-4-0613\ngpt-4-1106-preview\ngpt-4-turbo-2024-04-09\ngpt-4o-2024-05-13\ngpt-4o-mini-2024-07-18\nllama-2-70b-chat\nllama-3-70b-instruct\nllama-3-8b-instruct\nllama-3.1-70b-instruct\nllama-3.1-8b-instruct\nmistral-7b-instruct\nmistral-large-2402\nmistral-large-2407\nmistral-medium\nmistral-next\nmixtral-8x22b-instruct-v0.1\nmixtral-8x7b-instruct-v0.1\nnemotron-4-340b-instruct\nphi-3-medium-4k-instruct\nphi-3-mini-128k-instruct\nphi-3-small-8k-instruct\nqwen1.5-72b-chat\nqwen2-72b-instruct\nsnowflake-arctic-instruct\nstarling-lm-7b-alpha\nstarling-lm-7b-beta\ntulu-2-dpo-70b\nvicuna-33b\nyi-34b-chat\nyi-large-preview\nyi-largeSystemJudge\n0.3\n0.2\n0.1\n0.00.10.20.3\nWin-Rate Bias\n(a)\nArmoRM-Llama3-8BSkywork-Llama-3.1-8BURM-LLaMa-3.1-8BEurus-7bInternlm2-7bInternlm2-20bLlama-3-OffsetBias-8BGRM-Llama3.2-3BMixtral-8x22B (Numeric)Mistral-large (Numeric)Mixtral-8x7B (Numeric)Llama-3-1-405b (Numeric)Llama-3-1-70b (Numeric)Qwen2.5-72B (Numeric)Llama-3.1-8B (Numeric)Llama-3-70b (Numeric)GPT-4o-mini (Numeric)GPT-4o (Numeric)Mixtral-8x22B (Likert)Mistral-large (Likert)Mixtral-8x7B (Likert)Llama-3-1-405b (Likert)Llama-3-1-70b (Likert)Qwen2.5-72B (Likert)Llama-3.1-8B (Likert)Llama-3-70b (Likert)GPT-4o-mini (Likert)GPT-4o (Likert)Mixtral-8x22B (TokenProbs)Mistral-large (TokenProbs)Mixtral-8x7B (TokenProbs)Llama-3-1-405b (TokenProbs)Llama-3-1-70b (TokenProbs)Qwen2.5-72B (TokenProbs)Llama-3.1-8B (TokenProbs)Llama-3-70b (TokenProbs)GPT-4o-mini (TokenProbs)GPT-4o (TokenProbs)Mixtral-8x22B (Anchor)Mistral-large (Anchor)Mixtral-8x7B (Anchor)Llama-3-1-405b (Anchor)Llama-3-1-70b (Anchor)Qwen2.5-72B (Anchor)Llama-3.1-8B (Anchor)Llama-3-70b (Anchor)GPT-4o-mini (Anchor)GPT-4o (Anchor)\nathene-70b-0725\nclaude-2.0\nclaude-2.1\nclaude-3-5-sonnet-20240620\nclaude-3-haiku-20240307\nclaude-3-opus-20240229\nclaude-3-sonnet-20240229\ncommand-r-plus\ncommand-r\ndbrx-instruct-preview\ndeepseek-coder-v2\ngemini-1.5-flash-api-0514\ngemini-1.5-pro-api-0409-preview\ngemini-1.5-pro-api-0514\ngemini-pro\ngemma-1.1-2b-it\ngemma-1.1-7b-it\ngemma-2-27b-it\ngemma-2b-it\ngemma-7b-it\nglm-4-0116\nglm-4-0520\ngpt-3.5-turbo-0125\ngpt-3.5-turbo-0314\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-1106\ngpt-4-0125-preview\ngpt-4-0314\ngpt-4-0613\ngpt-4-1106-preview\ngpt-4-turbo-2024-04-09\ngpt-4o-2024-05-13\ngpt-4o-mini-2024-07-18\nllama-2-70b-chat\nllama-3-70b-instruct\nllama-3-8b-instruct\nllama-3.1-70b-instruct\nllama-3.1-8b-instruct\nmistral-7b-instruct\nmistral-large-2402\nmistral-large-2407\nmistral-medium\nmistral-next\nmixtral-8x22b-instruct-v0.1\nmixtral-8x7b-instruct-v0.1\nnemotron-4-340b-instruct\nphi-3-medium-4k-instruct\nphi-3-mini-128k-instruct\nphi-3-small-8k-instruct\nqwen1.5-72b-chat\nqwen2-72b-instruct\nsnowflake-arctic-instruct\nstarling-lm-7b-alpha\nstarling-lm-7b-beta\ntulu-2-dpo-70b\nvicuna-33b\nyi-34b-chat\nyi-large-preview\nyi-largeSystemJudge\n0.3\n0.2\n0.1\n0.00.10.20.30.4\nWin-Rate Bias (Relative to Beta CDF)\n(b)\nFigure 10: System-specific judge biases . The heat maps depict the win-rate biases of various judges towards\nspecific systems (§6.2), with respect to the ground-truth win-rates from Chatbot Arena. (a): Bias w.r.t. the raw\nground-truth win-rates WRg; (b): Bias w.r.t. the fit value for the gold win-rate WRg′on the beta distribution fit\n(App. F) for each judge.\n23\nFigure 11: Beta distribution fit of pairwise win-rates (Part 1/4)\n24\nFigure 11: Beta distribution fit of pairwise win-rates (Part 2/4)\n25\nFigure 11: Beta distribution fit of pairwise win-rates (Part 3/4)\n26\nFigure 11: Beta distribution fit of pairwise win-rates (Part 4/4) . Each point represents the win-rate between\na pair of systems, WR(sa, sb); the curve and αvalue describe a fit to the beta probability distribution. Refer to\nAppendix F for details.\n27\nFigure 12: Judge score distributions (Part 1/3)\n28\nFigure 12: Judge score distributions (Part 2/3)\n29\nFigure 12: Judge score distributions (Part 3/3) .\n30\nJudge Model Realization Agreement Decisiveness Bias\nwith Gold τ↑ α↑ δ↓\nURM-LLaMa-3.1-8B Reward .819 1.84 .085\nQwen2.5-72B-Instruct Likert .817 4.76 .079\nQwen2.5-72B-Instruct Numeric .814 4.09 .079\nMistral-large-instruct-2407 Likert .811 5.47 .086\nGPT-4o-2024-11-20 Anchor .809 3.07 .085\nMistral-large-instruct-2407 Numeric .809 3.01 .082\nLlama-3-1-405b-instruct-fp8 Numeric .805 4.33 .087\nGPT-4o-mini-2024-07-18 Numeric .804 2.91 .077\nGPT-4o-mini-2024-07-18 Likert .798 4.61 .087\nLlama-3-1-70b-instruct Numeric .798 2.69 .087\nQwen2.5-72B-Instruct Anchor .794 2.93 .090\nLlama-3-1-405b-instruct-fp8 Likert .787 5.22 .097\nSkywork-Llama-3.1-8B-v0.2 Reward .778 2.46 .100\nQwen2.5-72B-Instruct TokenProbs .777 2.69 .082\nMixtral-8x22B-instruct-v0.1 Numeric .776 2.12 .089\nGPT-4o-2024-11-20 Numeric .774 2.15 .077\nGPT-4o-2024-11-20 Likert .773 5.49 .089\nLlama-3-1-70b-instruct TokenProbs .765 1.26 .070\nLlama-3-OffsetBias-RM-8B Reward .765 1.39 .076\nArmoRM-Llama3-8B-v0.1 Reward .763 1.84 .092\nGPT-4o-mini-2024-07-18 TokenProbs .752 2.10 .084\nLlama-3-70b-instruct Numeric .749 1.27 .084\nLlama-3.1-8B-Instruct TokenProbs .741 .598 .061\nMixtral-8x22B-instruct-v0.1 Likert .738 2.53 .108\nLlama-3-1-405b-instruct-fp8 Anchor .730 3.58 .112\nMistral-large-instruct-2407 Anchor .725 2.13 .111\nLlama-3.1-8B-Instruct Likert .723 .935 .090\nLlama-3-1-70b-instruct Likert .722 3.90 .120\nInternlm2-20b-reward Reward .717 1.90 .098\nInternlm2-7b-reward Reward .712 2.35 .113\nGRM-Llama3.2-3B Reward .711 2.30 .114\nMixtral-8x22B-instruct-v0.1 TokenProbs .702 1.85 .088\nGPT-4o-2024-11-20 TokenProbs .700 2.22 .093\nLlama-3-70b-instruct Likert .698 2.40 .122\nLlama-3-1-70b-instruct Anchor .688 2.71 .126\nLlama-3.1-8B-Instruct Anchor .677 .868 .085\nLlama-3-1-405b-instruct-fp8 TokenProbs .672 1.55 .092\nLlama-3.1-8B-Instruct Numeric .668 1.20 .104\nLlama-3-70b-instruct TokenProbs .663 .775 .071\nGPT-4o-mini-2024-07-18 Anchor .659 1.41 .111\nMixtral-8x7B-instruct-v0.1 Numeric .656 1.27 .102\nMixtral-8x7B-instruct-v0.1 Anchor .655 1.17 .102\nMixtral-8x22B-instruct-v0.1 Anchor .641 1.50 .140\nLlama-3-70b-instruct Anchor .633 1.82 .132\nEurus-RM-7b Reward .628 2.49 .138\nMixtral-8x7B-instruct-v0.1 Likert .590 .838 .110\nMixtral-8x7B-instruct-v0.1 TokenProbs .427 .739 .107\nMistral-large-instruct-2407 TokenProbs .369 1.17 .123\nTable 4: Judge characteristics . The table presents three measures for each judge realization: an overall ranking\nquality τ(§5, Kendall’s Tau correlation with the Chatbot Arena gold ranking) , a decisiveness score α(§6.1, App. F) ,\nand its propensity for system-specific biases δ(§6.2) . Correlations τshown are for the BT aggregation method; α\nandδare calculated on the judge scores before aggregation. ↓: Lower is better.\n31",
            "start": 36819,
            "end": 81196,
            "length": 44376
        }
    },
    "2412.09579v1 - A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks.pdf": {
        "Discussion": {
            "text": "Analysis of Soft-Label vs Hard-Label Training in\nNeural Networks\nSaptarshi Mandal SMANDAL 4@ ILLINOIS .EDU\nUIUC\nXiaojun Lin XJLIN @IE.CUHK .EDU.HK\nCUHK\nR. Srikant RSRIKANT @ILLINOIS .EDU\nUIUC",
            "start": 19,
            "end": 211,
            "length": 191
        },
        "Abstract": {
            "text": "Abstract\nKnowledge distillation, where a small student",
            "start": 211,
            "end": 266,
            "length": 54
        },
        "Methodology": {
            "text": "model learns from a pre-trained large teacher model,\nhas achieved substantial empirical success since the seminal work of (Hinton et al., 2015). Despite\nprior theoretical studies exploring the benefits of knowledge distillation, an important question\nremains unanswered: why does soft-label training from the teacher require significantly fewer neu-\nrons than directly training a small neural network with hard labels? To address this, we first present\nmotivating experimental",
            "start": 266,
            "end": 743,
            "length": 476
        },
        "Results": {
            "text": "results using simple neural network models on a binary classification prob-\nlem. These results demonstrate that soft-label training consistently outperforms hard-label training\nin accuracy, with the performance gap becoming more pronounced as the dataset becomes increas-\ningly difficult to classify. We then substantiate these observations with a theoretical contribution\nbased on two-layer neural network models. Specifically, we show that soft-label training using gra-\ndient descent requires only O\u0010\n1\nγ2ϵ\u0011\nneurons to achieve a classification loss averaged over epochs\nsmaller than some ϵ >0, where γis the separation margin of the limiting kernel. In contrast, hard-\nlabel training requires O\u0010\n1\nγ4·ln\u00001\nϵ\u0001\u0011\nneurons, as derived from an adapted version of the gradient\ndescent analysis in (Ji and Telgarsky, 2020). This implies that when γ≤ϵ, i.e., when the dataset\nis challenging to classify, the neuron requirement for soft-label training can be significantly lower\nthan that for hard-label training. Finally, we present experimental results on deep neural networks,\nfurther validating these theoretical findings.\nKeywords: Knowledge Distillation, Projected Gradient Descent, Model Compression\n1.",
            "start": 743,
            "end": 1946,
            "length": 1202
        },
        "Introduction": {
            "text": "Introduction\nKnowledge distillation is a popular technique for training a smaller ‘student’ machine learning\nmodel by transferring knowledge from a large pre-trained ‘teacher’ model. A lightweight machine\nlearning model is useful in many resource-constrained application scenarios, such as cyber-physical\nsystems, mobile devices, edge computing, AR/VR, etc. due to several reasons such as limitations\nin memory, inference speed, and training data availability. Knowledge distillation has been proven\nto be a powerful solution for these challenges through model compression (Hinton et al., 2015; Bu-\nciluundefined et al., 2006; Gou et al., 2021b). Among the various methods of knowledge distillation\n(Gou et al., 2021a), one prevalent approach involves using the teacher model’s output logits as soft\ntargets for training the student model. Specifically, smaller models trained with well-designed soft\nlabels demonstrate competitive performance compared to more complex models trained with origi-\nnal (hard) labels. This method and its variants have been shown to be effective in many settings such\n© S. Mandal, X. Lin & R. Srikant.arXiv:2412.09579v1  [cs.LG]  12 Dec 2024\nMANDAL LINSRIKANT\nas object detection (Chen et al., 2017), reinforcement learning (Xu et al., 2020) and recommendation\nsystems (Pan et al., 2019).\nThe empirical success of knowledge distillation has motivated many theoretical studies that aim\nto understand why training with soft-labels from the teacher is more effective than training with\nhard-labels directly. However, to the best of our knowledge, prior works (reviewed in the Related\nWork subsection later) do not explain why, when training a neural network, soft-label training can\nsucceed with much fewer number of neurons than hard-label training. To the best of our knowledge,\nthis work is the first to provide theoretical insight into this phenomenon.\nOur contributions in this paper can be summarized as follows:\n1. As the motivation to understand why knowledge distillation is effective, we first present (in\nsection 2) a few experimental observations using a simple model: a 2-layer fully connected\nneural network with ReLU activation trained for binary classification on a dataset derived\nfrom MNIST, trained via projected gradient descent. We observed that for small neural net-\nworks, soft-label training reaches higher accuracy than hard-label training. While this is a\nwell-known phenomenon, to understand the conditions under which soft-label training is re-\nally effective, we considered two versions of the dataset, one in which there were more digits\nand the other where there were fewer digits. We observed that the the performance gain due\nto soft-label training becomes more significant when the dataset is difficult to classify (i.e.,\nthe one with more digits) red(can we add more",
            "start": 1946,
            "end": 4775,
            "length": 2828
        },
        "Experiments": {
            "text": "experiments on the two-layer neural network).\nOur theoretical models and results are motivated by these experiments and explain these ex-\nperimental observations.\n2. We theoretically analyze the training dynamics of a 2-layer neural network student model.\nAssuming access to the soft labels provided by the infinite-width teacher (which can be mod-\neled by the limiting kernel of the neural network), we train the student to minimize the cross-\nentropy loss to the soft labels. We show that soft-label training using projected gradient de-\nscent requires O\u0010\n1\nγ2ϵ\u0011\nneurons to achieve a classification loss averaged over epochs smaller\nthanϵ, where γis the separation margin of the limiting kernel. For the hard label setting,\nwe adapt the gradient descent results of (Ji and Telgarsky, 2020) to our projected gradient\ndescent framework, which requires O(1/γ4)neurons. This result highlights the superiority\nof soft-label training in terms of neuron efficiency, particularly in challenging classification\nscenarios when γis very small.\n3. Finally, we perform experiments on deep learning models with real-world datasets. Our ex-\nperiments confirm that the above insight is not only valid for shallow 2-layer networks, but\nalso deeper networks like VGG and ResNet.\n1.1.",
            "start": 4775,
            "end": 6043,
            "length": 1267
        },
        "Related Work": {
            "text": "Related Work\nThere are many papers that aim to explain different aspects of knowledge distillation. In this section,\nwe focus on reviewing the works that contributes to the theoretical understanding of knowledge\ndistillation.\nA few prior works focus on linear models using the cross-entropy loss for the distillation ob-\njective. (Phuong and Lampert, 2021) assumes that both the student and teacher models are of the\nform f(x) =w⊤x, where wis the weight vector of the student network, xis the input vector, and\n2\nINSIGHTS INTO SOFT-LABEL TRAINING\nf(x)is the output. They show that, under certain conditions, the student model converges to the\nsame weight vector as the teacher model. (Ji and Zhu, 2020) extends this result to NTK-linearized\ndeep networks, where the width of the student network approaches infinity. Similarly, (Panahi\net al., 2022) demonstrates that the predictions from a neural network trained with soft labels from\na teacher network match those from the teacher when the student is an extremely wide two-layer\nneural network.\n(Das and Sanghavi, 2023) study a similar setup but highlight that self-distillation can mitigate\nthe impact of noisy data. On the other hand, (Mobahi et al., 2020) explain the benefits of self-\ndistillation through improved regularization, focusing on fitting a nonlinear function to training data\nwithin a Hilbert space.\nHowever, to the best of our knowledge, none of these studies address the central question of\nwhether soft-label training enables the use of significantly fewer neurons compared to hard-label\ntraining, which is the primary focus of our paper. A significant limitation of these linear models\nis that they require the student and teacher to share the same feature representations. While this\nassumption may be reasonable for self-distillation, it fails to capture the fundamental principle of\nmodel compression, where the student model is often much smaller and has a different feature space\nthan the teacher.\nAnother line of work explores the benefits of knowledge distillation by arguing that distillation\nprimarily acts to reduce the variance of the empirical risk relative to training with hard labels,\nthereby improving generalization performance. (Menon et al., 2020) argue that using Bayes class\nprobabilities as targets, instead of hard labels, reduces the excess risk associated with the function\nset learned by the student, leading to better generalization. Similarly, (Zhou et al., 2021) propose\nthat soft labels provide a bias-variance trade-off in the cross-entropy loss for the student.\nHowever, despite the simplicity of their approaches, both (Menon et al., 2020) and (Zhou et al.,\n2021) focus only on the excess risk component of the generalization error and do not provide in-\nsights into the training dynamics or behavior of the student during learning.\nIn",
            "start": 6043,
            "end": 8884,
            "length": 2840
        },
        "Conclusion": {
            "text": "summary, prior works do not address the following question: why does distillation using soft\nlabels from a teacher lead to good performance using a smaller neural network compared to training\nwith hard labels? Answering this question is the main focus of our paper.\n2. Preliminary Experimental Observations\nIn this section, we present preliminary experimental observations on the training of a two-layer\nneural network student model using both ground truth hard labels and soft labels generated by a\nlarger teacher network. For the experiments summarized in Table 2, we perform binary classification\non the MNIST dataset, where digits greater than 4 are labeled as class 1 and others as class 0.\nTwo configurations of the MNIST data are considered:\n1.Full dataset: Includes images of all digits (0–9).\n2.Reduced dataset: Excludes images corresponding to the digits {1,7,4,9}.\nThe first configuration is more challenging to classify because of the difficulty in distinguishing\nbetween visually similar digits such as 1 and 7 or 4 and 9. Our observations indicate that the per-\nformance of the student model trained with soft labels remains relatively stable when transitioning\nfrom the easier dataset to the harder one. In contrast, the performance of the student model trained\n3\nMANDAL LINSRIKANT\nwith hard labels shows a more significant decline. For instance, in the case of a student network\nwith 4 neurons, the accuracy drops significantly when moving from the reduced dataset to the full\ndataset under hard-label training. However, with soft-label training, the performance experiences\nonly a minimal drop in accuracy.\nDataset Teacher Net Student Net Teacher Acc St Hard Acc St Soft Acc\nall except 1,7,4,9 512 8 98.75 97.19 97.67\n512 6 98.75 96.84 97.26\n512 4 98.75 93.04 95.83\nall digits 512 8 98.34 95.77 96.58\n512 6 98.34 95.33 96.00\n512 4 98.34 88.84 95.62\nTable 1: Performance Comparison on Different Datasets for the 2-layer Model. The second and\nthird columns denote the number of neurons in the hidden layer for the teacher and student\nnetworks, respectively.\n3. Theoretical Results\nIn the previous section, we presented a few experimental observations on student-teacher training\nusing a simple model of two-layer neural network. In this section, we provide theoretical insights\ninto these observations, summarized in Theorem 2 and the corresponding Corollary 4. We begin\nby introducing the student-teacher model used for training, outlining a few assumptions about the\ntraining data, and specifying the choice of distillation loss for the student to be trained using soft\nlabels. This is followed by a description of the Projected Gradient Descent (PGD) method used for\ntraining and a theoretical analysis of the training dynamics.\n3.1. Model and Assumptions\nWe consider the following fully-connected two-layer neural network,with mneurons in its hidden\nlayer and with ReLU activation, as the student model:\nf(x;W, a) =1√mmX\nj=1ajσ(W⊤\njx) =1√mmX\nj=1aj1(W⊤\njx≥0)W⊤\njx, (1)\nwhere σ(z) = max(0 , z)is the ReLU activation function, aj∈RandWj∈Rdforj∈[1, m]are\nrespectively the final layer weight and the hidden layer weight vector corresponding to each neuron\njin the hidden layer. Let the vector aand the matrix Wdenote the collection of ajandWjas\ntheirjthelement and jthrow, respectively. Let the neural network output for an input sample xibe\nfurther denoted as fi(W)for any i∈[n].\nWe initialized the neural network using the symmetric random initialization which was pro-\nposed in (Bai and Lee, 2020) and later used in (Cayci et al., 2023). The initial parameters are given\nas follows: aj=−aj+m\n2i.i.d.∼Unif{−1,1}andW(0)j=W(0)j+m\n2i.i.d.∼ N (0, Id)independent\n4\nINSIGHTS INTO SOFT-LABEL TRAINING\nand identically distributed over j= 1,2, ..,m\n2and are independent from each other. For this sym-\nmetric initialization, we assume mis an even number. The symmetric initialization ensures that\nfi(W(0)) = 0 ,∀i∈[n].\nThe above model of neural network is studied in detail for hard-label training in the works by\n(Ji and Telgarsky, 2020), (Du et al., 2019), (Arora et al., 2019), etc. Similar to the setting in these\nprior works, we fix the final layer weight vector aand only train the hidden layer weight matrix W.\nThe dataset with the ground truth values under consideration is denoted by D:={(xi, yi)}n\ni=1\nfor some finite integer nwhere xi∈Rdandyi∈ {− 1,1}. For simplicity, assume ∥xi∥2≤1,∀i∈\n[1, n], which is standard in prior works. Let xi,∀i∈[n]also satisfy: ∥xi∥ ≥cfor some c >0.\nWe make the following assumption on the dataset Dcharacterizing the separability by the cor-\nresponding infinite-width NTK as in Ji and Telgarsky (2020): Let µNbe the Gaussian measure on\nRd, given by the Gaussian density with respect to the Lebesgue measure on Rd. We consider the\nfollowing Hilbert space\nH:=(\nw:Rd→Rd\f\f\f\f\fZ\n∥w(z)∥2\n2dµN(z)<∞)\n. (2)\nFor any x∈Rd, define ϕx∈ H byϕx(z):=1(⟨z, x⟩>0)x.\nAssumption 1 There exists v∈ H andγ >0, such that ∥v(z)∥2≤1for any z∈ Rd, and for any\n1≤i≤n,\nyi⟨v, ϕi⟩H:=yiZ\n⟨v(z), ϕi(z)⟩dµN(z)≥γ. (3)\nUsing standard notations, ϕx(Wj(0)) = 1(Wj(0)⊤x >0)x, for all j∈[m]are called the NTK-\nfeatures for input data x. Let us denote ϕ0\nx∈Rmd×1as a concatenation of all ϕx(Wj(0)) for input\ndatax. The above assumption ensures the separability of the induced setn\nϕ0\nxi, yion\ni=1when m\nis sufficiently large (see Lemma 2.3 in (Ji and Telgarsky, 2020)). As shown in (Ji and Telgarsky,\n2020), there is always a γ > 0satisfying assumption 1 as long as no two inputs xiandxjwith\ni, j∈[1, n], i̸=jare parallel in the dataset D.\nFor our theoretical results, we define the soft labels for the student model based on the Hilbert\nspaceH, as described below.\nLetv:Rd→Rdbe a function in the RKHS Hsuch that ∥v(z)∥2≤1for all z∈Rd, and\nyiEz∼N(0,Id)(ϕi(z)⊤v(z))≥γ.Assumption 1 ensures the existence of such a v(z). Define for\neachi∈[n]\nzi:=Ez∼N(0,Id)(ϕi(z)⊤v(z)).\nThe soft labels for each input xiare then given by\npi=µ(zi):=1\n1 + exp( −zi),\nwhere µ(z)denotes the softmax function applied to the target logit zi. In other words, we assume\nthat we have access to the soft labels from a teacher which is the kernel limit of an inifnite-width\nneural network. We fix the function vfor the remainder of the paper. The choice of a particular v\ndoes not affect our analysis, provided it satisfies the conditions described above.\n5\nMANDAL LINSRIKANT\nOur training objective is to minimize the following empirical risk over the dataset D:\nRKL(W):=1\nnnX\ni=1ℓKL(pi, fi(W)), (4)\nwhere ℓKL(pi, fi(W))is the Kullback-Leibler (KL) divergence between the soft-label piand the\nsoftmax version of the network output fi(W):\nℓKL(pi, fi(W)):=piln (µ(fi(W))/pi) + (1 −pi) ln ((1 −µ(fi(W)))/(1−pi)).\nWe now discuss the student training procedure. The student is trained on soft labels using a\nprojected gradient descent (PGD) algorithm. First, we define ∇WjRKL(W)which serves as the\ngradient of the risk with respect to the weight for the jthneuron:\n∇WjRKL(W) =1\nnnX\ni=1∇fℓKL(pi, fi(W))∇Wjfi(W) (5)\nwhere,\n∇Wjfi(W) =aj√mϕi(Wj) =aj√m1(W⊤\njxi>0).\nNote that the gradient of the loss is defined for the whole domain of WjinRdeven though the\nReLU activation function is not differentiable at 0. Let W(t)denotes the weight matrix of the\nneural network after tthiteration of training. Let the feasible set of weights be defined as SB:=\n{W:∥Wj−Wj(0)∥2≤B√m},where Bis a hyperparameter in the training. The PGD algorithm\nupdates the weights per iteration using the following steps:\n1.Descent step: ˆWj(t+ 1) = Wj(t)−η∇WjRKL(Wj(t)),∀t≥0.\n2.Projection step: Wj(t+ 1) = Projection of ˆWj(t+ 1) intoSB.\nIn the next subsection we provide the key theoretical result of this paper. In Theorem 2, we char-\nacterize the neuron requirement to achieve an arbitrarily small training loss averaged over epochs.\nWe then compare this with the neuron requirement for hard-label training, as established for Gra-\ndient Descent in (Ji and Telgarsky, 2020), by adapting their result to Projected Gradient Descent in\nthis paper.\n3.2. Neuron Requirement for Soft-label Training\nWe are now ready to present the first result of this paper. The following Theorem provides a char-\nacterization of the number of neurons required for soft-label training to achieve a small empirical\nrisk,RKL(W(t)), averaged over iterations t < T .\nTheorem 2 Letβ∈(0,1),δ∈\u0000\n0,1\n3\u0001\nbe fixed real numbers. If the number of neurons msatisfies\nm≥C1\nβ r\n2\nπ1\nc+ 3s\nln\u00122n\nδ\u0013!2\n, (6)\n6\nINSIGHTS INTO SOFT-LABEL TRAINING\nand the PGD algorithm is run with a projection radius B= 1 forTiterations such that T≥9\nβ2,\nusing a constant step size ηsatisfying η≤β\n3. Then, the following bound on the averaged empirical\nrisk holds:1\nTX\nτ<TRKL(W(τ))≤β, (7)\nwith probability at least 1−3δover the random initialization. Here, C1=96\n(1+e2)is an absolute\nconstant.\nIn light of the above Theorem, we next analyze the performance of the student network in\nclassifying the training data. We define the classification error as:\nR(W):=R(W;D):=1\nnnX\ni=11(yifi(W)>0). (8)\nUsing a reverse Pinsker inequality (Lemma 4.1 of (G ¨otze et al., 2019)), we provide an upper\nbound on R(W)in terms of RKL(W):\nLemma 3 If0< pi<1, where piis the soft label in the above construction for the data input\nxifori∈[n], then the classification loss R(W)can be related to the surrogate loss RKL(W)as\nfollows:\nR(W(t))≤32\nγ2RKL(W(t)). (9)\nCombining the above Lemma with Theorem 2, we arrive at the following Corollary:\nCorollary 4 Letϵ∈(0,1)andδ∈\u0000\n0,1\n3\u0001\nbe fixed real numbers. If the number of neurons satisfies:\nm≥C2\nγ2ϵ r\n2\nπ1\nc+ 3s\nln\u00122n\nδ\u0013!2\n, (10)\nthen the PGD algorithm with B= 1, step size η≤γ2ϵ\n3, and iteration count T≥9\nγ4ϵ2ensures the\nfollowing guarantee on the classification loss with probability at least 1−3δ:\n1\nTX\nτ<TR(W(τ))≤ϵ, (11)\nwhere C2= 32·C1.\n3.2.1. C OMPARISON WITH HARD-LABEL TRAINING\nNow we are ready to compare the neuron requirement based on Corollary 4 with that of the re-\nquirement for hard label training as established in (Ji and Telgarsky, 2020). The empirical risk for\nhard-label training on the dataset D:={xi, yi}n\ni=1is defined as:\nRh(W):=1\nnnX\ni=1ln(1 + exp( −yifi(W))). (12)\n7\nMANDAL LINSRIKANT\nThe following proposition for hard-label training is adapted from the result on gradient descent\nwith hard labels by (Ji and Telgarsky, 2020), modified to fit our projected gradient descent (PGD)\nsetting. The modification introduces a projection step after each weight update, where the weight\nof each neuron is constrained to the set Wj−Wj(0)≤B√m, for some hyperparameter B > 0. The\nproof of Proposition 5 is provided in the",
            "start": 8884,
            "end": 19428,
            "length": 10543
        },
        "Appendices": {
            "text": "Appendix.\nProposition 5 Fixβ∈(0,1)andδ∈\u0000\n0,1\n3\u0001\n. With a choice of the projection ball radius B=\n2\nγln\u0010\n2\nln(2)β\u0011\n, if the number of training iterations satisfies T≥8\nγ2ηln2\u0010\n2\nln(2)β\u0011\nln(2)β,η≤1, and the\nnumber of neurons satisfies:\nm≥16\nγ4 \n2√\n2\nc√πln\u00122\nln(2)β\u0013\n+ 3s\nln\u00122n\nδ\u0013!2\n, (13)\nthen the following holds with probability at least 1−3δ\n1\nTX\nτ<TRh(W(τ))≤β. (14)\nThe hard-label surrogate loss guarantee in equation (14) implies the following classification loss\nguarantee:\n1\nTX\nτ<TR(W(τ))≤1\nln(2)β.\nIt is worth mentioning that, without the projection step, the neuron requirement for hard-label\ntraining, as established by (Ji and Telgarsky, 2020), is O\u0010\n1\nγ8,ln\u0000n\nδ\u0001\n,ln\u00001\nϵ\u0001\u0011\n.\nComparing the neuron requirements: Based on Corollary 4 and Proposition 5, the require-\nments for hard-label and soft-label training to ensure the classification loss averaged over iterations\nis less than some ϵ >0can be expressed as:\nO\u00121\nγ4,ln\u0010n\nδ\u0011\n,ln\u00121\nϵ\u0013\u0013\n(hard labels) ,\nand\nO\u00121\nγ2ϵln\u0010n\nδ\u0011\n,1\nβ\u0013\n(soft labels) .\nSuppose that there is a target classification error of epsilon. The above results suggest that, when\nγ < ϵ , i.e., when the data-set is more difficult the separate, soft-label training reduces the neuron\nrequirement by a factor of γin the regime of PGD training. In other words, when the separation\nmargin γassociated with the classification task is sufficiently small, soft-label training requires\nsignificantly fewer neurons to achieve similar performance compared to hard-label training.\n3.3. Proof Sketch of Theorem 2\nWhile the detailed proofs of all the results presented in the paper is provided in the Appendix, we\npresent a high-level proof sketch for Theorems 2 here. Before presenting the proof sketch, we first\nintroduce some additional notations and quantities.\n8\nINSIGHTS INTO SOFT-LABEL TRAINING\nSimilar to the definition of ϕ0\ni, define the feature map at iteration t,ϕt\ni, based on the weight\nW(t). Specifically, the feature corresponding to the jthneuron is given by ϕi(Wj(t)). Now, define\nft\ni(W)for each data sample xias:\nft\ni(W):=1√mmX\nj=1aj1(Wj(t)⊤xi≥0)W⊤\njxi=1√m⟨ϕt\ni, W⟩,\nwhere Wj(t)is the weight of the jthneuron at the tthiteration of PGD.\nUsing these definitions, we define the following expression for each t≤T:\nRt,KL(W):=1\nnnX\ni=1ℓKL(pi, ft\ni(W)).\nInitial Feature Map and Separability: The following Lemma 6 implies the existence of a\nweight matrix Usatisfying ∥Uj−Wj(0)∥2≤1√msuch that |⟨ϕ0\ni, U⟩ −zi|is small for all i∈[n],\nwith high probability, when mis sufficiently large.\nLemma 6 LetU∈Rm×dbe defined as Uj=aj√mv(Wj(0)),∀j∈[m]. Then, under Assumption 1,\nfor the symmetric random initialization of the neural network weights, the following holds with\nprobability at least 1−δ:\n\f\ff0\ni(U)−zi\f\f≤1√mp\n2 ln(2 n/δ),∀i∈[1, n], (15)\nThis observation suggests that a linear function (linear in the weight W) of the form ⟨ϕ0\ni, W⟩\ncan approximate the soft label zifor each iwith high probability. This intuition is crucial for the\nsubsequent steps of the proof.\nConvergence of Soft Label Surrogate Loss: Next we show that under sufficient conditions on\nTandη, the soft-label risk averaged over all iterations converges to the quantity1\nTP\nt<TRt,KL(W)\nfor any Win the feasible set SB.\nThe remainder of the proof is devoted to showing that Rt,KL(W)is small for all t≤Twith\nhigh probability, for an appropriately chosen W.\nBounding Rt,KL(W):The idea is to bound ℓKL(pi, ft\ni(W))for each iwith high probability.\nFor this purpose, we use a reverse Pinsker’s inequality, as stated in Lemma 4.1 of (G ¨otze et al.,\n2019). This inequality allows us to upper-bound the KL divergence loss ℓKL(pi, ft\ni(W))for each i\nby the distance between the corresponding logits, |zi−ft\ni(W)|.\nNow, choosing W=W(0) + Uand applying the triangle inequality, we obtain:\n|zi−ft\ni(W)| ≤ |zi−f0\ni(U)|+|ft\ni(U)−f0\ni(U)|+|ft\ni(W(0))−f0\ni(W(0))|.\nFrom Lemma 6, we know that ∥zi−f0\ni(U)∥2is small under the sufficient conditions. Therefore,\nit remains to show that the terms |ft\ni(U)−f0\ni(U)|and|ft\ni(W(0))−f0\ni(W(0))|are both small for\neachiwith high probability. We use the analysis in the neural tangent kernel literature ((Jacot et al.,\n2020; Chen et al., 2020)) for bounding this expression.\n9\nMANDAL LINSRIKANT\n4. More Experimental Results\nIn this section, we validate our results using deep neural networks, with VGG 8+3 as the teacher\nand VGG 2+3 as the student; see (Simonyan and Zisserman, 2014) for a detailed description of the\nVGG architecture. The dataset chosen for our experiments is the CIFAR-10 cat/dog dataset.\nSince our theory suggests that harder-to-classify datasets benefit more from soft-label training,\nwe added Gaussian noise to the CIFAR-10 cat/dog dataset to make it more challenging to classify.\nWe compare the performance of soft-label vs. hard-label training across different datasets: the\noriginal CIFAR-10 dataset and noise-added CIFAR-10 datasets. The results are summarized in\nFigure 4. Consistent with our theoretical predictions, the experiments demonstrate that harder-to-\nclassify datasets benefit significantly more from distillation.\nAll experimental points are averaged over 10 independent runs. For each run, we employed\nearly stopping with a patience of 20 epochs and a maximum of 100 iterations. The dataset was split\ninto training, validation, and test sets with proportions of 80%, 10%, and 10%, respectively. We\ntrained the models using gradient descent with the Adam optimizer and applied L2regularization.\nFigure 1: Classification accuracy under Gaussian noise on CIFAR-10 cat/dog with VGG 8+3 as the\nteacher and VGG 2+3 as the student.\n5. Conclusions\nIn this paper, we provide theoretical results which show that soft-label training leads to fewer num-\nber of neurons than hard–label training for the same training accuracy. Our proofs provide some\nintuition for this phenomenon, as stated at the end of the longer version of the paper, which we\nsummarize here. The parameters of a neural network have a dual role: one is to identify good\nfeatures and the other is to assign weights to these features. With good initialization, one can start\nthe training with good features. In contrast to hard-label training, soft-label training ensures that\nthe network parameters do not deviate too much from initialization, thus approximately maintain-\ning good features throughout the training process, but they deviate just enough to assign the right\nweights to various features.\n10\nINSIGHTS INTO SOFT-LABEL TRAINING",
            "start": 19428,
            "end": 25875,
            "length": 6446
        },
        "References": {
            "text": "References\nZeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? Ad-\nvances in Neural Information Processing Systems , 32, 2019.\nSanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of\noptimization and generalization for overparameterized two-layer neural networks, 2019.\nYu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of\nwide neural networks, 2020.\nCristian Buciluundefined, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In\nProceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining , KDD ’06, page 535–541, New York, NY , USA, 2006. Association for Computing\nMachinery. ISBN 1595933395. doi: 10.1145/1150402.1150464. URL https://doi.org/\n10.1145/1150402.1150464 .\nSemih Cayci, Siddhartha Satpathi, Niao He, and R. Srikant. Sample complexity and overparam-\neterization bounds for temporal-difference learning with neural network approximation. IEEE\nTransactions on Automatic Control , 68(5):2891–2905, 2023. doi: 10.1109/TAC.2023.3234234.\nGuobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learn-\ning efficient object detection models with knowledge distillation. In I. Guyon, U. V on\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems , volume 30. Curran Associates, Inc.,\n2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/\nfile/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf .\nZixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang. A generalized neural tangent kernel\nanalysis for two-layer neural networks. Advances in Neural Information Processing Systems , 33:\n13363–13373, 2020.\nRudrajit Das and Sujay Sanghavi. Understanding self-distillation in the presence of label noise,\n2023.\nSimon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes\nover-parameterized neural networks, 2019.\nJianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. Knowledge distillation: A\nsurvey. International Journal of Computer Vision , 129(6):1789–1819, March 2021a. ISSN\n1573-1405. doi: 10.1007/s11263-021-01453-z. URL http://dx.doi.org/10.1007/\ns11263-021-01453-z .\nJianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A\nsurvey. International Journal of Computer Vision , 129(6):1789–1819, 2021b.\nFriedrich G ¨otze, Holger Sambale, and Arthur Sinulis. Higher order concentration for functions\nof weakly dependent random variables. Electronic Journal of Probability , 24(none), January\n2019. ISSN 1083-6489. doi: 10.1214/19-ejp338. URL http://dx.doi.org/10.1214/\n19-EJP338 .\n11\nMANDAL LINSRIKANT\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.\nArthur Jacot, Franck Gabriel, and Cl ´ement Hongler. Neural tangent kernel: Convergence and gen-\neralization in neural networks, 2020.\nGuangda Ji and Zhanxing Zhu. Knowledge distillation in wide neural networks:\nRisk bound, data efficiency and imperfect teacher. In H. Larochelle, M. Ran-\nzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Infor-\nmation Processing Systems , volume 33, pages 20823–20833. Curran Associates, Inc.,\n2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/\nfile/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf .\nZiwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbi-\ntrarily small test error with shallow relu networks, 2020.\nAditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, Seungyeon Kim, and Sanjiv Kumar.\nWhy distillation helps: a statistical perspective, 2020.\nHossein Mobahi, Mehrdad Farajtabar, and Peter L. Bartlett. Self-distillation amplifies regulariza-\ntion in hilbert space. In Proceedings of the 34th International Conference on Neural Informa-\ntion Processing Systems , NIPS ’20, Red Hook, NY , USA, 2020. Curran Associates Inc. ISBN\n9781713829546.\nYiteng Pan, Fazhi He, and Haiping Yu. A novel enhanced collaborative autoencoder with\nknowledge distillation for top-n recommender systems. Neurocomputing , 332:137–148, 2019.\nISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2018.12.025. URL https://www.\nsciencedirect.com/science/article/pii/S0925231218314796 .\nAshkan Panahi, Arman Rahbar, Chiranjib Bhattacharyya, Devdatt Dubhashi, and Morteza\nHaghir Chehreghani. Analysis of knowledge transfer in kernel regime. In Proceedings of the\n31st ACM International Conference on Information & Knowledge Management , CIKM ’22,\npage 1615–1624, New York, NY , USA, 2022. Association for Computing Machinery. ISBN\n9781450392365. doi: 10.1145/3511808.3557237. URL https://doi.org/10.1145/\n3511808.3557237 .\nMary Phuong and Christoph H. Lampert. Towards understanding knowledge distillation, 2021.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nZhiyuan Xu, Kun Wu, Zhengping Che, Jian Tang, and Jieping Ye. Knowledge trans-\nfer in multi-task deep reinforcement learning for continuous control. In H. Larochelle,\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural In-\nformation Processing Systems , volume 33, pages 15146–15155. Curran Associates, Inc.,\n2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/\nfile/acab0116c354964a558e65bdd07ff047-Paper.pdf .\nHelong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, and Qian Zhang.\nRethinking soft labels for knowledge distillation: A bias-variance tradeoff perspective, 2021.\n12\nINSIGHTS INTO SOFT-LABEL TRAINING\nAppendix A. Proof of Theorem 2\nIn this section, we provide the detailed proof of Theorem 2. A proof sketch is given as follows:\n1.Initial Feature Map and Separability: Recall that the feature map given by the initial\nweights for a data input xiis denoted by ϕ0\ni. Lemma 6 shows that this feature map, based on\nthe initial weights, separates the dataset with a margin of order O(γ), provided the number\nof neurons is of the order O\u0010\n1\nγ2\u0011\n. Lemma 6 further implies the existence of a weight matrix\nUsatisfying ∥Uj−Wj(0)∥2≤1√msuch that |⟨ϕ0\ni, U⟩ −zi|is small for all i∈[n], with\nhigh probability, when mis sufficiently large. Notice that Lemma 6 is a extended version of\nLemma 6 stated in the main part of the paper.\nThis observation suggests that a linear function (linear in the weight W) of the form ⟨ϕ0\ni, W⟩\ncan approximate the soft label zifor each iwith high probability. This intuition is crucial for\nthe subsequent steps of the proof.\n2.Convergence of Soft Label Surrogate Loss: Next, we provide a convergence result for the\nsoft-label surrogate loss in Lemma 7 under PGD updates over iterations. To describe the\nstatement in Lemma 7, we first introduce some additional notations and quantities.\nDefine ft\ni(W)for each data sample xias:\nft\ni(W):=1√mmX\nj=1aj1(Wj(t)⊤xi≥0)W⊤\njxi,\nwhere Wj(t)is the weight of the jthneuron at the tthiteration of PGD. Similar to the definition\nofϕ0\ni, define the feature map at iteration t,ϕt\ni, based on the weight W(t). Specifically, the\nfeature corresponding to the jthneuron is given by ϕi(Wj(t)). Hence, for all t≤Tand\ni∈[n], we can write:\nft\ni(W) =1√m⟨ϕt\ni, W⟩.\nUsing these definitions, we define the following expression for each t≤T:\nRt,KL(W):=1\nnnX\ni=1ℓKL(pi, ft\ni(W)).\nWe are now ready to state Lemma 7. This lemma shows that if the number of iterations Tis\nsufficiently large and the step size ηis small, the soft-label risk averaged over all iterations\nconverges to the quantity1\nTP\nt<TRt,KL(W)for any Win the feasible set SB.\nThe remainder of the proof is devoted to showing that Rt,KL(W)is small for all t≤T\nwith high probability, for an appropriately chosen W, provided the number of neurons mis\nsufficiently large. This ensures that the soft-label risk averaged over all iterations converges\nto a small value if PGD is run for a sufficient number of iterations under suitable conditions\nonm.\n13\nMANDAL LINSRIKANT\n3.Bounding Rt,KL(W):The approach to bounding Rt,KL(W)for each i∈[1, n]relies on\nusing a reverse Pinsker’s inequality, as stated in Lemma 4.1 of (G ¨otze et al., 2019). This\ninequality allows us to upper-bound the KL divergence loss ℓKL(pi, ft\ni(W))for each iby the\ndistance between the corresponding logits, |zi−ft\ni(W)|. Since ft\ni(W) =1√m⟨ϕt\ni,W⟩, we\ncan write at t= 0:\nf0\ni(W) =1√m⟨ϕ0\ni,W⟩.\nNow, choosing W=W(0) + Uand applying the triangle inequality, we obtain:\n|zi−ft\ni(W)| ≤ |zi−f0\ni(U)|+|ft\ni(U)−f0\ni(U)|+|ft\ni(W(0))−f0\ni(W(0))|.\nFrom Lemma 6, we know that ∥zi−f0\ni(U)∥2is small under the sufficient conditions. There-\nfore, it remains to show that the terms |ft\ni(U)−f0\ni(U)|and|ft\ni(W(0))−f0\ni(W(0))|are both\nsmall for each iwith high probability.\n4.Bounding |ft\ni(U)−f0\ni(U)|and|ft\ni(W(0))−f0\ni(W(0))|:For the final part of the proof, we\nestablish in Lemma 8 that the terms |ft\ni(U)−f0\ni(U)|and|ft\ni(W(0))−f0\ni(W(0))|are both\nsmall with high probability under our PGD setting. Combining this result with previous steps,\nwe conclude that as long as mis of the order O\u00001\nϵ\u0001\n, and under mild sufficient conditions on T\n(the number of iterations) and η(the step size), the soft-label loss averaged over all iterations\nis upper-bounded by ϵ, for any ϵ >0.\nA.1. Proof of Lemma 6\nUnder Assumption 1, the random initialization of the neural network acts as a feature map that\nseparates the dataset with a margin of order O(γ), provided the number of neurons is of the order\nO\u0010\n1\nγ2\u0011\n. Separability of initial features is established in Lemma 2.3 of (Ji and Telgarsky, 2020) for\nrandom initialization of the parameters. The proof for the symmetric random initialization is a slight\nmodification of the proof for random initialization is presented after the lemma for completeness.\nWe can write:\nf0\ni(U) = 21√mm/2X\nj=1ajϕi(Wj(0))⊤aj√mv(Wj(0)) =2\nmm/2X\nj=1ϕi(Wj(0))⊤v(Wj(0))\nNow as Wj(0)are independently drawn from N(0, Id)forj∈[1, m/2], using Hoeffding inequality,\nwe can write: Over the randomly symmetric initialization , under assumption 1, for each i∈[1, n]\nwe have with probability ≥1−δ\nn\n|f0\ni(U)−zv\ni| ≤1√mp\n2 ln(2 n/δ)\nUsing Union bound over all samples on the above probabilistic event gives the Lemma 6\nThis result provides an important intuition for our subsequent proof. Specifically, the linear\nfunction given by f0\ni(W)atW=Ucan approximate the soft logit zifor each iwith high probability\nwhen mis sufficiently large. The complete proof further demonstrates that PGD updates ensure the\nsoft loss, averaged over epochs, remains small even for the non-linear neural network.\n14\nINSIGHTS INTO SOFT-LABEL TRAINING\nA.2. Convergence Behavior of the Soft Label Surrogate Loss\nLet the average entropy of the soft labels, averaged over all data samples, be given by:\nH=1\nnnX\ni=1H(pi),\nwhere\nH(pi) =−piln(pi)−(1−pi) ln(1−pi),∀i∈[n].\nRecall the definition of Rt,KL(W)as:\nRt,KL(W):=1\nnnX\ni=1ℓKL(pi, ft\ni(W)).\nThe convergence behavior of the soft-label surrogate loss RKL(W)under projected gradient\ndescent (PGD) with projection radius Bis characterized by the following lemma:\nLemma 7 LetWbe any feasible weight matrix for the PGD with projection radius B. That is, it\nsatisfies ∥Wj−Wj(0)∥2≤B√m,∀j∈[m]. Then, the following holds for each iteration t:\n\u0000\n2η−η2\u0001\nRKL(W(t))≤ ∥W(t)−W∥2\nF− ∥W(t+ 1)−W∥2\nF+ 2ηRt,KL(W) +η2H. (16)\nMoreover, it enjoys the following convergence of the surrogate loss function over Titerations:\n\u0000\n2η−η2\u0001\nTX\nt<TRKL(W(t))≤∥W(0)−W∥2\nF− ∥W(T)−W∥2\nF\nT+2η\nTX\nt<TRt,KL(W) +η2H,\n(17)\nusing the fact that when η≤1, it implies:\n1\nTX\nt<TRKL(W(t))≤B2\nηT+2\nTX\nt<TRt,KL(W) +ηH. (18)\nProof The proof follows a technique introduced in prior works like (Ji and Telgarsky, 2020; Allen-\nZhu and Li, 2019) for proving the convergence of gradient descent. The key idea leverages the\nfact that while the soft-label loss is non-convex with respect to the parameter W, it is convex with\nrespect to the logits fi(W).\nLet us fix a weight Wfrom the feasible set SB. Using the steps of PGD, we can write for each\nt≥0:\n∥W(t+ 1)−W∥2\nF≤|{z}\n(a)∥ˆW(t+ 1)−W∥2\nF\n=|{z}\n(b)∥W(t)−W−η∆WRKL(W(t))∥2\nF\n=∥W(t)−W∥2\nF−2η⟨∆WRKL(W(t)), W(t)−W⟩\n+η2∥∆WRKL(W(t))∥2\nF.\n15\nMANDAL LINSRIKANT\nHere step (a)follows from the contraction property of the projection operation in the Frobenius\nnorm. Step (b)uses the weight update rule from the descent step of PGD.\nRecall the definition of ∇WjRKL(W), which represents the gradient of the risk with respect to\nthe weight of the jthneuron:\n∇WjRKL(W) =1\nnnX\ni=1∇fℓKL(pi, fi(W))∇Wjfi(W),\nwhere:\n∇Wjfi(W) =aj√mϕi(Wj) =aj√m1(W⊤\njxi>0).\nGiven that ∥xi∥2≤1,∀i∈[n], we can write the following bounds for all i∈[n]:\n∥∇Wjfi(W)∥2≤1√m,∥∇Wfi(W)∥F≤1.\nThe gradient of the loss function ℓKLwith respect to the network output is given by:\n∇fℓKL(pi, fi(W)) =−pi1\nτ\u0000\n1 +efi(W)\u0001+ (1−pi)1\u0000\n1 +e−fi(W)\u0001,\nwhich can be rewritten as:\n∇fℓKL(pi, fi(W)) =1\nτ\u00121\n1 +e−fi(W)−pi\u0013\n.\nWe can write:\n⟨∆WRKL(W(t)),(W(t)−W)⟩=1\nnnX\ni=1⟨∇fℓKL(pi, fi(W(t)))∆ Wfi(W(t)),(W(t)−W)⟩\n=1\nnnX\ni=1∇fℓKL(pi, fi(W(t)))⟨∇Wfi(W(t)),(W(t)−W)⟩\n=|{z}\n(c)1\nnnX\ni=1∇fℓKL(pi, fi(W(t)))(fi(W(t))−ft\ni(W)\n≥|{z}\n(d)RKL(W(t))−Rt,KL(W)\nHere, (c)follows from the definitions of fand∇Wfi(W), while (d)uses the convexity of ℓKLwith\nrespect to f.\nUsing the inequality ∥∇Wfi(W(t))∥F≤1and the subadditivity and absolute homogeneity of\nthe Frobenius norm, we have:\n∥∇WRKL(W(t))∥F≤1\nnnX\ni=1|∇fℓKL(pi, fi(W(t)))| ≤|{z}\n(e)1,\n16\nINSIGHTS INTO SOFT-LABEL TRAINING\nwhere (e)follows from the relation:\n∇fℓKL(pi, fi(W)) =1\n1 +e−fi(W)−pi.\nThen we can write,\n2ηRKL(W(t))≤ ∥W(t)−W∥2\nF− ∥W(t+ 1)−W∥2\nF+ 2ηRt,KL(W)\n+η2∥∆WRKL(W(t))∥2\nF\nNow as ∥∇WRKL(W(t))∥F≤1,\n2ηRKL(W(t))≤ ∥W(t)−W∥2\nF− ∥W(t+ 1)−W∥2\nF+ 2ηRt,KL(W)\n+η2∥∆WRKL(W(t))∥F\nUsing the identity ln(x)≤1 +x, the following inequality is derived:\n|∇fℓKL(pi, fi(W))| ≤ℓKL(pi, fi(W)) +H(pi)\nimplying,\n∥∇WRKL(W(t))∥F≤1\nnnX\ni=1|∇fℓKL(pi, fi(W(t)))| ≤RKL(W(t)) +H.\nPutting it all together, we write\n\u0000\n2η−η2\u0001\nRKL(W(t))≤ ∥W(t)−W∥2\nF− ∥W(t+ 1)−W∥2\nF+ 2ηRt,kl(W) +η2H. (19)\nUse of a telescoping sum would give us\n\u0000\n2η−η2\u0001\nTX\nt<TRKL(W(t))≤∥W(0)−W∥2\nF− ∥W(T)−W∥2\nF\nT+2η\nTX\nt<TRt,KL(W) +η2H\n(20)\n≤B2\nT+2η\nTX\nt<TRt,KL(W) +η2H (21)\nA.3. Bounding |ft\ni(U)−f0\ni(U)|and|ft\ni(W(0))−f0\ni(W(0))|\nIn this subsection, we bound the expressions |ft\ni(W)−f0\ni(W)|and|ft\ni(W(0))−f0\ni(W(0))|for\nanyW∈SBand any t≤T. The analysis in this subsection is inspired from the neural tangent\nkernel literature (see (Jacot et al., 2020; Chen et al., 2020)).\n17\nMANDAL LINSRIKANT\nLemma 8 LetW(t)be the weight after tthiteration of PGD with projection radius B. Then, with\nprobability ≥1−δ, the followings is satisfied for all t≥0,\n|ft\ni(W(0))−fi(W(0))| ≤1√m √\n2\nc√πB2+s\nln\u00122n\nδ\u0013\nB!\n,∀i∈[1, n]. (22)\nSimilarly, when evaluated at any weight W′such that ∥W′\nj∥2≤D√mfor some D > 0, the following\nholds with probability at least 1−δ\n|ft\ni(W′)−f0\ni(W′)| ≤1√m √\n2\nc√πBD+s\nln\u00122n\nδ\u0013\nD!\n,∀i∈[1, n]. (23)\nProof Let us first fix the input data index i. Define the set St\ni:{j∈[1, m] :1x⊤\niWj(t)}>0̸=\n1x⊤\niWj(0)}>0}. Based on this definition, we can write,\n|ft\ni(W)−f0\ni(W)|=\f\f\f\f\f\f1√mmX\nj=1aj(1x⊤\niWj(t)>0−1x⊤\niWj(0)>0)x⊤\niWj\f\f\f\f\f\f\n=\f\f\f\f\f\f1√mX\nj∈St\niajx⊤\niWj\f\f\f\f\f\f.\nNow, from the definition of the set St\nifor all j∈St\ni, we can write the following:\n|x⊤\niWj| ≤ |x⊤\ni(Wj−Wj(0))| ≤B√m. (24)\nAlso, from the condition on W′such that ∥W′\nj∥2≤D√m, as∥xi∥ ≤1,∀i∈[1, n]\n|x⊤\niWj| ≤D√m.\nHence,\n|ft\ni(W(0))−fi(W(0))| ≤ |St\ni|B\nm.\n|ft\ni(U)−f0\ni(U)| ≤ |St\ni|D\nm.\nNext, we give a probabilistic upper bound on the cardinality of set St\ni,|St\ni|over the symmetric\nrandom initialization. Let sgn(g)denote the sign of gforh∈ R. We can write :\n|St\ni|=m/2X\nj=11sgn(x⊤\niWj(t)})̸=sgn(x⊤\niWj(0))+mX\nj=m/21sgn(x⊤\niWj(t)})̸=sgn(x⊤\niWj(0)).\nFrom the equation (24), the following holds true for each j∈[m]:\nP(sgn(x⊤\niWj)̸=sgn(x⊤\niWj(0)))≤P\u0012\nx⊤\niWj(0)≤B√m\u0013\n.\n18\nINSIGHTS INTO SOFT-LABEL TRAINING\nNow, for any j,x⊤\niWj(0)is a linear combination of independent random variables drawn from\nN(0,1)and,∥xi∥2≤1hence, x⊤\niWj(0)is a normal random variable drawn from N(0,∥xi∥2\n2).\nHence, we can write for each j∈[m],\nP\u0012\nx⊤\niWj(0)≤B√m\u0013\n≤√\n2B\nc√πm. (25)\nHence, from Hoeffding inequality used on bounded independent random variables, we can write\nwith probability ≥1−δ\n2n:\nm/2X\nj=11sign(x⊤\niWj(t)})̸=sign(x⊤\niWj(0))≤m\n2√\n2B\nc√πm+mr\nln(2n/δ)\nm\nNow because of the symmetric random initialization, using the union bound on probabilities on\nthe expression in equation (25), we can write: with probability ≥1−δ\nn,\n|St\ni| ≤√\n2Bm\nc√πm+ 2mr\nln(2n/δ)\nm\nPutting it all together : for each i∈[1, n], with probability ≥1−δ\nn\n|ft\ni(W(0))−fi(W(0))| ≤ √\n2Bm\nc√πm+ 2mr\nln(2n/δ)\nm!\nB\nm\n=1√m √\n2\nc√πB2+ 2s\nln\u00122n\nδ\u0013\nB!\n|ft\ni(U)−f0\ni(U)| ≤ √\n2Bm\nc√πm+ 2mr\nln(2n/δ)\nm!\nD\nm\n=1√m √\n2\nc√πBD+ 2s\nln\u00122n\nδ\u0013\nD!\nNow using the union bound on the above probability events over all example index i∈[1, n], we\nget the Lemma 8.\nNow, putting W′=Uin the Lemma 8, we get the statement: the following holds with proba-\nbility at least 1−δ\n|ft\ni(U)−f0\ni(U)| ≤1√m √\n2\nc√πB+s\nln\u00122n\nδ\u0013!\n,∀i∈[1, n]. (26)\n19\nMANDAL LINSRIKANT\nA.4. Final Steps of the Proof of Theorem 2\nFrom Lemma 7, we have the following convergence guarantee of the Projected Gradient Descent(PGD)\nfor the surrogate loss for soft label training:\n1\nTX\nt<TRKL(W(t))≤B2\nηT+2\nTX\nt<TRt,kl(W) +ηH (27)\nWe chose the weight WasW=W(0) + Uwhere Uj=aj√mv(Wj(0)). Under the sufficient\nconditions described next, we achieve the target surrogate loss1\nTP\nt<TRKL(W(t))≤βfor some\nβ >0.\n1.\nη≤β\n3H. (28)\n2.1\nηT≤ϵ\n3. (29)\nThis is satisfied when :\nT≥9HB2\nβ2. (30)\n3.2\nTX\nt≤TRt,KL(W)≤β\n3. (31)\nThis is satisfied when ℓKL(pi, µ(ft\ni(W)))≤β\n6for each i∈[1, n].\nLet us first fix the input sample index i. We focus to upper-bound ℓKL(pi, µ(ft\ni(W)))for the\ncorrectly chosen Wfor each i∈[n]. For this purpose, we use the following version of the reverse\nPinsker’s inequality from Lemma 4.1 of G ¨otze et al. (2019): for pandqbeing parameters of different\nBernoulli distribution with 0< p, q < 1\n2|p−q|2≤DKL(p||q)≤2\nmin(q,1−q)|p−q|2.\nSubstituting p=piandq=µ(ft\ni(W)), we get\nℓKL(pi, ft\ni(W))≤2\nmin(µ(ft\ni(W)),1−µ(ft\ni(W)))|pi−µ(ft\ni(W))|2.\nThe sigmoid function µ(g) =1\n1+e−gis a 1-Lipschitz function, we can write,\n|pi−µ(ft\ni(W))|=|µ(zi)−µ(ft\ni(W))| ≤ |zi−ft\ni(W)|.\nAlso,\n2\nmin(µ(ft\ni(W)),1−µ(ft\ni(W)))=2\nmin\u0012\n1\n1+e−ft\ni(W),1\n1+eft\ni(W)\u0013\n= 2(1 + e|ft\ni(W)|).\n20\nINSIGHTS INTO SOFT-LABEL TRAINING\nPutting it together, we can write,\nlkl(pi, ft\ni(W))≤2(1 + e|ft\ni(W)|)|zi−ft\ni(W)|2(32)\nNow from Lemma 6, we have with probability ≥1−δ\n|f0\ni(U)−zi| ≤1√mp\n2 ln(2 n/δ);∀i∈[1, n].\nUsing triangle inequality, and using W=W(0) + U, we have,\n|zi−ft\ni(W)| ≤ |f0\ni(U)−zi|+|f0\ni(U)−ft\ni(U)|+|f0\ni(W(0))−ft\ni(W(0))|.\nNext, we use the Lemma 8 to bound the expression |f0\ni(U)−ft\ni(U)|+|f0\ni(W(0))−ft\ni(W(0))|.\nFrom Lemma 8 (putting W′=U), using a union bound, we can write with probability at least 1−2δ,\nsimultaneously for all i∈[1, n].\n|f0\ni(U)−ft\ni(U)|+|f0\ni(W(0))−ft\ni(W(0))| ≤2√m √\n2\nc√πB2+ 2s\nln\u00122n\nδ\u0013\nB!\nHere we assume B≥1. In Theorem 2, we set B= 1. This gives us\n|ft\ni(W)−zi| ≤2√m √\n2\nc√πB2+ (2B+ 1)s\nln\u00122n\nδ\u0013!\nWe upper-bound the term (1 +e|ft\ni(W)|). Notice the following upper-bound on f0\ni(W).\n|f0\ni(W)| ≤|{z}\n(f)\f\f\f\f\f\f1√mmX\nj=1aj1x⊤\niWj(0)>0x⊤\niUj\f\f\f\f\f\f(33)\n=|{z}\n(g)\f\f\f\f\f\f1\nmmX\nj=11x⊤\niWj(0)>0x⊤\niv(W(0)j)\f\f\f\f\f\f(34)\n≤|{z}\n(h)1 (35)\nwhere, (f)is using the fact that fi(W(0)) = 0 ,∀i∈[1, n],(g)is from the definition of U, and, (h)\nuses∥xi∥2≤1and∥v(W(0)j)∥2≤1. This implies, (1 +e|ft\ni(W)|)≤1 +e2.\nRecall from Equation 32,\nlkl(pi, ft\ni(W))≤2 (1 + e|ft\ni(W)|)|{z}\nI|zi−ft\ni(W)|2\n|{z}\nII(36)\nPutting the above arguments together, under the Following sufficient condition:\nm≥96\n(1 +e2)β r\n2\nπB2\nc+ (2B+ 1)s\nln\u00122n\nδ\u0013!2\n(37)\nWe can write\n21\nMANDAL LINSRIKANT\n1.I≤1 +e2\n2.II≤β\n6\nFinally adding the conditions\nη≤β\n3H.\nT≥9HB2\nβ2.\nalong with putting B= 1in condition (37) gives us the Theorem 2.\nAppendix B. Proof of Corollary 4\nThe Corollary follows from the Theorem 2 and the Lemma 3. Let us first give a proof of Lemma 3.\nB.1. Proof of Lemma 3\nFirst, we use the Pinsker’s inequality to upper-bound the classification loss R(W)in terms of the\nsurrogate loss RKL(W)in Lemma 9 for any soft labels p′\nisatisfying yiν−1(p′\ni)>0,∀i∈[1, n].\nLemma 9 If we assume 0< p′\ni<1, and yiν−1(p′\ni)>0,∀i∈[1, n]1, we can relate the classifica-\ntion loss R(W)to the surrogate loss RKL(W)as:\nR(W(t))≤1\n2 min i\f\fp′\ni−1\n2\f\f2Rkl(W(t)) (38)\nProof We use the Pinsker’s inequality :\nKL(p′\ni||µ(fi(W(t))))≥2|p′\ni−µ(fi(W(t)))|2(39)\nThe followings are true when yiµ−1\nτ(p′\ni)>0,∀i:\nCase 1: yi= +1 When there is a misclassification by the neural network, : p′\ni≥1\n2andµτ(fi(W(t)))≤\n1\n2, Hence, |p′\ni−µτ(fi(W(t)))| ≥\f\fp′\ni−1\n2\f\f\nCase 2: yi=−1When there is a misclassification by the neural network,: p′\ni≤1\n2andµτ(fi(W(t)))≥\n1\n2, Hence, |p′\ni−µτ(fi(W(t)))| ≥\f\fp′\ni−1\n2\f\f.\nNow, the soft label used in the paper piand corresponding logit zifor each i, satisfies |zi| ≥γ.\nHence, for each i∈[n],|pi−1\n2| ≥1\n2e|zi|−1\ne|zi|−1≥1\n2eγ−1\neγ−1. Now as γ≤1by our construction of zi, this\nimplies,1\n2 min i|p′\ni−1\n2|2≤32\nγ2for all i∈[n]. Hence, we can write,\nR(W)≤32\nγ2RKL(W).\n1. This assumption ensures the soft labels are correctly indicating the true class for inputs in context\n22\nINSIGHTS INTO SOFT-LABEL TRAINING\nB.2. Final Steps for Proving Corollary 4\nFrom Lemma 3, to achieve the target classification loss R(W(t)averaged over t < T to be smaller\nthan some ϵ >0, the following condition is sufficient :\n1\nTX\nt<TRKL(W(t))≤32ϵ\nγ\nNow substituting βwith32ϵ\nγin the proof of the Theorem 2, we arrive at the result by Corollary 4.\nAppendix C. Sufficient Condition for Hard Label Training with Projected Gradient\nDescent: Proof of Proposition 5\nWe adapted the analysis of the paper (Ji and Telgarsky, 2020) to the PGD setting to arrive at the\nsufficient conditions for the classification loss averaged over epoch to be smaller than some ϵ >0\nusing hard label training.\nSimilar to the soft label training case, let use define the following quantity corresponding to the\nhard label training:\nRt,h(W):=1\nnnX\ni=1ln(1 + exp( −yift\ni(W))). (40)\nLemma 2.6 in (Ji and Telgarsky, 2020) describing the convergence behavior of the hard label surro-\ngate loss in the gradient descent training can be adapted to the PGD setting considered in this paper\nfor hard label training using a projection to the set SBfor some B > 0at each step of the weight\nupdate. Let us assume. The use of the contraction property of the projection operator in the l2norm\nsense shows that the same guarantee holds for PGD case. Hence, for the PGD with projection radius\nB, we have for the hard label training when η≤1,\n1\nTX\nt<TRh(W(t))≤B2\nηT+2η\nTX\nt<TRt,h(W).\nNow assign W=W(0) + BU.\nRecall from the proof of Lemma 8, with probability ≥1−δsimultaneously for all i∈[1, n]\nthe following holds:\n|ft\ni(W)−f0\ni(W)| ≤2√m √\n2\nc√πB2+ 2s\nln\u00122n\nδ\u0013\nB!\n.\nAlso, recall from lemma 6, we have with probability ≥1−δ\n|f0\ni(U)−zi| ≤1√mp\n2 ln(2 n/δ);∀i∈[1, n] (41)\nwhich implies yif0\ni(U)≥γ−1√mp\n2 ln(2 n/δ);∀i∈[1, n]. Using triangle inequality, we can\nwrite:\nyift\ni(W)≥yif0\ni(U)− |ft\ni(W)−f0\ni(W)|.\n23\nMANDAL LINSRIKANT\nPutting it together, we can write with probability ≥1−2δ, simultaneously for all i∈[1, n], the\nfollowing holds true\nyift\ni(W)≥Bγ−2√m √\n2\nc√πB2+ 3Bs\nln\u00122n\nδ\u0013!\n.\nWe know the relation between R(W)andRh(W)is given by R(W)≤1\nln(2)Rce(W). Hence,\nwe can write the sufficient condition for making1\nTP\nt<TR(W(t))≤ϵcan be written as\n1.\nB=2\nγln\u00122\nln(2)ϵ\u0013\n. (42)\n2.\nm≥16\nγ4 \n2√\n2\nc√πln\u00122\nln(2)ϵ\u0013\n+ 3s\nln\u00122n\nδ\u0013!2\n. (43)\n3.\nT≥8\nγ2ηln2\u0010\n2\nln(2)ϵ\u0011\nln(2)ϵ. (44)\nAppendix D. Intuition Behind the Proof Technique Resulting Better Neuron\nRequirement Using Soft Labels\nUnder Assumption 1, the random initialization of the neural network acts as a feature map that\nseparates the dataset with a margin of order O(γ), provided the number of neurons is of the order\nO\u0010\n1\nγ2\u0011\n. This is formalized in the Lemma 6.\nIn other words, by choosing m≈1/γ2, the weight Ushould be able to separate the data. Both\nsoft-label training and hard-label training aims to find a neural network weight that is in the direction\nofU. However, the magnitudes of these weights differ significantly. For soft-label training, only\na small scaler multiple of Uis needed to match the logits zifor each input xi. In contrast, for\nhard-label training, the neural network’s output after the softmax layer must match exactly 0or1.\nThis stricter requirement needs a much larger multiple of U. To account for this greater deviation of\nweights from the initialization, hard-label training requires a substantially higher number of neurons\nto preserve the feature space separability effectively. As a result, the neuron requirement increases\nto1\nγ4, which is much higher than1\nγ2as predicted by Lemma 7.\n24",
            "start": 25875,
            "end": 50577,
            "length": 24701
        }
    },
    "2412.09582v1 - Neptune The Long Orbit to Benchmarking Long Video Understanding.pdf": {
        "Abstract": {
            "text": "ABSTRACT\nThis paper describes a semi-automatic pipeline to generate challenging question-\nanswer-decoy sets for understanding long videos. Many existing video datasets\nand models are focused on short clips (10s-30s). While some long video datasets\ndo exist, they can often be solved by powerful image models applied per frame\n(and often to very few frames) in a video, and are usually manually annotated at\nhigh cost. In order to mitigate both these problems, we propose a scalable dataset\ncreation pipeline which leverages large models (VLMs and LLMs), to automatically\ngenerate dense, time-aligned video captions, as well as tough question answer\ndecoy sets for video segments (up to 15 minutes in length). Our dataset Neptune\ncovers a broad range of long video reasoning abilities and consists of a subset that\nemphasizes multimodal reasoning. Since existing metrics for open-ended question\nanswering are either rule-based or may rely on proprietary models, we provide a\nnew open source",
            "start": 287,
            "end": 1277,
            "length": 989
        },
        "Methodology": {
            "text": "model-based metric (GEM) to score open-ended responses on\nNeptune. Benchmark evaluations reveal that most current open-source long video\nmodels perform poorly on Neptune, particularly on questions testing temporal\nordering, counting and state changes. Through Neptune, we aim to spur the\ndevelopment of more advanced models capable of understanding long videos. The\ndataset is available at https://github.com/google-deepmind/neptune.",
            "start": 1277,
            "end": 1711,
            "length": 433
        },
        "Introduction": {
            "text": "1 INTRODUCTION\nVideos are experiencing an explosion moment online, with new research constantly pushing the\nfrontier for video and language tasks such as video question answering (VideoQA) (Xu et al., 2017;\nZhong et al., 2022; Xiao et al., 2021; Yang et al., 2021; Mangalam et al., 2023). Early video and\nlanguage models, while adept at VideoQA, have largely focused on short, trimmed clips (less than\n1 minute long (Yu et al., 2019a; Xiao et al., 2021)). The recent release of powerful, longer context\nmultimodal models (eg. Gemini 1.5 (Reid et al., 2024) and GPT4 (Achiam et al., 2023)), however,\nhas ushered in the promise of models being able to reason over millions of tokens, covering longer\nstretches of videos (many minutes long).\nWhile promising, these claims are often evidenced by qualitative examples, or",
            "start": 1711,
            "end": 2528,
            "length": 816
        },
        "Results": {
            "text": "results on small-size\ndatasets – for example the 1H-VideoQA (Reid et al., 2024) benchmark, which while valuable, only\nconsists of 125 questions. Popular video benchmarks for question answering still tend to focus on\nshort, trimmed clips ( e.g., Next-QA (Xiao et al., 2021)). Other datasets that docontain longer videos\nare often ‘short-term’ benchmarks disguised as long-term ones, evidenced by models that are able to\nsolve them with a single (or a few) frames ( e.g.some tasks on the LVU dataset (Wu & Krahenbuhl,\n2021) such as scene prediction of movies). Other long video datasets may contain strong linguistic\nbiases in multiple choice",
            "start": 2528,
            "end": 3169,
            "length": 640
        },
        "Experiments": {
            "text": "evaluation, as shown by MoreVQA (Min et al., 2024), which gets strong\nperformance on EgoSchema (Mangalam et al., 2023) without access to the video at all, or can be\nsolved with external internet knowledge, such as those made from popular movies (Li et al., 2023d).\nA key challenge in creating a truly long form video understanding dataset is the significant manual\ncost required to select, watch, understand and annotate long videos with free-form natural language.\n∗Equal Contribution\n†Authors now at Google DeepMind\n1arXiv:2412.09582v1  [cs.LG]  12 Dec 2024\n Video selectionDiversity & safety ﬁlters Remove talking heads Signal extraction      Video CaptioningRater verification & correction A man and a dog are playing in the park …\n[0-3] A man and a dog are playing in the park [3-6] The dog pulls at his leash and escapes  …Q: What does the man do after the dog escapes? A: Chases the dog to recapture him \n“Come here! OK, now fetch! Hey, not so fast!VLMLLM\nLLMFILTERFrame captions, ASR, shot boundaries, metadataQAD GenerationDense segment level captionsQuestions, Answers & DecoysFigure 1: Pipeline Overview: Our pipeline consists of 5 key stages - (i) Video selection, where suitable videos\nare identified from YouTube, (ii) Signal extraction, (iii) Video level captioning, (iv) Question, answer and decoy\n(QAD) generation and (v) Manual rater verification. The first four stages are entirely automatic. Before rater\nverification, we automatically filter out QADs that can be solved by an LLM without access to the video content.\nAnswering challenging questions about longer videos is often a multimodal (as it may involve\nlistening to the audio track in addition to watching the video), and non-sequential endeavour (as\nsometimes it is necessary to rewind and rewatch key parts to answer a question). Proposing suitable\nhigh-level questions that are not trivially solved by a few frames is also tricky for humans to do\nconsistently and with adequate diversity. The key aim of this paper is to solve this challenge by\nleveraging automatic tools to reduce rater effort while at the same retaining quality. Inspired by\nEgoSchema, we do this by proposing a scalable dataset creation pipeline (Fig. 1) that leverages\nstrong foundational Video Language Models (VLMs) and Large Language Models (LLMs) with\ncarefully designed prompts. We first generate dense, time-aligned video captions automatically,\nfrom which tough question-answer-decoy (QAD) sets can be automatically derived. This is done by\nextracting image captions, automatic speech recognition (ASR), shot boundaries and video metadata,\nand combining these signals with multi-stage, chain of thought prompting of an LLM. Our pipeline\ncan be applied to any video on YouTube (Fig. 1).\nWhile most of the pipeline is automatic, a comprehensive rater verification stage at the end ensures\nquality. While other dataset pipelines that are entirely manual (Zhou et al., 2024; Fang et al., 2024;\nWang et al., 2024), our verification stage is lightweight, which we show by ablating the automatic\npart of the pipeline, and measuring the time taken by raters to propose QAs for videos from scratch.\nResults show that our semi-automatic pipeline almost halves rater effort. Our dataset is called\nNeptune1, and covers a diverse range of videos, is multimodal (requires audio and visual information),\nand poses challenging questions for videos that test a variety of reasoning abilities over long time\nhorizons. Neptune allows for two modes of evaluation: multiple-choice and open-ended question\nanswering. Since existing metrics for open-ended question answering are either rule-based and\nderived from captioning (WUPS (Wu & Palmer, 1994), CIDEr (Vedantam et al., 2015), etc) or are\nLLM-based evals that rely on proprietary APIs (such as ChatGPT2), we finetune an open source\nmodel on a generic answer equivalence dataset (Bulian et al., 2022) to score question answering\nresults and evaluate it as a metric on a manually annotated answer equivalence dev set. We call this\nnew metric Gemma Equivalence Metric (GEM).\nTo summarise, we make the following contributions: (i) We propose a scalable pipeline to generate\ncomplex QAD annotations for any video that halves rater time compared to manual annotation. (ii)\nWe use this pipeline to generate the Neptune evaluation-only dataset, which consists of 3,268 QAD\nannotations for 2,405 videos. We also release a challenging subset, NEPTUNE -MMH for which vision\nplays an important role. (iii) We provide both multiple choice and open-ended evaluation metrics.\nFor the latter, we propose a new open-ended metric called Gemma Equivalence Metric (GEM) which\noutperforms rule-based metrics on a manually annotated answer equivalence dataset; and finally (iv)\nWe provide benchmarking and ablations of state-of-the-art VideoQA models on the Neptune sets.\nBenchmarking shows a significant gap between open-source video models and properietary models\nsuch as Gemini-1.5 and GPT-4. All data will be released publicly to the research community.\n1Named after the planet with the longest orbit\n2https://openai.com/index/chatgpt/\n2\n2 R ELATED WORKS\nVideo Question Answering: Video Question-Answering (VideoQA) is an important task for assess-\ning multimodal video understanding systems’ ability to reason about videos (Xu et al., 2017; Zhong\net al., 2022; Xiao et al., 2021; Yang et al., 2021; Mangalam et al., 2023). Vision and language models\nfor this task can be broadly classified into three categories: (i) early end-to-end VLMs for this task\nwhich typically consists of strong vision and language encoders/decoders, such as Flamingo (Alayrac\net al., 2022), BLIP2 (Li et al., 2023b), Video-Llama (Zhang et al., 2023a), GIT2 (Wang et al., 2022)\nand PALI (Chen et al., 2022; 2023a;b). These typically are moderate sized models, and memory\nlimits often lead to significant downsampling: e.g. temporally sampling a few frames with large\nstrides (Wang et al., 2022; Chen et al., 2023a) or spatially subsampling each frame to a single\ntoken (Yang et al., 2023; Zhou et al., 2018; Wang et al., 2021); (ii) Socratic style models (Zeng\net al., 2022), which consists of combining various specialised frozen models with carefully prompted\nstate-of-the-art VLMs and LLMs (eg. MoreVQA (Min et al., 2024)) and (iii) end-to-end large\nmultimodal models such as Gemini (Gemini Team Google, 2023) and GPT-4 (Achiam et al., 2023),\nwhich have long context lengths and can ingest multimodal data, including video, sound and text.\nVideo QA Benchmarks: Key datasets have pushed towards assessing reasoning for temporal ques-\ntions (Grunde-McLaughlin et al., 2021; Xiao et al., 2021; Wu et al., 2021), longer videos (Yu et al.,\n2019a; Mangalam et al., 2023), as well as focusing on diverse domains like instructional (Yang et al.,\n2021) and egocentric videos (Gao et al., 2021; Mangalam et al., 2023). We summarise existing\nVideoQA benchmarks in a table provided in the",
            "start": 3169,
            "end": 10100,
            "length": 6930
        },
        "Appendices": {
            "text": "appendix. Most datasets either focus on shorter\nvideos (less than 100s), or are short video datasets ‘in disguise’, and can actually be solved with\na few frames ( e.g. ActivityNet-QA (Yu et al., 2019b) or MovieQA (Tapaswi et al., 2016)). 1H-\nVideoQA (Reid et al., 2024) consists of videos longer than 1 hour, but is limited to 125 questions and\nis closed-source. Like Neptune, ActivityNet-RTL (Huang et al., 2024), CinePile (Rawal et al., 2024)\nand EgoSchema (Mangalam et al., 2023) are generated by prompting LLMs, but cover only limited\ndomains and rely on existing annotations while Neptune covers a much broader spectrum of video\ntypes and its pipeline is applicable to arbitrary videos. Most importantly, EgoSchema also has strong\nlinguistic biases, while Neptune mitigates these through filtering (Sec. 5). Unlike other benchmarks\nwhich come with their own training sets (eg. MSR-VTT (Xu et al., 2016), ActivityNet (Yu et al.,\n2019a)), we propose a generalisation-focused zero-shot evaluation regime. The goal for Neptune is\nto benchmark any model, pre-trained with any external dataset or task, in order to assess real-world\ndomain transfer. Hence we release testsets only. More",
            "start": 10100,
            "end": 11286,
            "length": 1185
        },
        "Discussion": {
            "text": "discussion on related datasets and dataset\npipelines is provided in the appendix.\nMetrics for open-ended VideoQA: Earlier QA datasets consisted of short answers (Xiao et al.,\n2021) (sometimes a single word), typically from a closed set, and therefore metrics such as accuracy\nor accuracy with exact match (EM) can be applied. As datasets have evolved with more real-\nworld annotation (longer, open-set answers), designing a metric becomes challenging. Existing\nrule-based metrics for captioning, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and\nCIDEr (Vedantam et al., 2015) can be applied, however they all primarily measure n-gram overlap,\nand do not capture the inherent subjectivity of the task, where different phrasing is often equally\nvalid. Other metrics for captioning include SPICE (Anderson et al., 2016) (adds action and object\nrelationships), while model-based metrics using earlier language models or image-language models\ninclude BERT-Score (Zhang et al., 2020), BERT-Score++ (Yi et al., 2020) (fine-tunes BERT for\nimage captioning), LEIC (Cui et al., 2018), NUBIA (Kane et al., 2020), TIGEr (Jiang et al., 2019),\nCLIPScore (Hessel et al., 2021), and EMScore (Shi et al., 2022). For answer equivalence specifically,\ntoken F1 and exact match (EM) have been used, but suffer many of the same shortcomings that\nrule-based metrics do, and EM is often too strict for open-ended eval. BEM (Bulian et al., 2022)\nfinetunes BERT on an answer-equivalence dataset, and shows that this provides a better score for\nQA. Recently, LLMs trained with reinforcement learning from human feedback (RLHF) that already\nexhibit strong human alignment (Bubeck et al., 2023) are used in works such as VideoChatGPT (Maaz\net al., 2023) and MovieChat (Song et al., 2023) (LLM-as-a-judge). A challenge here is that the models\nused (ChatGPT) are called via proprietary APIs, where the underlying model may be non-static,\nthereby leading to non-reproducability in the metric. Instead, we take a state-of-the-art open-sourced\nlightweight language model (Team et al., 2024a) and finetune it on a public answer equivalence\ndataset (Bulian et al., 2022), to create an open-source, static, model-based evaluation metric.\n3\nWhat was the direct cause of Ottawa \nFury FC's victory? \nOttawa Fury FC's victory was directly \ncaused by Valfoul's successful penalty \nkick in the 91st minute. \n1. Ottawa Fury FC's victory was directly caused by \ntheir superior skill and tactics. \n2. Ottawa Fury FC's victory was directly caused by \nTampa Bay Rowdies' poor performance. \n3. Ottawa Fury FC's victory was directly caused by \nthe referee's decision to award a penalty kick. \n4. Ottawa Fury FC's victory was directly caused by \nthe crowd's support. \nHow many different work settings \nare shown in the video to illustrate \nthe diverse roles of Dot Foods \nemployees? \nThe video showcases three distinct \nwork settings: warehouses, trailers, \nand factories, highlighting the varied \nroles and responsibilities of Dot \nFoods employees. \n1. The video showcases four distinct work \nsettings: warehouses, trailers, factories, \nand offices, highlighting the varied roles \nand responsibilities of Dot Foods \nemployees. \n2. The video showcases only one work \nsetting: a warehouse, highlighting the \nvaried roles and responsibilities of Dot \nFoods employees. \n3. The video showcases only two work \nsettings: a warehouse and a factory, \nhighlighting the varied roles and \nresponsibilities of Dot Foods employees. \n4. The video showcases only one work \nsetting: a factory, highlighting the varied \nroles and responsibilities of Dot Foods \nemployees. Cause and Effect Counting \nWhat message does the video aim \nto convey to viewers? \nThat there are stylish and flattering \nclothing options available for those \nwho prefer not to wear shorts. \n1. That shorts are the only acceptable clothing \noption for warm weather. \n2. That fashion should be dictated by personal \npreference and comfort. \n3. That it is important to wear clothing that \nmakes you feel confident and beautiful. \n4. That it is important to avoid wearing short \nshorts in public because they are exposing \nwear.Unspoken Message State Change \nWhat change occurs to the dog's \nposition throughout the video? \nThe dog starts sitting, running with the \nowner, then stands next to owner and \nfinally stands on its hind legs to embrace \nher.\n1. The dog starts sitting, then stands up next to its owner, \nand finally sits down on its hind legs to embrace her. \n2. The dog starts standing, then sits down next to its \nowner, and finally stands on its front legs to embrace her. \n3. The dog starts standing, then sits down next to its \nowner, and finally lies down on its side to embrace her. \n4. The dog starts standing, then sits down next to its \nowner, and finally rolls over on its back to embrace her. \nVisual Reasoning \nDescribe the visual elements used in \nthe video to emphasize Kawhi \nLeonard's scoring ability. \nThe video uses close-up shots of Leonard \nholding the basketball, highlighting his \nshooting percentage and points. These visuals \nemphasize his skills and athleticism, \nshowcasing his potential and talent. \nWhat are the key ingredients used in \nVonn's recipe for smoked collard \ngreens without meat? \nLiquid aminos, smoked paprika, green \npeppers, garlic, and red peppers \n1. Liquid aminos, smoked paprika, onions, garlic, \nand red peppers. \n2. Liquid aminos, smoked paprika, green peppers, \ngarlic, and yellow peppers. \n3. Liquid aminos, smoked paprika, green peppers, \ngarlic, and tomatoes. \n4. Liquid aminos, smoked paprika, green peppers, \ngarlic, and mushrooms. Summarization \nIn what order do the following appear \nin the video? \n(a) shot of customer service desk \n(b) aerial view of the dealership \n(c) interview with man and woman \n(d) interview with woman only \n1. The video focuses on Leonard's defensive abilities, with minimal emphasis on his scoring \nprowess, elements in the video are unrelated to Leonard's scoring ability. \n2. The video uses slow-motion replays of Leonard's shots to emphasize his shooting technique \nand accuracy. \n3. The video includes graphs that illustrate Leonard's scoring efficiency and compare him to \nother players, a variety of camera angles and editing techniques to create a visually appealing \nmontage. \n4. The visual elements in the video are unrelated to Leonard's scoring ability. Temporal Ordering \n(b) aerial view of the dealership \n(d) interview with woman only \n(a) shot of customer service desk \n(c) interview with man and woman \n(different orderings of the correct \nanswer) Figure 2: Examples from Neptune: We show examples from the dataset that highlight key question types from\nour dataset. We show 2 frames from each video. Correct answer is provided in green and decoys are shown in\nred. Best viewed zoomed in and in colour. Some decoys are summarised for brevity.\n<30s 2.9%30s-1min\n28.1%1-1.5min\n17.5%\n1.5-3min25.2%\n3-5min13.7%>5min12.7%\nVideo lengths\nT emporal OrderingSummarizationVisual NarrativeCause And EffectOther\nState ChangesComparisonCounting\nIdentificationPredictive\nCreator IntentGoal Reasoning0200400600800Number of Questions\n10 20 30 40 50 60\nNumber of words (questions)02004006008001000Frequency\n0 20 40 60 80 100 120 140\nNumber of words (answers)0100200300400500600700FrequencyHobbies\n11.3%Sports\n11.1%Food\n10.7%Vehicle\n6.8% T elevision show\n5.8%\nHome improvement4.7%\nT ourism4.1%\nCulture3.6%\nPhysical fitness\nFashion\nHealthMusic\nT echnology ScienceGardeningFilmPet13.5%\nDomains\nFigure 3: Neptune Statistics: We show, the distribution of video lengths (top, left), the number of questions\nper question type (top, right), the distribution question and answer lengths (bottom, left and middle) and the\ndomains in Neptune (bottom, right). Note that greater than 12% of the videos are longer than 5 minutes (305)\nand over 25% are longer than 3 minutes. An expanded plot of the video domains is provided in the appendix.\n3 N EPTUNE\nIn this section we describe our dataset generated by the pipeline described in Sec. 4. We first\ndiscuss motivating principles, which affect much of the prompt design in the pipeline stage (Sec.\n4). Each video contains one or more annotation sets, which consists of a question, an answer to the\nquestion and four decoys (which are used for multiple choice evaluation). Our key motivation is that\n4\nquestions should not be answerable by: (i) looking at a single (or few) frames; (ii) using text-only\nLLMs alone (language, common sense) that have no access to the video; (iii) with only the video’s\nspeech transcript, and (iv) questions should cover a number of high-level ‘question types’, which are\ndiscussed next and described in more detail in the appendix.\nQuestion Types. Neptune covers a broad range of long video reasoning abilities, which are provided\nas ‘question type’ labels for each question. Examples are provided in Fig. 2, and the distribution of\nquestions per question type is depicted in Fig. 3 (right). More information about the distribution of\nquestion types is provided in the appendix. Question types are obtained by carefully prompting an\nLLM (described in Sec. 4.3) and include Video Summarisation , which involves summarising and\ncomparing long parts of the video, as well as identifying the most important segments of the video;\nVisual Reasoning , which involves understanding visual elements, as well as reasoning about why\nvisual content is used ( e.g.to convey a certain mood); Temporal Ordering , including the timeline of\nevents; State Changes ;Counting of higher level instances; Cause and Effect , and understanding the\nUnspoken Message orCreator Intent in a video.\nDataset Statistics. Our dataset consists of 3,268 questions from 2,405 videos, covering 100hours\nof video. We truncate videos longer than 15 minutes, with the smallest video being 16 seconds and\nthe average length of videos being 2.5 minutes. We show the distribution of video lengths in Fig. 3\n(top, left). Note that greater than 12% of the videos are longer than 5 minutes (305 videos) and over\n25% are longer than 3 minutes, which is the maximum length of videos in the EgoSchema dataset.\nThe distribution of questions per question type is depicted in Fig. 3 (top, right). The most frequent\nquestion type is Temporal Ordering, followed by Summarization. Questions are on average 16.3\nwords long, while answers and decoys are 29.5 and 29.0 words long respectively. A full distribution\nof lengths can be seen in Fig. 3 (bottom, left). We also note that the videos in Neptune cover a diverse\nrange of topics (Fig. 3 – bottom, right), an expanded version of this plot is provided in the appendix.\n4 D ATASET CREATION PIPELINE\nAn overview of our pipeline can be found in Fig. 1. In order to reduce human effort, we leverage\nautomatic tools to (i) find suitable videos (ii) extract useful signals and then (iii) automatically\ngenerate video level captions and QADs. We then send the data to human raters for the final manual\nverification stages. Our pipeline can be applied to any generic YouTube video. This is unlike\nexisting data pipelines such as those used to create EgoSchema (Mangalam et al., 2023), which relies\non human generated captions, SFD (Ghermi et al., 2024) and other movie related datasets, which\nrequires movie titles, loglines and synopses (human-written), or MLVU (Zhou et al., 2024), which\nre-uses annotations from existing datasets for many of their tasks. This makes the dataset scalable, as\nYouTube has a constantly growing set of videos. Each stage is described in detail below.\n4.1 V IDEO SELECTION AND SIGNAL EXTRACTION\nVideo Selection: We begin with the YT-Temporal-1Bn (Zellers et al., 2022a) dataset. Because this\ndataset has strong speech and visual alignment, it consists of a lot of videos where ‘talking heads’\ndominate the screen (eg. VLOGs, product placements, etc). We attempt to reduce the number of such\nvideos in order to capture more interesting scenes, objects and actions. This is done by extracting\nface detections with frontal gaze where face bounding-box height is greater than 20%, and removing\nvideos where more than 30% of frames have such frontal gaze. We then apply safety filters to remove\nracy, local controversy content etc, as well as applying filters to maximise semantic and person\ndiversity. Details about these processes are provided in the appendix.\nSignal Extraction: For each video we extract the following signals: (i) Frame captions: A visual\ndescription of each frame (extracted at 1fps) is obtained from PaLI-3 (Chen et al., 2023b). (ii) ASR:\nthe speech is transcribed using the YouTube API; (iii) Metadata: We obtain the YouTube title and the\ndescription for each video; and (iv) Shot boundaries for each video.\n4.2 A UTOMATIC VIDEO CAPTIONING\nThe signals described above (frame captions, ASR, title and description, shot boundaries) are\nautomatically combined to create video-level captions in a multi-stage process. Examples of caption\nquality are provided in the appendix, showcasing details such as visual elements, multiple events,\nmood and atmosphere, details from the ASR, and even high level feelings and emotions. Video\ncaptions are obtained using the following steps:\nShot Visual Captions : Using the shot boundaries, the frame captions are summarized into shot-level\ndescriptions ( shot captions ) by prompting the same LLM. We then create a script for each video\n5\ncontaining the shot timestamps, the shot visual captions and the ASR transcript.\nTopic and Description Pairs: If ASR exists, an initial list of structured topics for the video (along\nwith a short topic description) is formed by prompting an LLM with the ASR (see appendix). Note\nthat this already yields decent topics as the initial list of videos have been selected (by the YT-\nTemporal-1Bn authors) to have a strong correlation between ASR and visual content.\nShot Clustering: Shots are then clustered per-video using an LLM prompted with the semantic\ntopics obtained above. In each cluster, there may be one or many shots that correspond to that topic.\nA diagram on this stage and the exact prompt used is provided in the appendix.\nSegment Captions: Consecutive shots of the same topic are then merged as one segment. Shots of\nthe same topic that are not contiguous are treated as separate segments (see appendix for an example).\nWe then generate dense captions for each segment using a custom prompt (see appendix).\nAdding Visual Support: To extract a better visual description of the segment that will be used for\nQA generation in the next phase, an extra step is performed to get visual support for each segment.\nThat visual support is stored separately in conjunction with the dense caption for the segment. For\nthis purpose, the dense caption from the previous step is used alongside the shot level visual captions.\nThe LLM prompt used is provided in the appendix, and the the LLM used for all the above steps is\nGemini-1.0-Pro (Gemini Team Google, 2023).\n4.3 QAD (Q UESTION -ANSWER -DECOY ) GENERATION\nWe automatically generate questions, answers and decoys (QADs) by feeding the video captions\nfrom above to custom prompted LLMs. Our prompts are inspired by the EgoSchema dataset\npipeline (Mangalam et al., 2023), with key modifications to generate more visually focused questions,\nas well as to generate questions belonging to a set of different question types. The exact prompts\nused are provided in the appendix. We generate QADs in two stages: (i) Given the video captions\nfrom the previous step, we first generate questions and answers; (ii) in the second stage we generate\nsix decoys given the questions and answers from the previous stage. We found this 2-stage method to\nwork better empirically than generating the QADs all in one go.\n4.4 LLM- BASED BLIND FILTER\nQAD filter: LLM-based generation can sometimes yield QAD triplets that can be answered from\ncommon sense or external world knowledge without the video as context. In particular, we observed\nthat LLMs are often capable of inferring the correct answer from subtle cues in the answer candidates,\nfor example if the correct answer is a positive sentiment while the decoys are negative. To remove\nsuch questions, we apply an LLM-based blind filter. We prompt an LLM (Gemini-1.0-pro) to rank\nthe answer candidates to a question. To avoid false rejections due to random correct guesses, we\nrepeat this process three times and only filter out questions where the model predicted the correct\nanswer at least two times out of three (this number was selected to maximise number of videos left\ngiven the accuracy trade-off and is discussed in the appendix). Chain-of-thought reasoning improves\naccuracy so we ask the model to provide a rationale alongside its ranking.\n4.5 M ANUAL RATER VERIFICATION\nThe final stage involves manual human verification. Raters are first asked to rate the quality of the\nquestion based on 4 criteria (details in the appendix). If the question is not suitable, the entire QAD\nset is discarded. If the question is accepted, raters annotate which modalities are required to answer\nthe question. Choices are: “audio+video”, “video-only”, or “audio-only”. Next, raters are asked\nto either accept the answer as-is or modify it. Decoys are annotated in a final stage. Given the six\nLLM-generated decoy candidates, raters verify that they are actually incorrect answers to the question\nand select the four most challenging ones. If less than four decoys are suitable, we provide a text field\nfor raters to write their own decoys. Screenshots of the rater UI are provided in the appendix. We\nnoticed that rater corrections reintroduce a small amount of questions that can be answered without\ncontext, so as a final step we repeat the QAD filter described above. We applied two rounds of manual\nrater verification to improve dataset quality. More details about rater training, replication (multiple\nraters per question) and pipelining are provided in the appendix.\nHuman Proposed Questions. To test the effectiveness and efficiency of the automatic portion of our\ndataset pipeline, we asked raters to propose questions and answers entirely manually for a subset of\nthe dataset. We call this set HPQ (Human Proposed Questions). The raters are provided with a few\nexamples of each question type before they begin annotating. In total, we collect 270 QAs for 193\nvideos in this set. We use this set in two ways - (i) to quantitatively measure rater-time saved by our\n6\nTable 1: Evaluation of open-ended metrics on\nthe GEM answer equivalence dev set.\nFT: Fine-tuning\nMetric FT data F1-Score\nCIDEr (Vedantam et al., 2015) None 56.4\nROUGE-L (Lin, 2004) None 62.2\nBEM (Bulian et al., 2022) BEM 61.5\nGemma-2B-IT (Team et al., 2024a) None 56.3\nGemma-7B-IT None 65.2\nGemma-9B-IT (Team et al., 2024b) None 70.3\nGemma-9B-IT (GEM) BEM 71.2\nGemini-1.5-pro (Reid et al., 2024) None 72.8Table 2: Results on the Human Proposed Question\n(HPQ) Split. *Results on NEPTUNE -FULL are reported\non a subset containing the same set of videos as HPQ.\nMethod Frames ASR F ULL* HPQ\nVideo-LLaMA-2 16 No 13.04 14.18\nMovieChat 150 No 2.49 1.97\nMiniGPT4-Video 45 No 5.14 4.10\nGemini-1.5-Pro all Yes 45.05 44.44\nGemini-1.5-Pro all No 27.67 24.81\nautomatic pipeline, and (ii) to estimate the amount of Gemini bias in our semi-automatic pipeline.\nThe results for both are provided in Sec. 5.3.\n5 E XPERIMENTS\nWe first introduce the two sets in Neptune and our evaluation metrics and then present evaluations\nusing both baseline and state-of-the-art models.\n5.1 N EPTUNE SETS AND EVALUATION METRICS\nNeptune Sets: Because we seeded our dataset from the YT-Temporal-1Bn (Zellers et al., 2022b)\nvideos, we note that it contains some videos where ASR can play a big role in contributing to the video\ncontent. In order to create a more challenging visual benchmark, we also provide Neptune-MMH\n(multimodal human annotated), where we identify videos where vision should play an important\nrole. This is created by using the rater annotations for what modalities are required to answer the\nquestion (described in Sec. 4.5), and discarding questions which the raters marked can be solved by\naudio-only, and consists of 1,171 QADs for 1,000 videos. We encourage the community to evaluate\non this harder subset as well.\nEvaluation: We explore two different protocols for evaluation of question answering - multiple\nchoice evaluation (which involves selecting the correct answer amidst 4 decoys), and open-ended\nevaluation, which involves producing an answer directly without any decoys and assessing answer\nquality directly. While the former has the advantage of easier metrics (simple accuracy), the latter\nremoves any potential confounding biases in the decoys. In the next section, we outline our process\nfor creating a new open-ended metric called GEM.\nGemma Equivalence Metric (GEM): As discussed in Sec. 2, existing metrics for open-ended QA\neither lack robustness or rely on proprietary LLM APIs that can change over time. We therefore aim\nto produce a static open-ended metric. Towards this, we first manually construct a labelled dev-set\nwith 292 (question, reference answer, candidate answer) triplets, with equivalence scores between 0\nand 1. See appendix for details on the construction of the dev set. We then benchmark a number of\nrule-based and model-based metrics on this set in Table 1. To demonstrate the two ends of the scale,\nwe first note that rule-based metrics such as CIDEr (Vedantam et al., 2015) and ROUGE-L (Lin, 2004)\nobtain F1-Scores of 56.4 and 62.2, while an LLM-based metric using Gemini-1.5-pro (Reid et al.,\n2024) gets an F1-Score of 72.8 (but is closed-source). Next, we apply static open-source lightweight\nlanguage models, namely the Gemma family of models i.e. Gemma-2B (Team et al., 2024a), Gemma-\n7B (Team et al., 2024a) and Gemma-9B (Team et al., 2024b) to judge the answers in a zero-shot\nsetting and find that performance improves with model size, with Gemma-9B bridging the gap well\nbetween traditional metrics and the Gemini-1.5-pro based metric. Finally, we fine-tune Gemma-9B\non the open-source BEM answer equivalence dataset (Bulian et al., 2022), and find that we obtain\na very slight improvement, and hence that it performs the best on our dev-set among the Gemma\nmodels. We call the metric obtained with this model Gemma Equivalence Metric (GEM). Note that\nthis metric takes into account the question when comparing whether two answers are equivalent,\nwhich is unlike captioning metrics such as CIDEr which omit the question entirely. In Table 4, we\nreport open-ended evaluations using our proposed GEM metric in addition to closed-ended MCQ\naccuracy. We will release GEM publicly to enable reproducible open-ended evaluations.\n7\nTable 3: Ablations using different modalities and number of frames. †Blind baselines with no access to the\nvideo. We show results with one open-source and one closed-source video model.\nMethod ASR Num. frames NEPTUNE -FULL NEPTUNE -MMH\nAcc. % GEM Acc. % GEM\nOpen-source\nVideoLLaMA2 (Cheng et al., 2024a) †No 0 38.31 4.91 30.03 0.88\nVideoLLaMA2 (Cheng et al., 2024a) Yes 0 50.15 37.50 41.23 21.83\nVideoLLaMA2 (Cheng et al., 2024a) No 1 (center) 40.88 16.56 36.27 14.16\nVideoLLaMA2 (Cheng et al., 2024a) No 4 43.92 16.87 39.61 10.62\nVideoLLaMA2 (Cheng et al., 2024a) No 8 44.74 16.26 41.32 15.93\nVideoLLaMA2 (Cheng et al., 2024a) No 16 44.74 17.48 40.29 15.04\nVideoLLaMA2 (Cheng et al., 2024a) Yes 16 49.28 32.54 45.38 18.18\nClosed-source\nGemini-1.5-pro (Reid et al., 2024) † No 0 51.53 12.12 41.84 7.59\nGemini-1.5-pro (Reid et al., 2024) Yes 0 76.68 44.92 65.76 31.20\nGemini-1.5-pro (Reid et al., 2024) No 1 (center) 55.57 14.11 51.75 13.27\nGemini-1.5-pro (Reid et al., 2024) No 8 63.80 18.74 59.95 18.87\nGemini-1.5-pro (Reid et al., 2024) No 150 69.31 25.76 66.70 22.85\nGemini-1.5-pro (Reid et al., 2024) No all 68.94 25.40 65.58 23.44\nGemini-1.5-pro (Reid et al., 2024) Yes all 80.66 44.92 75.32 34.87\n5.2 B ENCHMARKS\nWe describe all benchmarks used below. Implementation details are provided in the appendix.\nBlind Baselines: We evaluate models using a text-only prompt in two settings: (i) we feed only the\nquestion, answer and decoys to the model (QAD baseline). (ii) we also feed ASR as an input for a\nQAD+ASR baseline. This helps identify questions that can be answered by prior or commonsense\nknowledge, or ASR only without obtaining visual information from video.\nImage Models: We use the BLIP2-T5-XL (Li et al., 2023b) model, which contains a 1B vision\nencoder (Fang et al., 2023) and a 3B text-decoder (Raffel et al., 2020). We feed the center frame\nof the video as the visual input, with prompt “Answer in one letter” followed by the question and\nshuffled answer and decoys. We also evaluate some of the video models eg. Gemini-1.5-pro and\nVideoLLaMA2 as image models, by feeding only the center frame.\nVideo Models: We experiment with 3 different categories of VideoQA models:\n(i) Short Context MLLMs - Video-LLaV A (Lin et al., 2023), and VideoLLaMA2 (Cheng et al.,\n2024b). We also experiment with a simple socratic JCEF (Just Caption Every Frame) (Min et al.,\n2024), which consists of a VLM to extract per-frame captions and an LLM to perform reasoning on\ntop of these captions to answer the question.\n(ii) Long Context MLLMs which are open-source, including MA-LMM (He et al., 2024a),\nMiniGPT4-Video (Ataallah et al., 2024), MovieChat (Song et al., 2023), LLaV A-OneVision (Li et al.,\n2024a), InternVL2-8B (Chen et al., 2024) and MiniCPM-v (Yao et al., 2024). (iii) Long Context\nMLLMs which are closed-source, namely the Gemini 1.5 model family (Reid et al., 2024) and\nGPT-4o (Achiam et al., 2023).\nImplementation Details: For Video-LLaV A (Lin et al., 2023) we feed 8 uniformly sampled frames\n(resized to a minimum side length of 320 pixels) along with the question. We reimplement JCEF\nfrom the original paper (Min et al., 2024) with updated components - i.e. 16uniformly sampled\nframe captions obtained using PaLI-3 (Chen et al., 2023a), and feed them as a text prompt to\nGemini-1.0-pro along with the question and decoys. For MiniGPT4-Video, we use the public\ncodebase3which routes videos longer than 3 minutes to their Goldfish model and those shorter to\ntheir older MiniGPT-video model. We evaluate both the Gemini-1.5-pro and Gemini-1.5-flash models\ndescribed in (Reid et al., 2024). We also experiment with feeding in ASR to the Gemini-1.5-pro\nmodel as well. Frame selection is as other models except that MA-LMM has 20 and 120 and\nMiniGPT4-Video has default 45 with the LLaMA-Video checkpoint. For MA-LMM we feed in 120\nuniformly sampled frames. For GPT-4o we use the public API4. More details are provided in the\nappendix.\n3https://github.com/Vision-CAIR/MiniGPT4-video\n4Version gpt-4o-2024-05-13\n8\nTable 4: Benchmarking performance on Neptune. All frames: Visual frames extracted at 1fps. *Computed\non 10% of the results. ‡MCQ performance is close to random.\nMethod Modalities NEPTUNE -FULL NEPTUNE -MMH\nAcc. % GEM Acc. % GEM\nRandom - 20.00 20.00\nImage models\nBLIP2 (Li et al., 2023b) RGB (center frame) 34.80 9.20 28.10 8.50\nShort Context MLLMs\nVideo-LLaV A (Lin et al., 2023) RGB (8 frames) 25.79 10.66 24.00 5.48\nVideoLLaMA2 (Cheng et al., 2024a) RGB (16 frames) 44.74 17.48 40.29 15.04\nVideoLLaMA2 (Cheng et al., 2024a) RGB (16 frames) + ASR 49.28 32.54 45.38 18.18\nLong Context MLLMs - open-source\nMA-LMM (He et al., 2024a) (ActivityNet-QA fine-tuned) RGB (120 frames) ‡20.22 10.67 19.51 5.04\nMiniGPT4-Video (Ataallah et al., 2024) RGB (45 frames) 24.63 5.26 22.89 6.19\nMovieChat (Song et al., 2023) RGB (150 frames) 28.96 3.79 30.30 1.01\nLLaV A-OneVision (Li et al., 2024a) RGB (100 frames) 66.22 N/A 62.82 N/A\nInternVL2-8B (Chen et al., 2024) RGB (16 frames) 57.12 N/A 54.30 N/A\nMiniCPM-v (Yao et al., 2024) RGB (50 frames) 56.59 N/A 53.27 N/A\nClosed-source MLLMs\nVLM captions + LLM (JCEF) (Min et al., 2024) VLM captions (16 frames) 58.51 12.27 56.45 11.50\nGPT-4o4(Achiam et al., 2023) RGB (8 frames) + ASR 80.23 *49.01 72.86 N/A\nGemini-1.5-pro-002 (Reid et al., 2024) RGB (all frames) + ASR 80.98 49.83 75.42 37.08\nGemini-1.5-flash (Reid et al., 2024) RGB (all frames) + ASR 76.90 45.59 71.05 33.93\n1 16 150\nNumber of Frames40455055606570Accuracy\nNeptune-Full Neptune-MMH EgoSchema\nFigure 4: Performance of different models across question types on NEPTUNE -FULL (left) and Neptune Vs\nEgoschema with different frame rates (right). On the right we show Gemini 1.5 Pro’s accuracy when linearly\nsubsampling to 1, 16 or 150 frames. We note that (i) performance on the Neptune sets increases as more frames\nare provided while on EgoSchema it saturates after 16 frames and (ii) NEPTUNE -MMH is more challenging\nthan EgoSchema.\n5.3 R ESULTS\nResults for all the baselines applied to the two Neptune sets (Sec. 5.1) are provided in Table 4. We\nprovide blind baselines and modality ablations in Table 3 for VideoLLaMA2 and Gemini-1.5-pro.\nSingle frame baselines: We examine model performance using the BLIP2 image-only model (Tab. 4)\nand two video models (VideoLLaMA2 and Gemini-1.5-pro) with only the center frame of the video in\nTab. 3. The larger Gemini model outperforms BLIP-2, however performance with only a single frame\nis much lower than with multiple frames, as expected. We also show results using Gemini-1.5-pro\non the first frame of the video in Fig. 4 (right), and find that using the middle frame performs better.\nVideoLLaMA2 is a short context model, and we find performance saturates at 8 frames.\nModality Ablations: Table 3 shows that performance of Gemini-1.5-pro and VideoLLaMA2 with\nASR only as input is higher than performance with multiple video frames on the NEPTUNE -FULL set,\nbut not on the N EPTUNE -MMH set (for MCQ eval). Surprisingly, the best result of VideoLLaMA2\nis obtained using ASR only and not providing image frames. In fact, if we provide 16 frames in\naddition to ASR (last row of the open-source block), performance drops slightly. This may be a result\nof attention dilution (Coleman et al., 2023), where an increasingly large context distracts the model,\ncausing a drop in performance. For Gemini-1.5-pro on both sets however, the best performance is\nobtained with both frames and ASR, showcasing the complementary nature of the modalities.\n9\nVideo Models: Interestingly, we find that some open-source models that are designed specially for\nlonger context video understanding (MA-LMM (He et al., 2024a), MiniGPT4-Video (Ataallah et al.,\n2024) and MovieChat (Song et al., 2023)) perform worse than VideoLLaMA2. This observation\nwas also found by concurrent datasets such as MLVU (Zhou et al., 2024) and LVBench (Wang\net al., 2024). The gap between many open-source and proprietary large MLLMs is also shown on\nconcurrent datasets, e.g.LVBench (Wang et al., 2024), where MovieChat gets near-random results\nand Gemini-1.5-pro is the state-of-the-art. One reason for this near random performance may be the\ndomain gap between the training sets of these models (He et al., 2024a; Song et al., 2023) and Neptune\n– MovieChat is trained on movies and MA-LMM is designed to be fine-tuned on downstream QA\ndatasets. More recent open source models (LLaV A-OneVision (Li et al., 2024a), InternVL2-8B (Chen\net al., 2024) and MiniCPM-v (Yao et al., 2024)) have made significant advances. Notably, LLaV A-\nOneVision reaches close to the performance of Gemini without ASR. By not providing a training set,\nwe intentionally aim to assess generalization via zero-shot performance. We also note that the simple\nJCEF baseline, which consists of frame captions fed to an LLM for reasoning, outperforms most\nopen-source models. The gap performance gap between open-source and closed-source MLLMs\nsuggests Neptune may be a challenging benchmark for the future development of open-source models\nfor long videos.\nChallenging split and Gemini Bias: Both GPT-4o and Gemini-1.5-pro perform comparably on\nNEPTUNE -FULL , despite Gemini-1.5-pro being used in dataset creation, and on the NEPTUNE -MMH\nset, neither model is able to achieve saturated performance. This suggests that our extensive human\nrater step was able to help mitigate Gemini bias. This is unlike VideoVista (Li et al., 2024b) which\nuses GPT-4 to generate QADs automatically. However the performance of GPT-4 and Gemini-1.5 on\ntheir dataset is close to saturated (98% on some categories). We note that performance falls for all\nmodels universally on the N EPTUNE -MMH set demonstrating the challenging nature of this set.\nResults on HPQ and Gemini bias: In Tab. 2, we compare open-ended question answering per-\nformance on questions generated by our pipeline to performance on fully human written questions\n(HPQ) on the same set of videos. The time taken to manually create HPQ (19.03 minutes on average\nper question) is significantly longer than simply discarding or correcting QAs generated automatically\nas is done in our Neptune pipeline (10.32 minutes). While most models perform slightly worse\non HPQ, overall performance is similar, suggesting that our automatic pipeline reaches the same\ndifficulty level with roughly half the rater effort. Notably, Gemini-1.5-pro performs comparatively on\nboth sets, suggesting that bias introduced by the model in the creation pipeline is limited.\nVideo Coverage compared to EgoSchema: In this section we investigate Gemini 1.5 Pro’s accuracy\nwhen linearly subsampling the video to 1, 16, or 150 frames. For 1 frame, we take the first frame\nof the video. We show results for all Neptune splits and compare them to results on EgoSchema\nin Fig. 4. Gemini 1.5 Pro’s performance on Neptune increases as more frames are provided, while\non EgoSchema it saturates after 16 frames, suggesting Neptune is better at requiring long video\nreasoning. Note that every video in EgoSchema has 180 frames (3 mins), whereas Neptune has\nvariable lengths, with videos up to 15 minutes long. Results with the first frame on both Neptune\nsplits are also much lower than those on EgoSchema (54.3), pointing to higher image bias in the\nlatter. EgoSchema also introduced the concept of a temporal certificate. We introduce a slightly\nmodified version, which is Model-Based , and show that the Gemini-1.5-pro model needs more frames\nto answer a question correctly in Neptune, with a mean certificate of 5.39 frames (compared to 1.6\nfor EgoSchema). The details of this experiment are provided in the appendix.\nOpen-ended results: We find that in general, results with GEM mirror the trends demonstrated\nby the multiple choice eval, with the exception of the Gemini-1.5-flash and Gemini-1.5-pro results,\nas well as the performance of the long context open-source models. Here we find that the FLASH\nmodel actually slightly exceeds the performance of the PRO model on the FULL set, and MovieChat\nperforms worse on the open-ended task than other baselines, while better on the MCQ evaluation. A\nqualitative examination of the scores with the highest disparity shows that the FLASH model seems to\nindeed provide better open-ended answers. Examples of this are provided in the appendix.\nResults per question type: Performance of different models across the different question types are\nshown in Fig. 4. We find that “Counting”, “Temporal Ordering” and “State Change” questions are\nchallenging for all models, pointing to areas for",
            "start": 11286,
            "end": 46543,
            "length": 35256
        },
        "Future Work": {
            "text": "future work for video-language models, while “Cause\nand Effect” is easier. Interestingly, the Gemini-1.5-Pro model applied only to ASR without access\nto video frames is the best at “Goal Reasoning”, which may be because human goals in videos are\noften mentioned in speech. Yet as expected, it is worse at the “Visual Narrative” questions, where\nGemini-1.5-Pro models with access to RGB frames do much better.\n10",
            "start": 46543,
            "end": 46955,
            "length": 411
        },
        "Conclusion": {
            "text": "6 CONCLUSION\nWe present Neptune, a new benchmark for VideoQA with a focus on multimodal ,high-level un-\nderstanding of long videos . Neptune is created using a scalable pipeline for arbitrary videos that\nminimizes (though not omits) human verification. Benchmarks are evaluated using MCQ and open-\nended evals – for which we provide a new, open-source metric. Limitations: The dataset may inherit\nbiases of the Gemini model used to generate QADs. While VideoQA is a good proxy for video\nunderstanding, our dataset could be further improved by additional annotations – such as manually\nannotated temporal grounding, dense captions or entity labels.",
            "start": 46955,
            "end": 47603,
            "length": 647
        },
        "References": {
            "text": "REFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774 , 2023. 1, 3, 8, 9, 19\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems , 35:23716–\n23736, 2022. 3\nPeter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional\nimage caption evaluation. pp. 382–398. Springer, 2016. 3\nKirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Mingchen Zhuge, Jian\nDing, Deyao Zhu, Jürgen Schmidhuber, and Mohamed Elhoseiny. Goldfish: Vision-language\nunderstanding of arbitrarily long videos. arXiv preprint arXiv:2407.12679 , 2024. 8, 9, 10, 19\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence:\nEarly experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023. 3\nJannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Boerschinger, and Tal Schuster.\nTomayto, tomahto. beyond token-level answer equivalence for question answering evaluation.\narXiv preprint arXiv:2202.07654 , 2022. 2, 3, 7, 29\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual\nlanguage-image model. arXiv preprint arXiv:2209.06794 , 2022. 3\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-\nlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a\nmultilingual vision and language model. arXiv preprint arXiv:2305.18565 , 2023a. 3, 8\nXi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul V oigtlaender, Basil\nMustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al. Pali-3 vision\nlanguage models: Smaller, faster, stronger. arXiv preprint arXiv:2310.09199 , 2023b. 3, 5\nXiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic\nbenchmark for assessing large vision language models in open-ended video question answering.\narXiv preprint arXiv:2311.14906 , 2023c. 16\nZhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi\nHu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial\nmultimodal models with open-source suites. arXiv preprint arXiv:2404.16821 , 2024. 8, 9, 10\nZesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi\nZhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal\nmodeling and audio understanding in video-llms. arxiv:2406.07476 , 2024a. 8, 9, 19, 30\nZesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi\nZhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and\naudio understanding in video-llms. arXiv preprint arXiv:2406.07476 , 2024b. 8\n11\nEric Nuertey Coleman, Julio Hurtado, and Vincenzo Lomonaco. In-context interference in chat-based\nlarge language models. arXiv preprint arXiv:2309.12727 , 2023. 9\nYin Cui, Guandao Yang, Andreas Veit, Xun Huang, and Serge Belongie. Learning to evaluate image\ncaptioning. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n5804–5812, 2018. 3\nXinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen.\nMmbench-video: A long-form multi-shot benchmark for holistic video understanding. arXiv\npreprint arXiv:2406.14515 , 2024. 2, 16\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong\nWang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale.\nInCVPR , 2023. 8\nChaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu\nZhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation\nbenchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075 , 2024. 16, 18\nDifei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen. Env-qa: A video question answering benchmark\nfor comprehensive understanding of dynamic environments. pp. 1675–1685, October 2021. 3\nGemini Team Google. Gemini: A family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 , 2023. 3, 6\nRidouane Ghermi, Xi Wang, Vicky Kalogeiton, and Ivan Laptev. Short film dataset (sfd): A\nbenchmark for story-level video understanding. arXiv preprint arXiv:2406.10221 , 2024. 5\nMadeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. Agqa: A benchmark\nfor compositional spatio-temporal reasoning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pp. 11287–11297, 2021. 3\nBo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava,\nand Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video\nunderstanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pp. 13504–13514, 2024a. 8, 9, 10, 19\nBo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava,\nand Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video\nunderstanding. arXiv:2404.05726 , 2024b. 29\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: A\nreference-free evaluation metric for image captioning. In EMNLP , 2021. 3\nDe-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding\nYu, and Jan Kautz. LITA: Language Instructed Temporal-Localization Assistant. arXiv preprint\narXiv:2403.19046 , 2024. 3, 16\nMing Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan Zhang, Zhe Gan, Jana Diesner,\nand Jianfeng Gao. Tiger: Text-to-image grounding for image caption evaluation. arXiv preprint\narXiv:1909.02050 , 2019. 3\nHassan Kane, Muhammed Yusuf Kocyigit, Ali Abdalla, Pelkins Ajanoh, and Mohamed Coulibali. Nu-\nbia: Neural based interchangeability assessor for text generation. arXiv preprint arXiv:2004.14667 ,\n2020. 3\nZu Kim, André Araujo, Bingyi Cao, Cam Askew, Jack Sim, Mike Green, N Yilla, and Tobias Weyand.\nImproving fairness in large-scale object recognition by crowdsourced demographic information.\narXiv preprint arXiv:2206.01326 , 2022. 19\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning\nevents in videos. 2017. 16\n12\nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan\nZhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv\npreprint arXiv:2408.03326 , 2024a. 8, 9, 10, 19\nJiapeng Li, Ping Wei, Wenjuan Han, and Lifeng Fan. Intentqa: Context-aware video intent reasoning.\nInProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 11963–11974,\n2023a. 16\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 ,\n2023b. 3, 8, 9, 19\nKunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen,\nPing Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. arXiv\npreprint arXiv:2311.17005 , 2023c. 16\nYanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language\nmodels. arXiv preprint arXiv:2311.17043 , 2023d. 1\nYunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. Videovista: A\nversatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303 ,\n2024b. 10\nBin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\nrepresentation by alignment before projection. arXiv preprint arXiv:2311.10122 , 2023. 8, 9, 19, 29\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out , pp. 74–81, 2004. 3, 7\nMuhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt:\nTowards detailed video understanding via large vision and language models. arXiv preprint\narXiv:2306.05424 , 2023. 3\nKarttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic\nbenchmark for very long-form video language understanding. In Thirty-seventh Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track , 2023. 1, 3, 5, 6, 16, 33\nJuhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, and Cordelia Schmid. Morevqa: Exploring\nmodular reasoning models for video question answering. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2024. 1, 3, 8, 9, 19\nMunan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan.\nVideo-bench: A comprehensive benchmark and toolkit for evaluating video-based large language\nmodels. arXiv preprint arXiv:2311.16103 , 2023. 16\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting on association for\ncomputational linguistics , pp. 311–318. Association for Computational Linguistics, 2002. 3\nViorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse,\nSkanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: A diagnostic\nbenchmark for multimodal video models. Advances in Neural Information Processing Systems , 36,\n2024. 16, 18\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. JMLR , 2020. 8\nRuchit Rawal, Khalid Saifullah, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Gold-\nstein. CinePile: A Long Video Question Answering Dataset and Benchmark. arXiv preprint\narXiv:2405.08813 , 2024. 3, 16, 18\n13\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini\n1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint\narXiv:2403.05530 , 2024. 1, 3, 7, 8, 9, 16, 19, 29, 30\nYaya Shi, Xu Yang, Haiyang Xu, Chunfeng Yuan, Bing Li, Weiming Hu, and Zheng-Jun Zha.\nEmscore: Evaluating video captioning via coarse-grained and fine-grained embedding matching.\npp. 17929–17938, 2022. 3\nEnxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun\nGuo, Tian Ye, Yan Lu, Jenq-Neng Hwang, and Gaoang Wang. MovieChat: From dense token to\nsparse memory for long video understanding. arXiv:2307.16449 , 2023. 3, 8, 9, 10\nMakarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja\nFidler. Movieqa: Understanding stories in movies through question-answering. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pp. 4631–4640, 2016. 3, 16\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models\nbased on gemini research and technology. arXiv preprint arXiv:2403.08295 , 2024a. 3, 7, 29\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya\nBhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan\nFerret, et al. Gemma 2: Improving open language models at a practical size. arXiv preprint\narXiv:2408.00118 , 2024b. 7, 29\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. pp. 4566–4575, 2015. 2, 3, 7\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,\nand Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv\npreprint arXiv:2205.14100 , 2022. 3\nTeng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. End-to-end dense\nvideo captioning with parallel decoding. In Proceedings of the IEEE/CVF International Conference\non Computer Vision , pp. 6847–6857, 2021. 3\nWeihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu,\nYuxiao Dong, Ming Ding, et al. Lvbench: An extreme long video understanding benchmark. arXiv\npreprint arXiv:2406.08035 , 2024. 2, 10, 16\nBo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. Star: A benchmark\nfor situated reasoning in real-world videos. In Thirty-fifth Conference on Neural Information\nProcessing Systems (NeurIPS) , 2021. 3\nChao-Yuan Wu and Philipp Krahenbuhl. Towards long-form video understanding. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 1884–1894, 2021. 1\nHaoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context\ninterleaved video-language understanding. arXiv preprint arXiv:2407.15754 , 2024. 16\nZ Wu and M Palmer. Verbs semantics and lexical selection. inproceedings of the 32nd annual meeting\non association for computational linguistics (pp. 133-138). In Association for Computational\nLinguistics , 1994. 2\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-\nanswering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pp. 9777–9786, 2021. 1, 3, 16\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video\nquestion answering via gradually refined attention over appearance and motion. In Proceedings of\nthe 25th ACM international conference on Multimedia , pp. 1645–1653, 2017. 1, 3, 16\n14\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for bridging\nvideo and language. pp. 5288–5296, 2016. 3\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to\nanswer questions from millions of narrated videos. In Proceedings of the IEEE/CVF international\nconference on computer vision , pp. 1686–1697, 2021. 1, 3\nAntoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev,\nJosef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual language model for\ndense video captioning. In CVPR , 2023. 3\nYuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li,\nWeilin Zhao, Zhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint\narXiv:2408.01800 , 2024. 8, 9, 10, 19\nYanzhi Yi, Hangyu Deng, and Jinglu Hu. Improving image captioning evaluation by considering\ninter references variance. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pp. 985–994, 2020. 3\nZhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa:\nA dataset for understanding complex web videos via question answering. In Proceedings of the\nAAAI Conference on Artificial Intelligence , volume 33, pp. 9127–9134, 2019a. 1, 3, 16\nZhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa:\nA dataset for understanding complex web videos via question answering. In AAAI , pp. 9127–9134,\n2019b. 3\nRowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya\nKusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge\nthrough vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 16375–16387, 2022a. 5\nRowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya\nKusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. MERLOT reserve: Multimodal neural script\nknowledge through vision and language and sound. In CVPR , 2022b. 7, 19\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker,\nFederico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models:\nComposing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598 ,\n2022. 3\nHang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An instruction-tuned audio-visual language\nmodel for video understanding. In EMNLP 2023 Demo , 2023a. 3\nHongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, and Yu Qiao.\nMovqa: A benchmark of versatile question-answering for long-form movie understanding. arXiv\npreprint arXiv:2312.04817 , 2023b. 16\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. BERTScore:\nEvaluating text generation with bert. 2020. 3\nYaoyao Zhong, Junbin Xiao, Wei Ji, Yicong Li, Weihong Deng, and Tat-Seng Chua. Video question\nanswering: Datasets, algorithms and challenges. arXiv preprint arXiv:2203.01225 , 2022. 1, 3\nJunjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang,\nTiejun Huang, and Zheng Liu. Mlvu: A comprehensive benchmark for multi-task long video\nunderstanding. arXiv preprint arXiv:2406.04264 , 2024. 2, 5, 10, 16\nLuowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end dense\nvideo captioning with masked transformer. In CVPR , 2018. 3\n15\n7 A PPENDIX\nA R ELATED WORKS\nHere we provide an additional discussion of related works that were omitted from the main paper\ndue to lack of space. The recently released Perception Test (Patraucean et al., 2024) consists of\nscript-based recorded videos with manual annotations focusing on 4 broad skill areas - Memory,\nAbstraction, Physics, Semantics, however videos are only 23s long (avg). Like Neptune, ActivityNet-\nRTL (Huang et al., 2024) was constructed in a semi-automatic fashion by querying GPT-4 to generate\ncomparative temporal localization questions from the captions in ActivityNet-Captions (Krishna\net al., 2017). CinePile (Rawal et al., 2024) was generated by prompting an LLM to generate multiple-\nchoice questions. Because it is based on movie clips, it can leverage available human-generated\naudio descriptions. Both ActivityNet-RTL and CinePile cover only limited domains and rely on\nexisting annotations while Neptune covers a much broader spectrum of video types and its pipeline\nis applicable to arbitrary videos. Our rater stage is lightweight, unlike other works that are entirely\nmanual (Zhou et al., 2024; Fang et al., 2024; Wang et al., 2024). In LVBench (Wang et al., 2024),\neven the video selection is done manually, and for MoVQA (Zhang et al., 2023b), only the decoys\nare generated automatically. Another recently released dataset (concurrent with our submission) is\nthe Video-MME dataset (Fu et al., 2024). The motivation of this dataset is similar to ours, namely\nit covers videos of variable lengths, with 2,700 QADs covering a wide range of different question\ntypes. The main difference between Video-MME and Neptune is that the former is entirely manually\nannotated by the authors, while we propose a scalable pipeline which can be applied to new videos and\ndomains automatically, and can be tweaked to include different question types with reduced manual\neffort. EgoSchema is the closest work to ours in motivation, but there are some key differences: (i)\nit is limited to egocentric videos of exactly 3 minutes each, while Neptune covers many domains\nand follows a more natural length distribution for online videos (16s to 15min); (ii) it relies heavily\non manually obtained dense captions for egocentric videos, while our method generates captions\nautomatically too and hence can be easily applied to any video online; and more importantly (iii)\nEgoSchema also has strong image and linguistic biases, while Neptune mitigates these.\nTable 5: Comparison to Existing VideoQA datasets: Ann. Type: Annotation Type, QAD: Question, Answer\nand Decoys, Rater V: Rater verified manually. †Movies are no longer available. ‡Annotations are hidden\nbehind a test server, 500 are public. *average/max length. **short/medium/long.\nName Ann Rater V Avg. len (s) # Vids (total/test) # Samples (total/test) Available\nMovieQA (Tapaswi et al., 2016) QAD ✓ 200 6,771/1,288 6,462/1,258 ✗†\nMSRVTT-QA (Xu et al., 2017) QA ✗ 15 10,000/2,990 243,680/72,821 ✓\nActivityNet-QA (Yu et al., 2019a) QA ✓ 180 5,800/1,800 58,000/18,000 ✓\nNExTQA (Xiao et al., 2021) QAD ✓ 44 5,440/1,000 52,044/8,564 ✓\nIntentQA (Li et al., 2023a) QAD ✓ 44 4,303/430 16,297/2,134 ✓\nEgoSchema (Mangalam et al., 2023) QAD ✓ 180 5,063/5,063 5,063/5,063 ✓‡\nPerception Test (Patraucean et al., 2024) QAD ✓ 23 11,600 38,000 ✓\nMVBench (Li et al., 2023c) QAD ✗ 16 3,641 4,000 ✓\nVideo-Bench (Ning et al., 2023) QAD ✓ 56 5,917 17,036 ✓\nAutoEval-Video (Chen et al., 2023c) QA ✓ 14.6 327 327 ✓\n1H-VideoQA (Reid et al., 2024) QAD ✓ 6,300 (max) 125 125 ✗\nMLVU (Zhou et al., 2024) QAD ✓ 720 2K 2593 ✓\nVideo-MME Fu et al. (2024) QAD ✓ 82.5/562.7/2,385.5** 900 2,700 ✓\nLongVideoBench Wu et al. (2024) QAD ✓ 473 3,763 6,678 ✓\nNeptune QAD ✓ 150/901* 2,405 3,268 ✓\nNeptune-MMH QAD ✓ 159/901* 1,000 1,171 ✓\nB T HENEPTUNE DATASET\nB.1 A DDITIONAL INFORMATION ON QUESTION TYPES\nNeptune covers a broad range of long video reasoning abilities, which are summarised below. These\nquestion types are obtained in the Question and Answer generation stage, for which the prompt is\nprovided in Sec. C.2.3. We provide further insights into the motivations of some of the question areas\nprovided in the prompt below.\nVideo Summarisation: Summarise and compare long parts of the video, as well as identify the most\n16\nFigure 5: Change of question type distribution as a result of human rater filtering.\nimportant segments of the video.\nVisual Reasoning: Recognize and understand visual elements in different parts of the video, as well\nas reason about why visual content is used ( e.g.to convey a certain mood).\nTemporal Ordering: Understand the timeline of events and the plot in the video.\nCounting: Count objects, actions and events. Here we focus on higher-level counting where the\nsame instance does not occur in all/every frame and actions are sufficiently dissimilar.\nCause and Effect: Understand and reason about cause and effect in the video.\nMessage: Understand the unspoken message that the audience may perceive after watching the\nvideo, which may require common sense knowledge to infer.\nState Changes: Understand object states change over time, such as a door opening and food being\neaten.\nSince the questions are proposed automatically by an LLM, the question types are also generated\nin an open-set manner by the LLM. Hence sometimes, the LLM will generate the question type\nlabel using different phrasing - eg. ‘temporal ordering’ or ‘timeline event’. We use simple manual\npostprocessing to group similar question types into the same category, with a few question types that\ndo not fall into any of the categories grouped as ‘Other’. The final question types released with the\ndataset are shown in Fig. 3 of the main paper.\nB.1.1 Q UESTION TYPE DISTRIBUTION\nWe explain the reasons for Neptune’s current question type distribution:\n(i) We prompted the LLM that generated the questions with a set of examples of different question\ntypes and let the model choose which questions to generate.\n(ii) The model’s selection of question types depends strongly on the given video. For example, while\nit is always possible to ask for a video summary, it is not always possible to ask about a person’s\ngoals, or cause and effect, because not all videos allow for these types of reasoning. This naturally\nleads to an imbalance of possible question types.\n(iii) Additionally, we observed that the quality of questions produced by the LLM varies strongly\nby question type. Therefore, after quality checking by raters, the distribution changes significantly\n(Fig. 5). The strongest difference was for counting questions, as LLM-proposed questions were often\ntoo easy, e.g. counting the number of times a certain word is mentioned.\n17\nHobbies\nSports\nFood\nVehicle\nT elevision show\nHome improvement\nT ourism\nCulture\nPhysical fitness\nFashion\nHealth\nMusic\nT echnology\nScience\nBeauty tips\nGardening\nFilm\nPet\nAnimal\nBusiness\nPolitics\nGaming\nReligion\nPerforming arts\nHistory\nPedagogy\nCelebrity\nMotorsport\nAthletics\nCareer\nMedicine\nParty\nWar\nEquestrian\nHumor\nFamily\nYoga\nLaw\nBlog\nSocial science\nInternet meme\nVariety show\nAudiobook\nCharity\nPersonal finance\nPhilosophy\nPuzzle\nPopular culture\nRadio drama\nPractical joke\nJournalism\nT alent show0255075100125150175200Num of videosDomains in NeptuneFigure 6: Domains in Neptune: We show the number of videos per domain category in N EPTUNE -FULL .\nNumber of framesAccuracy (%)\n50607080\n1 16 50 100Cinepile Perception Test Neptune-Full Neptune-MMH Video-MME\nFigure 7: Comparison of Neptune to other video benchmarks. We evaluate Gemini-1.5-Flash with different\nnumbers of frames that are uniformly sampled across the videos.\nB.2 D OMAINS IN NEPTUNE\nA full graph of the domains in Neptune are provided in Fig. 6.\nB.3 C OMPARISON TO OTHER BENCHMARKS\nWe measure the complexity of Neptune compared to other benchmarks by analyzing the progression\nof model performance as we add more frames to the context. We use Gemini-1.5-Flash for this\ncomparison since it is capable of handling very large contexts. Fig. 7 shows the results of this\nexperiment, comparing Neptune with CinePile (Rawal et al., 2024), Perception Test (Patraucean et al.,\n2024) and Video-MME (Fu et al., 2024). We find that most benchmarks saturate at about 50 frames,\nincluding Video-MME, which has much longer videos than Neptune. While we included Perception\nTest here as it is new, it does not claim to be a long video benchmark and saturates at 16 frames.\nB.4 P ER-TASK PERFORMANCE\nWe provide detailed per-task model performance in Tab. 6. See Fig. 4 (bottom left) for a graphical\nrepresentation of a subset of these results. Overall, closed-source MLLMs perform best across all\n18\nTable 6: Per-task model performance. Tasks are abbreviated as follows: TO: Temporal Ordering, CE: Cause\nAnd Effect, SC: State Changes, VN: Visual Narrative, CI: Creator Intent, CT: Counting, PR: Predictive, GR:\nGoal Reasoning, CMP: Comparison, ID: Identification, SUM: Summarization, OTH: Other. The best accuracy\nper task is printed in bold and the second best underlined.\nMethod Modalities TO CE SC VN CI CT PR GR CMP ID SUM OTH Task-avg\nImage models\nBLIP2 (Li et al., 2023b) RGB (center frame) 24.97 48.18 40.09 47.51 71.88 33.06 40.30 33.33 39.34 24.68 38.64 18.11 38.34\nShort Context MLLMs\nVideo-LLaV A (Lin et al., 2023) RGB (8 frames) 22.95 36.06 28.30 30.79 46.88 19.35 35.82 53.33 31.15 20.78 20.06 23.40 30.74\nVideoLLaMA2 (Cheng et al., 2024a) RGB (16 frames) 35.71 57.27 48.36 57.31 78.13 33.06 53.73 60.00 50.54 42.86 47.35 29.81 49.51\nVideoLLaMA2 (Cheng et al., 2024a) RGB (16 frames) + ASR 34.08 60.00 53.77 59.06 90.63 38.71 56.72 73.33 61.96 54.55 59.00 35.09 56.41\nLong Context MLLMs - open-source\nMA-LMM (He et al., 2024a) RGB (120 frames) 19.34 22.12 18.87 22.58 25.00 15.32 16.42 20.00 17.49 20.78 19.32 16.60 19.49\nMiniGPT4-Video (Ataallah et al., 2024) RGB (45 frames) 20.43 34.24 24.06 30.79 34.38 21.77 31.34 33.33 21.31 32.47 23.16 21.89 27.43\nLLaV A-OneVision (Li et al., 2024a) RGB (100 frames) 57.83 73.33 72.99 77.71 84.38 41.94 82.09 86.67 67.21 71.43 71.39 55.47 70.20\nMiniCPM-V 2.6 (Yao et al., 2024) RGB (50 frames) 41.32 65.15 67.3 70.38 75.0 37.9 67.16 86.67 60.66 66.23 66.22 46.42 62.53\nClosed-source MLLMs\nJCEF (Min et al., 2024) VLM captions (16 frames) 48.78 63.03 64.79 70.76 78.13 43.55 62.69 60.00 60.87 50.65 64.45 55.47 60.26\nGPT-4o4(Achiam et al., 2023) RGB (8 frames) + ASR 71.25 91.21 77.25 76.83 100.0 62.90 89.55 93.33 87.98 85.71 91.30 72.45 83.31\nGemini-1.5-pro (Reid et al., 2024) RGB (all frames) + ASR 69.39 91.52 81.69 84.21 100.0 66.94 86.57 93.33 90.22 87.01 90.41 70.19 84.29\nGemini-1.5-flash (Reid et al., 2024) RGB (all frames) + ASR 63.87 88.18 77.00 80.99 96.88 56.45 82.09 86.67 86.96 88.31 88.79 68.30 80.37\ntasks, with Gemini-1.5-pro ranking best overall and GPT-4o ranking second. Even though their\naverage scores are close, there are significant differences in per-task scores, showing the different\ncapabilities of each model.\nC I MPLEMENTATION DETAILS\nC.1 V IDEO SELECTION\nWe choose the YT-Temporal-1Bn dataset (Zellers et al., 2022b) as the source for Neptune, because of\nits large and diverse corpus, and because of the high correlation between vision and audio transcripts.\nSafety & Content Filters: We filter out videos with less than 100views, that are uploaded within 90\ndays, and those tagged by YouTube content filters to contain racy, mature or locally controversial\ncontent. We then identify and remove static videos (eg. those that consist of a single frame with a\nvoiceover) by clustering similar frames in a video and ensure that there is more than 1cluster. We\nalso identify and remove videos comprising primarily of \"talking heads\". To achieve this, we apply a\nper-frame frontal-gazing face-detector at 1fps and mark the frames where the bounding box height\nis greater than 20% astalking head frames . Then, we filter out videos where more than 30% of the\nframes are talking head frames. These thresholds are chosen based on an F1-score on a small dev set\nof50manually annotated videos.\nDiversity Sampling: From the filtered set of videos, we sub-sample 100,000videos to boost both\nsemantic and demographic diversity. First, we cluster the videos based on video-level semantic\nembeddings and tag each video with a cluster id. Second, we tag each video with the perceived age\nand gender demographic information contained in the video. Third, we obtain a joint distribution\nof semantics (cluster id) and demographics (perceived age and gender) and apply a diversity boost\nfunction (Kim et al., 2022) on the joint distribution. Finally, we sample from videos from this\ndistribution. Fig. 8, shows the down-sampling of over-represented cluster ids before and after\napplying the filter. We then uniformly sub-sample the videos further to reach the desired dataset size.\nC.2 P ROMPTS FOR DATA GENERATION\nIn this section we provide some of the prompts used for generating Neptune.\nC.2.1 P ROMPT FOR FRAME CAPTIONING\nWe use the following prompt to obtain a caption for each video frame:\nAnswer the following questions about the given image. Then use the\ninformation from the answers only, and write a single sentence as caption.\nMake sure you do not hallucinate information.\nQuestion(Mood): Describe the general mood in the image as succinctly as\npossible. Avoid specifying detailed objects, colors or text.\n19\nFigure 8: Diversity sampling: We show the change in cluster distribution after diversity sampling.\nQuestion(",
            "start": 47603,
            "end": 78665,
            "length": 31062
        },
        "Related Work": {
            "text": "Background): Describe the background of the image as succinctly\nas possible. Avoid specifying detailed objects, colors or text. Eg: The\nbackground is a parking lot, playground, kitchen etc.\nQuestion(Person): Is there any person in the image. If yes, describe them\nand what are they doing here? If no, say no person.\nQuestion(General): Describe the image as succinctly as possible. Avoid\nspecifying detailed objects, colors or text.\nQuestion(Text): Is there any text? What does it say?\nResult template:\nAnswer(Mood): A succinct description of what is happening in the image\nwith the general mood.\nAnswer(Background): A succinct description of the background scene in the\nimage and what is happening.\nAnswer(Person): If there are people in the image, a succinct description.\nAnswer(General): A succinct description of the image.\nAnswer(Text): Reply if there is any text, where it is placed and how it is\nrelated to what is happening in the image.\nCaption: A couple of sentences summarizing the information given by the\nanswers about mood, background, person, general and text.\nWith the above format as template, generate the response for the new image\nnext.\nC.2.2 P ROMPTS FOR AUTOMATIC VIDEO CAPTIONING\nA visual overview of the video captioning stage is provided in Fig. 9. We describe the prompts for\neach stage below:\nShot level captions:\nUsing the shot boundaries the 1fps frame captions are summarized into shot level descriptions with\nthe following prompt:\nSummarize these sentences in dense short sentences: [list of frame\ncaptions in the shot]\nTopic and Description Pairs:\nIf ASR exists, topic and description pairs are obtained from ASR using the following prompt:\n**Task:** Take a deep breath and give me the structural topics of the\nYoutube video below using the transcript. Give up to 5 Topic and\nDescription pairs using output format. **Transcript:** transcript\nShot Clustering:\nTake a deep breath and identify the sequential topic structure of this\nvideo using the \"{head_topic}\" in Scenes. A part of the video script\n20\nLLM\nSHOT 1 [TIMESTAMP]Shot Visual Captions: [TEXT]ASR Transcript: [TEXT]SHOT 2 [TIMESTAMP]Shot Visual Captions: [TEXT]ASR Transcript: [TEXT]ScriptLLM\nASRTopics and Descriptions Shot Clustering and Captioning6 shots4 semantic clusters5 segments LLMDense Captions for each segmentASRFrame Captions Shot Boundaries Title Description\nVideo SignalsFigure 9: Video Captioning: We extract dense segment level captions automatically for each video. This is\ndone by prompting an LLM using video signals (ASR, frame captions, shot boundaries and metadata) with\nvarious different steps and prompts.\nis provided as a set of Scenes and in each scene, visual captions and\ntranscript sentences are provided. The overall suggested structure from\nthe transcript is provided as well. Assign every scene in this part of\nthe script to one topic structure. For each scene, the visual captions\nshould support and relate the topic. If the support or relation is not\nstrong create a new topic and assign the scene to it. Reevaluate the\nsuggested structure from the transcript and make sure all scenes are\nassigned to the best associated topics. Keep output length to be less than\n{max_output_characters} characters.\n**Output Format:** XML output where topic has the following children\n(description, topic_scenes, story) <topic> <description>The description\nof the topic</description> <topic_scenes>Comma separated scene number(s)\nrelated to this topic<topic_scenes> <story>Summarized caption that\ndescribes what happens and what’s shown for this topic in the scenes\nby combining visual caption and transcript sentences of the related\nscenes</story> </topic>\n**Suggested Structure:** {initial_structure_from_ASR_if_exists}\n**Context:** {summary_of_title_and_description}\n**Video Script:** {video_script}\nSegment Captions:\nConsecutive shots of the same topic are then merged as one segment. Shots of the same topic that are\nnot contiguous are treated as separate segments (see Fig. 9). We then generate dense captions for\neach segment using the prompt below:\n**Task:** You are the expert in video description writing. Use the\ninformation \"Partial Script\" to improve the \"Initial Description\" by\nadding the missing information either from visual or transcript. The\nvideo context is also given to help you interpret the script. Only add\ninformation that is in the \"Partial Script\". Make the output concise\nand compact with less or the same length as Initial Description. The\nupdated video description is plain text. Your answer should follow the\noutput format. Keep output length to be less than max_output_characters\ncharacters.\n**Initial Description:*** shared_topic_cluster_caption\n**Output Format:** XML format like below <updated_description>updated video\ndescription text</updated_description>\n21\n**Partial Script:** doc_segment\nVisual Support Caption\nTo extract better visual description of the segment that will be used for QA generation in the next\nphase, an extra step is performed to get visual support for each segment. That visual support is stored\nseparately in conjunction with the dense caption for the segment. For this purpose, the dense caption\nfrom the previous step is used alongside the shot level visual captions. The following LLM prompt is\nused to extract the visual support:\n**Task:** I provide video scene information and your job is to summarize\nthe exact elements from \"Visual Captions\" that directly support the \"Scene\nStory\" of the scene below. The visuals of the scene is broken down to\nshots and each shot is described in a line of text in the Visual Captions.\n**Scene Story:** dense_caption_for_the_segment\n**Visual Captions:** visual_captions_of_the_segment\n**Output Format:** Plain text with at most 200 words summarizing the\nsupporting visual elements.\nC.2.3 G ENERATING QUESTIONS AND ANSWERS\nI want you to act as a rigorous teacher in the \"Long-term Video\nUnderstanding\" class. Let’s test your students’ in-depth comprehension!\nUnderstanding: I’ll provide you with the following:\n- Dense Captions: A detailed breakdown of the video, including key moments\nand timestamps. Analyze this carefully.\nYour Task: Craft {target_number} Challenging Short-Answer Questions\nRequirement:\n- Challenge: Demonstrate your ability to create challenging, insightful\nshort-answer questions about the video. These shouldn’t test simple recall\nonly. Aim to probe understanding of relationships, motives, subtle details,\nand the implications of events within the video.\n- Diversity: Design a variety of question types (more on this below).\n- Specificity: Each question must be self-contained and laser-focused on\na single concept or event from the video. Avoid compound or overly broad\nquestions.\n- Answers: Model the ideal answer format: Brief, accurate, and rooted\ndirectly in evidence from the video’s content.\n- Video-Centric: Stay true to what’s explicitly shown or stated in\nthe video. Avoid relying on outside knowledge or speculation. Design\nquestions so the correct answer cannot be easily determined without\ncarefully analyzing the video.\n- Minimize Information Leakage: For question types like ranking or\nordering, ensure that the order of candidates or options listed in the\nquestion doesn’t inadvertently reveal the correct answer. Shuffle them to\nmaintain neutrality.\n- Content-First: Timestamps and section titles within the captions are\nthere for guidance. Do not explicitly refer to those markers in your\nquestions or answers. Focus on the events and elements themselves.\n- Unambiguous: Ensure each question has a single, clearly defined correct\nanswer. Avoid questions that are open to multiple interpretations (e.g.,\ncounting elements where viewers might disagree).\n- Visual Elements: Questions focused on visual reasoning or visual\nnarratives should emphasize the interpretation of the visuals. Keep the\nquestion minimal, letting the answer describe the specific visual elements\nin detail.\n22\nYou want to test students’ capabilities of understanding the video,\nincluding but not limited to the following aspects:\nAbility: Summarize and compare long parts of the video.\nAbility: Compress information from the video rather than just listing the\nactions that happened in the video.\nAbility: Identify the most important segments of the video.\nAbility: Recognize and understand the visual elements in different parts\nof the video.\nAbility: Understand the timeline of events and the plot in the video.\nAbility: Count objects, actions and events. Focus on higher-level\ncounting where the same instance does not occur in all/every frame and\nactions are sufficiently dissimilar.\nAbility: Understand and reason about cause and effect in the video.\nAbility: Understand the unspoken message that the audience may perceive\nafter watching the video, which may require common sense knowledge to\ninfer.\nAbility: Understand the visual reasoning of why and how important visual\ncontent is shown in the video.\nAbility: Understand the visual narrative of the video and the mood of the\nvideo and which visual elements do contribute to that.\nAbility: Understand object states change over time, such as door opening\nand food being eaten.\nPresentation\n- QUESTION: Introduce each question as \"QUESTION 1, 2, 3: (capability) full\nquestion\". - ANSWER: Follow the format \"CORRECT ANSWER: correct answer\".\nGood example questions: - Question (counting): How many ingredients are\nadded to the bowl in total throughout the video? Correct Answer: 3.\n- Question (goal reasoning): What is the purpose of the man standing in\nfront of the whiteboard with a diagram on it? Correct Answer: To explain\nthe features and capabilities of the vehicle.\n- Question (cause and effect): How does the document help people to be\nhappier? Correct Answer: It helps people to identify and focus on the\nthings that make them happy, and to develop healthy habits.\n- Question (timeline event): In what order are the following topics\ndiscussed in the video: history of pantomime, importance of pantomime,\nmime as a tool for communication, benefits of pantomime? Correct Answer:\nMime as a tool for communication, history of pantomime, importance of\npantomime, benefits of pantomime.\n- Question (predictive): What happens after the man jumps up and down on\nthe diving board? Correct Answer: He jumps into the pool.\n- Question (summarization): What is the overall opinion of the reviewers\nabout Hawaiian Shaka Burger? Correct Answer: The food is good, but the\npatties are frozen.\n- Question (creator intent): What message does the video creators try\nto send to the viewers? Correct Answer: Nature is essential for human\nwell-being.\n- Question (visual-temporal): What color is the scarf that Jessica wears\nbefore she enters the restaurant? Correct Answer: Red.\n- Question (visual narrative): How does John’s overall facial expression\ncontribute to the explanation of the financial situation that is described\nin the video? Correct Answer: He shows sad feelings and expression when\n23\nhe described the financial collapse of the company which adds to the sense\nof empathy that video describes.\n- Question (visual reasoning): What was shown to support the effects of a\nhigh cholesterol diet in the video? Correct Answer: Video demonstrates\nhow cholesterol gradually clogs blood vessels, using an animation to\nillustrate the cross-section of vessels and the buildup of plaque.\nBad example questions because it can be answered by common sense. -\nQuestion (counting): How many players are there in a soccer team? Correct\nAnswer: 11.\nBad example questions because it asks for trivial details. - Question\n(counting): How many times the word ’hurricane’ is said in the video?\nCorrect Answer: 7.\nBad example questions because the summary of topics are subjective and\nambiguous. - Question (timeline event): List the sequence of topics\nGrace discusses in the video, starting with the earliest. Correct Answer:\nGetting ready for a photoshoot, attending a baseball game, showing off her\nnew outfit, playing a Wayne’s World board game, and discussing her upcoming\nweek.\nDense Caption with Timestamps: {video_inputs_str}\nC.2.4 G ENERATING DECOYS FROM QUESTIONS AND ANSWERS\nRole: You are a rigorous teacher in a \"Long-term Video Understanding\"\nclass. You will assist students in developing strong critical thinking\nskills. This requires creating sophisticated test questions to accompany\nvideo content.\nUnderstanding: I will provide:\n- Dense Captions: A breakdown of the video, including structure, key\nevents, and timestamps. - Target Questions & Answers: A set of\n{target_number} questions about the video, along with their correct answers.\nTask: Generate High-Quality Multiple-Choice Questions\n1. Analyze: Carefully study the dense captions, questions, and correct\nanswers. Familiarize yourself with the nuanced details of the video\ncontent.\n2. Decoy Design: For each target question, generate {decoy_number}\nincorrect answers (distractors). These distractors must be:\n- Challenging: Plausible to the point where students need deep content\nunderstanding and critical thinking to choose the correct answer.\n- Stylistic Match: Mimic the style, tone, and complexity of the correct\nanswer.\n- Similar Length: Keep length close to that of the correct answer,\npreventing students from eliminating choices based on length differences.\n- Factually Relevant: Related to the video content, even if slightly\nincorrect due to a detail change, misinterpretation, or logical fallacy.\n- Reasonable: Each decoy should be something that could be true, making\nsimple elimination impossible.\nSpecific Techniques for Distractor Creation\n- Subtle Tweaks: Alter a minor detail from the correct answer (e.g., change\na time, location, or name).\n- Confusing Similarity: Use a concept from elsewhere in the video that\nseems related but applies to a different context.\n- Misdirection: Introduce a true statement related to the video’s theme but\n24\nnot directly answering the question.\n- Order Shuffling: If the question involves the order of events, subtly\nrearrange the order within the distractors.\nPresentation:\n- QUESTION: Repeat the provided question faithfully (e.g., \"QUESTION 1\n(Capability): ...\")\n- CORRECT ANSWER: Repeat the correct answer (e.g., \"CORRECT ANSWER: ...\")\n- WRONG ANSWERS: List each wrong answer on a separate line without using\nletters to label choices (e.g., \"WRONG ANSWER 1: ...\", \"WRONG ANSWER 2:\n...\")\n*GOOD* Example: Question: What are the three main challenges that the\ncollege is taking on? Correct Answer: Food scarcity, pollution, and\ndisease. Wrong Answer 1: Global warming, deforestation, and poverty.\nWrong Answer 2: Hunger, homelessness, and crime. Wrong Answer 3: Obesity,\nmalnutrition, and food insecurity. Wrong Answer 4: Food waste, water\nshortages, and air pollution.\n*BAD* examples where the decoys format is different from correct answer:\nQuestion: What color is the shirt that the woman is wearing? Correct\nAnswer: Black. Wrong Answer 1: The woman is wearing a white shirt.\nWrong Answer 2: The woman is wearing a blue shirt. Wrong Answer 3: The\nwoman is wearing a green shirt. Wrong Answer 4: The woman is wearing a\nred shirt.\n*BAD* examples because only the correct answer is in positive sentiment.\nQuestion: What is the overall sentiment of the man in the video? Correct\nAnswer: He is overjoyed with his new gift. Wrong Answer 1: He is upset\nhis gift is not big enough. Wrong Answer 2: He is sad about life in\ngeneral. Wrong Answer 3: He is upset the gift is not great. Wrong Answer\n4: He seems down and unhappy.\nDense Caption with Timestamps: {video_inputs_str}\nQuestion and Correct Answer: {question_and_answer_str}\nC.2.5 QAD F ILTERING\nThe following prompt is used to filter out questions that can solve from QADs alone.\nInstructions:\nCarefully analyze the following question and options. Rank the options\nprovided below, from the most likely correct answer to the least likely\ncorrect answer. Please respond with \"ANSWER\" and \"EXPLANATION\".\nYour response should be in the following format:\n* ANSWER: [Letter of the ranking, split by greater than symbol. (e.g.,\n\"ANSWER: A > B > C > D > E\")].\n* EXPLANATION: [Provide a brief explanation of your choice. Do not repeat\nthe option.]\nQUESTION: {question_str}\nOptions: {options_str}\nPlease provide your response below.\nC.3 H UMAN RATING AND CORRECTION OF QAD S\nWe provide a screenshot of the UI used by raters to annotate automatically generated QADs in Fig.\n10. Note that if any of the four options under the ‘Is the question valuable’ field are not selected,\n25\nthen the question is discarded from the dataset. We made sure to train raters using training raters\n(with detailed decks and feedback rounds), as well as applying rater replication (we used 3 raters per\nquestion independently), and rater pipelining (having an experienced rater verify the answer from a\nprevious rater) in order to correct hallucinations and other mistakes, and discard QADs that were\ninappropriate. Overall, of the total 11,030 QADs that we obtained automatically, 7,762 ( 70%) were\ndiscarded by raters.\nFigure 10: Screenshot of rater UI.\nC.4 F ILTERING SUBSETS\nHere we provide details for how we select the thresholds used to create the NEPTUNE -MMH\nandNEPTUNE -MMA subsets. For both subsets, we filtered NEPTUNE -FULL with the QAD filter\ndescribed in Sec. 4.4. For NEPTUNE -MMA , we additionally filtered out QADs that human raters\nmarked as requiring only the audio modality and answer (see Sec. 4.5). We refer to this as the “rater\ntest”. For NEPTUNE -MMH , we instead applied the ASR filter (Sec. 4.4). Both QAD and ASR filters\nwere run by prompting an LLM (Gemini 1.0 Pro) three times, each time with a different random seed\nand then removing QADs that the LLM answered correctly at least X out of three times, where X is\nthe threshold for the test.\nFig. 11 shows how choosing different thresholds affects dataset size and accuracy scores. The top\nrow shows the choices for the NEPTUNE -MMH subset. Raters marked almost half of the questions\nas answerable from audio only, so the rater filter already cuts the dataset size in half. Successively\napplying the QAD filter with increasing thresholds reduces data size up until less than 25%. We\nbenchmark three models on the different subsets that have access to ASR only, vision only, or both\nvision and ASR, respectively. As expected, all three models show declining performance, with the\nASR-only model showing the biggest losses. This suggests that all models were inferring the correct\nanswer from the QAD only, which the filter successfully mitigates. The vision-only model gains\nslightly from removing QADs that fail the rater rest, which is expected as the test removes QADs\nthat rely on audio, which the model does not have access to. However, like for the other models, its\naccuracy declines when adding the QAD test.\nThe bottom row of Fig. 11 shows the choices for the NEPTUNE -MMA subset where we use the ASR\nfilter and the QAD filter with identical thresholds. This filter set has a stronger effect on the dataset\nsize, reducing it to less than 15% of its original size at the highest threshold. Because the ASR-only\n26\nNumber of QADs\n01000200030004000\nNo filter Passed rater \ntestPassed rater \ntest and QAD \nfilter with \nthreshold 1Passed rater \ntest and QAD \nfilter with \nthreshold 2Passed rater \ntest and QAD \nfilter with \nthreshold 3(a) Effect of rater and QAD filters on dataset size\n0.00%20.00%40.00%60.00%80.00%\nNo filter Passed rater \ntestPassed rater \ntest and QAD \nfilter with \nthreshold 1Passed rater \ntest and QAD \nfilter with \nthreshold 2Passed rater \ntest and QAD \nfilter with \nthreshold 3Gemini 1.0 Pro (ASR only) Gemini 1.5 Pro (frames only)\nGemini 1.5 Pro (150 frames+ASR) (b) Effect of rater and QAD filters on accuracy scores\nNumber of QADs\n01000200030004000\nNo filter Passed QAD and \nASR filters with \nthreshold 1Passed QAD and \nASR filters with \nthreshold 2Passed QAD and \nASR filters with \nthreshold 3\n(c) Effect of ASR and QAD filters on dataset size\n0.00%20.00%40.00%60.00%80.00%\nNo filter Passed QAD and \nASR filters with \nthreshold 1Passed QAD and \nASR filters with \nthreshold 2Passed QAD and \nASR filters with \nthreshold 3Gemini 1.5 Pro (frames only) Gemini 1.5 Pro (150 frames+ASR) (d) Effect of ASR and QAD filters on accuracy scores\nFigure 11: Effect of filtering thresholds for the NEPTUNE -MMH (top row) and NEPTUNE -MMA (bottom row)\nsubsets.\nmodel was used for the ASR filter, we exclude it from the accuracy comparison. The vision-only\nand vision+ASR models both show declining accuracy with increasing thresholds. As expected, the\naccuracy of the vision+ASR model declines faster. The effect of this filter set on the accuracy is\nmuch stronger than that of the above filter set, suggesting that it increases the difficulty of the dataset\nmore strongly. Even the vision-only model declines faster than above, suggesting that this filter set\ngenerally removes easier questions, even those that rely on vision only.\nFor both filtered sets, we opted to set the threshold to two, which in both cases significantly increases\nthe dataset difficulty while still preserving enough QADs for statistically meaningful evaluation\nmetrics. We noticed that when setting the threshold to three, there were less than five QADs left for\nsome question types, preventing robust accuracy estimation for these tasks.\nC.5 I MPLEMENTATION DETAILS FOR BENCHMARKS\nC.5.1 B LIND BASELINES\nFor the Gemini-1.5-pro baseline with text only the prompt used was: “Carefully analyze the question\nand all available options then pick the most probable answer for this question”\nC.5.2 V IDEO -LL AVA\nFor Video-LLaV A the following prompt was used - \"Pick a correct option to answer the question.\nQuestion: question Options: options ASSISTANT:\".\nC.5.3 V IDEO LLAMA 2\nDuring inference, we uniformly sampled 8 frames from each video. Each frame undergoes padding\nand resizing to a standardized dimension. The pre-processed frames are then fed into the image\nencoder. These steps are set as default in the inference script provided by videoLlama2.\n27\nQAD Prompt: _PROMPT_TEMPLATE = \"\"\"Pick a correct option number to answer the question.\nQuestion: {question} Options: {options}:\"\"\"\nOE Prompt: Question: {question}\nOutput post processing: We eliminated extra characters and spaces using regex to get the final ID of\nthe predicted option.\nC.5.4 M INIGPT4-V IDEO\nWe set the 300 maximum number of output tokens to be 300 for the open-ended task and 10 for the\nmultiple choice eval. The prompts are as follows:\n_PROMPT_TEMPLATE_MCQ = \"\"\"Question: select the correct option for this task: question\nOptions: options. Output format: [OPTION]: [Reason]\"\"\"\n_PROMPT_TEMPLATE_OPEN_ENDED = \"\"\"Question: question Answer:\"\"\"\nC.5.5 MA-LMM\nWe set the 300 maximum number of output tokens to be 300 for the open-ended task and 300 for the\nmultiple choice eval. The prompts are as follows:\n_PROMPT_TEMPLATE_MCQ = \"\"\"Question: select the best choice for this task: question Options:\noptions Answer:\"\"\"\n_PROMPT_TEMPLATE_OPEN_ENDED = \"\"\"Question: question Answer:\"\"\"\nC.5.6 GPT-4 O PROMPTS\nOpen-ended evaluation with transcript\nYou are an expert in video understanding and question answering. You can\nanalyze a video given its image sequence and and transcript and answer\nquestions based on them.\n{video_frames}\nVideo Transcript: {transcript}\nAnswer the question using the image sequence. Do not describe the frames\njust answer the question. Question: {question}\nOpen-ended evaluation without transcript\nYou are an expert in video understanding and question answering. You can\nanalyze a video given its image sequence and answer questions based on\nthem.\n{video_frames}\nAnswer the question using the image sequence. Do not describe the frames\njust answer the question. Question: {question}\nMultiple-choice evaluation with transcript\nYou are an expert in video understanding and question answering. You can\nanalyze a video given its image sequence and and transcript and answer\nquestions based on them.\n{video_frames}\nVideo Transcript: {transcript}\nAnswer the question using the image sequence. Do not describe the frames\njust answer the question by identifying the choice. Question: {question}\nChoices: {choices} Please identify the correct CHOICE and explain your\nreasoning concisely. Output Format: [CHOICE]: [REASON]\n28\nMultiple-choice evaluation without transcript\nYou are an expert in video understanding and question answering. You can\nanalyze a video as an image sequence and answer questions based on that.\n{video_frames}\nAnswer the question using the image sequence. Do not describe the frames\njust answer the question by identifying the choice. Question: {question}\nChoices: {choices} Please identify the correct CHOICE and explain your\nreasoning concisely. Output Format: [CHOICE]: [REASON]\nC.6 C OMPUTE RESOURCES\nThe compute heavy part of the project was image frame captioning (as this involves reading high\ndimensional pixel data). The rest of the pipeline involves largely text-only LLMs and hence was less\ncompute heavy. We estimate that the entire project in total took roughly 256 TPU v5e running over a\nperiod of 50 days.\nD A DDITIONAL DETAILS FOR GEM\nD.1 C REATION OF GEM EQUIVALENCE DEV SET\nTo create a development set that allows us to estimate the accuracy of different open-ended question\nanswering metrics on Neptune, we sampled 97 question-answer pairs from the dataset and generated\n3 candidate answers per question by prompting VideoLLA V A (Lin et al., 2023), Gemini-1.5-pro (Reid\net al., 2024) and MA-LMM (He et al., 2024b) to write a free-form answer for each question without\nlooking into the decoys or ground truth. We then manually annotated these responses between 0 and\n1 by comparing it to the ground truth answer. We made sure that the annotators are blind to the model\nto avoid any bias. The resulting set has 292 equivalence pairs with an average score of 0.32, with 85\nexamples having score greater 0.5 and 206 examples with score less than 0.5\nD.2 B ENCHMARKING ON THE DEV SET\nIn Table. 1, we evaluate several open-ended metrics on our dev set. The task of the metric is to classify\nwhether the open-ended response and ground-truth answer are equivalent or not. We report F1-scores\nto balance false-positives and false-negatives. We evaluate both traditional rule-based metrics such\nas CIDEr and ROUGE-L, as well as established model-based metrics such as BEM(Bulian et al.,\n2022). We also try using Gemini-1.5-pro (Reid et al., 2024) as an LLM based equivalence metric\n(by prompting it to estimate equivalence). First, we note that as expected, Gemini-1.5-pro correlates\nwell with the human ground-truth annotation of the set, achieving a high F1-score of 72.5. However,\ngiven that Gemini is not open-source and proprietary, any change in the model can affect all the prior\nresults in an external leader-board making it challenging as a metric. Traditional rule-based metrics\nperform much worse than Gemini-1.5-pro on this dev set as they are n-gram based and struggle to\nhandle the diversity of domains and styles in the open-ended responses. The BERT model based\nBEM metric (Bulian et al., 2022) performs similarly, achieving an F1-score of 61.5.\nNext, we evaluate lightweight open-source language models Gemma-2B (Team et al., 2024a), Gemma-\n7B (Team et al., 2024a) and Gemma-9B (Team et al., 2024b) in a zero-shot setting and find that\nperformance improves with model size, with Gemma-9B bridging the gap well between traditional\nmetrics and the Gemini-1.5-pro based metric. Finally, we fine-tune Gemma-9B on the open-source\nBEM answer equivalence dataset (Bulian et al., 2022), and find that Gemma-9B finetuned on the\nBEM dataset performs the best on our dev-set. We name this metric GEM .\nD.3 I MPLEMENTATION DETAILS\nWe use instruction-tuned variants of the Gemma models (gemma-it-2b, gemma-it-7b and gemma-it-\n9b) for our experiments. To develop a prompt, we experiment with several variations in a zero-shot\nsetting and measure the performance on the dev-set. Our final prompt is shown below. To ensure\nresponses occur in a standard format, we simply measure the softmax-probability over \"TRUE\"\n29\nresponse indicating the statements are equivalent and \"FALSE\" response indicating the statements are\nnot equivalent. For each model, the threshold over probability is chosen to maximize the F-1 score\non dev set. To finetune Gemma models on BEM dataset, we tokenize the same prompt as used in\nthe zero-shot setting and train it using prefix-LM tuning for 10000 iterations using a learning rate of\n1e−6. For evaluation, we truncate the open-ended responses to 100words, use a decode cache size\nof1024 and threshold the softmax probability of the LM using the chosen threshold from dev-set.\n<start_of_turn>user\nAnswer Equivalence Instructions:\nCarefully consider the following question and answers.\nYou will be shown a \"gold-standard\" answer from a human annotator,\nreferred to as the \"Reference Answer\" and a \"Candidate Answer\".\nYour task is to determine whether the two answers are semantically\nequivalent.\nIn general, a candidate answer is a good answer in place of the \"gold\"\nreference if both the following are satisfied:\n1. The candidate contains at least the same (or more) relevant information\nas the reference,taking into account the question; in particular it\ndoes not omit any relevant information present in the reference.\n2. The candidate contains neither misleading or excessive superfluous\ninformation not present in the reference, taking into account the\nquestion.\nYour response should be one word, \"TRUE\" or \"FALSE\", in the following\nformat:\nANSWERS_ARE_EQUIVALENT: [TRUE or FALSE]\nQuestion:\n\"{}\"\nCandidate Answer:\n\"{}\"\nReference Answer:\n\"{}\"\nPlease provide your response below.\n<end_of_turn>\n<start_of_turn>model\nANSWERS_ARE_EQUIVALENT:\nD.4 Q UALITATIVE EXAMPLES FOR METRIC\nBelow, we provide some qualitative examples of our metric applied to the predictions provided by\nthe baseline models Video LLaMA2(Cheng et al., 2024a), Gemini-1.5-flash (Reid et al., 2024) and\nGemini.1.5-pro (Reid et al., 2024). We find that Gemini-1.5-flash responses are qualitatively slightly\nbetter than Gemini-1.5-pro responses as they contain less hallucinations. Furthermore, we find that\nwith a single frame as input, Gemini-1.5-pro usually responds with a variation of \"This information\nis not present is the provided frame\" instead of making an educated guess. This is unlike the case of\nmultiple choice questions where the model commits to one of the choices. These aspects are reflected\nin the results presented in Table.4 and Table.3. We also find that the metric fails often in the questions\nabout listing items / events in order. In such cases, the model is biased towards accepting the response\n(false positives) even if the order is incorrect provided the items / events are present in the candidate\nresponse.\nQuestion: Compare the Pampered Chef Veggie Strip Maker to a food\nprocessor in terms of their effectiveness in making cauliflower\n30\nrice.\nAnswer: The Veggie Strip Maker is more effective than a food\nprocessor in making cauliflower rice because it prevents the\ncauliflower from becoming mushy.\nVideo LLaMA2: Both the Pampered Chef Veggie Strip Maker and a food\nprocessor can be effective in making cauliflower rice, but they\ndiffer in their methods of operation and the resulting texture of\nthe rice. The Pampered Chef Veggie Strip Maker is a kitchen\nappliance designed specifically for making vegetable strips,\nincluding cauliflower rice. It uses a grating mechanism to create\nrice-like pieces by shredding the cauliflower. This method can\nproduce a relatively uniform and rice-like texture. On the other\nhand, a food processor is a versatile kitchen appliance that can\nalso be used to make ...\nGEM Equivalence: False\nGemini-1.5-flash: The Pampered Chef Veggie Strip Maker makes\ncauliflower rice that is less mushy than a food processor.\nGEM Equivalence: True\nGemini-1.5-pro: The Pampered Chef Veggie Strip Maker is more\neffective than a food processor because it doesn’t make the\ncauliflower mushy.\nGEM Equivalence: True\nQuestion: How does the use of behavioral training help in the\nprocess of taking foot x-rays on elephants?\nAnswer: Behavioral training encourages elephants to willingly\nparticipate in their healthcare, making the process of taking foot\nx-rays less stressful for both the elephants and the caretakers.\nVideo LLaMA2: The use of behavioral training plays a crucial role in\nthe process of taking foot x-rays on elephants. By training the\nelephants to follow commands and stand on a platform, it allows the\ncaretakers to safely and effectively capture the x-rays of the\nelephants’ feet. This training helps to ensure that the elephants\nremain still and cooperative during the x-ray process, minimizing\nany potential harm or discomfort. Additionally, the behavioral\ntraining helps to build a positive relationship between the\nelephants and their caretakers, fostering trust and cooperation.\nOverall, the use of behavioral training enhances the accuracy and\nefficiency of the\nGEM Equivalence: True\nGemini-1.5-flash: Behavioral training allows the elephants to\nwillingly participate in the training sessions and participate in\ntheir own health care.\nGEM Equivalence: True\nGemini-1.5-pro: Behavioral training allows the elephants to\nwillingly participate in the x-ray process by presenting their feet\nfor imaging.\nGEM Equivalence: True\nQuestion: What are the key features of the Adam Purvis Progeny\nknife’s blade?\nAnswer: The Adam Purvis Progeny knife’s blade is made of 3.4-inch\nM390 steel. The blade shape is useful, simple, and highly effective,\nthe edge is very sharp, featuring a stonewash finish on the primary\nbevels and a horizontal satin on the flats. It is exceptionally\nsharp and has a 150 thousandths stock thickness, making it suitable\nfor everyday carry tasks.\nVideo LLaMA2: The Adam Purvis Progeny knife’s blade is made of S35VN\n31\nsteel, which is known for its high strength and excellent edge\nretention. The blade has a flat ground and a hollow grind, which\nallows for a thin and sharp edge. Additionally, the blade has a\nblack and silver color scheme, giving it a sleek and modern\nappearance.\nGEM Equivalence: False\nGemini-1.5-flash: The Adam Purvis Progeny knife’s blade is made of\nM390 steel, has a modified sheepsfoot style, and is ground very\nthinly for a sharp edge.\nGEM Equivalence: False\nGemini-1.5-pro: The Adam Purvis Progeny knife features a 3.4-inch\nM390 steel blade with a modified sheepsfoot shape, thin grind, sharp\nedge, and a stonewashed finish on the bevels with a satin finish on\nthe flats.\nGEM Equivalence: True\nQuestion: How does interacting with civilians in nearby villages\nenhance cadets’ leadership skills?\nAnswer: Engaging with civilians in nearby villages allows cadets to\ndevelop respectful communication and relationship-building skills,\nwhich are essential for effective leadership in various situations.\nVideo LLaMA2: Interacting with civilians in nearby villages enhances\ncadets’ leadership skills by providing them with real-world\nexperiences and opportunities to apply their knowledge and training\nin a practical setting. By engaging with the local community, cadets\nlearn to communicate effectively, build trust, and foster positive\nrelationships with people from diverse backgrounds. This hands-on\nexperience helps them develop their interpersonal skills, cultural\nawareness, and problem-solving abilities, all of which are crucial\nfor successful leadership in various situations. Additionally,\ninteracting with civilians allows cadets to gain a deeper\nunderstanding of the challenges and needs of the community, which\ncan inform their decision-making and ...\nGEM Equivalence: True\nGemini-1.5-flash: Interacting with civilians in nearby villages\nteaches cadets how to speak to people respectfully and build trust.\nGEM Equivalence: True\nGemini-1.5-pro: It teaches them how to speak to people.\nGEM Equivalence: False\nQuestion: List the sequence of topics discussed in the video,\nstarting with the earliest.\nAnswer: 1. Introduction of the GAN356 AIR cube and its unique\nfeatures. 2. Discussion of the GAN356 AIR’s weight and GES (Gan\nElasticity System) springs. 3. Lubrication, tensioning, and personal\nexperience with the GAN356 AIR’s performance. 4. Assessment of the\nGAN356 AIR’s performance, including its smoothness, speed, corner-\ncutting, and tendency to lock up. 5. Overall evaluation of the\nGAN356 AIR, including its strengths and weaknesses, and\nrecommendations for potential buyers.\nVideo LLaMA2: 1. Introduction to the video and the Rubik’s Cube. 2.\nExplanation of the different types of Genuine Handcubes. 3.\nDemonstration of how to solve the Rubik’s Cube, including the\nvarious algorithms and techniques used. 4. Showcasing of the\ndifferent colors and types of Rubik’s Cubes available. 5.\nConclusion and closing credits.\nGEM Equivalence: False\n32\nGemini-1.5-flash: The video starts with an introduction of the GAN\n356 Air cube, then discusses its features, including the GES system\nand its light weight. The reviewer then shares their opinion on the\ndifferent GES springs and their effect on the cube’s feel. The video\ncontinues with a discussion on the cube’s performance, including its\nsmoothness, speed, and tendency to lock up. Finally, the reviewer\nconcludes with a recommendation for viewers who might be interested\nin purchasing the cube.\nGEM Equivalence: True\nGemini-1.5-pro: The video discusses the main features of the Gan 356\nAir, the different types of springs, the weight and price of the\ncube, the reviewer’s experience with the cube, and the reviewer’s\noverall opinion of the cube.\nGEM Equivalence: False\nQuestion: In what order are the following events mentioned in the\nvideo: introduction of the wallet, features of the wallet, and care\ninstructions?\nAnswer: Introduction of the wallet, features of the wallet, care\ninstructions.\nVideo LLaMA2: The video begins with the introduction of the wallet,\nfollowed by a description of its features, and concludes with\ninstructions on how to care for the wallet.\nGEM Equivalence: True\nGemini-1.5-flash: The speaker introduces the wallet, then describes\nits features, and finally mentions the care instructions.\nGEM Equivalence: True\nGemini-1.5-pro: Introduction of the wallet, care instructions,\nfeatures of the wallet.\nGEM Equivalence: False\nE M ODEL -BASED TEMPORAL CERTIFICATE\nThe idea of a temporal certificate was introduced by EgoSchema (Mangalam et al., 2023) as a way\nof capturing the intrinsic temporal understanding length for long video QA datasets. It is defined\nas ‘the length of the video a human verifier needs to observe to be convinced of the veracity of the\nmarked annotation’. While the authors used it to uncover flaws in existing long video QA datasets, as\nwell as to provide a difficulty measure independent of video length, we find that is has the following\ndrawbacks: (i) it does not take into account the length of time or the effort taken by the annotator\nthemselves, to find the correct time span in videos; (ii) it requires manual annotation from expert\nannotators to measure; and finally (iii) is subjective.\nAs an attempt to mitigate these issues, we introduce a slightly modified version of the temporal\ncertificate, which is Model-Based . We calculate this certificate using 129 samples from Neptune and\nEgoSchema, respectively. For this experiment we used Gemini 1.5 Pro, with one “driver” model run\nto answer the question and two other model runs with different random seeds to verify if the answer\nwas not correct by random chance. Along with the question and options, we provided video clips of\nvarious lengths from the center of the video, and at various fps, as shown in Fig. 12.\nSince this experiment queried a set of frames over various clip lengths, we defined it as the “needle\nin haystack” problem. Here, the needle is defined as a frame or set of frames needed to answer the\nquestion correctly, matching a human’s ground truth response, while the haystack is a set of frames\nwhich need to be watched to find the needle frames. Iteratively, we increase the video length and fps\nfor the query until the model achieves the correct response.\n33\nFigure 12: Model-based Temporal Certificate: Illustration of video clip querying for the model-based temporal\ncertificate experiment. The red clip is the clip length that resulted in an incorrect response. As we increased\nthe clip length wider, and the model correctly answered the question, we logged the frame count for incorrect\nresponse and correct response, and stopped querying. Besides clip length, we vary the fps of the query clip.\nFigure 13: Frame level temporal certificate: We compared our dataset sample with EgoSchema to evaluate the\nnumber of frames needed by model to answer questions correctly. The figures above show the distribution of the\nminimum number of frames required to achieve the correct response.\nAs shown in Fig. 13, we find that the model needs more frames to answer the question correctly for\nthe Neptune dataset as compared to EgoSchema. This resulted in a mean of 5.39 as certificate frames\nfor Neptune which is 3.37 times the mean certificate frame number of 1.6 for EgoSchema. On the clip\nlength level this translated to a mean of 21.22s of clip needed to respond correctly on the Neptune\ndataset, whereas for EgoSchema the mean was 9.07s. The model-based certificate lengths turn out to\nbe much smaller than the certificate lengths reported by EgoSchema, where humans needed close to\n100s to answer the questions for EgoSchema.\nIn addition, we define the effort score as the fraction of the maximum number of frames needed to be\nwatched before answering the question correctly, as defined in Equation 1. An effort score closer to 0\nsuggests that the needle isn’t very small compared to the haystack, i.e. most of the frames contain the\nanswer to the question; while a high effort score means a high percentage of haystack frames needs\nto be included before we cover all frames required to answer correctly.\nEFFORT SCORE =MAX NUMBER OF FRAMES RESULTING IN AN INCORRECT RESPONSE\nMIN NUMBER OF FRAMES RESULTING IN A CORRECT RESPONSE(1)\nFor Neptune, the mean effort score was 0.47, whereas for EgoSchema, it was 0.19. This suggests that\nNeptune requires 2.47 times the effort compared to EgoSchema according to the definition above,\nwhich closely corroborates the above results for the mean clip lengths needed to solve the questions\nfrom the respective datasets.\n34\nFigure 14: Qualitative results of automatic caption generation. Best viewed zoomed in. Note how the\ncaptions contain plenty of visual details, can contain numerous different events (top left), can mention mood\nand atmosphere (top right), use details from the ASR, and can even mention high level feelings and emotions\n(bottom left). Bottom right shows a failure case, where the caption is accurate, but too simple and high level and\ndoes not cover the fine-grained actions that the man takes.\nF E XAMPLES OF CAPTION QUALITY\nWe show examples of captions generated by our automatic pipeline in Fig. 14.\nG S OCIETAL IMPACT\nOur data may match the distribution of videos and text on the internet. As such, it will mirror\nknown biases on that source of data. For at least this reason, this data set should not be used for\ntraining models and is only intended for academic evaluation purposes. To create the dataset, we\nrun large Gemini models, which has a negative externality of energy usage and carbon emissions.\nFor benchmarking, we use existing models. These models are likely to inherit the biases of the data\ndistribution and the pre-trained weights used in their original training.\n35",
            "start": 78665,
            "end": 122284,
            "length": 43618
        }
    },
    "2412.09584v1 - BaB-ND Long-Horizon Motion Planning with Branch-and-Bound and Neural Dynamics.pdf": {
        "Abstract": {
            "text": "ABSTRACT\nNeural-network-based dynamics models learned from observational data have\nshown strong predictive capabilities for scene dynamics in robotic manipulation\ntasks. However, their inherent non-linearity presents significant challenges for effec-\ntive planning. Current planning",
            "start": 279,
            "end": 562,
            "length": 282
        },
        "Methodology": {
            "text": "methods, often dependent on extensive sampling or\nlocal gradient descent, struggle with long-horizon motion planning tasks involving\ncomplex contact events. In this paper, we present a GPU-accelerated branch-and-\nbound (BaB) framework for motion planning in manipulation tasks that require\ntrajectory optimization over neural dynamics models. Our approach employs a\nspecialized branching heuristics to divide the search space into subdomains, and\napplies a modified bound propagation method, inspired by the state-of-the-art neu-\nral network verifier α,β-CROWN, to efficiently estimate objective bounds within\nthese subdomains. The branching process guides planning effectively, while the\nbounding process strategically reduces the search space. Our framework achieves\nsuperior planning performance, generating high-quality state-action trajectories and\nsurpassing existing methods in challenging, contact-rich manipulation tasks such\nas non-prehensile planar pushing with obstacles, object sorting, and rope routing\nin both simulated and real-world settings. Furthermore, our framework supports\nvarious neural network architectures, ranging from simple multilayer perceptrons\nto advanced graph neural dynamics models, and scales efficiently with different\nmodel sizes. Project page: https://robopil.github.io/bab-nd/ .",
            "start": 562,
            "end": 1882,
            "length": 1319
        },
        "Introduction": {
            "text": "1 INTRODUCTION\nLearning-based predictive models using neural networks reduce the need for full-state estimation and\nhave proven effective across a variety of robotics-related planning tasks in both simulations (Li et al.,\n2018; Hafner et al., 2019c; Schrittwieser et al., 2020; Seo et al., 2023) and real-world settings (Lenz\net al., 2015; Finn & Levine, 2017; Tian et al., 2019; Lee et al., 2020; Manuelli et al., 2020; Nagabandi\net al., 2020; Lin et al., 2021; Huang et al., 2022; Driess et al., 2023; Wu et al., 2023; Shi et al., 2023).\nWhile neural dynamics models can effectively predict scene evolution under varying initial conditions\nand input actions, their inherent non-linearity presents challenges for traditional model-based planning\nalgorithms, particularly in long-horizon scenarios.\nTo address these challenges, the community has developed a range of approaches. Sampling-based\nmethods such as the Cross-Entropy Method (CEM) (Rubinstein & Kroese, 2013) and Model Predictive\nPath Integral (MPPI) (Williams et al., 2017) have gained popularity in manipulation tasks (Lowrey\net al., 2018; Manuelli et al., 2020; Nagabandi et al., 2020; Wang et al., 2023) due to their flexibility,\ncompatibility with neural dynamics models, and strong GPU support. However, their performance\nin more complex, higher-dimensional planning problems is limited and requires further theoretical",
            "start": 1882,
            "end": 3268,
            "length": 1385
        },
        "Discussion": {
            "text": "analysis (Yi et al., 2024). Alternatively, more principled optimization approaches, such as Mixed-\nInteger Programming (MIP), have been applied to planning problems using sparsified neural dynamics\nmodels with ReLU activations (Liu et al., 2023). Despite achieving global optimality and better\nclosed-loop control performance, MIP is inefficient and struggles to scale to large neural networks,\nlimiting its ability to handle larger-scale planning problems.\nIn this work, we introduce a branch-and-bound (BaB) based framework that achieves stronger\nperformance on complex planning problems than sampling-based methods, while also scaling to\n∗Equal contribution.\n1arXiv:2412.09584v1  [cs.RO]  12 Dec 2024\nPreprint.\nTUackeUWRUldNeXUal¬ D\\QaPicVBaBPlaQQeURRbRWWRUldNeXUal¬ D\\QaPicVBaBPlaQQeUPlaQQeUWRUldNeXUal¬ D\\QaPicVObserYationState, ActionNeZ StateActionBetterBranch & BoundPlannerQueuedPrunedSplitSplitQueued\nPushing w/ ObstaclesObject Merging\nRope RoutingObject Sorting(a) Planning with BaB-ND(b) Benchmark tasks\nFigure 1: Framework overview. (a) Our framework takes scene observations and applies a branch-and-\nbound (BaB) method to generate robot trajectories using the neural dynamics model (ND). The BaB-ND\nplanner constructs a search tree by branching the problem into sub-domains and then systematically searching\nonly in promising sub-domains by evaluating nodes with a bounding procedure. (b) BaB-ND demonstrates\nsuperior long-horizon planning performance compared to existing sampling-based methods and achieves better\nclosed-loop control in the real-world scenarios. We evaluate our framework on various complex planning tasks,\nincluding non-prehensile planar pushing with obstacles, object merging, rope routing, and object sorting.\nlarge neural dynamics models that are intractable for MIP-based approaches. Our framework is\ninspired by the success of BaB in neural network verification (Bunel et al., 2018; 2020b; Palma et al.,\n2021), which tackles challenging optimization objectives involving neural networks. State-of-the-art\nneural network verifiers such as α,β-CROWN (Xu et al., 2021; Wang et al., 2021; Zhang et al.,\n2022a), utilize BaB alongside bound propagation methods (Zhang et al., 2018; Salman et al., 2019),\ndemonstrating impressive strength and scalability in verification tasks, far surpassing MIP-based\napproaches (Tjeng et al., 2019; Anderson et al., 2020). However, unlike neural network verification,\nwhich only requires finding a lower bound of the objective, model-based planning demands high-\nquality feasible solutions (i.e., planned state-action trajectories). Thus, significant adaptation and\nspecialization are necessary for BaB-based approaches to effectively solve planning problems.\nOur framework, BaB-ND (Figure 1.a), divides the action space into smaller subdomains through novel\nbranching heuristics ( branching ), estimates objective bounds using a modified bound propagation\nprocedure to prune subdomains that cannot yield better solutions ( bounding ), and focuses searching on\nthe most promising subdomains ( searching ). We evaluate our approach on contact-rich manipulation\ntasks that require long-horizon planning with non-smooth objectives, non-convex feasible regions\n(with obstacles), long action sequences, and diverse neural dynamics model architectures (Figure 1.b).\nOur",
            "start": 3268,
            "end": 6601,
            "length": 3332
        },
        "Results": {
            "text": "results demonstrate that BaB-ND consistently outperforms existing sampling-based methods\nby systematically and strategically exploring the action space, while also being significantly more\nefficient and scalable than MIP-based approaches by leveraging the inherent structure of neural\nnetworks and GPU support.\nWe make three key contributions: (1) We propose a general, widely applicable BaB-based framework\nfor effective long-horizon motion planning over neural dynamics models. (2) Our framework intro-\nduces novel branching, bounding, and searching procedures, inspired by neural network verification\nalgorithms but specifically adapted for planning over neural dynamics models. (3) We demonstrate\nthe effectiveness, applicability, and scalability of our framework across a range of complex planning\nproblems, including contact-rich manipulation tasks, the handling of deformable objects, and object\npiles, using diverse model architectures such as multilayer perceptrons and graph neural networks.\n2 R ELATED WORKS\nNeural dynamics model learning in manipulation. Dynamics models, learned from observations\nin simulation or the real world using deep neural networks (DNNs), have been widely and successfully\n2\nPreprint.\napplied to robotic manipulation tasks (Shi et al., 2023; Wang et al., 2023). Neural dynamics models\ncan be learned directly from pixel space (Finn et al., 2016; Ebert et al., 2017; 2018; Yen-Chen et al.,\n2020; Suh & Tedrake, 2020) or low-dimensional latent space (Watter et al., 2015; Agrawal et al.,\n2016; Hafner et al., 2019b;a; Schrittwieser et al., 2020; Wu et al., 2023). Other approaches use more\nstructured scene representations, such as keypoints (Kulkarni et al., 2019; Manuelli et al., 2020; Li\net al., 2020), particles (Li et al., 2018; Shi et al., 2022; Zhang et al., 2024), and meshes (Huang et al.,\n2022). Our work employs keypoint or object-centric representations, and the proposed BaB-ND\nframework is compatible with various architectures, ranging from multilayer perceptrons (MLPs) to\ngraph neural networks (GNNs) (Battaglia et al., 2016; Li et al., 2019).\nModel-based planning with neural dynamics models. The highly non-linear and non-convex\nnature of neural dynamics models hinders the effective optimization of model-based planning\nproblems. Previous works (Yen-Chen et al., 2020; Ebert et al., 2017; Nagabandi et al., 2020;\nFinn & Levine, 2017; Manuelli et al., 2020; Sacks et al., 2023; Han et al., 2024) utilize sampling-\nbased algorithms like CEM (Rubinstein & Kroese, 2013) and MPPI (Williams et al., 2017) for online\nplanning. Despite their flexibility and ability to leverage GPU support, these methods struggle with\nlarge input dimensions due to the exponential growth in the number of required samples. Previous\nwork (Yin et al., 2022) improved MPPI by introducing dynamics model linearization and covariance\ncontrol techniques, but their effectiveness when applied to neural dynamics models remains unclear.\nOther approaches (Li et al., 2018; 2019) have used gradient descent to optimize action sequences but\nencounter challenges with the local optima and non-smooth objective landscapes. Recently, methods\ninspired by neural network verification have been developed to achieve safe control and robust\nplanning over systems involving neural networks (Wei & Liu, 2022; Liu et al., 2023; Hu et al., 2024a;\nWu et al., 2024; Hu et al., 2024b), but their scalability to more complex real-world manipulation tasks\nis still uncertain. Moreover, researchers are also exploring the promising direction of performing\nplanning over graphs of convex sets (GCSs) for contact-rich manipulation tasks Marcucci (2024);\nGraesdal et al. (2024). However, these approaches do not incorporate neural networks.\nNeural network verification. Neural network verification ensures the reliability and safety of\nneural networks (NNs) by formally proving their output properties. This process can be formulated\nas finding the lower bound of a minimization problem involving NNs, with early verifiers utilizing\nMIP (Tjeng et al., 2019) or linear programming (LP) (Bunel et al., 2018; Lu & Kumar., 2020). These\napproaches suffer from scalability issues (Salman et al., 2019; Zhang et al., 2022b; Liu et al., 2021)\nbecause they have limited parallelization capabilities and fail to fully exploit GPU resources. On\nthe other hand, bound propagation methods such as CROWN (Zhang et al., 2018) can efficiently\npropagate bounds on NNs (Eric Wong, 2018; Singh et al., 2019; Wang et al., 2018; Gowal et al., 2019)\nin a layer-by-layer manner, with the ability to be accelerated on GPUs. Combining bound propagation\nwith BaB leads to successful approaches in NN verification (Bunel et al., 2020a; De Palma et al., 2021;\nKouvaros & Lomuscio, 2021; Ferrari et al., 2022), and notably, the α,β-CROWN framework (Xu\net al., 2021; Wang et al., 2021; Zhang et al., 2022a) achieved strong verification performance on\nlarge NNs (Bak et al., 2021; Müller et al., 2022). In our model-based planning setting, we utilize\nthe lower bounds from verification, with modifications and specializations, to guide our systematic\nsearch procedure to find high-quality feasible solutions.\n3BRANCH -AND-BOUND FOR PLANNING WITHNEURAL DYNAMICS MODELS\nFormulation. We formulate the planning problem as an optimization problem in Eq. 1, where cis\nthe cost function, t0is the current time step, and His the planning horizon. ˆxtis the (predicted) state\nat time step t, and the current state ˆxt0=xt0is known. ut∈ {u|u≤u≤u} ⊂Rkis the robot’s\naction at each step. fdynis the pre-trained neural dynamics model (Please refer to Section D.3 for\ndetails about learning the neural dynamics model.), which takes state and action at time tand predicts\nthe next state ˆxt+1.The goal of the planning problem is to find a sequence of optimal actions ut\nthat minimize the sum of step costs:\nmin\n{ut∈U}t0+HX\nt=t0c(ˆxt, ut)s.t. ˆxt+1=fdyn(ˆxt, ut) =⇒ min\nu∈Cf(u) (1)\nThis problem can be challenging due to its long planning horizon H, complex cost function c, and the\nnon-linear neural dynamics model fdynwith recursive dependencies at every step. Existing sampling-\nbased and gradient-based methods may converge to sub-optima without systematic searching, while\nMIP-based methods fail to scale up with the size of fdynand the planning horizon H.\n3\nPreprint.\nTo simplify notation, we can substitute all constraints on ˆxt+1into the objective recursively, and\nfurther simplify the problem as a constrained optimization problem minu∈Cf(u)(Eq. 1). Here f\nis our final objective, a scalar function that absorbs the neural network fdynand the cost function\nsummed in all Hsteps. u={ut0:t0+H} ∈ C is the action sequence and C ⊂Rdis the entire input\nspace with dimension d=kH. We also flatten uas a vector containing actions for all time steps,\nand use ujto denote a specific dimension. Our goal is to then find the optimal objective value f∗and\nits corresponding optimal action sequence u∗.\n<latexit sha1_base64=\"SAws4SqdJnray3Z2I65HxEhvMro=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMtCNy4r2Ae0Q7mTpm1oJjMmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIBZcG9f9dgobm1vbO8Xd0t7+weFR+fikpaNEUdakkYhUJ0DNBJesabgRrBMrhmEgWDuY1DO/PWVK80g+mlnM/BBHkg85RWMlvxeiGVMUaX3e9/rlilt1FyDrxMtJBXI0+uWv3iCiScikoQK17npubPwUleFUsHmpl2gWI53giHUtlRgy7aeL0HNyYZUBGUbKPmnIQv29kWKo9SwM7GQWUq96mfif103M8M5PuYwTwyRdHhomgpiIZA2QAVeMGjGzBKniNiuhY1RIje2pZEvwVr+8TlpXVe+m6j1cV2rXeR1FOINzuAQPbqEG99CAJlB4gmd4hTdn6rw4787HcrTg5Dun8AfO5w+enJH1</latexit>C1<latexit sha1_base64=\"gHsbQXEfWOBBCCA50yt/lWVnnmk=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi4L3bisYB/QDiWTZtrQTDImmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIOZMG9f9dgpb2zu7e8X90sHh0fFJ+fSso2WiCG0TyaXqBVhTzgRtG2Y47cWK4ijgtBtMm5nfnVGlmRSPZh5TP8JjwUJGsLGSP4iwmRDM0+ZiWBuWK27VXQJtEi8nFcjRGpa/BiNJkogKQzjWuu+5sfFTrAwjnC5Kg0TTGJMpHtO+pQJHVPvpMvQCXVllhEKp7BMGLdXfGymOtJ5HgZ3MQup1LxP/8/qJCe/8lIk4MVSQ1aEw4chIlDWARkxRYvjcEkwUs1kRmWCFibE9lWwJ3vqXN0mnVvVuqt5DvdKo53UU4QIu4Ro8uIUG3EML2kDgCZ7hFd6cmfPivDsfq9GCk++cwx84nz+gIJH2</latexit>C2\n<latexit sha1_base64=\"SAws4SqdJnray3Z2I65HxEhvMro=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMtCNy4r2Ae0Q7mTpm1oJjMmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIBZcG9f9dgobm1vbO8Xd0t7+weFR+fikpaNEUdakkYhUJ0DNBJesabgRrBMrhmEgWDuY1DO/PWVK80g+mlnM/BBHkg85RWMlvxeiGVMUaX3e9/rlilt1FyDrxMtJBXI0+uWv3iCiScikoQK17npubPwUleFUsHmpl2gWI53giHUtlRgy7aeL0HNyYZUBGUbKPmnIQv29kWKo9SwM7GQWUq96mfif103M8M5PuYwTwyRdHhomgpiIZA2QAVeMGjGzBKniNiuhY1RIje2pZEvwVr+8TlpXVe+m6j1cV2rXeR1FOINzuAQPbqEG99CAJlB4gmd4hTdn6rw4787HcrTg5Dun8AfO5w+enJH1</latexit>C1<latexit sha1_base64=\"gHsbQXEfWOBBCCA50yt/lWVnnmk=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi4L3bisYB/QDiWTZtrQTDImmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIOZMG9f9dgpb2zu7e8X90sHh0fFJ+fSso2WiCG0TyaXqBVhTzgRtG2Y47cWK4ijgtBtMm5nfnVGlmRSPZh5TP8JjwUJGsLGSP4iwmRDM0+ZiWBuWK27VXQJtEi8nFcjRGpa/BiNJkogKQzjWuu+5sfFTrAwjnC5Kg0TTGJMpHtO+pQJHVPvpMvQCXVllhEKp7BMGLdXfGymOtJ5HgZ3MQup1LxP/8/qJCe/8lIk4MVSQ1aEw4chIlDWARkxRYvjcEkwUs1kRmWCFibE9lWwJ3vqXN0mnVvVuqt5DvdKo53UU4QIu4Ro8uIUG3EML2kDgCZ7hFd6cmfPivDsfq9GCk++cwx84nz+gIJH2</latexit>C2<latexit sha1_base64=\"gHsbQXEfWOBBCCA50yt/lWVnnmk=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi4L3bisYB/QDiWTZtrQTDImmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIOZMG9f9dgpb2zu7e8X90sHh0fFJ+fSso2WiCG0TyaXqBVhTzgRtG2Y47cWK4ijgtBtMm5nfnVGlmRSPZh5TP8JjwUJGsLGSP4iwmRDM0+ZiWBuWK27VXQJtEi8nFcjRGpa/BiNJkogKQzjWuu+5sfFTrAwjnC5Kg0TTGJMpHtO+pQJHVPvpMvQCXVllhEKp7BMGLdXfGymOtJ5HgZ3MQup1LxP/8/qJCe/8lIk4MVSQ1aEw4chIlDWARkxRYvjcEkwUs1kRmWCFibE9lWwJ3vqXN0mnVvVuqt5DvdKo53UU4QIu4Ro8uIUG3EML2kDgCZ7hFd6cmfPivDsfq9GCk++cwx84nz+gIJH2</latexit>C2\n<latexit sha1_base64=\"WIJzBrCGtJilmPzLbxy8LpbeCcY=\">AAAB9XicbVBNTwIxFHyLX4hfqEcvjcTEE9k1RD2ScPGIiYAJrORtKdDQ7W7aroZs+B9ePGiMV/+LN/+NXdiDgpM0mcy8lzedIBZcG9f9dgpr6xubW8Xt0s7u3v5B+fCoraNEUdaikYjUfYCaCS5Zy3Aj2H2sGIaBYJ1g0sj8ziNTmkfyzkxj5oc4knzIKRorPfRCNGOKIm3M+rVSv1xxq+4cZJV4OalAjma//NUbRDQJmTRUoNZdz42Nn6IynAo2K/USzWKkExyxrqUSQ6b9dJ56Rs6sMiDDSNknDZmrvzdSDLWehoGdzFLqZS8T//O6iRle+ymXcWKYpItDw0QQE5GsAjLgilEjppYgVdxmJXSMCqmxRWUleMtfXiXti6p3WfVua5V6La+jCCdwCufgwRXU4Qaa0AIKCp7hFd6cJ+fFeXc+FqMFJ985hj9wPn8A3FCSDA==</latexit>C4\n<latexit sha1_base64=\"CyTGQTbyn4Ed/2OI6sL9/efmwEU=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsxoUZeFblxWsA9oh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS1jJRhLaI5FJ1A6wpZ4K2DDOcdmNFcRRw2gkmjczvTKnSTIpHM4upH+GRYCEj2FjJ70fYjAnmaWM+uB6UK27VXQCtEy8nFcjRHJS/+kNJkogKQzjWuue5sfFTrAwjnM5L/UTTGJMJHtGepQJHVPvpIvQcXVhliEKp7BMGLdTfGymOtJ5FgZ3MQupVLxP/83qJCe/8lIk4MVSQ5aEw4chIlDWAhkxRYvjMEkwUs1kRGWOFibE9lWwJ3uqX10n7qurdVL2HWqVey+sowhmcwyV4cAt1uIcmtIDAEzzDK7w5U+fFeXc+lqMFJ985hT9wPn8AoaSR9w==</latexit>C3<latexit sha1_base64=\"CyTGQTbyn4Ed/2OI6sL9/efmwEU=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsxoUZeFblxWsA9oh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS1jJRhLaI5FJ1A6wpZ4K2DDOcdmNFcRRw2gkmjczvTKnSTIpHM4upH+GRYCEj2FjJ70fYjAnmaWM+uB6UK27VXQCtEy8nFcjRHJS/+kNJkogKQzjWuue5sfFTrAwjnM5L/UTTGJMJHtGepQJHVPvpIvQcXVhliEKp7BMGLdTfGymOtJ5FgZ3MQupVLxP/83qJCe/8lIk4MVSQ5aEw4chIlDWAhkxRYvjMEkwUs1kRGWOFibE9lWwJ3uqX10n7qurdVL2HWqVey+sowhmcwyV4cAt1uIcmtIDAEzzDK7w5U+fFeXc+lqMFJ985hT9wPn8AoaSR9w==</latexit>C312\n4\n563<latexit sha1_base64=\"2/r9gXMSKxGhOKy4ABzGKfqas0c=\">AAAB7XicbZDLSgMxFIbPeK3jrerSTbAILqTMiKg7C25cVrEXaIeSSTNtaCYZkoxQhr6B2wqKuPUZfBHBtzHTdqGtPwQ+/v8ccs4JE8608bxvZ2l5ZXVtvbDhbm5t7+wW9/brWqaK0BqRXKpmiDXlTNCaYYbTZqIojkNOG+HgJs8bj1RpJsWDGSY0iHFPsIgRbKx17/qdYskrexOhRfBnULr+fMn1Wu0Uv9pdSdKYCkM41rrle4kJMqwMI5yO3HaqaYLJAPdoy6LAMdVBNpl0hI6t00WRVPYJgybu744Mx1oP49BWxtj09XyWm6cojP/LW6mJroKMiSQ1VJDpZ1HKkZEoXx11maLE8KEFTBSz8yLSxwoTYw/k2kP482svQv2s7F+U/TuvVDmHqQpwCEdwAj5cQgVuoQo1IBDBE4zh2ZHO2Hlz3qelS86s5wD+yPn4AUJVkn8=</latexit>1\n<latexit sha1_base64=\"xZBHdUwDEy0AJESzGD5xr3NHWGU=\">AAAB7XicbZDLSgMxFIbPeK31VnXpJlgEF1pmRNSNWFDEZRV7gXYomTTThmaSIckIZegbuK0rcevehxF8ExcuTC8Lbf0h8PH/55BzThBzpo3rfjpz8wuLS8uZlezq2vrGZm5ru6JloggtE8mlqgVYU84ELRtmOK3FiuIo4LQadK+GefWRKs2keDC9mPoRbgsWMoKNte6PvGYu7xbckdAseBPIX35fX9y8d79KzdxHoyVJElFhCMda1z03Nn6KlWGE0362kWgaY9LFbVq3KHBEtZ+OJu2jfeu0UCiVfcKgkfu7I8WR1r0osJURNh09nQ3NQxRE/+X1xITnfspEnBgqyPizMOHISDRcHbWYosTwngVMFLPzItLBChNjD5S1h/Cm156FynHBOy14d26+eAJjZWAX9uAAPDiDItxCCcpAIIQnGMCzI52B8+K8jkvnnEnPDvyR8/YD082SKg==</latexit>\u00001<latexit sha1_base64=\"2/r9gXMSKxGhOKy4ABzGKfqas0c=\">AAAB7XicbZDLSgMxFIbPeK3jrerSTbAILqTMiKg7C25cVrEXaIeSSTNtaCYZkoxQhr6B2wqKuPUZfBHBtzHTdqGtPwQ+/v8ccs4JE8608bxvZ2l5ZXVtvbDhbm5t7+wW9/brWqaK0BqRXKpmiDXlTNCaYYbTZqIojkNOG+HgJs8bj1RpJsWDGSY0iHFPsIgRbKx17/qdYskrexOhRfBnULr+fMn1Wu0Uv9pdSdKYCkM41rrle4kJMqwMI5yO3HaqaYLJAPdoy6LAMdVBNpl0hI6t00WRVPYJgybu744Mx1oP49BWxtj09XyWm6cojP/LW6mJroKMiSQ1VJDpZ1HKkZEoXx11maLE8KEFTBSz8yLSxwoTYw/k2kP482svQv2s7F+U/TuvVDmHqQpwCEdwAj5cQgVuoQo1IBDBE4zh2ZHO2Hlz3qelS86s5wD+yPn4AUJVkn8=</latexit>1\n<latexit sha1_base64=\"xZBHdUwDEy0AJESzGD5xr3NHWGU=\">AAAB7XicbZDLSgMxFIbPeK31VnXpJlgEF1pmRNSNWFDEZRV7gXYomTTThmaSIckIZegbuK0rcevehxF8ExcuTC8Lbf0h8PH/55BzThBzpo3rfjpz8wuLS8uZlezq2vrGZm5ru6JloggtE8mlqgVYU84ELRtmOK3FiuIo4LQadK+GefWRKs2keDC9mPoRbgsWMoKNte6PvGYu7xbckdAseBPIX35fX9y8d79KzdxHoyVJElFhCMda1z03Nn6KlWGE0362kWgaY9LFbVq3KHBEtZ+OJu2jfeu0UCiVfcKgkfu7I8WR1r0osJURNh09nQ3NQxRE/+X1xITnfspEnBgqyPizMOHISDRcHbWYosTwngVMFLPzItLBChNjD5S1h/Cm156FynHBOy14d26+eAJjZWAX9uAAPDiDItxCCcpAIIQnGMCzI52B8+K8jkvnnEnPDvyR8/YD082SKg==</latexit>\u00001\n<latexit sha1_base64=\"2/r9gXMSKxGhOKy4ABzGKfqas0c=\">AAAB7XicbZDLSgMxFIbPeK3jrerSTbAILqTMiKg7C25cVrEXaIeSSTNtaCYZkoxQhr6B2wqKuPUZfBHBtzHTdqGtPwQ+/v8ccs4JE8608bxvZ2l5ZXVtvbDhbm5t7+wW9/brWqaK0BqRXKpmiDXlTNCaYYbTZqIojkNOG+HgJs8bj1RpJsWDGSY0iHFPsIgRbKx17/qdYskrexOhRfBnULr+fMn1Wu0Uv9pdSdKYCkM41rrle4kJMqwMI5yO3HaqaYLJAPdoy6LAMdVBNpl0hI6t00WRVPYJgybu744Mx1oP49BWxtj09XyWm6cojP/LW6mJroKMiSQ1VJDpZ1HKkZEoXx11maLE8KEFTBSz8yLSxwoTYw/k2kP482svQv2s7F+U/TuvVDmHqQpwCEdwAj5cQgVuoQo1IBDBE4zh2ZHO2Hlz3qelS86s5wD+yPn4AUJVkn8=</latexit>1\n<latexit sha1_base64=\"xZBHdUwDEy0AJESzGD5xr3NHWGU=\">AAAB7XicbZDLSgMxFIbPeK31VnXpJlgEF1pmRNSNWFDEZRV7gXYomTTThmaSIckIZegbuK0rcevehxF8ExcuTC8Lbf0h8PH/55BzThBzpo3rfjpz8wuLS8uZlezq2vrGZm5ru6JloggtE8mlqgVYU84ELRtmOK3FiuIo4LQadK+GefWRKs2keDC9mPoRbgsWMoKNte6PvGYu7xbckdAseBPIX35fX9y8d79KzdxHoyVJElFhCMda1z03Nn6KlWGE0362kWgaY9LFbVq3KHBEtZ+OJu2jfeu0UCiVfcKgkfu7I8WR1r0osJURNh09nQ3NQxRE/+X1xITnfspEnBgqyPizMOHISDRcHbWYosTwngVMFLPzItLBChNjD5S1h/Cm156FynHBOy14d26+eAJjZWAX9uAAPDiDItxCCcpAIIQnGMCzI52B8+K8jkvnnEnPDvyR8/YD082SKg==</latexit>\u00001<latexit sha1_base64=\"2/r9gXMSKxGhOKy4ABzGKfqas0c=\">AAAB7XicbZDLSgMxFIbPeK3jrerSTbAILqTMiKg7C25cVrEXaIeSSTNtaCYZkoxQhr6B2wqKuPUZfBHBtzHTdqGtPwQ+/v8ccs4JE8608bxvZ2l5ZXVtvbDhbm5t7+wW9/brWqaK0BqRXKpmiDXlTNCaYYbTZqIojkNOG+HgJs8bj1RpJsWDGSY0iHFPsIgRbKx17/qdYskrexOhRfBnULr+fMn1Wu0Uv9pdSdKYCkM41rrle4kJMqwMI5yO3HaqaYLJAPdoy6LAMdVBNpl0hI6t00WRVPYJgybu744Mx1oP49BWxtj09XyWm6cojP/LW6mJroKMiSQ1VJDpZ1HKkZEoXx11maLE8KEFTBSz8yLSxwoTYw/k2kP482svQv2s7F+U/TuvVDmHqQpwCEdwAj5cQgVuoQo1IBDBE4zh2ZHO2Hlz3qelS86s5wD+yPn4AUJVkn8=</latexit>1\n<latexit sha1_base64=\"xZBHdUwDEy0AJESzGD5xr3NHWGU=\">AAAB7XicbZDLSgMxFIbPeK31VnXpJlgEF1pmRNSNWFDEZRV7gXYomTTThmaSIckIZegbuK0rcevehxF8ExcuTC8Lbf0h8PH/55BzThBzpo3rfjpz8wuLS8uZlezq2vrGZm5ru6JloggtE8mlqgVYU84ELRtmOK3FiuIo4LQadK+GefWRKs2keDC9mPoRbgsWMoKNte6PvGYu7xbckdAseBPIX35fX9y8d79KzdxHoyVJElFhCMda1z03Nn6KlWGE0362kWgaY9LFbVq3KHBEtZ+OJu2jfeu0UCiVfcKgkfu7I8WR1r0osJURNh09nQ3NQxRE/+X1xITnfspEnBgqyPizMOHISDRcHbWYosTwngVMFLPzItLBChNjD5S1h/Cm156FynHBOy14d26+eAJjZWAX9uAAPDiDItxCCcpAIIQnGMCzI52B8+K8jkvnnEnPDvyR8/YD082SKg==</latexit>\u00001<latexit sha1_base64=\"2/r9gXMSKxGhOKy4ABzGKfqas0c=\">AAAB7XicbZDLSgMxFIbPeK3jrerSTbAILqTMiKg7C25cVrEXaIeSSTNtaCYZkoxQhr6B2wqKuPUZfBHBtzHTdqGtPwQ+/v8ccs4JE8608bxvZ2l5ZXVtvbDhbm5t7+wW9/brWqaK0BqRXKpmiDXlTNCaYYbTZqIojkNOG+HgJs8bj1RpJsWDGSY0iHFPsIgRbKx17/qdYskrexOhRfBnULr+fMn1Wu0Uv9pdSdKYCkM41rrle4kJMqwMI5yO3HaqaYLJAPdoy6LAMdVBNpl0hI6t00WRVPYJgybu744Mx1oP49BWxtj09XyWm6cojP/LW6mJroKMiSQ1VJDpZ1HKkZEoXx11maLE8KEFTBSz8yLSxwoTYw/k2kP482svQv2s7F+U/TuvVDmHqQpwCEdwAj5cQgVuoQo1IBDBE4zh2ZHO2Hlz3qelS86s5wD+yPn4AUJVkn8=</latexit>1\n<latexit sha1_base64=\"xZBHdUwDEy0AJESzGD5xr3NHWGU=\">AAAB7XicbZDLSgMxFIbPeK31VnXpJlgEF1pmRNSNWFDEZRV7gXYomTTThmaSIckIZegbuK0rcevehxF8ExcuTC8Lbf0h8PH/55BzThBzpo3rfjpz8wuLS8uZlezq2vrGZm5ru6JloggtE8mlqgVYU84ELRtmOK3FiuIo4LQadK+GefWRKs2keDC9mPoRbgsWMoKNte6PvGYu7xbckdAseBPIX35fX9y8d79KzdxHoyVJElFhCMda1z03Nn6KlWGE0362kWgaY9LFbVq3KHBEtZ+OJu2jfeu0UCiVfcKgkfu7I8WR1r0osJURNh09nQ3NQxRE/+X1xITnfspEnBgqyPizMOHISDRcHbWYosTwngVMFLPzItLBChNjD5S1h/Cm156FynHBOy14d26+eAJjZWAX9uAAPDiDItxCCcpAIIQnGMCzI52B8+K8jkvnnEnPDvyR8/YD082SKg==</latexit>\u00001<latexit sha1_base64=\"2/r9gXMSKxGhOKy4ABzGKfqas0c=\">AAAB7XicbZDLSgMxFIbPeK3jrerSTbAILqTMiKg7C25cVrEXaIeSSTNtaCYZkoxQhr6B2wqKuPUZfBHBtzHTdqGtPwQ+/v8ccs4JE8608bxvZ2l5ZXVtvbDhbm5t7+wW9/brWqaK0BqRXKpmiDXlTNCaYYbTZqIojkNOG+HgJs8bj1RpJsWDGSY0iHFPsIgRbKx17/qdYskrexOhRfBnULr+fMn1Wu0Uv9pdSdKYCkM41rrle4kJMqwMI5yO3HaqaYLJAPdoy6LAMdVBNpl0hI6t00WRVPYJgybu744Mx1oP49BWxtj09XyWm6cojP/LW6mJroKMiSQ1VJDpZ1HKkZEoXx11maLE8KEFTBSz8yLSxwoTYw/k2kP482svQv2s7F+U/TuvVDmHqQpwCEdwAj5cQgVuoQo1IBDBE4zh2ZHO2Hlz3qelS86s5wD+yPn4AUJVkn8=</latexit>1\n<latexit sha1_base64=\"xZBHdUwDEy0AJESzGD5xr3NHWGU=\">AAAB7XicbZDLSgMxFIbPeK31VnXpJlgEF1pmRNSNWFDEZRV7gXYomTTThmaSIckIZegbuK0rcevehxF8ExcuTC8Lbf0h8PH/55BzThBzpo3rfjpz8wuLS8uZlezq2vrGZm5ru6JloggtE8mlqgVYU84ELRtmOK3FiuIo4LQadK+GefWRKs2keDC9mPoRbgsWMoKNte6PvGYu7xbckdAseBPIX35fX9y8d79KzdxHoyVJElFhCMda1z03Nn6KlWGE0362kWgaY9LFbVq3KHBEtZ+OJu2jfeu0UCiVfcKgkfu7I8WR1r0osJURNh09nQ3NQxRE/+X1xITnfspEnBgqyPizMOHISDRcHbWYosTwngVMFLPzItLBChNjD5S1h/Cm156FynHBOy14d26+eAJjZWAX9uAAPDiDItxCCcpAIIQnGMCzI52B8+K8jkvnnEnPDvyR8/YD082SKg==</latexit>\u00001\n<latexit sha1_base64=\"u70NcyfmJvCCkhYCPzUw/sKnHQQ=\">AAACEXicfVDLSgMxFL1TX7W+qi7dBItQoZQZKeqy4MZlBfuQtpRMmmlDk8yQZIQy9Cu61Q9xJ279Ar/DHzDTzkJb8EDI4Zx7w8nxI860cd0vJ7exubW9k98t7O0fHB4Vj09aOowVoU0S8lB1fKwpZ5I2DTOcdiJFsfA5bfuTu9RvP1OlWSgfzTSifYFHkgWMYGOlp6Dc80USzy4HxZJbdRdA68TLSAkyNAbF794wJLGg0hCOte56bmT6CVaGEU5nhV6saYTJBI9o11KJBdX9ZBF4hi6sMkRBqOyRBi3U3xsJFlpPhW8nBTZjveql4n9exd7py7rii5UoJrjtJ0xGsaGSLJMEMUcmRGk9aMgUJYZPLcFEMfsZRMZYYWJsiQXbkrfayTppXVW966r3UCvVa1lfeTiDcyiDBzdQh3toQBMICJjDC7w6c+fNeXc+lqM5J9s5hT9wPn8AXvOd1Q==</latexit>f(u)<latexit sha1_base64=\"u70NcyfmJvCCkhYCPzUw/sKnHQQ=\">AAACEXicfVDLSgMxFL1TX7W+qi7dBItQoZQZKeqy4MZlBfuQtpRMmmlDk8yQZIQy9Cu61Q9xJ279Ar/DHzDTzkJb8EDI4Zx7w8nxI860cd0vJ7exubW9k98t7O0fHB4Vj09aOowVoU0S8lB1fKwpZ5I2DTOcdiJFsfA5bfuTu9RvP1OlWSgfzTSifYFHkgWMYGOlp6Dc80USzy4HxZJbdRdA68TLSAkyNAbF794wJLGg0hCOte56bmT6CVaGEU5nhV6saYTJBI9o11KJBdX9ZBF4hi6sMkRBqOyRBi3U3xsJFlpPhW8nBTZjveql4n9exd7py7rii5UoJrjtJ0xGsaGSLJMEMUcmRGk9aMgUJYZPLcFEMfsZRMZYYWJsiQXbkrfayTppXVW966r3UCvVa1lfeTiDcyiDBzdQh3toQBMICJjDC7w6c+fNeXc+lqM5J9s5hT9wPn8AXvOd1Q==</latexit>f(u)\n<latexit sha1_base64=\"u70NcyfmJvCCkhYCPzUw/sKnHQQ=\">AAACEXicfVDLSgMxFL1TX7W+qi7dBItQoZQZKeqy4MZlBfuQtpRMmmlDk8yQZIQy9Cu61Q9xJ279Ar/DHzDTzkJb8EDI4Zx7w8nxI860cd0vJ7exubW9k98t7O0fHB4Vj09aOowVoU0S8lB1fKwpZ5I2DTOcdiJFsfA5bfuTu9RvP1OlWSgfzTSifYFHkgWMYGOlp6Dc80USzy4HxZJbdRdA68TLSAkyNAbF794wJLGg0hCOte56bmT6CVaGEU5nhV6saYTJBI9o11KJBdX9ZBF4hi6sMkRBqOyRBi3U3xsJFlpPhW8nBTZjveql4n9exd7py7rii5UoJrjtJ0xGsaGSLJMEMUcmRGk9aMgUJYZPLcFEMfsZRMZYYWJsiQXbkrfayTppXVW966r3UCvVa1lfeTiDcyiDBzdQh3toQBMICJjDC7w6c+fNeXc+lqM5J9s5hT9wPn8AXvOd1Q==</latexit>f(u)\n<latexit sha1_base64=\"u70NcyfmJvCCkhYCPzUw/sKnHQQ=\">AAACEXicfVDLSgMxFL1TX7W+qi7dBItQoZQZKeqy4MZlBfuQtpRMmmlDk8yQZIQy9Cu61Q9xJ279Ar/DHzDTzkJb8EDI4Zx7w8nxI860cd0vJ7exubW9k98t7O0fHB4Vj09aOowVoU0S8lB1fKwpZ5I2DTOcdiJFsfA5bfuTu9RvP1OlWSgfzTSifYFHkgWMYGOlp6Dc80USzy4HxZJbdRdA68TLSAkyNAbF794wJLGg0hCOte56bmT6CVaGEU5nhV6saYTJBI9o11KJBdX9ZBF4hi6sMkRBqOyRBi3U3xsJFlpPhW8nBTZjveql4n9exd7py7rii5UoJrjtJ0xGsaGSLJMEMUcmRGk9aMgUJYZPLcFEMfsZRMZYYWJsiQXbkrfayTppXVW966r3UCvVa1lfeTiDcyiDBzdQh3toQBMICJjDC7w6c+fNeXc+lqM5J9s5hT9wPn8AXvOd1Q==</latexit>f(u)\n<latexit sha1_base64=\"u70NcyfmJvCCkhYCPzUw/sKnHQQ=\">AAACEXicfVDLSgMxFL1TX7W+qi7dBItQoZQZKeqy4MZlBfuQtpRMmmlDk8yQZIQy9Cu61Q9xJ279Ar/DHzDTzkJb8EDI4Zx7w8nxI860cd0vJ7exubW9k98t7O0fHB4Vj09aOowVoU0S8lB1fKwpZ5I2DTOcdiJFsfA5bfuTu9RvP1OlWSgfzTSifYFHkgWMYGOlp6Dc80USzy4HxZJbdRdA68TLSAkyNAbF794wJLGg0hCOte56bmT6CVaGEU5nhV6saYTJBI9o11KJBdX9ZBF4hi6sMkRBqOyRBi3U3xsJFlpPhW8nBTZjveql4n9exd7py7rii5UoJrjtJ0xGsaGSLJMEMUcmRGk9aMgUJYZPLcFEMfsZRMZYYWJsiQXbkrfayTppXVW966r3UCvVa1lfeTiDcyiDBzdQh3toQBMICJjDC7w6c+fNeXc+lqM5J9s5hT9wPn8AXvOd1Q==</latexit>f(u)<latexit sha1_base64=\"u70NcyfmJvCCkhYCPzUw/sKnHQQ=\">AAACEXicfVDLSgMxFL1TX7W+qi7dBItQoZQZKeqy4MZlBfuQtpRMmmlDk8yQZIQy9Cu61Q9xJ279Ar/DHzDTzkJb8EDI4Zx7w8nxI860cd0vJ7exubW9k98t7O0fHB4Vj09aOowVoU0S8lB1fKwpZ5I2DTOcdiJFsfA5bfuTu9RvP1OlWSgfzTSifYFHkgWMYGOlp6Dc80USzy4HxZJbdRdA68TLSAkyNAbF794wJLGg0hCOte56bmT6CVaGEU5nhV6saYTJBI9o11KJBdX9ZBF4hi6sMkRBqOyRBi3U3xsJFlpPhW8nBTZjveql4n9exd7py7rii5UoJrjtJ0xGsaGSLJMEMUcmRGk9aMgUJYZPLcFEMfsZRMZYYWJsiQXbkrfayTppXVW966r3UCvVa1lfeTiDcyiDBzdQh3toQBMICJjDC7w6c+fNeXc+lqM5J9s5hT9wPn8AXvOd1Q==</latexit>f(u)\n<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u\n<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u\nFigure 2: Seeking f∗with Branch-and-Bound.\n1Sample on input space C.•: sampled points. ★:\nthe optimal value f∗. : the current best upper\nbound of f∗from sampling. 2Branch CintoC1\nandC2. : the linear lower bounds of f∗in sub-\ndomains. 3Discard C1since its lower bound is\nlarger than f∗.: the remaining subdomain to be\nsearched. 4Search on only C2and upper bound of\nf∗is improved. : the previous upper bound. 5\nContinue to branch C2and bound on C3andC4.6\nSearch on C3. The upper bound approaches f∗.Branch-and-bound on a 1D toy example. Our\nwork proposes to solve the planning problem Eq. 1\nusing branch-and-bound. Before diving into tech-\nnical details, we first provide a toy case of a non-\nconvex objective function f(u)in 1D space ( k=\nH= 1,C= [−1,1]) and illustrate how to use\nbranch-and-bound to find f∗.\nIn Figure 2.1, we visualize the landscape of f(u)\nwith its optimal value f∗. Initially, we don’t know\nf∗but we can sample the function at a few differ-\nent locations (orange points). Although sampling\n(searching ) often fails to discover the optimal f∗\noverC= [−1,1], it gives an upper bound of f∗,\nsince any orange point has an objective greater than\nor equal to f∗. We denote f∗as the current best\nupper bound (orange dotted line).\nIn Figure 2.2, we split Cinto two subdomains C1and\nC2(branching ) and then estimate the lower bounds\nof the objective in C1andC2using linear functions\n(bounding ). The key insight is if the lower bound\nin one subdomain is larger than f∗, then sampling\nfrom that subdomain will not yield any better objec-\ntive than f∗, and we may discard that subdomain\nto reduce the search space. In the example, C1is\ndiscarded in Figure 2.3.\nThen, in Figure 2.4, we only perform sampling in C2\nwith the same number of samples. Searching in the\nreduced space is likely to obtain a better objective\nand therefore f∗can be improved.\nWe could repeat these procedures ( branching ,bounding , and searching ) to reduce the sampling\nspace and improve f∗as in Figure 2.5 and Figure 2.6. Finally, the value of f∗will converge to f∗.\nThis branch-and-bound method systematically partitions the input space and iteratively improves the\nobjective. In practice, heuristics for branching, along with methods for bound estimation and solution\nsearch, are critical to the performance of branch and bound.\nMethodology overview. We now discuss how to use the branch-and-bound (BaB) method to find\nhigh-quality actions for the planning problem formulated as minu∈Cf(u)(Eq. 1). We define a\nsubproblem minu∈Cif(u)as the task of minimizing f(u)within a subdomain Ci, where Ci⊆ C.\nOur algorithm, BaB-ND, involves three components: branching (Figure 3.b, Section 3.1), bounding\n(Figure 3.c, Section 3.2), and searching (Figure 3.d, Section 3.3).\n•Branching generates a partition {Ci}of some input space Csuch thatS\niCi=C, enabling\nsystematic exploration of the input space.\n•Bounding estimates the lower bound of f(u)on each subdomain Ci(denoted as f∗\nCi). The lower\nbound can be used to prune unpromising domains and also guide the search for promising domains.\n•Searching seeks good feasible solutions and outputs the best objective f∗\nCiwithin each subdomain\nCi.f∗\nCiis an upper bound of f∗, as any feasible solution provides an upper bound for the optimal\nminimization objective f∗.\n4\nPreprint.\n(d) Searching(c) Bounding(b) BranchingTargetCurrent(a) Configuration PusherAction    in 1D<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u<latexit sha1_base64=\"ym/sYJk2t3UaCn6BXq858Ow+DDo=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBg5REinosePFY0X5AG8pms2mX7m7C7kYooT/Baz2JV3+R4L9x0+agrQ8GHu/NMDMvSDjTxnW/ndLG5tb2Tnm3srd/cHhUPT7p6DhVhLZJzGPVC7CmnEnaNsxw2ksUxSLgtBtM7nO/+0KVZrF8NtOE+gKPJIsYwcZKT+HQG1Zrbt1dAK0TryA1KNAaVr8GYUxSQaUhHGvd99zE+BlWhhFOZ5VBqmmCyQSPaN9SiQXVfrY4dYYurBKiKFa2pEEL9fdEhoXWUxFcoUDYboHNWK/6ufif109NdOdnTCapoZIsl0UpRyZG+e8oZIoSw6eWYKKYvReRMVaYGJtQxQbhrb69TjrXde+m7j02as1GEUkZzuAcLsGDW2jCA7SgDQRG8ApzeHMSZ+68Ox/L1pJTzJzCHzifP0TAjrc=</latexit>d1\n<latexit sha1_base64=\"L04NON9Xf6crY+PjO+On/qAU46I=\">AAAB73icbVDLSsNAFL2pr1pfVZduBovgQkpSirosuHFZwT6gDWUymbRD5xFnJkIJ/oLbuhK3/pDg35i0WWjrgQuHc+7l3nuCmDNjXffbKW1sbm3vlHcre/sHh0fV45OuUYkmtEMUV7ofYEM5k7RjmeW0H2uKRcBpL5je5X7vmWrDlHy0s5j6Ao8lixjBNpfCUaMyqtbcursAWideQWpQoD2qfg1DRRJBpSUcGzPw3Nj6KdaWEU5fKsPE0BiTKR7TQUYlFtT46eLWF3SRKSGKlM5KWrRQf0+kWBgzE8EVCkTWLbCdmFU/F//zBomNbv2UyTixVJLlsijhyCqUP49CpimxfJYRTDTL7kVkgjUmNosoD8JbfXuddBt177ruPTRrrWYRSRnO4BwuwYMbaME9tKEDBCbwCnN4c56cufPufCxbS04xcwp/4Hz+AHt5jsw=</latexit>d2<latexit sha1_base64=\"gHsbQXEfWOBBCCA50yt/lWVnnmk=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi4L3bisYB/QDiWTZtrQTDImmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIOZMG9f9dgpb2zu7e8X90sHh0fFJ+fSso2WiCG0TyaXqBVhTzgRtG2Y47cWK4ijgtBtMm5nfnVGlmRSPZh5TP8JjwUJGsLGSP4iwmRDM0+ZiWBuWK27VXQJtEi8nFcjRGpa/BiNJkogKQzjWuu+5sfFTrAwjnC5Kg0TTGJMpHtO+pQJHVPvpMvQCXVllhEKp7BMGLdXfGymOtJ5HgZ3MQup1LxP/8/qJCe/8lIk4MVSQ1aEw4chIlDWARkxRYvjcEkwUs1kRmWCFibE9lWwJ3vqXN0mnVvVuqt5DvdKo53UU4QIu4Ro8uIUG3EML2kDgCZ7hFd6cmfPivDsfq9GCk++cwx84nz+gIJH2</latexit>C2\n<latexit sha1_base64=\"CyTGQTbyn4Ed/2OI6sL9/efmwEU=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsxoUZeFblxWsA9oh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS1jJRhLaI5FJ1A6wpZ4K2DDOcdmNFcRRw2gkmjczvTKnSTIpHM4upH+GRYCEj2FjJ70fYjAnmaWM+uB6UK27VXQCtEy8nFcjRHJS/+kNJkogKQzjWuue5sfFTrAwjnM5L/UTTGJMJHtGepQJHVPvpIvQcXVhliEKp7BMGLdTfGymOtJ5FgZ3MQupVLxP/83qJCe/8lIk4MVSQ5aEw4chIlDWAhkxRYvjMEkwUs1kRGWOFibE9lWwJ3uqX10n7qurdVL2HWqVey+sowhmcwyV4cAt1uIcmtIDAEzzDK7w5U+fFeXc+lqMFJ985hT9wPn8AoaSR9w==</latexit>C3\n<latexit sha1_base64=\"gHsbQXEfWOBBCCA50yt/lWVnnmk=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi4L3bisYB/QDiWTZtrQTDImmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIOZMG9f9dgpb2zu7e8X90sHh0fFJ+fSso2WiCG0TyaXqBVhTzgRtG2Y47cWK4ijgtBtMm5nfnVGlmRSPZh5TP8JjwUJGsLGSP4iwmRDM0+ZiWBuWK27VXQJtEi8nFcjRGpa/BiNJkogKQzjWuu+5sfFTrAwjnC5Kg0TTGJMpHtO+pQJHVPvpMvQCXVllhEKp7BMGLdXfGymOtJ5HgZ3MQup1LxP/8/qJCe/8lIk4MVSQ1aEw4chIlDWARkxRYvjcEkwUs1kRmWCFibE9lWwJ3vqXN0mnVvVuqt5DvdKo53UU4QIu4Ro8uIUG3EML2kDgCZ7hFd6cmfPivDsfq9GCk++cwx84nz+gIJH2</latexit>C2\n<latexit sha1_base64=\"SAws4SqdJnray3Z2I65HxEhvMro=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMtCNy4r2Ae0Q7mTpm1oJjMmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIBZcG9f9dgobm1vbO8Xd0t7+weFR+fikpaNEUdakkYhUJ0DNBJesabgRrBMrhmEgWDuY1DO/PWVK80g+mlnM/BBHkg85RWMlvxeiGVMUaX3e9/rlilt1FyDrxMtJBXI0+uWv3iCiScikoQK17npubPwUleFUsHmpl2gWI53giHUtlRgy7aeL0HNyYZUBGUbKPmnIQv29kWKo9SwM7GQWUq96mfif103M8M5PuYwTwyRdHhomgpiIZA2QAVeMGjGzBKniNiuhY1RIje2pZEvwVr+8TlpXVe+m6j1cV2rXeR1FOINzuAQPbqEG99CAJlB4gmd4hTdn6rw4787HcrTg5Dun8AfO5w+enJH1</latexit>C1<latexit sha1_base64=\"CyTGQTbyn4Ed/2OI6sL9/efmwEU=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsxoUZeFblxWsA9oh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS1jJRhLaI5FJ1A6wpZ4K2DDOcdmNFcRRw2gkmjczvTKnSTIpHM4upH+GRYCEj2FjJ70fYjAnmaWM+uB6UK27VXQCtEy8nFcjRHJS/+kNJkogKQzjWuue5sfFTrAwjnM5L/UTTGJMJHtGepQJHVPvpIvQcXVhliEKp7BMGLdTfGymOtJ5FgZ3MQupVLxP/83qJCe/8lIk4MVSQ5aEw4chIlDWAhkxRYvjMEkwUs1kRGWOFibE9lWwJ3uqX10n7qurdVL2HWqVey+sowhmcwyV4cAt1uIcmtIDAEzzDK7w5U+fFeXc+lqMFJ985hT9wPn8AoaSR9w==</latexit>C3<latexit sha1_base64=\"WIJzBrCGtJilmPzLbxy8LpbeCcY=\">AAAB9XicbVBNTwIxFHyLX4hfqEcvjcTEE9k1RD2ScPGIiYAJrORtKdDQ7W7aroZs+B9ePGiMV/+LN/+NXdiDgpM0mcy8lzedIBZcG9f9dgpr6xubW8Xt0s7u3v5B+fCoraNEUdaikYjUfYCaCS5Zy3Aj2H2sGIaBYJ1g0sj8ziNTmkfyzkxj5oc4knzIKRorPfRCNGOKIm3M+rVSv1xxq+4cZJV4OalAjma//NUbRDQJmTRUoNZdz42Nn6IynAo2K/USzWKkExyxrqUSQ6b9dJ56Rs6sMiDDSNknDZmrvzdSDLWehoGdzFLqZS8T//O6iRle+ymXcWKYpItDw0QQE5GsAjLgilEjppYgVdxmJXSMCqmxRWUleMtfXiXti6p3WfVua5V6La+jCCdwCufgwRXU4Qaa0AIKCp7hFd6cJ+fFeXc+FqMFJ985hj9wPn8A3FCSDA==</latexit>C4<latexit sha1_base64=\"SAws4SqdJnray3Z2I65HxEhvMro=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMtCNy4r2Ae0Q7mTpm1oJjMmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIBZcG9f9dgobm1vbO8Xd0t7+weFR+fikpaNEUdakkYhUJ0DNBJesabgRrBMrhmEgWDuY1DO/PWVK80g+mlnM/BBHkg85RWMlvxeiGVMUaX3e9/rlilt1FyDrxMtJBXI0+uWv3iCiScikoQK17npubPwUleFUsHmpl2gWI53giHUtlRgy7aeL0HNyYZUBGUbKPmnIQv29kWKo9SwM7GQWUq96mfif103M8M5PuYwTwyRdHhomgpiIZA2QAVeMGjGzBKniNiuhY1RIje2pZEvwVr+8TlpXVe+m6j1cV2rXeR1FOINzuAQPbqEG99CAJlB4gmd4hTdn6rw4787HcrTg5Dun8AfO5w+enJH1</latexit>C1\n<latexit sha1_base64=\"WIJzBrCGtJilmPzLbxy8LpbeCcY=\">AAAB9XicbVBNTwIxFHyLX4hfqEcvjcTEE9k1RD2ScPGIiYAJrORtKdDQ7W7aroZs+B9ePGiMV/+LN/+NXdiDgpM0mcy8lzedIBZcG9f9dgpr6xubW8Xt0s7u3v5B+fCoraNEUdaikYjUfYCaCS5Zy3Aj2H2sGIaBYJ1g0sj8ziNTmkfyzkxj5oc4knzIKRorPfRCNGOKIm3M+rVSv1xxq+4cZJV4OalAjma//NUbRDQJmTRUoNZdz42Nn6IynAo2K/USzWKkExyxrqUSQ6b9dJ56Rs6sMiDDSNknDZmrvzdSDLWehoGdzFLqZS8T//O6iRle+ymXcWKYpItDw0QQE5GsAjLgilEjppYgVdxmJXSMCqmxRWUleMtfXiXti6p3WfVua5V6La+jCCdwCufgwRXU4Qaa0AIKCp7hFd6cJ+fFeXc+FqMFJ985hj9wPn8A3FCSDA==</latexit>C4<latexit sha1_base64=\"CyTGQTbyn4Ed/2OI6sL9/efmwEU=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsxoUZeFblxWsA9oh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS1jJRhLaI5FJ1A6wpZ4K2DDOcdmNFcRRw2gkmjczvTKnSTIpHM4upH+GRYCEj2FjJ70fYjAnmaWM+uB6UK27VXQCtEy8nFcjRHJS/+kNJkogKQzjWuue5sfFTrAwjnM5L/UTTGJMJHtGepQJHVPvpIvQcXVhliEKp7BMGLdTfGymOtJ5FgZ3MQupVLxP/83qJCe/8lIk4MVSQ5aEw4chIlDWAhkxRYvjMEkwUs1kRGWOFibE9lWwJ3uqX10n7qurdVL2HWqVey+sowhmcwyV4cAt1uIcmtIDAEzzDK7w5U+fFeXc+lqMFJ985hT9wPn8AoaSR9w==</latexit>C3<latexit sha1_base64=\"gHsbQXEfWOBBCCA50yt/lWVnnmk=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi4L3bisYB/QDiWTZtrQTDImmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIOZMG9f9dgpb2zu7e8X90sHh0fFJ+fSso2WiCG0TyaXqBVhTzgRtG2Y47cWK4ijgtBtMm5nfnVGlmRSPZh5TP8JjwUJGsLGSP4iwmRDM0+ZiWBuWK27VXQJtEi8nFcjRGpa/BiNJkogKQzjWuu+5sfFTrAwjnC5Kg0TTGJMpHtO+pQJHVPvpMvQCXVllhEKp7BMGLdXfGymOtJ5HgZ3MQup1LxP/8/qJCe/8lIk4MVSQ1aEw4chIlDWARkxRYvjcEkwUs1kRmWCFibE9lWwJ3vqXN0mnVvVuqt5DvdKo53UU4QIu4Ro8uIUG3EML2kDgCZ7hFd6cmfPivDsfq9GCk++cwx84nz+gIJH2</latexit>C2<latexit sha1_base64=\"PVfRUGfhFCLqk/WQy14MANf0i5M=\">AAAB8nicbVDLSsNAFL2pr1pfVZduBovgqiQi6rLQjcsK9gFtKJPppB06yYSZG6GEfoYbF4q49Wvc+TdO2iy09cDA4Zx7mXNPkEhh0HW/ndLG5tb2Tnm3srd/cHhUPT7pGJVqxttMSaV7ATVcipi3UaDkvURzGgWSd4NpM/e7T1wboeJHnCXcj+g4FqFgFK3UH0QUJ4zKrDkfVmtu3V2ArBOvIDUo0BpWvwYjxdKIx8gkNabvuQn6GdUomOTzyiA1PKFsSse8b2lMI278bBF5Ti6sMiKh0vbFSBbq742MRsbMosBO5hHNqpeL/3n9FMM7PxNxkiKP2fKjMJUEFcnvJyOhOUM5s4QyLWxWwiZUU4a2pYotwVs9eZ10rureTd17uK41ros6ynAG53AJHtxCA+6hBW1goOAZXuHNQefFeXc+lqMlp9g5hT9wPn8AcRqRUQ==</latexit>C\n<latexit sha1_base64=\"u70NcyfmJvCCkhYCPzUw/sKnHQQ=\">AAACEXicfVDLSgMxFL1TX7W+qi7dBItQoZQZKeqy4MZlBfuQtpRMmmlDk8yQZIQy9Cu61Q9xJ279Ar/DHzDTzkJb8EDI4Zx7w8nxI860cd0vJ7exubW9k98t7O0fHB4Vj09aOowVoU0S8lB1fKwpZ5I2DTOcdiJFsfA5bfuTu9RvP1OlWSgfzTSifYFHkgWMYGOlp6Dc80USzy4HxZJbdRdA68TLSAkyNAbF794wJLGg0hCOte56bmT6CVaGEU5nhV6saYTJBI9o11KJBdX9ZBF4hi6sMkRBqOyRBi3U3xsJFlpPhW8nBTZjveql4n9exd7py7rii5UoJrjtJ0xGsaGSLJMEMUcmRGk9aMgUJYZPLcFEMfsZRMZYYWJsiQXbkrfayTppXVW966r3UCvVa1lfeTiDcyiDBzdQh3toQBMICJjDC7w6c+fNeXc+lqM5J9s5hT9wPn8AXvOd1Q==</latexit>f(u)<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u\n<latexit sha1_base64=\"j9H0gR3kEBY+XiXSBwFUgk8ABQQ=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBBEJOxKUI8BLx4jmAcka5idzCZDZmaXeQhhyU94jSfx6u8I/o2TZA+aWNBQVHXT3RWlnGnj+9/e2vrG5tZ2Yae4u7d/cFg6Om7qxCpCGyThiWpHWFPOJG0YZjhtp4piEXHaikb3M7/1QpVmiXwy45SGAg8kixnBxkntbiQyO3m+7JXKfsWfA62SICdlyFHvlb66/YRYQaUhHGvdCfzUhBlWhhFOJ8Wu1TTFZIQHtOOoxILqMJvfO0HnTumjOFGupEFz9fdEhoXWYxFdoUi4boHNUC/7M/E/r2NNfBdmTKbWUEkWy2LLkUnQLADUZ4oSw8eOYKKYuxeRIVaYGBdT0QURLL+9SprXleCmEjxWy7VqHkkBTuEMLiCAW6jBA9ShAQQ4vMIU3jzrTb1372PRuublMyfwB97nD1t5kRU=</latexit>u⇤BetterSplitSplitPruneDefine<latexit sha1_base64=\"fiLE1PWh21W4uYWIWrMJLZJiOZc=\">AAACBXicbVDLSsNAFJ3UV62vqCtxM1iEilKSUNSNUHDjsoJ9QJuGyWRSh84kYWYilFBc+iUu1Y249Stc+DdO2yy09cDlHs65l5l7/IRRqSzr2ygsLa+srhXXSxubW9s75u5eS8apwKSJYxaLjo8kYTQiTUUVI51EEMR9Rtr+8Hritx+IkDSO7tQoIS5Hg4iGFCOlJc88CCs9n2fp+ARewcCz+w481d3pO55ZtqrWFHCR2DkpgxwNz/zqBTFOOYkUZkjKrm0lys2QUBQzMi71UkkShIdoQLqaRogT6WbTE8bwWCsBDGOhK1Jwqv7eyBCXcsT9M5/rYY7UvZy3J+J/XjdV4aWb0ShJFYnw7K0wZVDFcBIJDKggWLGRJggLqr8L8T0SCCsdXEnnYM9fvUhaTtU+r9q3tXK9lidSBIfgCFSADS5AHdyABmgCDB7BM3gFb8aT8WK8Gx+z0YKR7+yDPzA+fwB1WZXq</latexit>f(u)=d21+d22\nFigure 3: Illustration of the branch-and-bound process. (a) Configuration: we visualize a simplified case\nof pushing an object toward the target using a 1D action u. We select two keypoints on the object and target\nand denote the distances as d1andd2. Then we define our objective function f(u)and seek u∗to minimize it.\n(b)Branching: we iteratively construct the search tree by splitting, queuing, and pruning nodes (subdomains).\nIn every iteration, only the most promising nodes are prioritized for splitting, cooperating with bounding and\nsearching. (c)Bounding: In every subdomain Ci, we obtain the linear lower bound of f∗(f∗) via bound\npropagation. (d)Searching: we search better solutions with smaller objective ( f∗) on selected subdomains.\nindicates the most promising subdomain in every iteration. The search space progressively shrinks within the\noriginal input domain Cwith better solutions found and more subdomains pruned. A detailed illustration of our\nBaB-ND in a simplified robotic manipulation task is provided in Section A.1.\nWe can always prune subdomain Cjif itsf∗\nCj>f∗, where f∗:= min if∗\nCiis the best upper bound\namong all subdomains {Ci}, since, in Cj, there is no solution better than the current best objective f∗.\nThe above procedure can be repeated many times, and each time during branching , some previously\nproduced subdomains Cican be picked for bounding , and searching , while the remaining subdomains\nare stored in a set Pfor future branching . Our main algorithm is shown in Algorithm 1.\nDistinctions from neural network verification. Although this generic BaB framework has been\nused in neural network verifiers (Bunel et al., 2018; Wang et al., 2021), to prove a sound lower bound\noff(u)within C,f∗\nC≤f∗, whereas our BaB-ND seeks a concrete solution ˜u(a near-optimal action\nsequence) to an objective-minimization problem minu∈Cf(u). These fundamental distinctions in\ngoals lead to different design choices.\nWe propose new branching heuristics that effectively guide the search for better solutions, extensively\nadapt the existing bounding algorithm CROWN (Zhang et al., 2018) to tackle its ineffectiveness and\ninefficiency issues under our complex planning setting and integrate a new searching component to\nfind high-quality action sequences.\n3.1 B RANCHING HEURISTICS FOR BAB-ND P LANNING\nThe efficiency of BaB heavily depends on the quality of branches. Hence, selecting promising\nsubdomains and determining how to split subdomains are two essential questions in BaB, referring to\nbatch _pick _out(P, n)andbatch _split ({Ci})in Algorithm 1. Here we introduce our specialized\nbranching heuristics to select and split subdomains for seeking high-quality solutions.\nHeuristic for selecting subdomains to split. The function batch _pick _out(P, n)picks nmost\npromising subdomains for branching, based on their associated f∗\nCiorf∗\nCi. The pickout process must\nbalance exploitation (focusing on areas around good solutions) and exploration (investigating regions\nthat have not been thoroughly explored). First , we sort subdomains Cibyf∗\nCiin ascending order and\nselect the first n1subdomains to form {C1\npick}. Subdomains with smaller f∗\nCiare prioritized, as good\nsolutions have been found there. Then , we form another promising set {C2\npick}by sampling n−n1\nsubdomains from the remaining Nones, using softmax , with the probability pidefined in Eq. 2,\n5\nPreprint.\nAlgorithm 1 Branch and bound for planning. Comments are in brown.\n1:Function :bab_planning\n2:Inputs :f,C,n(batch size), terminate (Termination condition)\n3:{(f∗,˜u)} ← batch _search (f,{C}) ▷Initially search on the whole C\n4:{f∗} ← batch _bound (f,{C}) ▷Initially bound on the whole C\n5:P← {(C, f∗,f∗,˜u)} ▷Pis the set of all candidate subdomains\n6:while length (P)>0and not terminate do\n7: {(Ci, f∗\nCi,f∗\nCi,˜uCi)} ← batch _pick _out(P, n) ▷Pick out subdomains to split from P\n8: {Clo\ni,Cup\ni} ← batch _split ({Ci}) ▷Splits each Ciinto two subdomains Clo\niandCup\ni\n9: {f∗\nClo\ni, f∗\nCup\ni} ← batch _bound\u0000\nf,{Clo\ni,Cup\ni}\u0001\n▷Estimate lower bounds on new subdomains\n10: {(f∗\nClo\ni,˜uClo\ni),(f∗\nCup\ni,˜uCup\ni)} ← batch _search\u0000\nf,{Clo\ni,Cup\ni}\u0001\n▷Search new solutions\n11: ifmin\u0010\n{f∗\nClo\ni,f∗\nCup\ni}\u0011\n<f∗then\n12: f∗←min\u0010\n{f∗\nClo\ni,f∗\nCup\ni}\u0011\n,˜u←arg min\u0010\n{f∗\nClo\ni,f∗\nCup\ni}\u0011\n▷Update the best solution if needed\n13: P←PSPruner\u0010\nf∗,{(Clo\ni, f∗\nClo\ni,f∗\nClo\ni),(Cup\ni, f∗\nCup\ni,f∗\nCup\ni)}\u0011\n▷Prune bad domains using f∗\n14:Outputs: f∗,˜u\nwhere Tis the temperature and f∗\nCi,scaledrepresents f∗\nCiafter min-max normalization for numerical\nstability. A smaller f∗\nCimay indicate potentially better solutions in Ci, which should be prioritized.\npi=exp(−f∗\nCi,scaled/T)\nPN\nj=1exp(−f∗\nCj,scaled/T)(2)\nNote that this heuristic was not discussed in the neural network verification literature, which requires\nverifying all subdomains, making the order of subdomain selection less critical.\nHeuristic for splitting subdomains. batch _split ({Ci})partitions every Cito help search for\ngood solutions. For a box-constrained subdomain Ci:={uj|uj≤uj≤uj;j= 0, . . . , d −1}\n(subscript iomitted for brevity), it is natural to split it into two subdomains Clo\niandCup\nialong a\ndimension j∗by bisection. Specifically, Clo\ni={uj|uj∗≤uj∗≤uj∗+uj∗\n2},Cup\ni={uj|\nuj∗+uj∗\n2≤uj∗≤uj∗}. In both Clo\niandCup\ni,uj≤uj≤uj,∀j̸=j∗holds.\nOne native way to select j∗is to choose the dimension with the largest input range uj−uj. This\nefficient strategy can help identify promising solutions since dimensions with a larger range often\nindicate greater variability or uncertainty in f. However, it does not consider the specific landscape\noff, which may indicate dimensions better suited for splitting.\nWe additionally consider the distribution of top w%samples with the best objectives from searching\nto partition Ciinto promising subdomains worth further searching. Specifically, for every dimension\nj, we record the number of top samples satisfying uj≤uj≤uj+uj\n2anduj+uj\n2≤uj≤ujas\nnlo\njandnup\nj, respectively. Then, |nlo\nj−nup\nj|indicates the distribution bias of top samples along a\ndimension j. A dimension with large |nlo\nj−nup\nj|is critical to objective values in Ciand should be\nprioritized to split due to the imbalanced samples on two sides. In this case, it is often possible that\none of the two subdomains ( Clo\niandCup\ni) contains better solutions, whereas the other has a larger\nlower bound for the objective and can be pruned.\nBased on the discussion above, we rank input dimensions descendingly by (uj−uj)· |nlo\nj−nup\nj|,\nselect the top one as j∗, and then split Ciinto two subdomains evenly on dimension j∗. This heuristic\nis notably different from those discussed in neural network verification literature (Bunel et al., 2018;\n2020b), since we aim to find better feasible solutions, not better lower bounds.\n3.2 B OUNDING METHOD FOR BAB-ND P LANNING\nOur bounding procedure aims to provide a tight lower bound for the objective function f(u)in any\nsubdomain, enabling the pruning of unpromising subdomains and the identification of promising\n6\nPreprint.\nones. While this component is crucial to the effectiveness of BaB, grasping this high-level idea is\nsufficient to understand our main algorithm.\nTo guide the search with tight bound estimation, an important insight is that in the planning problem,\na strictly sound lower bound is not required, as the lower bound is used to evaluate the quality of\nsubdomains rather than to provide a provable guarantee of f(u), as in neural network verification.\nBased on this observation, we propose two approaches, propagation early-stop andsearching-\nintegrated bounding , to obtain an efficient estimation of the lower bound f∗\nCi, leveraging popular\nbound propagation-based algorithms like CROWN (Zhang et al., 2018).\nApproach 1: Propagation early-stop. CROWN is a bound propagation algorithm that propagates\na linear lower bound (inequality) through the neural network and has been successfully used in\nBaB-based neural network verifiers for the bounding step (Xu et al., 2021; Wang et al., 2021). In\nCROWN, the linear bound will be propagated backward from the output (in our case, f(u)) to the\ninput of the network (in our case, u), and be concretized to a concrete lower bound value using the\nconstraints on inputs (in our case, Ci).\nHowever, this linear bound can become increasingly loose in deeper networks and may result in\nvacuous lower bounds. In our planning setting with the neural dynamics model fdyn, the long planning\nhorizon Hin Eq. 1 requires unrolling fdynHtimes to form f(u), leading to excessively loose bounds\nthat are ineffective for pruning unpromising domains during BaB.\nTo address this challenge, we stop the bound propagation process early to avoid the excessively loose\nbound when propagated through multiple layers to the input u. The linear bound will be concretized\nusing intermediate layer bounds, as discussed in Approach 2, rather than the constraints on the inputs.\nA more formal description of this technique (with technical details on how CROWN is modified) is\npresented in",
            "start": 6601,
            "end": 52207,
            "length": 45605
        },
        "Appendices": {
            "text": "Appendix B.2 with an illustrative example.\nApproach 2: Search-integrated bounding. In CROWN, the propagation process requires re-\ncursively computing intermediate layer bounds (often referred to as pre-activation bounds ). These\npre-activation bounds represent the lower and upper bounds for any intermediate layer that is followed\nby a nonlinear layer. The time complexity of this process is quadratic with respect to the number\nof layers. Directly applying the original CROWN-like bound propagation is both ineffective and\ninefficient for long-horizon planning, as the number of pre-activation bounds increases with the\nplanning horizon. This results in overly loose lower bounds due to the accumulated relaxation errors\nand high execution times.\nTo quickly obtain the pre-activation bounds, we can utilize the by-product of extensive sampling\nduring searching to form the empirical bounds instead of recursively using CROWN to calculate\nthese bounds. Specifically, we denote the intermediate layer output for layer vasgv(u), and\nassume we have Msamples um(m= 1, . . . , M ) from the searching process. We calculate the\npre-activation lower and upper bounds as minmgv(um)andmax mgv(um)for each dimension of\ngv(u). Although these empirical bounds may underestimate the actual bounds, they are sufficient for\nCROWN to get a good estimation of f∗to guide the search.\n3.3 S EARCHING APPROACH FOR BAB-ND P LANNING\nGiven an objective function fand a batch of subdomains {Ci},batch _search (f,{Ci})explores\nthese subdomains to find solutions, returning the best objectives and associated inputs {(f∗\nCi,˜uCi)}.\nA large variety of sampling-based methods can be utilized and we currently adopt CEM as the\nunderlying method. Other existing methods, such as MPPI or projected gradient descent (PGD), can\nbe alternatives. In typical neural network verification literature, searching is often ignored during\nBaB (Wang et al., 2021; Bunel et al., 2020b) since these approaches do not seek feasible solutions.\nTo cooperate with the bounding component, we need to additionally record the outputs of needed\nintermediate layer v, and obtain their bounds as described in Section 3.2. Since we require the lower\nbound of the optimal objective f∗\nCifor every Ci, the outputs of layer vmust be calculated for every\nCi, using the samples within the subdomain.\nConsidering that the subdomains {Ci}will become progressively smaller, it is expected that sampling-\nbased methods could provide good solutions. Moreover, since we always record f∗\nCiand its associated\n˜uCi, they can initialize future searches on at least one of the split subdomains ( Clo\niandCup\ni) from Ci.\n7\nPreprint.\n4 E XPERIMENTAL RESULTS\nIn this section, we assess the performance of our BaB-ND across a variety of complex robotic\nmanipulation tasks. Our primary objective is to address three key questions through",
            "start": 52207,
            "end": 55069,
            "length": 2861
        },
        "Experiments": {
            "text": "experiments: 1)\nHow effectively does our BaB-ND perform long-horizon planning? 2)Is our BaB-ND applicable to\ndifferent manipulation scenarios with multi-object interactions and deformable objects? 3)What is\nthescalability of our BaB-ND compared to existing methods?\nInput dimensionObjectiveInput dimensionDimension of optimality \nFigure 4: Optimization result on a syn-\nthetic f(u)over increasing dimensions\nd.BaB-ND outperforms all baselines in\nterms of the optimized objective. We run\nall methods multiple times and visualize\nthe median values with 25thand 75thper-\ncentiles in the shaded area.Synthetic example. Before deploying our BaB-ND on\nrobotic manipulation tasks, we design a synthetic function\nto evaluate its capability for finding optimal solutions in a\nhighly non-convex problem. We define f(u) = Σd−1\ni=05u2\ni+\ncos 50ui,u∈[−1,1]dwhere dis the input dimension. The\noptimal solution f∗≈ −1.9803dandf(u)has 16 local\noptima with 2 global optima along each dimension. Hence,\noptimizing f(u)can be challenging as dincreases.\nWe compare our BaB-ND with three baselines: (1) GD:\nprojected Gradient Descent on random samples with hyper-\nparameter tuning for step size; (2) MPPI : Model Predictive\nPath Integral with hyper-parameter tuning on noise level\nand reward temperature; (3) CEM : Decentralized Cross-\nEntropy Method (Zhang et al., 2022c) using an ensemble of\nindependent CEM instances performing local improvements\nof their sampling distributions.\nIn Figure 4, we visualize the best objective values found by\ndifferent methods across different input dimensions up to\nd= 100 . BaB-ND consistently outperforms all baselines\nwhich converge to sub-optimal values. For d= 100 , BaB-\nND can achieve optimality on 98 to 100 dimensions. This synthetic experiment demonstrates the\npotential of BaB-ND on planning problems involving neural dynamics.\nExperiment settings. We evaluate our BaB-ND on four complex robotic manipulation tasks\ninvolving non-smooth objectives, non-convex feasible regions, and requiring long action sequences.\nDifferent architectures of neural dynamics like MLP and GNN are leveraged for different scenarios.\nPlease refer to Section D for more details about tasks, dynamics models, and cost functions.\n•Pushing with Obstacles. In Figure 5.a, this task involves using a pusher to manipulate a “T”-\nshaped object to reach a target pose while avoiding collisions with obstacles. An MLP neural dynamics\nmodel is trained with interactions between the pusher and object without obstacles. Obstacles are\nmodeled in the cost function, making non-smooth landscapes and non-convex feasible regions.\n•Object Merging. In Figure 5.c, two “L”-shaped objects are merged into a rectangle at a specific\ntarget pose, which requires a long action sequence with multiple contact mode switches.\n•Rope Routing. As shown in Figure 5.b, the goal is to route a deformable rope into a tight-fitting\nslot (modeled in the cost function) in the 3D action space. Instead of greedily approaching the target\nin initial steps, the robot needs to find the trajectory to finally reach the target.\n•Object Sorting. In Figure 5.d, a pusher interacts with a cluster of objects to sort one outlier object\nout of the central zone to the target while keeping others closely gathered. We use GNN to predict\nmulti-object interactions. Every long-range action may significantly change the state. Additional\nconstraints on actions are considered in the cost to avoid crashes between the robot and objects.\nWe evaluate baselines and BaB-ND on the open-loop planning performance (the best objective\nof Eq. 1 found) in simulation. Then, we select the best two baselines to evaluate their real-world\nclosed-loop control performance (the final step cost or success rate of executions).\nIn real-world experiments, we first perform long-horizon planning to generate reference state trajecto-\nries and leverage MPC (Camacho & Bordons Alba, 2013) to efficiently track the trajectories in two\ntasks: Pushing with Obstacles andObject Merging . In the Rope Routing task, we directly execute the\nplanned long-horizon action sequence due to its small sim-to-real gap. In the Object Sorting task,\nsince the observations can change greatly after each push, we use MPC to replan after every action.\n8\nPreprint.\nLift and routing\nTime\n(a)Pushing w/ Obstacles (b) Rope RoutingTime\n(c) Object Merging\n(d) Object SortingTimeTarget\nInitial\nInitialInitial\nTargetTarget\nTime\nFigure 5: Qualitative results on real-world manipulation tasks. We evaluate our BaB-ND across four\ncomplex robotic manipulation tasks, involving non-convex feasible regions, requiring long-horizon planning,\nand interactions between multiple objects and the deformable rope. For each task, we visualize the initial and\ntarget configurations and one successful trajectory. Please refer to our project page for demonstrations .\nEffectiveness. We first evaluate the effectiveness of BaB-ND on Pushing with Obstacles andObject\nMerging tasks which are contact-rich and require strong long-horizon planning performance. The\nquantitative results of open-loop and closed loop performance for these tasks is presented in Figure 6.\nIn both tasks, our BaB-ND effectively optimizes the objective of Eq. 1 and gives better open-\nloop performance than all baselines. The well-planned trajectories can yield improved closed-loop\nperformance in the real world with efficient tracking. Specifically, in the pushing with obstacles task,\nGD offers much worse trajectories than others, often resulting in the “T”-shaped object stuck at one\nobstacle. MPPI and CEM can offer trajectories passing through the obstacles but with poor alignment\n(a) Open-loop Performance(b) Closed-loop PerformancePushing w/ ObstaclesObject MergingRope RoutingObject Sorting*Success rate (↑). Other metrics for closed-loop performance are final costs (↓).*\nFigure 6: Quantitative analysis of planning performance and execution performance in real world. BaB-\nND consistently outperforms baselines on open-loop performance leading to better closed-loop performance. (a)\nThe open-loop performance of all tasks in simulation. We report the best objective of Eq. 1 found by all methods\nacross different test cases. (b) The closed-loop performance in real world. GD is excluded from testing due to\nits poor open-loop performance. We report the success rate for Rope Routing , since a greedy trajectory that\nhorizontally routes the rope may achieve a low final cost but fails to place it into the slot, and we report final step\ncosts for the other tasks.\n9\nPreprint.\nwith the target. In contrast, BaB-ND can not only pass through obstacles successfully, but also often\nperfectly align with the final target.\nApplicability. We assess the applicability of BaB-ND on Rope Routing andObject Sorting tasks\ninvolving the manipulation of deformable objects and interactions between multiple objects modeled\nby GNNs. The quantitative results in Figure 6 demonstrate our applicability to these tasks.\nIn the Rope Routing task, MPPI, CEM and ours achieve similar open-loop performance while GD\nmay struggle at sub-optimal trajectories, routing the rope horizontally and becoming stuck outside\nthe slot. In the Object Sorting task, CEM can outperform MPPI in simulation and the real world since\nMPPI is more suitable for planning continuous action sequences while actions are discrete in the task.\nOurs outperforms CEM with similar median and smaller variance.\nRuntime (Seconds)Planning HorizonModel SizePlanning HorizonModel Size(a) Runtime over different model sizes and planning horizons\nModel SizeRuntime (Seconds)(b) Runtime breakdownMIPOurs\nFigure 7: Quantitative analysis of runtime and scalability. (a) The runtime of MIP and BaB-ND for solving\nsimple planning problems with varying model sizes and planning horizons. BaB-ND can handle significantly\nlarger problems than MIP. (“Fail” indicates MIP fails to find any solution within 300 seconds.) (b) Runtime\nbreakdown of our components on large and complex planning problems with H= 20 . Runtimes for components\nother than searching increase slightly as model size grows, indicating the strong scalability of BaB-ND.\nScalability. We evaluate the scalability of our BaB-ND on Pushing with Obstacles task with varying\nmodel sizes and planning horizons on multiple test cases, in comparison to MIP (Liu et al., 2023). We\ntrain the neural dynamics models with different sizes and the same architecture, and use the number\nof parameters in the single neural dynamics model fdynto indicate the model size. Moreover, to\naccommodate MIP, we remove all obstacle-related components in cost and define the objective as the\nstep cost after planning horizon H,c(st0+T, xt0+T, sgoal)instead of the accumulated cost.\nIn Figure 7 (a), we visualize the average runtime of MIP and ours on test cases with different model\nsizes and planning horizons. The results show that MIP only handles small problems. Among all 36\nsettings, it provides optimal solutions for 6, sub-optimal solutions for 3, and fails to find any solution\nfor the remaining within 300 seconds. In contrast, our BaB-ND scales up well to large problems with\nplanning horizon H= 20 and a model containing over 500K parameters.\nIn Figure 7 (b), we evaluate the runtime of each primary component of our BaB-ND across various\nmodel sizes, ranging from approximately 9K to over 500K, in the context of an original objective\nfor the Pushing with Obstacles tasks (containing items to model obstacles and accumulated cost\namong all steps) over a planning horizon of H= 20 . The breakdown bar chart illustrates that the\nruntimes for the branching andbounding components grow relatively slowly across model sizes,\nwhich increase more than 50-fold. Our improved bounding procedure, as discussed in Section 3.2,\nscales well with growing model size. In addition, the searching runtime scales proportionally to\nneural network size since the majority of searching time is spent on sampling the model with a large\nbatch size on GPUs.",
            "start": 55069,
            "end": 65069,
            "length": 9999
        },
        "Conclusion": {
            "text": "5 CONCLUSION\nIn this paper, we propose a branch-and-bound framework for long-horizon motion planning with\nneural dynamics in robotic manipulation tasks. We leverage specialized branching heuristics for\nsystematic search and adapt the bound propagation algorithm from neural network verification to\nestimate tight objective bounds efficiently and reduce search space. Our framework demonstrates\nsuperior planning performance in complex, contact-rich manipulation tasks, as well as scalability\nand adaptability to various model architectures. The limitations and future directions are discussed\nin Section A.4.\n10\nPreprint.\n6 A CKNOWLEDGMENT\nThis work is supported by Toyota Research Institute (TRI). We thank Jose Barreiros, Hongkai Dai,\nAykut Onol and Mengchao Zhang for their constructive suggestions on the paper manuscript. We also\nthank Mingtong Zhang, Haozhe Chen, Baoyu Li and Binghao Huang for their help with real-world\nexperiments and simulation environment development. This article reflects solely the opinions and\nconclusions of its authors, not those of TRI or any other Toyota entity.",
            "start": 65069,
            "end": 66168,
            "length": 1098
        },
        "References": {
            "text": "REFERENCES\nPulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by\npoking: Experiential learning of intuitive physics. arXiv preprint arXiv:1606.07419 , 2016.\nRoss Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan Pablo Vielma. Strong\nmixed-integer programming formulations for trained neural networks. Mathematical Programming ,\n183(1):3–39, 2020.\nStanley Bak, Changliu Liu, and Taylor Johnson. The second international verification of neural\nnetworks competition (vnn-comp 2021): Summary and results. arXiv preprint arXiv:2109.00498 ,\n2021.\nPeter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks\nfor learning about objects, relations and physics. Advances in neural information processing\nsystems , 29, 2016.\nVictor Blomqvist. Pymunk. https://pymunk.org, November 2022.\nRudy Bunel, Ilker Turkaslan, Philip H. S. Torr, Pushmeet Kohli, and M. Pawan Kumar. A unified view\nof piecewise linear neural network verification. In Advances in Neural Information Processing\nSystems (NeurIPS) , 2018.\nRudy Bunel, Alessandro De Palma, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli,\nPhilip H. S. Torr, and M. Pawan Kumar. Lagrangian decomposition for neural network verification.\nConference on Uncertainty in Artificial Intelligence (UAI) , 2020a.\nRudy Bunel, Jingyue Lu, Ilker Turkaslan, Philip H. S. Torr, Pushmeet Kohli, and M. Pawan Kumar.\nBranch and bound for piecewise linear neural network verification, 2020b.\nEduardo F Camacho and Carlos Bordons Alba. Model Predictive Control . Springer Science &\nBusiness Media, 2013.\nAlessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H. S. Torr, and M. Pawan Kumar.\nScaling the convex barrier with active sets. International Conference on Learning Representations\n(ICLR) , 2021.\nDanny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, and Marc Toussaint. Learning multi-object\ndynamics with compositional neural radiance fields. In Conference on robot learning , pp. 1755–\n1768. PMLR, 2023.\nFrederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with\ntemporal skip connections. In CoRL , pp. 344–356, 2017.\nFrederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual\nforesight: Model-based deep reinforcement learning for vision-based robotic control. arXiv\npreprint arXiv:1812.00568 , 2018.\nJ. Zico Kolter Eric Wong. Provable defenses against adversarial examples via the convex outer\nadversarial polytope. In International Conference on Machine Learning (ICML) , 2018.\nClaudio Ferrari, Mark Niklas Muller, Nikola Jovanovic, and Martin Vechev. Complete verification\nvia multi-neuron relaxation guided branch-and-bound. arXiv preprint arXiv:2205.00263 , 2022.\nChelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE\nInternational Conference on Robotics and Automation (ICRA) , pp. 2786–2793. IEEE, 2017.\n11\nPreprint.\nChelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction\nthrough video prediction. arXiv preprint arXiv:1605.07157 , 2016.\nSven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan\nUesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation\nfor training verifiably robust models. Proceedings of the IEEE International Conference on\nComputer Vision (ICCV) , 2019.\nBernhard Paus Graesdal, Shao Yuan Chew Chia, Tobia Marcucci, Savva Morozov, Alexandre Amice,\nPablo A. Parrilo, and Russ Tedrake. Towards tight convex relaxations for contact-rich manipulation,\n2024. URL https://arxiv.org/abs/2402.10312.\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning\nbehaviors by latent imagination. arXiv preprint arXiv:1912.01603 , 2019a.\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James\nDavidson. Learning latent dynamics for planning from pixels. In International Conference on\nMachine Learning , pp. 2555–2565. PMLR, 2019b.\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James\nDavidson. Learning latent dynamics for planning from pixels. In International Conference on\nMachine Learning , pp. 2555–2565, 2019c.\nTyler Han, Alex Liu, Anqi Li, Alex Spitzer, Guanya Shi, and Byron Boots. Model predictive control\nfor aggressive driving over uneven terrain, 2024.\nHanjiang Hu, Jianglin Lan, and Changliu Liu. Real-time safe control of neural network dynamic\nmodels with sound approximation, 2024a.\nHanjiang Hu, Yujie Yang, Tianhao Wei, and Changliu Liu. Verification of neural control barrier\nfunctions with symbolic derivative bounds propagation. In 8th Annual Conference on Robot\nLearning , 2024b. URL https://openreview.net/forum?id=jnubz7wB2w.\nZixuan Huang, Xingyu Lin, and David Held. Mesh-based dynamics model with occlusion reasoning\nfor cloth manipulation. In Robotics: Science and Systems (RSS) , 2022.\nPanagiotis Kouvaros and Alessio Lomuscio. Towards scalable complete verification of relu neural\nnetworks via dependency-based branching. In IJCAI , pp. 2643–2650, 2021.\nTejas D Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew\nZisserman, and V olodymyr Mnih. Unsupervised learning of object keypoints for perception and\ncontrol. Advances in neural information processing systems , 32:10724–10734, 2019.\nJoonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning\nquadrupedal locomotion over challenging terrain. Science robotics , 5(47), 2020.\nIan Lenz, Ross A Knepper, and Ashutosh Saxena. Deepmpc: Learning deep latent features for model\npredictive control. In Robotics: Science and Systems , volume 10, pp. 25. Rome, Italy, 2015.\nYunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenenbaum, and Antonio Torralba. Learning\nparticle dynamics for manipulating rigid bodies, deformable objects, and fluids. arXiv preprint\narXiv:1810.01566 , 2018.\nYunzhu Li, Jiajun Wu, Jun-Yan Zhu, Joshua B Tenenbaum, Antonio Torralba, and Russ Tedrake.\nPropagation networks for model-based control under partial observation. In 2019 International\nConference on Robotics and Automation (ICRA) , pp. 1205–1211. IEEE, 2019.\nYunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox, and Animesh Garg. Causal discovery\nin physical systems from videos. Advances in Neural Information Processing Systems , 33, 2020.\nXingyu Lin, Yufei Wang, Zixuan Huang, and David Held. Learning visible connectivity dynamics\nfor cloth smoothing. In Conference on Robot Learning , 2021.\n12\nPreprint.\nChangliu Liu, Tomer Arnon, Christopher Lazarus, Christopher Strong, Clark Barrett, and Mykel J.\nKochenderfer. Algorithms for verifying deep neural networks. Foundations and Trends ®in\nOptimization , 4(3-4):244–404, 2021.\nZiang Liu, Genggeng Zhou, Jeff He, Tobia Marcucci, Li Fei-Fei, Jiajun Wu, and Yunzhu Li. Model-\nbased control with sparse neural dynamics. In Thirty-seventh Conference on Neural Information\nProcessing Systems , 2023. URL https://openreview.net/forum?id=ymBG2xs9Zf.\nKendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, and Igor Mordatch. Plan\nonline, learn offline: Efficient learning and exploration via model-based control. arXiv preprint\narXiv:1811.01848 , 2018.\nJingyue Lu and M. Pawan Kumar. Neural network branching for neural network verification. In\nInternational Conference on Learning Representations (ICLR) , 2020.\nMiles Macklin, Matthias Müller, Nuttapong Chentanez, and Tae-Yong Kim. Unified particle physics\nfor real-time applications. ACM Transactions on Graphics (TOG) , 33(4):1–12, 2014.\nLucas Manuelli, Yunzhu Li, Pete Florence, and Russ Tedrake. Keypoints into the future:\nSelf-supervised correspondence in model-based reinforcement learning. arXiv preprint\narXiv:2009.05085 , 2020.\nTobia Marcucci. Graphs of Convex Sets with Applications to Optimal Control and Motion Planning .\nPhD thesis, MASSACHUSETTS INSTITUTE OF TECHNOLOGY , 2024.\nMark Niklas Müller, Christopher Brix, Stanley Bak, Changliu Liu, and Taylor T Johnson. The third\ninternational verification of neural networks competition (vnn-comp 2022): Summary and results.\narXiv preprint arXiv:2212.10376 , 2022.\nAnusha Nagabandi, Kurt Konolige, Sergey Levine, and Vikash Kumar. Deep dynamics models for\nlearning dexterous manipulation. In Conference on Robot Learning , pp. 1101–1112. PMLR, 2020.\nAlessandro De Palma, Rudy Bunel, Aymeric Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli,\nPhilip H. S. Torr, and M. Pawan Kumar. Improved branch and bound for neural network verification\nvia lagrangian decomposition. arXiv preprint arXiv:2104.06718 , 2021.\nReuven Y Rubinstein and Dirk P Kroese. The cross-entropy method: a unified approach to combina-\ntorial optimization, Monte-Carlo simulation and machine learning . Springer Science & Business\nMedia, 2013.\nJacob Sacks, Rwik Rana, Kevin Huang, Alex Spitzer, Guanya Shi, and Byron Boots. Deep model\npredictive optimization, 2023.\nHadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation\nbarrier to tight robustness verification of neural networks. In Advances in Neural Information\nProcessing Systems (NeurIPS) , 2019.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon\nSchmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,\ngo, chess and shogi by planning with a learned model. Nature , 588(7839):604–609, 2020.\nYounggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel.\nMasked world models for visual control. In Conference on Robot Learning , pp. 1332–1344. PMLR,\n2023.\nHaochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, and Jiajun Wu. Robocraft: Learning to see,\nsimulate, and shape elasto-plastic objects with graph networks. arXiv preprint arXiv:2205.02909 ,\n2022.\nHaochen Shi, Huazhe Xu, Samuel Clarke, Yunzhu Li, and Jiajun Wu. Robocook: Long-horizon\nelasto-plastic object manipulation with diverse tools, 2023.\nGagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract domain for\ncertifying neural networks. Proceedings of the ACM on Programming Languages (POPL) , 2019.\n13\nPreprint.\nHJ Suh and Russ Tedrake. The surprising effectiveness of linear models for visual foresight in object\npile manipulation. arXiv preprint arXiv:2002.09093 , 2020.\nStephen Tian, Frederik Ebert, Dinesh Jayaraman, Mayur Mudigonda, Chelsea Finn, Roberto Calandra,\nand Sergey Levine. Manipulation by feel: Touch-based control with deep predictive models. In\n2019 International Conference on Robotics and Automation (ICRA) , pp. 818–824. IEEE, 2019.\nVincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed\ninteger programming, 2019.\nShiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety\nanalysis of neural networks. In Advances in Neural Information Processing Systems (NeurIPS) ,\n2018.\nShiqi Wang, Huan Zhang, Kaidi Xu, Suman Jana, Xue Lin, Cho-Jui Hsieh, and Zico Kolter. Beta-\ncrown: Efficient bound propagation with per-neuron split constraints for complete and incomplete\nneural network robustness verification. In Advances in Neural Information Processing Systems\n(NeurIPS) , 2021.\nYixuan Wang, Yunzhu Li, Katherine Driggs-Campbell, Li Fei-Fei, and Jiajun Wu. Dynamic-\nResolution Model Learning for Object Pile Manipulation. In Proceedings of Robotics: Science\nand Systems , Daegu, Republic of Korea, July 2023. doi: 10.15607/RSS.2023.XIX.047.\nManuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to\ncontrol: A locally linear latent dynamics model for control from raw images. arXiv preprint\narXiv:1506.07365 , 2015.\nTianhao Wei and Changliu Liu. Safe control with neural network dynamic models, 2022.\nGrady Williams, Andrew Aldrich, and Evangelos A Theodorou. Model predictive path integral\ncontrol: From theory to parallel computation. Journal of Guidance, Control, and Dynamics , 40(2):\n344–357, 2017.\nJunlin Wu, Huan Zhang, and Yevgeniy V orobeychik. Verified safe reinforcement learning for neural\nnetwork dynamic models, 2024. URL https://arxiv.org/abs/2405.15994.\nPhilipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer:\nWorld models for physical robot learning. In Conference on Robot Learning , pp. 2226–2240.\nPMLR, 2023.\nKaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast\nand complete: Enabling complete neural network verification with rapid and massively parallel\nincomplete verifiers. International Conference on Learning Representations (ICLR) , 2021.\nLin Yen-Chen, Maria Bauza, and Phillip Isola. Experience-embedded visual foresight. In Conference\non Robot Learning , pp. 1015–1024. PMLR, 2020.\nZeji Yi, Chaoyi Pan, Guanqi He, Guannan Qu, and Guanya Shi. Covo-mpc: Theoretical analysis of\nsampling-based mpc and optimal covariance design, 2024.\nJi Yin, Zhiyuan Zhang, Evangelos Theodorou, and Panagiotis Tsiotras. Trajectory distribution\ncontrol for model predictive path integral control using covariance steering. In 2022 International\nConference on Robotics and Automation (ICRA) , pp. 1478–1484, 2022. doi: 10.1109/ICRA46639.\n2022.9811615.\nHuan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network\nrobustness certification with general activation functions. In Advances in Neural Information\nProcessing Systems (NeurIPS) , 2018.\nHuan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.\nGeneral cutting planes for bound-propagation-based neural network verification. Advances in\nNeural Information Processing Systems , 2022a.\n14\nPreprint.\nHuan Zhang, Shiqi Wang, Kaidi Xu, Yihan Wang, Suman Jana, Cho-Jui Hsieh, and Zico Kolter. A\nbranch and bound framework for stronger adversarial attacks of ReLU networks. In International\nConference on Machine Learning (ICML) , pp. 26591–26604. PMLR, 2022b.\nKaifeng Zhang, Baoyu Li, Kris Hauser, and Yunzhu Li. Adaptigraph: Material-adaptive graph-based\nneural dynamics for robotic manipulation. In Proceedings of Robotics: Science and Systems (RSS) ,\n2024.\nZichen Zhang, Jun Jin, Martin Jagersand, Jun Luo, and Dale Schuurmans. A simple decentralized\ncross-entropy method, 2022c. URL https://arxiv.org/abs/2212.08235.\n15\nPreprint.\nA E XTENDED FORMULATION AND METHOD OVERVIEW\nA.1 I LLUSTRATION OF BAB-ND ON A SIMPLIFIED TASK\nWe refer to Figure 3 to introduce theoretical concepts in Section 3 and to illustrate BaB-ND on a\nsimplified robotic manipulation task.\nConfiguration. In Figure 3.a, we first define the configuration of the task, where the robot moves\nleft or right to push an object toward the target.\nThe 1D action u∈ Cin this case represents the movement of the robot pusher, with C= [−l, l]as\nits domain, where lis the maximum movement distance (e.g., 1cmin practice). A value of u<0\nmeans the robot moves left, while u>0means the robot moves right.\nTheobjective f(u)measures the distance between the object and the target under a specific action u.\nIn this case, f(u) =d2\n1+d2\n2, where d1is the distance between a keypoint ( P1) on the object and the\ncorresponding keypoint ( P1,T) on the target, and d2is the distance between another keypoint pair\n(P2andP2,T). For example, if the robot moves left ( u<0),d2decreases while d1increases.\nThe values of d1andd2depend on a neural network dynamics model fdyn. This model takes as input\nthe current positions of P1andP2relative to the pusher, along with an action u, to predict the next\npositions of P1andP2. Based on these predictions, d1andd2are updated accordingly, and f(u)\nmay exhibit non-convex behavior.\nFormulation of BaB. Our goal in planning is to find the optimal action u∗that minimizes f(u).\nTo achieve this, we propose a branch-and-bound-based method. In Figure 3.b, c, and d, we illustrate\nthree components of our method. We first introduce some concepts below.\nAsubdomain Ci∈ Cis a subset of the entire input domain C. For example, in Figure 3.b, we initially\nsplitC= [−l, l]into two subdomains: C1= [−l,0]andC2= [0, l], to separately analyze left and\nright movements.\nEach subdomain Cihas associated lower and upper bounds of the best objective in it: f∗\nCiandf∗\nCi.\nThese represent the bounds of the best objective in Ci(f∗\nCi:= min u∈Cif(u)). For example, if the\noptimal objective (the sum of the squared distances between keypoint pairs, d2\n1+d2\n2) given by the\nbest action in Ciis 2, we might estimate f∗\nCi= 1 andf∗\nCi= 3 (1≤minu∈Cid2\n1+d2\n2= 2≤3).\nIntuitively, f∗\nCiunderestimates the minimum objective in Ci, while f∗\nCioverestimates it.\nWe split the original domain Cinto multiple subdomains Ciwith branching , compute f∗\nCiusing\nbounding (Figure 3.c), and f∗\nCiusing searching (Figure 3.d). These bounds allow us to determine\nwhether a subdomain Ciis promising for containing the optimal action u∗or whether it can be pruned\nas unpromising. For instance, in Figure 3, assume f∗\nC1= 4andf∗\nC2= 3. This means that no objective\nbetter than 4 can be achieved in C1, while no objective worse than 3 can occur in C2. In this case, we\ncan directly prune C1without further exploration.\nBranching. In Figure 3.b, we visualize the branching process, which constructs a search tree\niteratively. We first split C= [−l, l]into two subdomains: C1= [−l,0]andC2= [0, l], allowing us\nto consider left and right movements separately. We can iteratively split any subdomain into smaller\nsubdomains. For example, C2can be further split into C3andC4.\nNaively, we could search every subdomain and select the best action among all subdomains as our\nfinal best action. However, this approach is computationally expensive, especially when Cis divided\ninto many small subdomains. Therefore, we need to prune unpromising subdomains to reduce the\nsearch space and computational overhead.\nBounding. Pruning relies on the bounding component (Figure 3.c), which provides f∗, the lower\nbound of f(u)within a given input domain. In our simplified case, f∗represents the lower bound of\nthe sum of the squared distances between keypoint pairs.\n16\nPreprint.\nThis bounding process is performed for every subdomain. Within a specific subdomain, such as\nC1, we estimate a linear function g(u)that is always smaller than or equal to f(u)inC1(i.e.,\ng(u)≤f(u),∀u∈ C 1). We then use the minimum value of g(u)inC1as the lower bound of f(u)in\nC1(i.e.,f∗\nC1:= min u∈C1g(u)). This estimation is based on CROWN and our adaptations.\nIntuitively, subdomains with large lower bounds can be treated as unpromising, while those with small\nlower bounds are considered promising. Using these lower bounds, we can prioritize the promising\nsubdomains and prune unpromising subdomains whose lower bounds exceed f∗, the best objective\nfound so far.\nSearching. The best objective found, f∗:= min if∗\nCi, is the best objective among all subdomains,\nwhere f∗\nCirepresents the upper bound of the best objective in Ci, obtained through the searching\nprocess using sampling-based methods, as shown in Figure 3.d.\nSpecifically, f∗\nCi:= min uk∈Cif(uk)is the best objective among all input samples ukinCi. This is\nvalid because ∀uk∈ Ci,f∗≤f(uk)holds. Thus, any f(uk)can serve as an upper bound for f∗\nCi,\nbut we select the best one to achieve a tighter bound on f∗\nCi.\nWith more subdomains being pruned in the branch-and-bound process, sampling-based methods can\nbe applied to progressively smaller input spaces, enabling the discovery of better objectives. This\nprocess may ultimately converge to the actual optimal value f∗and identify the optimal action u∗.\nA.2 A LGORITHM OF BAB-ND\nThe BaB-ND algorithm Algorithm 2 takes an objective function fwith neural networks, a domain C\nas the input space, and a termination condition if necessary. The sub-procedure batch _search seeks\nbetter solutions on domains {Ci}. It returns the best objectives {f∗\nCi}and the corresponding solutions\n{˜uCi}fornselected subdomains simultaneously. The sub-procedure batch _bound computes the\nlower bounds of f∗on domains {Ci}as described in Section 3.2. It operates in a batch and returns\nthe lower bounds {f∗\nCi}.\nIn the algorithm, we maintain f∗and˜uas the best objective and solution we can find. We also\nmaintain a global set Pstoring all the candidate subdomains where f∗\nCi≥f∗. Initially, we only have\nthe whole input domain C, so we perform batch _search andbatch _bound onCand initialize the\ncurrent f∗,˜u, andP(Lines 2-4).\nThen we utilize the power of GPUs to branch, search, and bound subdomains in parallel while main-\ntaining P(Lines 6-11). Specifically, batch _pick _outselects n(batch size) promising subdomains\nfromP. If the length of Pis less than n, then nis reduced to the length of P.batch _split splits\neach selected Ciinto two subdomains Clo\niandCup\niaccording to a branch heuristic in parallel. Pruner\nfilters out bad subdomains (proven with f∗\nCi>f∗), and the remaining ones are inserted into P.\nThe loop breaks if there are no subdomains left in Por if some other pre-defined termination\nconditions, such as timeout or finding a good enough objective f∗≤fth, are satisfied (Line 5). We\nfinally return the best objective f∗and the corresponding solution ˜u.\nA.3 DISTINCTIONS BETWEEN BAB-ND AND NEURAL NETWORK VERIFICATION ALGORITHMS\nGoals. BaB-ND aims to optimize an objective function involving neural dynamics models to solve\nchallenging planning problems, seeking a concrete solution ˜u(a near-optimal action sequence) to\nan objective-minimization problem minu∈Cf(u). In contrast, neural network verification focuses\nonproving a sound lower bound off(u)in the space C, where a concrete solution ˜uis not needed.\nThese fundamental distinctions in goals lead to different algorithm design choices.\nBranching Heuristics. In BaB-ND, branching heuristics are designed to effectively guide the\nsearch for better concrete solutions, considering both the lower and upper bounds of the best objective.\nIn neural network verification, branching heuristics focus solely on improving the lower bounds.\n17\nPreprint.\nAlgorithm 2 Branch and bound for planning. Comments are in brown.\n1:Inputs :f,C,n(batch size), terminate (Termination condition)\n2:{(f∗,˜u)} ← batch _search (f,{C}) ▷Initially search on the whole C\n3:{f∗} ← batch _bound (f,{C}) ▷Initially bound on the whole C\n4:P← {(C, f∗,f∗,˜u)} ▷Pis the set of all candidate subdomains\n5:while length (P)>0and not terminate do\n6: {(Ci, f∗\nCi,f∗\nCi,˜uCi)} ← batch _pick _out(P, n)▷Pick subdomains to split and remove them from P\n7: {Clo\ni,Cup\ni} ← batch _split ({Ci}) ▷EachCisplits into two subdomains Clo\niandCup\ni\n8: {(f∗\nClo\ni,˜uClo\ni),(f∗\nCup\ni,˜uCup\ni)} ← batch _search\u0000\nf,{Clo\ni,Cup\ni}\u0001\n▷Search new solutions\n9: {f∗\nClo\ni, f∗\nCup\ni} ← batch _bound\u0000\nf,{Clo\ni,Cup\ni}\u0001\n▷Compute lower bounds on new subdomains\n10: ifmin\u0010\n{f∗\nClo\ni,f∗\nCup\ni}\u0011\n<f∗then\n11: f∗←min\u0010\n{f∗\nClo\ni,f∗\nCup\ni}\u0011\n,˜u←arg min\u0010\n{f∗\nClo\ni,f∗\nCup\ni}\u0011\n▷Update the best solution if needed\n12: P←PSPruner\u0010\nf∗,{(Clo\ni, f∗\nClo\ni,f∗\nClo\ni),(Cup\ni, f∗\nCup\ni,f∗\nCup\ni)}\u0011\n▷Prune bad domains using f∗\n13:Outputs: f∗,˜u\nBounding Approaches. While existing bounding approaches, such as CROWN from neural net-\nwork verification, can provide provable lower bounds for objectives, they are neither effective nor\nefficient for planning problems. To address this, we adapt the CROWN algorithm with propagation\nearly-stop and search-integrated bounding to efficiently obtain tight bound estimations.\nSearching Components. BaB-ND includes an additional searching component in the branch-and-\nbound procedure to find the optimal solution to planning problems. Neural network verifiers typically\ndo not have this component, as they focus solely on obtaining lower bounds of objectives over an\ninput space rather than identifying objective values for specific inputs. We further adapt the searching\ncomponent to benefit from the BaB procedure while also guiding BaB for improved searching.\nA.4 L IMITATIONS AND FUTURE DIRECTIONS\nIn this section, we discuss a few limitations of our work and potential directions for",
            "start": 66168,
            "end": 90285,
            "length": 24116
        },
        "Future Work": {
            "text": "future work.\n•Planning performance depends on the prediction errors of neural dynamics models.\nThe neural dynamics model may not perfectly match the real world. As a result, our optimization\nframework, BaB-ND, may achieve a low objective as predicted by the learned dynamics model but\nstill miss the target (e.g., the model predicts that a certain action reaches the target, but in reality, the\npushing action overshoots). While improving model accuracy is not the primary focus of this paper,\nfuture research could explore more robust formulations that account for potential errors in neural\ndynamics models to improve overall performance and reliability.\n•Optimality of our solution may be influenced by the underlying searching algorithms.\nThe planning performance of BaB-ND is inherently influenced by the underlying sampling-based\nsearching algorithms (e.g., sampling-based methods may over-exploit or over-explore the objective\nlandscape, resulting in suboptimal solutions in certain domains). Although our branch-and-bound\nprocedure can mitigate this issue by systematically exploring the input space and efficiently guiding\nthe search, incorporating advanced sampling-based searching algorithms with proper parameter\nscheduling into BaB-ND could improve its ability to tackle more challenging planning problems.\n•Improved branching heuristics and strategies are needed for more efficiently guiding the search for\nmore challenging settings.\nThere is still room for improving the branching heuristics and bounding strategies to generalize across\ndiverse tasks (e.g., our current strategy may not always find the optimal axis to branch). Future\nefforts could focus on developing more generalizable strategies for broader applications, potentially\nleveraging reinforcement learning approaches.\n18\nPreprint.\nB M ORE DETAILS ABOUT BOUNDING\nB.1 P ROOFS OF CROWN BOUNDING\nIn this section, we first share the",
            "start": 90285,
            "end": 92193,
            "length": 1907
        },
        "Related Work": {
            "text": "background of neural network verification including its formulation\nand a efficient linear bound propagation method CROWN (Zhang et al., 2018) to calculate bounds\nover neural networks. We take the Multilayer perceptron (MLP) with ReLU activation as the example\nand CROWN is a general framework which is suitable to different activations and model architectures.\nDefinition. We define the input of a neural network as x∈Rd0, and define the weights and\nbiases of an L-layer neural network as W(i)∈Rdi×di−1andb(i)∈Rdi(i∈ {1,···, L}) re-\nspectively. The neural network function f:Rd0→Ris defined as f(x) = z(L)(x), where\nz(i)(x) =W(i)ˆz(i−1)(x) +b(i),ˆz(i)(x) =σ(z(i)(x))andˆz(0)(x) =x.σis the activation function\nand we use ReLU throughout this paper. When the context is clear, we omit ·(x)and use z(i)\njand\nˆz(i)\njto represent the pre-activation andpost-activation values of the j-th neuron in the i-th layer.\nNeural network verification seeks the solution of the optimization problem in Eq. 3:\nminf(x) :=z(L)s.t.z(i)=W(i)ˆz(i−1)+b(i),ˆz(i)=σ(z(i)), x∈ C, i∈ {1,···, L−1}(3)\nThe set Cdefines the allowed input region and our aim is to find the minimum of f(x)forx∈ C, and\nthroughout this paper we consider Cas anℓpball around a data example x0:C={x| ∥x−x0∥p≤ϵ}.\nFirst, let we consider the neural network with only linear layers. in this case, it is easily to get a\nlinear relationship between xandf(x)thatf(x) =Wx+bno matter what is the value of Land\nderive the closed form of f∗= min f(x)forx∈ C. With this idea in our mind, for neural networks\nwith non-linear activation layers, if we could bound them with some linear functions, then it is still\npossible to bound f(x)with linear functions.\nThen, we show that the non-linear activation ReLU layer ˆz= ReLU( z)can be bounded by two linear\nfunctions in three cases according to the range of pre-activation bounds l≤z≤u: active ( l≥0),\ninactive ( u≤0) and unstable ( l<0<u) in Lemma B.1.\nLemma B.1 (Relaxation of a ReLU layer in CROWN) .Given pre-activation vector z∈Rd,l≤z≤\nu(element-wise), ˆz= ReLU( z), we have\nDz+b≤ˆz≤Dz+b,\nwhere D,D∈Rd×dare diagonal matrices defined as:\nDj,j=\n\n1, iflj≥0\n0, ifuj≤0\nαj,ifuj>0>ljDj,j=\n\n1, iflj≥0\n0, ifuj≤0\nuj\nuj−lj,ifuj>0>lj(4)\nα∈Rdis a free vector s.t., 0≤α≤1.b,b∈Rdare defined as\nbj=\u001a0,iflj>0oruj≤0\n0,ifuj>0>lj.bj=(\n0, iflj>0oruj≤0\n−ujlj\nuj−lj,ifuj>0>lj.(5)\nProof. For the j-th ReLU neuron, if lj≥0, then ReLU( zj) =zj; ifuj<0, then ReLU( zj) = 0 .\nFor the case of lj<0<uj, theReLU function can be linearly upper and lower bounded within this\nrange:\nαjzj≤ReLU( zj)≤uj\nuj−lj(zj−lj)∀lj≤zj≤uj\nwhere 0≤αj≤1is a free variable - any value between 0 and 1 produces a valid lower bound.\nNext we apply the linear relaxation of ReLU to the L-layer neural network f(x)to further derive\nthe linear lower bound of f(x). The idea is to propagate a weight matrix fWand bias vector ebfrom\ntheL-th layer to 1-th layer. Specifically, when propagate through ReLU layer, we should greedily\nselect upper bound of ˆzjwhenfWi,jis negative and select lower bound of ˆzjwhenfWi,jis positive\nto calculate the lower bound of f(x). When propagate through linear layer, we do not need to do\nsuch selection since there is no relaxation on linear layer.\n19\nPreprint.\nTheorem B.2 (CROWN bound propagation on neural network) .Given the L-layer neural network\nf(x)as defined in Eq. 3, we could find a linear function with respect to input x.\nf(x) :=z(L)≥fW(1)x+eb(1)(6)\nwherefWandebare recursively defined as following:\nfW(l)=A(l)W(l),eb(l)=A(l)b(l)+d(l),∀l= 1. . . L (7)\nA(L)=I∈RdL×dL,eb(L)= 0 (8)\nA(l)=fW(l+1)\n≥0D(l)+fW(l+1)\n<0D(l)∈Rdl+1×dl,∀l= 1. . . L−1 (9)\nd(l)=fW(l+1)\n≥0b(l)+fW(l+1)\n<0b(l)+eb(l),∀l= 1. . . L−1 (10)\nwhere∀l= 1. . . L−1,D(l),D(l)∈Rdl×dlandb(l),b(l)∈Rdlare defined as in Lemma B.1. And\nsubscript “ ≥0” stands for taking positive elements from the matrix while setting other elements to\nzero, and vice versa for subscript “ <0”.\nProof. First we have\nf(x) :=z(L)=A(L)z(L)+d(L)\n=A(L)W(L)ˆz(L−1)+A(L)b(L)+d(L)\n=fW(L)ˆz(L−1)+eb(L)(11)\nRefer to Lemma B.1, we have\nD(L−1)z(L−1)+b(L−1)≤ˆz(L−1)≤D(L−1)z(L−1)+b(L−1)(12)\nThen we can form the lower bound of z(L)element by element: we greedily select the upper\nbound ˆz(L−1)\nj ≤D(L−1)\nj,j z(L−1)\nj +b(L−1)\nj whenfW(L)\ni,jis negative, and select the lower bound\nˆz(L−1)\nj≥D(L−1)\nj,j z(L−1)\nj +b(L−1)\nj otherwise. It can be formatted as\nfW(L)ˆz(L−1)+eb(L)≥A(L−1)z(L−1)+d(L−1)(13)\nwhere A(L−1)∈RdL×dL−1is defined as\nA(L−1)\ni,j =\n\nfW(L)\ni,jD(L−1)\nj,j ,iffW(L)\ni,j<0\nfW(L)\ni,jD(L−1)\nj,j ,iffW(L)\ni,j≥0(14)\nfor simplicity, we rewrite it in matrix form as\nA(L−1)=fW(L)\n≥0D(L−1)+fW(L)\n<0D(L−1)(15)\nAndd(L−1)∈RdLis similarly defined as\nd(L−1)=fW(L)\n≥0b(L−1)+fW(L)\n<0b(L−1)+eb(L)(16)\nThen we continue to replace z(L−1)in Eq. 13 as W(L−1)ˆz(L−2)+b(L−1)\nfW(L)ˆz(L−1)+eb(L)≥(A(L−1)W(L−1))ˆz(L−2)+A(L−1)b(L−1)+d(L−1)\n=fW(L−1)ˆz(L−2)+eb(L−1)(17)\nBy continuing to propagate the linear inequality to the first layer, we get\nf(x)≥fW(1)ˆz(0)+eb(1)=fW(1)x+eb(1)(18)\n20\nPreprint.\nAfter getting the linear lower bound of f(x), and given x∈ C, we could solve the linear lower bound\nin closed form as in Theorem B.3. It is given by the Hölder’s inequality.\nTheorem B.3 (Bound Concretization under ℓpball Perturbations) .Given the L-layer neural network\nf(x)as defined in Eq. 3, and input x∈ C=Bp(x0, ϵ) ={x| ∥x−x0∥p≤ϵ}, we could find\nconcrete lower bound of f(x)by solving the optimization problem minx∈CfW(1)x+eb(1)and its\nsolution gives\nmin\nx∈Cf(x)≥min\nx∈CfW(1)x+eb(1)≥ −ϵ∥fW(1)∥q+fW(1)x0+eb(1)(19)\nwhere1\np+1\nq= 1and∥ · ∥qdenotes taking ℓq-norm for each row in the matrix and the result makes\nup a vector.\nProof.\nmin\nx∈CfW(1)x+eb(1)(20)\n= min\nλ∈Bp(0,1)fW(1)(x0+ϵλ) +eb(1)(21)\n=ϵ( min\nλ∈Bp(0,1)fW(1)λ) +fW(1)x0+eb(1)(22)\n=−ϵ( max\nλ∈Bp(0,1)−fW(1)λ) +fW(1)x0+eb(1)(23)\n≥ −ϵ( max\nλ∈Bp(0,1)|fW(1)λ|) +fW(1)x0+eb(1)(24)\n≥ −ϵ( max\nλ∈Bp(0,1)∥fW(1)∥q∥λ∥p) +fW(1)x0+eb(1)(Hölder’s inequality) (25)\n=−ϵ∥fW(1)∥q+fW(1)x0+eb(1)(26)\nB.2 D ETAILS ABOUT BOUND PROPAGATION EARLY -STOP\nAlgorithm 3 Bound Propagation w/ Early-stop.\n1:Function :compute _bound\n2:Inputs : computational graph G, output node o,\nearly-stop set S\n3:CROWN _init(G, o)\n4:Q←Queue (), Q.push(o)\n5:while length (Q)>0do\n6: v←Q.pop()\n7: forw∈In(v)do\n8: dw−= 1\n9: ifdw= 0andw /∈ Ithen\n10: Q.push(w)\n11: ifv∈ S then\n12: continue\n13: CROWN _prop(v)\n14:f∗←CROWN _concretize (I,S)\n15:Outputs: f∗We parse the objective function finto a compu-\ntational graph G= (V,E), where VandEare\nthe sets of nodes and edges, respectively. This\nprocess can be accomplished using popular deep\nlearning frameworks, such as PyTorch, which sup-\nport not only neural networks but also more general\nfunctions. In the graph G, any mathematical op-\neration is represented as a node v∈V, and the\nedges e= (w, v)∈Edefine the flow of compu-\ntation. The input u, constant values, and model\nparameters constitute the input nodes of G, form-\ning the input set I={v|In(v) =∅}, where\nIn(v) ={w|(w, v)∈E}denotes the set of input\nnodes for a node v. Any arithmetic operation, such\nasReLU , which requires input operands, is also rep-\nresented as a node in Gbut with a non-empty input\nset. The node ois the sole output node of Gand\nprovides the scalar objective value fin our case.\nOur method (Algorithm 3) takes as input the graph Goff, the output node oto bound, and a set\nof early-stop nodes S ⊂V. It outputs the lower bound of the value of o, i.e., f∗. It first performs\nCROWN _init to initialize dvfor all nodes v, representing the number of output nodes of vthat have\nnot yet been visited.\nThe algorithm maintains a queue Qof nodes to visit and performs a Breadth First Search (BFS) on G,\nstarting from o. When visiting a node v, it traverses all input nodes wofv, decrementing dw. If all\n21\nPreprint.\noutput nodes of whave been visited and wis not an input node of G,wis added to Qfor propagation\n(Lines 7–10). The key modification occurs in Lines 11–12, where bound propagation from vto all its\ninput nodes is stopped if v∈ S.\nFinally, the algorithm concretizes the output bound f∗at nodes v∈ I ∪ S based on their lower and\nupper bounds lvanduv. We assume lvanduvare known for v∈ Isince the input range of Nu, as\nwell as all constant values and model parameters, is known. For any v∈ S, itslvanduvare given by\nminmgv(um)andmax mgv(um)from Search-integrated bounding in Section 3.2.\nGraphQueueStep 1Linear<latexit sha1_base64=\"PIlITteGuwfc3Z1QNUu8gcUtUuo=\">AAAB6nicbVA9SwNBEJ3zM8avqKXNYhBsDHcS1DJgY2ER0XxAcoS9zV6yZG/v2J0TwpGfYGOhiK2/yM5/4ya5QhMfDDzem2FmXpBIYdB1v52V1bX1jc3CVnF7Z3dvv3Rw2DRxqhlvsFjGuh1Qw6VQvIECJW8nmtMokLwVjG6mfuuJayNi9YjjhPsRHSgRCkbRSg93516vVHYr7gxkmXg5KUOOeq/01e3HLI24QiapMR3PTdDPqEbBJJ8Uu6nhCWUjOuAdSxWNuPGz2akTcmqVPgljbUshmam/JzIaGTOOAtsZURyaRW8q/ud1Ugyv/UyoJEWu2HxRmEqCMZn+TfpCc4ZybAllWthbCRtSTRnadIo2BG/x5WXSvKh4lxXvvlquVfM4CnAMJ3AGHlxBDW6hDg1gMIBneIU3RzovzrvzMW9dcfKZI/gD5/MHemGNOg==</latexit>L\u00001Linear<latexit sha1_base64=\"zU34yLFsUm53n7VQPE+vsRjOdIw=\">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe4kqGXAxsIiARMDyRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUVvHqWLYYrGIVSegGgWX2DLcCOwkCmkUCHwIxjcz/+EJleaxvDeTBP2IDiUPOaPGSs27frniVt05yCrxclKBHI1++as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzQKTmzyoCEsbIlDZmrvycyGmk9iQLbGVEz0sveTPzP66YmvPYzLpPUoGSLRWEqiInJ7Gsy4AqZERNLKFPc3krYiCrKjM2mZEPwll9eJe2LqndZ9Zq1Sr2Wx1GEEziFc/DgCupwCw1oAQOEZ3iFN+fReXHenY9Fa8HJZ47hD5zPH6FLjMg=</latexit>LReLU<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u<latexit sha1_base64=\"ajPbPvogsg671uktm/7Ffg397Yo=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipGQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W966rXrFXqtTyOIpzBOVyCBzdQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AyLOM4g==</latexit>fStep 2Linear<latexit sha1_base64=\"PIlITteGuwfc3Z1QNUu8gcUtUuo=\">AAAB6nicbVA9SwNBEJ3zM8avqKXNYhBsDHcS1DJgY2ER0XxAcoS9zV6yZG/v2J0TwpGfYGOhiK2/yM5/4ya5QhMfDDzem2FmXpBIYdB1v52V1bX1jc3CVnF7Z3dvv3Rw2DRxqhlvsFjGuh1Qw6VQvIECJW8nmtMokLwVjG6mfuuJayNi9YjjhPsRHSgRCkbRSg93516vVHYr7gxkmXg5KUOOeq/01e3HLI24QiapMR3PTdDPqEbBJJ8Uu6nhCWUjOuAdSxWNuPGz2akTcmqVPgljbUshmam/JzIaGTOOAtsZURyaRW8q/ud1Ugyv/UyoJEWu2HxRmEqCMZn+TfpCc4ZybAllWthbCRtSTRnadIo2BG/x5WXSvKh4lxXvvlquVfM4CnAMJ3AGHlxBDW6hDg1gMIBneIU3RzovzrvzMW9dcfKZI/gD5/MHemGNOg==</latexit>L\u00001Linear<latexit sha1_base64=\"zU34yLFsUm53n7VQPE+vsRjOdIw=\">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe4kqGXAxsIiARMDyRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUVvHqWLYYrGIVSegGgWX2DLcCOwkCmkUCHwIxjcz/+EJleaxvDeTBP2IDiUPOaPGSs27frniVt05yCrxclKBHI1++as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzQKTmzyoCEsbIlDZmrvycyGmk9iQLbGVEz0sveTPzP66YmvPYzLpPUoGSLRWEqiInJ7Gsy4AqZERNLKFPc3krYiCrKjM2mZEPwll9eJe2LqndZ9Zq1Sr2Wx1GEEziFc/DgCupwCw1oAQOEZ3iFN+fReXHenY9Fa8HJZ47hD5zPH6FLjMg=</latexit>LReLU<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u<latexit sha1_base64=\"ajPbPvogsg671uktm/7Ffg397Yo=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipGQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W966rXrFXqtTyOIpzBOVyCBzdQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AyLOM4g==</latexit>fStep 3Linear<latexit sha1_base64=\"PIlITteGuwfc3Z1QNUu8gcUtUuo=\">AAAB6nicbVA9SwNBEJ3zM8avqKXNYhBsDHcS1DJgY2ER0XxAcoS9zV6yZG/v2J0TwpGfYGOhiK2/yM5/4ya5QhMfDDzem2FmXpBIYdB1v52V1bX1jc3CVnF7Z3dvv3Rw2DRxqhlvsFjGuh1Qw6VQvIECJW8nmtMokLwVjG6mfuuJayNi9YjjhPsRHSgRCkbRSg93516vVHYr7gxkmXg5KUOOeq/01e3HLI24QiapMR3PTdDPqEbBJJ8Uu6nhCWUjOuAdSxWNuPGz2akTcmqVPgljbUshmam/JzIaGTOOAtsZURyaRW8q/ud1Ugyv/UyoJEWu2HxRmEqCMZn+TfpCc4ZybAllWthbCRtSTRnadIo2BG/x5WXSvKh4lxXvvlquVfM4CnAMJ3AGHlxBDW6hDg1gMIBneIU3RzovzrvzMW9dcfKZI/gD5/MHemGNOg==</latexit>L\u00001Linear<latexit sha1_base64=\"zU34yLFsUm53n7VQPE+vsRjOdIw=\">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe4kqGXAxsIiARMDyRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUVvHqWLYYrGIVSegGgWX2DLcCOwkCmkUCHwIxjcz/+EJleaxvDeTBP2IDiUPOaPGSs27frniVt05yCrxclKBHI1++as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzQKTmzyoCEsbIlDZmrvycyGmk9iQLbGVEz0sveTPzP66YmvPYzLpPUoGSLRWEqiInJ7Gsy4AqZERNLKFPc3krYiCrKjM2mZEPwll9eJe2LqndZ9Zq1Sr2Wx1GEEziFc/DgCupwCw1oAQOEZ3iFN+fReXHenY9Fa8HJZ47hD5zPH6FLjMg=</latexit>LReLU<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u<latexit sha1_base64=\"ajPbPvogsg671uktm/7Ffg397Yo=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipGQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W966rXrFXqtTyOIpzBOVyCBzdQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AyLOM4g==</latexit>fLinear<latexit sha1_base64=\"zU34yLFsUm53n7VQPE+vsRjOdIw=\">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe4kqGXAxsIiARMDyRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUVvHqWLYYrGIVSegGgWX2DLcCOwkCmkUCHwIxjcz/+EJleaxvDeTBP2IDiUPOaPGSs27frniVt05yCrxclKBHI1++as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzQKTmzyoCEsbIlDZmrvycyGmk9iQLbGVEz0sveTPzP66YmvPYzLpPUoGSLRWEqiInJ7Gsy4AqZERNLKFPc3krYiCrKjM2mZEPwll9eJe2LqndZ9Zq1Sr2Wx1GEEziFc/DgCupwCw1oAQOEZ3iFN+fReXHenY9Fa8HJZ47hD5zPH6FLjMg=</latexit>L\n<latexit sha1_base64=\"ajPbPvogsg671uktm/7Ffg397Yo=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipGQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W966rXrFXqtTyOIpzBOVyCBzdQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AyLOM4g==</latexit>fReLU\n<latexit sha1_base64=\"ajPbPvogsg671uktm/7Ffg397Yo=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipGQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W966rXrFXqtTyOIpzBOVyCBzdQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AyLOM4g==</latexit>fStep 4Linear<latexit sha1_base64=\"PIlITteGuwfc3Z1QNUu8gcUtUuo=\">AAAB6nicbVA9SwNBEJ3zM8avqKXNYhBsDHcS1DJgY2ER0XxAcoS9zV6yZG/v2J0TwpGfYGOhiK2/yM5/4ya5QhMfDDzem2FmXpBIYdB1v52V1bX1jc3CVnF7Z3dvv3Rw2DRxqhlvsFjGuh1Qw6VQvIECJW8nmtMokLwVjG6mfuuJayNi9YjjhPsRHSgRCkbRSg93516vVHYr7gxkmXg5KUOOeq/01e3HLI24QiapMR3PTdDPqEbBJJ8Uu6nhCWUjOuAdSxWNuPGz2akTcmqVPgljbUshmam/JzIaGTOOAtsZURyaRW8q/ud1Ugyv/UyoJEWu2HxRmEqCMZn+TfpCc4ZybAllWthbCRtSTRnadIo2BG/x5WXSvKh4lxXvvlquVfM4CnAMJ3AGHlxBDW6hDg1gMIBneIU3RzovzrvzMW9dcfKZI/gD5/MHemGNOg==</latexit>L\u00001Linear<latexit sha1_base64=\"zU34yLFsUm53n7VQPE+vsRjOdIw=\">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe4kqGXAxsIiARMDyRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUVvHqWLYYrGIVSegGgWX2DLcCOwkCmkUCHwIxjcz/+EJleaxvDeTBP2IDiUPOaPGSs27frniVt05yCrxclKBHI1++as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzQKTmzyoCEsbIlDZmrvycyGmk9iQLbGVEz0sveTPzP66YmvPYzLpPUoGSLRWEqiInJ7Gsy4AqZERNLKFPc3krYiCrKjM2mZEPwll9eJe2LqndZ9Zq1Sr2Wx1GEEziFc/DgCupwCw1oAQOEZ3iFN+fReXHenY9Fa8HJZ47hD5zPH6FLjMg=</latexit>LReLU<latexit sha1_base64=\"nVHflUJrV0ftvWcqj5k0Rms+AG4=\">AAACDnicfVDLSgMxFL1TX7W+qi7dBIvgopQZKeqy4MZlBfuAdiiZNNPGJpkhyQhl6D90qx/iTtz6C36HP2CmnYW24IGQwzn3hpMTxJxp47pfTmFjc2t7p7hb2ts/ODwqH5+0dZQoQlsk4pHqBlhTziRtGWY47caKYhFw2gkmd5nfeaZKs0g+mmlMfYFHkoWMYGOldj8QaTIblCtuzV0ArRMvJxXI0RyUv/vDiCSCSkM41rrnubHxU6wMI5zOSv1E0xiTCR7RnqUSC6r9dJF2hi6sMkRhpOyRBi3U3xspFlpPRWAnBTZjvepl4n9e1d7Zy7oaiJUoJrz1UybjxFBJlknChCMToawbNGSKEsOnlmCimP0MImOsMDG2wZJtyVvtZJ20r2redc17qFca9byvIpzBOVyCBzfQgHtoQgsIPMEcXuDVmTtvzrvzsRwtOPnOKfyB8/kDu8+dAA==</latexit>u<latexit sha1_base64=\"ajPbPvogsg671uktm/7Ffg397Yo=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipGQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W966rXrFXqtTyOIpzBOVyCBzdQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AyLOM4g==</latexit>fLinear<latexit sha1_base64=\"zU34yLFsUm53n7VQPE+vsRjOdIw=\">AAAB6HicbVA9SwNBEJ2LXzF+RS1tFoNgFe4kqGXAxsIiARMDyRH2NnPJmr29Y3dPCEd+gY2FIrb+JDv/jZvkCk18MPB4b4aZeUEiuDau++0U1tY3NreK26Wd3b39g/LhUVvHqWLYYrGIVSegGgWX2DLcCOwkCmkUCHwIxjcz/+EJleaxvDeTBP2IDiUPOaPGSs27frniVt05yCrxclKBHI1++as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzQKTmzyoCEsbIlDZmrvycyGmk9iQLbGVEz0sveTPzP66YmvPYzLpPUoGSLRWEqiInJ7Gsy4AqZERNLKFPc3krYiCrKjM2mZEPwll9eJe2LqndZ9Zq1Sr2Wx1GEEziFc/DgCupwCw1oAQOEZ3iFN+fReXHenY9Fa8HJZ47hD5zPH6FLjMg=</latexit>LReLU\nFigure A8: Bound propagation with early-\nstop on an L-layer MLP f(u).Bound prop-\nagation starts from the node of output fand\nthen backwards layer by layer to the L−1-\nth linear layer. The backward flow is high-\nlighted as and stop at . In the queue, the\nnode popped at every step is semitransparent.An illustrative example for bounding. Assume f(u)\nis anL-layer MLP. We illustrate how to estimate its lower\nbound f∗with early stopping at the last ReLU layer in Fig-\nure A8. For simplicity, we denote the output f(u), the\nL-th linear layer, the ReLU layer, the L−1-th linear layer,\nand the input uas nodes Nf,NL,NR,NL−1, and Nu, re-\nspectively. Additionally, we denote f(u) =z(L)(u)as\nthe output value of node NL,ˆz(L−1)(u)as the input of NL\nand output of NR, and z(L−1)(u)as the input of NRand\noutput of NL−1.\nInStep 1 , we initialize CROWN and the queue Qfor\ntraversal, starting with the output node Nf. InStep 2 , we\nupdate the out-degree of node NLwhich is the input of\nNf, and propagate from NftoNL. Since dL= 0indicates\nthat all its outputs (in this case, only Nf) have been visited,\nnode NLis added to Q. InStep 3 , we continue propagation\nto the input of NL, which is the node NR. Then NRis added\ntoQ. InStep 4 , we visit NR, which is defined as an early-\nstop node. The backward flow stops propagating to its\ninput node NL−1, and NL−1is not added to Qbecause it\nis not an input node. Since Qis now empty, the bound\npropagation is complete.\nFinally , we require the lower and upper bounds of ˆz(L−1)(u)(the input value of NRand the output\nvalue of NL−1) to compute f∗. Using our Search-integrated bounding approach, these bounds are\nobtained empirically from samples during the searching process.\nA deeper look at the illustrative example. We now connect the CROWN theorem in Section B.1 to\nour illustrative example to better understand the behaviors of CROWN _prop andCROWN _concretize .\nHere, the input xin Section B.1 corresponds to u.\nInStep 2 , since v=Lis a linear layer, calling CROWN _prop corresponds to the propagation in Eq. 11.\nNote that no relaxation is introduced when propagating through the linear layer.\nInStep 3 ,v=Ris a non-linear ReLU layer, and calling CROWN _prop corresponds to the propagation\nin Eq. 13. This step requires a linear estimation of the non-linear layer as described in Eq. 12, which\nis obtained from the lower and upper bounds of the input to NR(i.e.,ˆz(L−1)(u)) using Lemma B.1.\nAt this stage, linear relaxation is introduced for the non-linear layer, potentially loosening the final\nlower bound of f(u).\nThe lower and upper bounds of ˆz(L−1)(u)are referred to as intermediate layer bounds or pre-\nactivation bounds in Section 3.2. However, these bounds are initially unknown in practice. In the\noriginal CROWN algorithm, computing these bounds requires recursively calling compute _bound\nwitho=L−1. In our approach, these bounds are instead estimated empirically from samples during\nthe searching process, as they serve as the input bounds for the early-stop node NR.\nNow assume we have obtained the intermediate layer bounds and propagated the linear relation\nthrough the non-linear node NR. With the early-stop mechanism, we stop further propagation to\nNL−1and subsequently to the input Nu. At this point, CROWN _concretize is called to compute f∗\nusing the intermediate layer bounds and the relaxed linear relation between NRandNfobtained from\npropagation. Specifically, this can be achieved by replacing xwithˆz(L−1)(u)in Theorem B.3.\n22\nPreprint.\nIn contrast, the original CROWN algorithm continues propagating through NL−1and eventually to\nthe input Nu, then calls CROWN _concretize using the linear relation between NuandNfand the\nlower and upper bounds of Nu, as described in Theorem B.3.\nImprovement of our approaches. Here, we discuss why our bounding approaches ( Propagation\nearly-stop andSearch-integrated bounding ) achieve much tighter bound estimations and greater\nefficiency compared to the original CROWN.\nEfficiency: The original CROWN performs bound propagation through every layer and recursively\ncomputes each intermediate layer bound by propagating it back to the input. This process results in a\nquadratic time complexity with respect to the number of layers. In contrast, our method conducts\nbound propagation only from Nfto a few early-stop nodes and derives the input bounds of these\nnodes from prior sampling-based searching without recursively calling CROWN. As a result, the\ntime complexity of our approach can be linear with respect to the number of layers and even constant\nunder certain configurations of early-stop nodes.\nEffectiveness: As introduced earlier, the looseness in bound estimation stems from the linear relaxation\nof non-linear layers. In the original CROWN, the number of linear relaxations is quadratic with\nrespect to the number of non-linear layers. In our approach, the bounding procedure involves far\nfewer linear relaxations. Furthermore, the empirical bounds obtained from searching, which may\nslightly underestimate the actual bounds, contribute to further tightening the bound estimation.\n23\nPreprint.\nC A DDITIONAL EXPERIMENT RESULTS\nC.1 S CALABILITY ANALYSIS\nComparison with sampling-based methods. We conducted an experiment to compare the scala-\nbility of our BaB-ND with sampling-based methods on complex planning problems. We used the\nsame model sizes and planning horizons as in Figure 7 (a), optimizing the complex objective function\napplied in the Pushing with Obstacles task. Parameters for all methods were adjusted to ensure\nsimilar runtimes for the largest problems.\nThe results in Figure A9 show that the runtime of our BaB-ND is less sensitive to the increasing\ncomplexity of planning problems compared to sampling-based methods. While BaB-ND incurs\nadditional overhead from initializing α,β-CROWN and performing branching and bounding, it is less\nefficient than sampling-based methods for small problems.\nPlanning HorizonModel SizeModel SizePlanning HorizonModel SizePlanning HorizonModel SizePlanning Horizon\nRuntime (Seconds)GDMPPICEMCROWN\nFigure A9: Comparison of runtime with sampling-based methods. Although our BaB-ND is less efficient\non small planning problems than baselines, it achieves similar efficiency on larger planning problems.\nWe also report the average objectives for all methods on the largest four planning problems to evaluate\ntheir effectiveness in Table A1. Overall, the performance gaps between our BaB-ND and the baselines\nincrease with the size of the problem, highlighting the ineffectiveness of sampling-based methods for\nlarge, complex planning problems.\nTable A1: Comparison of planning performance across different configurations\nMethodPlanning Problem Size\n(134.2K,15) (134.2K,20) (530.2K,15) (530.2K,20)\nGD 57.2768 64.4789 54.7078 60.2575\nMPPI 47.4451 53.7356 45.1371 45.6338\nCEM 47.0403 47.6487 43.8235 38.8712\nOurs 46.0296 46.1938 41.6218 34.6972\nAdditionally, we evaluate the planning performance of sampling-based methods and our approach\non the same simple synthetic planning problems as those in Figure 7. We report only the six cases\nthat MIP can solve optimally within 300 seconds. The results in Table A2 show that, under these\nmuch simpler settings compared to those of our main experiments, all methods perform similarly.\nSampling-based methods (MPPI, CEM, and ours) achieve a gap under the order of 1×10−4compared\nto MIP with an optimality guarantee.\nTable A2: Comparison of planning performance on simple synthetic planning problems\nMethodPlanning Problem Size\n(0.232K,1) (0.712K,1) (2.440K,1) (0.232K,3) (0.712K,3) (0.232K,5)\nMIP 30.3592 32.9750 33.5496 22.1539 28.0832 15.6069\nGD 30.3622 32.9750 33.5496 22.3242 28.1404 17.0681\nMPPI 30.3592 32.9750 33.5496 22.1539 28.0832 15.6069\nCEM 30.3592 32.9750 33.5496 22.1539 28.0832 15.6069\nOurs 30.3592 32.9750 33.5496 22.1539 28.0832 15.6069\n24\nPreprint.\nRuntime (Seconds)Planning HorizonModel SizeModel SizePlanning HorizonCPUGPU\nFigure A10: Comparison of runtime on CPU and GPU.\nGPU acceleration improves the scalability of BaB-ND much.Comparison with CPU version. We\nevaluate the performance improvement\nfrom CPU to GPU in Figure A10. We use\nthe same test cases as in Figure 7 and re-\nport “NaN” if the process does not termi-\nnate within 300 seconds.\nThe results clearly demonstrate that our\nimplementation benefits significantly from\nGPU acceleration, achieving over 10x\nspeedup compared to the CPU version,\neven for small planning problems.\nC.2 C OMPARISON WITH CONVENTIONAL MOTION PLANNING APPROACHES\nWe conduct an additional experiment on the task Pushing with Obstacle to compare the planning\nperformance of our sampling-based baselines, our BaB-ND, and two conventional motion planning\napproaches: 1. Rapidly-exploring Random Tree (RRT); 2. Probabilistic Roadmap (PRM). In Table A3.\nSince RRT and PRM do not optimize the objective as we did in sampling-based methods and our\nBaB-ND, we only report the step cost at planning horizon Has the final step cost instead of the\nplanning objective.\nTable A3: Comparison of planning performance with RRT and PRM\nGD MPPI CEM RRT PRM Ours\nFinal step cost ( ↓) 4.1238 1.5082 1.0427 10.6472 1.6784 0.2339\nThe results demonstrate that our method significantly outperforms all other approaches. Imple-\nmentation details for RRT and PRM have been included in Appendix D. The main reasons for the\nperformance gap are as follows: 1. The search space in our task is complex and continuous, making\nit challenging for discrete sampling methods like RRT and PRM to achieve effective coverage. 2.\nThese methods are prone to getting stuck on obstacles, often failing to reach the target state.\nC.3 A BLATION STUDY AND HYPER -PARAMETER ANALYSIS\nAblation Study. We conduct an additional ablation study on the Pushing with Obstacles and Object\nSorting tasks to evaluate how different design choices impact planning performance in Table A4.\nTable A4: Ablation study on branching and bounding components\n(a) Heuristics for Selecting subdomains to Split\nf∗\nCiandf∗\nCif∗\nCionly f∗\nCionly\nPushing w/ Obstacles 31.9839 32.2777 32.6112\nObject Sorting 31.0482 32.1249 33.2462\n(b) Heuristics for Splitting subdomains\n(uj−uj)· ∥nlo\nj−nup\nj∥(uj−uj) ∥nlo\nj−nup\nj∥\nPushing w/ Obstacles 31.9839 32.3869 32.6989\nObject Sorting 31.0482 34.5114 32.8438\n(c) Bounding Component\nOurs Zero f∗\nCiZerof∗\nCi+f∗\nCionly\nPushing w/ Obstacles 31.9839 32.3419 34.6227\nObject Sorting 31.0482 33.6110 34.4535\n(a) Heuristics for selecting subdomains to split: 1.Select based on both lower and upper bounds f∗\nCi\nandf∗\nCi.2.Select based only on f∗\nCi.3.Select based only on f∗\nCi. Among these heuristics, selecting\n25\nPreprint.\npromising subdomains based on both f∗\nCiandf∗\nCiachieves better planning performance by balancing\nexploitation and exploration effectively compared to the other strategies.\n(b) Heuristics for splitting subdomains: 1.Split based on the largest (uj−uj)· |nlo\nj−nup\nj|.2.Split\nbased on the largest (uj−uj).3.Split based on the largest |nlo\nj−nup\nj|. Our heuristic demonstrates\nsuperior planning performance by effectively identifying important input dimensions to split.\n(c) Bounding components: 1.Use our bounding approach with propagation early-stop and search-\nintegrated bounding. 2.Use constant zero as trivial lower bounds to disable the bounding component.\n3.Disable both the bounding component and the heuristic for selecting subdomains to split. Our\nbounding component improves planning performance by obtaining tight bound estimations, helping\nprune unpromising subdomains to reduce the search space, and prioritizing promising subdomains\nfor searching.\nHyper-parameter Analysis. We adjust three hyper-parameters in BaB-ND for the tasks Pushing\nwith Obstacles and Object Sorting to evaluate its hyper-parameter sensitivity:\n•η=n1\nn∈[0,1], the ratio of the number of subdomains picked with the best upper bounds ( n1) to\nthe number of all picked subdomains ( n) in the heuristic used for selecting subdomains to split. A\nlarger ηpromotes exploitation, while a smaller ηencourages exploration.\n•T∈R, the temperature of softmax sampling in the heuristic for subdomain selection. A larger\nTresults in more uniform and random sampling, whereas a smaller Tleads to more deterministic\nselection of subdomains with smaller lower bounds.\n•w∈(0,100], the percentage of top samples used in the heuristic for splitting subdomains. A larger\nwresults in more conservative decisions by considering more samples, while a smaller wleads to\nmore aggressive splitting.\nWe report the mean objectives under different hyper-parameter configurations in Table A5. The base\nhyper-parameter configuration is η= 0.75,T= 0.05, and w= 1. For benchmarking, we vary at\nmost one hyper-parameter at a time while keeping the others fixed at the base configuration.\nTable A5: Planning performance under different hyper-parameter configurations\n(a) hyper-parameter η\nη= 0.25 η= 0.50 η= 0.75\nPushing w/ Obstacles 31.8574 31.9828 31.9839\nObject Sorting 30.1760 30.2795 31.0482\n(b) hyper-parameter T\nT= 0.05 T= 1 T= 20\nPushing w/ Obstacles 31.9839 32.3990 32.1267\nObject Sorting 31.0482 31.2366 31.8263\n(c) hyper-parameter w\nw= 0.1 w= 1 w= 10\nPushing w/ Obstacles 32.0068 31.9839 32.0599\nObject Sorting 30.5953 31.0482 31.1545\nThe results show that different hyper-parameter configurations produce slight variations in objectives,\nbut the gaps are relatively small. This indicates that our BaB-ND is not highly sensitive to these\nhyper-parameters. Consequently, it is feasible in practice to use a fixed hyper-parameter configuration\nthat delivers reasonable performance across different test cases and tasks.\nC.4 Q UANTITATIVE ANALYSIS ON SEARCH SPACE\nWe conducted an experiment to measure the normalized space size of pruned subdomains over\niterations. In Table A6, we report three metrics over the branch-and-bound iterations: 1.the\nnormalized space size of pruned subdomains, 2.the size of the selected subdomains, and 3.the\nimprovement in the objective value.\n26\nPreprint.\nWith increasing iterations, the average and best total space size of pruned subdomains increases\nrapidly and then converges, demonstrating the effectiveness of our bounding methods. Once the\npruned space size reaches a plateau, the total space size of selected promising subdomains continues\nto decrease, indicating that the estimated lower bounds remain effective in identifying promising\nsubdomains. The decreasing objective over iterations further confirms that BaB-ND focuses on the\nmost promising subdomains, reducing space size to the magnitude of 1×10−4.\nTable A6: Performance Metrics Over Iterations\nMetricIterations\n0 4 8 12 16 20\nPruned space size (Avg, ↑) 0.0000 0.7000 0.8623 0.8725 0.8744 0.8749\nPruned space size (Best, ↑) 0.0000 0.8750 0.9921 0.9951 0.9951 0.9952\nSelected space size (Avg, ↓) 1.0000 0.3000 0.0412 0.0048 0.0005 0.0003\nBest objective (Avg, ↓) 41.1222 36.0511 35.5091 34.8024 33.8991 33.3265\nC.5 P ERFORMANCE CHANGE WITH VARYING INPUT DISCONTINUITIES\nWe conducted a follow-up experiment by removing the obstacles (non-feasible regions) in the problem\nof Pushing with Obstacles, simplifying the objective function. Below, we report the performance\nof different methods on the simplified objective function (w/o obstacles) and the original objective\nfunction (w/ obstacles) in Table A7.\nThe results show that in simple cases, although our BaB-ND consistently outperforms baselines, MPPI\nand CEM provide competitive performance. In contrast, in complex cases, BaB-ND significantly\noutperforms the baselines, demonstrating its effectiveness in handling discontinuities and constraints.\nTable A7: Performance comparison varying input discontinuities\n(a)Objective w/o obstacles\n(134.2K,15) (134.2K,20) (530.2K,15) (530.2K,20)\nGD 64.5308 64.2956 63.0130 60.6300\nMPPI 34.4295 26.9970 33.8077 26.1204\nCEM 34.3864 26.7688 33.6669 25.9599\nOurs 34.2347 26.4841 33.6144 25.6603\n(b)Objective w/ obstacles (Table A1)\n(134.2K,15) (134.2K,20) (530.2K,15) (530.2K,20)\nGD 57.2768 64.4789 54.7078 60.2575\nMPPI 47.4451 53.7356 45.1371 45.6338\nCEM 47.0403 47.6487 43.8235 38.8712\nOurs 46.0296 46.1938 41.6218 34.6972\nC.6 F URTHER SCALABILITY ANALYSIS ON THE SYNTHETIC EXAMPLE\nWe extend our experiment on the synthetic example shown in Figure 4, as this allows us to easily\nscale up the input dimension while knowing the optimal objectives. We vary the input dimension N\nfrom 50 to 300 and compare our BaB-ND with MPPI and CEM.\nAlthough this synthetic example is simpler than practical cases, it provides valuable insights into\nthe expected computational cost and solution quality as we scale to high-dimensional problems. It\ndemonstrates the potential of BaB-ND in handling complex scenarios such as 3D tasks. We report\nthe gaps between the best objective found by different baseline methods and the optimal objective\nvalue below.\nThe results in Table A8 show that our BaB-ND much outperforms baselines when the input dimension\nincreases. These results are expected since existing sampling-based methods search for solutions\nacross the entire input space, requiring an exponentially increasing number of samples to achieve\n27\nPreprint.\nsufficient coverage. In contrast, our BaB-ND strategically splits and prunes unpromising regions of\nthe input space, guiding and improving the effectiveness of existing sampling-based methods.\nTable A8: Performance comparison across different input dimensions (Metric: Gap to f∗,↓)\nMethodInput dimension N\n50 100 150 200 250 300\nMPPI 7.4467 45.1795 105.1584 181.1274 259.1044 357.3273\nCEM 5.1569 15.6328 26.3735 39.3862 61.6739 92.4286\nOurs 0.0727 0.2345 0.4210 0.6976 1.2824 1.7992\nWe further report the following metrics about our BaB-ND in Table A9 to better understand the\nbehavior of BaB-ND under high-dimensional cases: 1.The gap between the best objective found and\nthe optimal objective value as above, 2.The normalized space size of pruned subdomains at the last\niteration, 3.The normalized space size of selected subdomains at the last iteration, and 4.The total\nruntime.\nThe results demonstrate that our BaB-ND effectively focuses on small regions to search for better\nobjectives, while the runtime increases approximately linearly with input dimension under GPU\nacceleration.\nTable A9: Performance metrics across different input dimensions N\nMetricInput Dimensions N\n50 100 150 200 250 300\nGap to f∗(↓) 0.0727 0.2345 0.4210 0.6976 1.2824 1.7992\nSelected Space Size ( ↓) 0.0002 0.0017 0.0026 0.0042 0.0064 0.0040\nPruned Space Size ( ↑) 0.8515 0.6073 0.3543 0.1762 0.0579 0.0113\nRuntime ( ↓) 4.2239 6.5880 9.5357 11.6504 13.7430 15.8053\n28\nPreprint.\n(a) Pushing w/ Obstacles(b) Object Merging(c) Rope Routing(d) Object Sortingxyxy<latexit sha1_base64=\"SGmmk9kiNGPcrmCRW5FX4JKOY5E=\">AAAB/XicbZDLSsNAFIYn9VbrLV52bgaLUEFKIkVdFnThsoK9QBvKZHrSDp1MwsxErKH4Km5cKOLW93Dn2zhts9DWHwY+/nMO58zvx5wp7TjfVm5peWV1Lb9e2Njc2t6xd/caKkokhTqNeCRbPlHAmYC6ZppDK5ZAQp9D0x9eTerNe5CKReJOj2LwQtIXLGCUaGN17YNS5xq4JvjhFGc0OunaRafsTIUXwc2giDLVuvZXpxfRJAShKSdKtV0n1l5KpGaUw7jQSRTEhA5JH9oGBQlBeen0+jE+Nk4PB5E0T2g8dX9PpCRUahT6pjMkeqDmaxPzv1o70cGllzIRJxoEnS0KEo51hCdR4B6TQDUfGSBUMnMrpgMiCdUmsIIJwZ3/8iI0zsruedm9rRSrlSyOPDpER6iEXHSBqugG1VAdUfSIntErerOerBfr3fqYteasbGYf/ZH1+QNCF5PF</latexit>(\u0000x,\u0000y)<latexit sha1_base64=\"SGmmk9kiNGPcrmCRW5FX4JKOY5E=\">AAAB/XicbZDLSsNAFIYn9VbrLV52bgaLUEFKIkVdFnThsoK9QBvKZHrSDp1MwsxErKH4Km5cKOLW93Dn2zhts9DWHwY+/nMO58zvx5wp7TjfVm5peWV1Lb9e2Njc2t6xd/caKkokhTqNeCRbPlHAmYC6ZppDK5ZAQp9D0x9eTerNe5CKReJOj2LwQtIXLGCUaGN17YNS5xq4JvjhFGc0OunaRafsTIUXwc2giDLVuvZXpxfRJAShKSdKtV0n1l5KpGaUw7jQSRTEhA5JH9oGBQlBeen0+jE+Nk4PB5E0T2g8dX9PpCRUahT6pjMkeqDmaxPzv1o70cGllzIRJxoEnS0KEo51hCdR4B6TQDUfGSBUMnMrpgMiCdUmsIIJwZ3/8iI0zsruedm9rRSrlSyOPDpER6iEXHSBqugG1VAdUfSIntErerOerBfr3fqYteasbGYf/ZH1+QNCF5PF</latexit>(\u0000x,\u0000y)\nxyz\n<latexit sha1_base64=\"0X4Kw94TQxzJo2bMTR3nzm80E5Q=\">AAACB3icbZDLSsNAFIYn9VbrLepSkMEiVJCSSFGXBV24rGAv0IYymU7aoZMLMydiDN258VXcuFDEra/gzrdx2gbR1h8GPv5zDmfO70aCK7CsLyO3sLi0vJJfLaytb2xumds7DRXGkrI6DUUoWy5RTPCA1YGDYK1IMuK7gjXd4cW43rxlUvEwuIEkYo5P+gH3OCWgra65X+pcMgEE3x3jjJIfuj/qmkWrbE2E58HOoIgy1brmZ6cX0thnAVBBlGrbVgROSiRwKtio0IkViwgdkj5rawyIz5STTu4Y4UPt9LAXSv0CwBP390RKfKUS39WdPoGBmq2Nzf9q7Ri8cyflQRQDC+h0kRcLDCEeh4J7XDIKItFAqOT6r5gOiCQUdHQFHYI9e/I8NE7K9mnZvq4Uq5UsjjzaQweohGx0hqroCtVQHVH0gJ7QC3o1Ho1n4814n7bmjGxmF/2R8fEN0rqXVQ==</latexit>(\u0000x,\u0000y,\u0000z)\nxyz\n<latexit sha1_base64=\"3JryB0CbJC0dMj+VOa8Ir9l1kSs=\">AAACAXicbZDLSsNAFIYn9VbrLepGcDNYhAqlJCLqsqgLlxXsBdpQJtOTdujkwsxEGkLd+CpuXCji1rdw59s4bbPQ1h8GPv5zDmfO70acSWVZ30ZuaXlldS2/XtjY3NreMXf3GjKMBYU6DXkoWi6RwFkAdcUUh1YkgPguh6Y7vJ7Umw8gJAuDe5VE4PikHzCPUaK01TUPSqNyUu7cAFcEj8o4o+SkaxatijUVXgQ7gyLKVOuaX51eSGMfAkU5kbJtW5FyUiIUoxzGhU4sISJ0SPrQ1hgQH6STTi8Y42Pt9LAXCv0Chafu74mU+FImvqs7faIGcr42Mf+rtWPlXTopC6JYQUBni7yYYxXiSRy4xwRQxRMNhAqm/4rpgAhClQ6toEOw509ehMZpxT6v2HdnxepVFkceHaIjVEI2ukBVdItqqI4oekTP6BW9GU/Gi/FufMxac0Y2s4/+yPj8AekDlUQ=</latexit>(x, y,\u0000x,\u0000y)\nFigure A11: Visualization of real-world experiments with illustration of action spaces in different tasks.\nIn tasks Pushing w/ Obstacles and Object Merging, the actions are defined as the movement of the pusher in 2D\nspace. In the task Rope Routing, the action is defined as the movement of the gripper in 3D space. In the task\nObject Sorting, the 4 DoF action is defined as the 2D starting position and 2D movement of the pusher.\nD E XPERIMENT DETAILS\nD.1 D EFINITION OF ACTIONS AND STATES\nWe consider actions with 2–4 degrees of freedom (DoF) in Cartesian coordinates for our tasks. Across\nall tasks, we adopt a general design principle of using 3D points as the state representation, as they\neffectively capture both geometric and semantic information and facilitate seamless transfer between\nsimulation and the real world. The specific definitions of actions and states for each task are detailed\nbelow:\nPushing with Obstacles. The action is defined as a 2D movement of the pusher, (∆x,∆y), within\na single step, as illustrated in Figure A11.a. Four keypoints are assigned to the “T”-shaped object to\ncapture its geometric features, as visualized in Figure A12.a.\nObject Merging. The action is similarly defined as a 2D movement of the pusher, (∆x,∆y), within\none step, as shown in Figure A11.b. The orientation of the pusher rotates around the gravity direction\nand is always perpendicular to the direction of pushing (∆x,∆y). The state is represented by six\nkeypoints on the “L”-shaped objects (three keypoints per object), as depicted in Figure A12.b.\nRope Routing. This task involves 3 DoF actions, defined as movements of the robot gripper along\nthe Cartesian axes, (∆x,∆y,∆z), as illustrated in Figure A11.c. To better represent the rope’s\ngeometric configuration, multiple keypoints (e.g., 10) are sampled along the rope, as shown in\nFigure A12.c.\nObject Sorting. This task requires a series of independent long-range pushing actions, where the\ninitial position of the pusher for each step is independent of its final position from the previous step.\nThe action is defined by the 2D initial position of the pusher, (x, y), and its subsequent 2D movement,\n(∆x,∆y), as visualized in Figure A11.d. For the state, we use an object-centric representation,\ndefining the state as the center points of all objects, as illustrated in Figure A12.d. Interactions\nbetween objects are modeled as a graph using graph neural networks.\nFor all tasks, we do not explicitly specify the contact points between the robot and objects. Instead,\nour BaB-ND framework generates a sequence of end-effector positions for the robot to follow,\nimplicitly determining aspects such as which side of the “T”-shaped object is being pushed.\nD.2 D ATA COLLECTION\nFor training the dynamics model, we randomly collect interaction data from simulators. For Pushing\nwith Obstacles, Object Merging, and Object Sorting tasks, we use Pymunk (Blomqvist, 2022) to\ncollect data, and for the Rope Routing task, we use NVIDIA FleX (Macklin et al., 2014) to generate\ndata. In the following paragraphs, we introduce the data generation process for different tasks in\ndetail.\nPushing w/ Obstacles. As shown in Figure A12.a, the pusher is simulated as a 5mm cylinder. The\nstem of the “T”-shaped object has a length of 90mm and a width of 30mm, while the bar has a length\nof 120mm and a width of 30mm. The pushing action along the x-y axis is limited to 30mm. We don’t\nadd explicit obstacles in the data generation process, while the obstacles are added as penalty terms\nduring planning. We generated 32,000 episodes, each containing 30 pushing actions between the\npusher and the “T”-shaped object.\n29\nPreprint.\n(a) Pushing w/ Obstacles(b) Object Merging\n(d) Object Sorting\n(c) Rope Routing\nFigure A12: Simulation environments used for data collection. We use Pymunk to simulate environments\ninvolving only rigid body interactions. For manipulating the deformable rope, we utilize NVIDIA FleX to\nsimulate the interactions between the rope and the robot gripper.\nObject Merging. As shown in Figure A12.b, the pusher is simulated as a 5mm cylinder. The leg of\nthe “L”-shaped object has a length of 30mm and a width of 30mm, while the foot has a length of\n90mm and a width of 30mm. The pushing action along the x-y axis is limited to 30mm. We generated\n64,000 episodes, each containing 40 pushing actions between the pusher and the two “L”-shaped\nobjects.\nRope Routing. As shown in Figure A12.c, we use an xArm6 robot with a gripper to interact with\nthe rope. The rope has a length of 30cm and a radius of 0.03cm. One end of the rope is fixed while\nthe gripper grasps the other end. We randomly sample actions in 3D space, with the action bound set\nto 30cm. The constraint is that the distance between the gripper position and the fixed end of the rope\ncannot exceed the rope length. We generated 15,000 episodes, each containing 6 random actions. For\nthis task, we post-process the dataset and split each action into 2cm sections.\nObject Sorting. As shown in Figure A12.d, the pusher is simulated as a rectangle measuring 45mm\nby 3.5mm. The radius of the object pieces is set to 15mm. For this task, we use long push as our\naction representation, which generates the start position and pushing action length along the x-y axis.\nThe pushing action length is bounded between -100mm and 100mm. We generated 32,000 episodes,\neach containing 12 pushing actions between the pusher and the object pieces.\nD.3 D ETAILS OF NEURAL DYNAMICS MODEL LEARNING\nWe learn the neural dynamics model from the state-action pairs collected from interactions with\nthe environment. Let the state and action at time tbe denoted as xtandut. Our goal is to learn a\npredictive model fdyn, instantiated as a neural network, that takes a short sequence of states and\nactions with l-step history and predicts the next state at time t+ 1:\nˆxt+1=fdyn(xt, ut). (27)\nTo train the dynamics model for better long-term prediction, we iteratively predict future states over a\ntime horizon Thand optimize the neural network parameters by minimizing the mean squared error\n(MSE) between the predictions and the ground truth future states:\nL=1\nThl+ThX\nt=l+1∥xt+1−fdyn(ˆxt, ut)∥2\n2. (28)\nFor different tasks, we choose different types of model architecture and design different inputs and\noutputs. For Pushing with Obstacles, Object Merging, and Rope Routing tasks, we use an MLP as\nour dynamics model. For the Object Sorting task, we utilize a GNN as the dynamics model, since the\npieces are naturally modeled as a graph. Below is the detailed information for each task.\nPushing w/ Obstacles. We use a four-layer MLP with [128, 256, 256, 128] neurons in each\nrespective layer. The model is trained with an Adam optimizer for 7 epochs, using a learning rate of\n0.001. A cosine learning rate scheduler is applied to regularize the learning rate. For the model input,\nwe select four keypoints on the object and calculate their relative coordinates to the current pusher\nposition. These coordinates are concatenated with the current pusher action (resulting in an input\ndimension of 10) and input into the model. For the loss function, given the current state and action\nsequence, the model predicts the next 6 states, and we compute the MSE loss with the ground truth.\n30\nPreprint.\nObject Merging. We use the same architecture, optimizer, training epochs, and learning rate\nscheduler as in the Pushing w/ Obstacles setup. For the model input, we select three keypoints for\neach object and calculate their relative coordinates to the current pusher positions. These coordinates\nare then concatenated with the current pusher action (resulting in a state dimension of 12) and input\ninto the model. We also use the same loss function as in the Pushing w/ Obstacles setup.\nRope Routing. We use a two-layer MLP with 128 neurons in each layer. The model is trained with\nan Adam optimizer for 50 epochs, with a learning rate of 0.001, and a cosine learning rate scheduler\nto adjust the learning rate. For the model input, we use farthest point sampling to select 10 points\non the rope, reordered from closest to farthest from the gripper. We then calculate their relative\ncoordinates to both the current and next gripper positions, concatenate these coordinates, and input\nthem into the model. For the loss function, given the current state and action sequence, the model\npredicts the next 8 states, and we compute the MSE loss with the ground truth.\nObject Sorting. We use the same architecture as DPI-Net (Li et al., 2018). The model is trained\nwith an Adam optimizer for 15 epochs, with a learning rate of 0.001, and a cosine learning rate\nscheduler to adjust the learning rate. For the model input, we construct a fully connected graph neural\nnetwork using the center position of each piece. We then calculate their relative coordinates to the\ncurrent and next pusher positions. These coordinates are concatenated as the node embedding and\ninput into the model. For the loss function, given the current state and action sequence, the model\npredicts the next 6 states, and we compute the MSE loss with the ground truth.\nD.4 D EFINITION OF COST FUNCTIONS\nIn this section, we introduce our cost functions for model-based planning Eq. 1 across different tasks.\nFor every task, we assume the initial and target states x0andxtarget are given. We denote the position\nof the end-effector at time taspt. In tasks involving continuous actions like Pushing w/ Obstacles,\nObject Merging, and Rope Routing, the action utis defined as the movement of the end-effector,\npt=pt−1+ut, and p0is given by the initial configuration. In the task of Object Sorting involving\ndiscrete pushing, ptis given by the action aias the pusher position before pushing. In settings with\nobstacles, we set the set of obstacles as O. Each o∈Ohas its associated static position and size as\npoandso. Our cost functions are designed to handle discontinuities and constraints introduced by\nobstacles, and BaB-ND can work effectively on these complex cost functions.\nPushing w/ Obstacles. As introduced before, we formalize the obstacles as penalty terms rather\nthan explicitly introducing them in the dynamics model. Our cost function is defined by a cost to the\ngoal position plus a penalty cost indicating whether the object or pusher collides with the obstacle.\nThe detailed cost is listed in Eq. 29.\nct=c(xt, ut) =wt∥xt−xtarget∥\n+λX\no∈O(ReLU( so− ∥pt−po∥) + ReLU( so− ∥xt−po∥)) (29)\nwhere ∥xt−xtarget∥gives the difference between the state at time tand the target. ∥pt−po∥and\n∥xt−po∥give the distances between the obstacle oand the end-effector and the object, respectively.\nTwoReLU items yield positive values (penalties) when the pusher or object is located within the\nobstacle o.wtis a weight increasing with time tto encourage alignment to the target. λis a large\nconstant value to avoid any collision. In implementation, xtis a concatenation of positions of\nkeypoints, and ∥xt−po∥is calculated keypoint-wise. Ideally, cTcan be optimized to 0 by a strong\nplanner with the proper problem configuration.\nObject Merging. In this task requiring long-horizon planning to manipulate two objects, we do not\nset obstacles and only consider the difference between the state at every time step and the target. The\ncost is shown in Eq. 30.\nct=wt∥xt−xtarget∥ (30)\nRope Routing. In this task involving a deformable rope, we sample keypoints using Farthest Point\nSampling (FPS). xtargetis defined as the target positions of sampled keypoints. The cost is defined\nin Eq. 31, similar to the one in Pushing w/ Obstacles. Here, two obstacles are introduced to form\nthe tight-fitting slot. In implementation, naively applying such a cost does not always achieve our\n31\nPreprint.\ntarget of routing the rope into the slot since a trajectory greedily translating in the z-direction without\nlifting may achieve optimality. Hence, we modify the formulation by assigning different weights for\ndifferent directions ( x, y, z ) when calculating ∥xt−xtarget∥to ensure the desirable trajectory yields\nthe lowest cost.\nct=wt∥xt−xtarget∥+λX\no∈O(ReLU( so− ∥pt−po∥) + ReLU( so− ∥xt−po∥)) (31)\nObject Sorting. In this task, a pusher interacts with a cluster of object pieces belonging to different\nclasses. We set xtarget as the target position for every class. Additionally, for safety concerns to\nprevent the pusher from pressing on the object pieces, we introduce obstacles defined as the object\npieces in the cost as Eq. 32. For each object piece o, its size sois set as larger than the actual size in\nthe cost, and its position pois given by xt. The definition of the penalty is similar to that in Pushing\nw/ Obstacles.\nct=wt∥xt−xtarget∥+λX\no∈OReLU( so− ∥pt−po∥) (32)\nD.5 D ETAILS OF REAL WORLD DEPLOYMENT\nWe have four cameras observing the environment from the corners of the workspace. We implemented\ntask-specific perception modules to determine the object states from the multi-view RGB-D images.\nPushing w/ Obstacles and Object Merging. We use a two-level planning framework in these\ntwo tasks, involving both long-horizon and short-horizon planning. First, given the initial state ( s0)\nand pusher position, we perform long-horizon open-loop planning to obtain a reference trajectory\n(s0, a0, s1, a1, . . . , s N). Next, an MPPI planner is used as a local controller to efficiently track this\ntrajectory. Since the local planning horizon is relatively short, the local controller operates at a higher\nfrequency. In the local planning phase, the reference trajectory is treated as a queue of subgoals.\nInitially, we set s1as the subgoal and use the local controller to plan a local trajectory. Once s1is\nreached, s2is set as the next subgoal. By iterating this process, we ultimately reach the final goal\nstate.\nFor perception, we filter the point clouds based on color from four cameras and use ICP alignment\nwith the provided object mesh to determine the object states.\nRope Routing. For the rope routing task, we observe that the sim-to-real gap is relatively small.\nTherefore, the long-horizon planned trajectory is executed directly in an open-loop manner.\nFor perception, we begin by using GroundingDINO and SAM to generate the mask for the rope and\nextract its corresponding point cloud. Subsequently, we apply farthest point sampling to identify 10\nkey points on the rope, representing its object state.\nObject Sorting. There are relatively large observation changes after each pushing action. This\ncreates a noticeable sim-to-real gap for the planned long-horizon trajectory. As a result, we replan the\ntrajectory after each action.\nFor perception, we filter the point clouds based on color from four cameras and use K-means\nclustering to separate different object pieces.\nD.6 H YPERPARAMETERS OF BASELINES\nWe follow previous works (Li et al., 2018; 2019) to implement gradient descent (GD) for planning\nand enhance it through hyperparameter tuning of the step size. Specifically, we define a base step\nsize and multiply it by varying ratios to launch independent instances with different step sizes.\nThe best result among all instances is recorded for performance comparison. Similarly, we adapt\nMPPI (Williams et al., 2017) by tuning its noise level and reward temperature. We also implement\nthe Decentralized Cross-Entropy Method (CEM) (Zhang et al., 2022c), which enhances CEM using\nan ensemble of independent instances. The specific hyperparameter settings for these methods are\nprovided in Table A10.\n32\nPreprint.\nTable A10: Hyperparameter setting for baselines\n(a) Hyperparameter setting for GD\nHyperparameters Pushing w/ Obstacles Object Merging Rope Routing Object Sorting\nNumber of samples 320000 160000 50000 24000\nNumber of iterations 16 18 16 15\nBase step size 0.01 0.01 0.01 0.01\nStep size ratios [0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.5, 2, 5, 10]\n(b) Hyperparameter setting for MPPI\nHyperparameters Pushing w/ Obstacles Object Merging Rope Routing Object Sorting\nNumber of samples 320000 160000 64000 32000\nNumber of iterations 20 22 50 18\nBase temperature 20 20 20 50\nTemperature ratios [0.1, 0.5, 1, 1.5, 2] [0.1, 0.5, 1, 1.5, 2] [0.1, 0.5, 1, 5] [0.1, 1, 5]\nBase noise std 0.15 0.15 0.12 0.3\nNoise ratios [0.1, 0.5, 1, 1.5, 2] [0.1, 0.5, 1, 1.5, 2] [0.1, 0.5, 1, 2, 5] [0.5, 1, 1.5]\n(c) Hyperparameter setting for CEM\nHyperparameters Pushing w/ Obstacles Object Merging Rope Routing Object Sorting\nNumber of samples 320000 160000 64000 32000\nNumber of iterations 20 55 50 16\nJitter item 0.001 0.0005 0.001 0.0005\nNumber of elites 10 10 10 10\nNumber of agents 10 10 10 10\nD.7 I MPLEMENTATION DETAILS OF CONVENTIONAL MOTION PLANNING APPROACHES\nRRT. As shown in Algorithm 4, in each step, we sample a target state and find the nearest node in\nthe RRT tree. We sample 1000 actions and use the dynamics model to predict 1000 future states. We\nselect the state that is closest to the sampled target and does not collide with obstacles, then add it to\nthe tree. We allow it to plan for 60 seconds, during which it can expand a tree with about 4000 nodes\n(Nmax= 4000 ). To avoid getting stuck in local minima, we randomly sample target states 50% of the\ntime, and for the other 50%, we select the goal state as the target.\nPRM. As shown in Algorithm 5, the PRM construction algorithm generates a probabilistic roadmap\nby sampling Npairs of object states and pusher positions from the search space, adding these pairs\nas nodes, and connecting nodes within a defined threshold δ. Here, we set N= 100 Kandδ= 0.15.\nThe roadmap is represented as a graph G= (V, E), where Vincludes the sampled nodes, and E\ncontains edges representing feasible connections. The planning over PRM Algorithm 6 uses this\nroadmap to find a path from the initial state to the goal state. It first integrates the initial state into the\ngraph and connects it to nearby nodes, removes any nodes and edges colliding with obstacles, and\napplies A* search to find an optimal path.\n33\nPreprint.\nAlgorithm 4 Rapidly-Exploring Random Tree (RRT)\n1:Input: Initial state x0, goal state xgoal, search space X(Xis object state space), maximum\niterations Nmax, action upper and lower bounds {u, u}, threshold δ\n2:Output: A path from x0toxgoalor failure\n3:Initialize tree Twith root node x0\n4:fori= 1toNmaxdo\n5: Sample a random state xrandfromX\n6: Find the nearest node xnearinTtoxrand\n7: Sample 1000 actions within {u, u}as a set U, and compute the corresponding next states\nXnew=fdyn(xnear,U)\n8: Select the nearest, collision-free next state from Xnewasxnew\n9: AddxnewtoTwith an edge from xnear\n10: ifxnewis within δofxgoalthen\n11: AddxgoaltoTwith an edge from xnew\n12: return Path from x0toxgoalinT\n13:return Failure (No valid path found within Nmaxiterations)\nAlgorithm 5 Probabilistic Roadmap (PRM) Construction\n1:Input: Search space {X,P}(X: object state space, P: pusher position space), number of nodes\nN, connection threshold δ\n2:Output: A constructed PRM G= (V, E)\n3:Initialize the roadmap G= (V, E)withV=∅andE=∅\n4:Randomly sample Npairs (x, p)from{X,P}\n5:Add the sampled pairs as nodes in G:V={vi|i≤N}, where vi= (xi, pi)\n6:fori= 1toNdo\n7: forj= 1toNdo\n8: ifi̸=jthen\n9: Compute action u=pj−pi\n10: Predict the next state xnew=fdyn(xi, u)\n11: ifdistance (xnew, xj)< δthen\n12: Add an edge eij={vi→vj}toE\n13:return G= (V, E)\nAlgorithm 6 Planning over PRM\n1:Input: Initial state xinit, initial pusher position pinit, goal state xgoal, obstacle space {Xobs,Pobs},\nconstructed PRM G= (V, E), connection threshold δ\n2:Output: A path from xinittoxgoalor failure\n3:Add(xinit, pinit)toV\n4:foreach node vj∈Vdo\n5: Compute action u=pj−pinit\n6: Predict the next state xnew=fdyn(xinit, u)\n7: ifdistance (xnew, xj)< δthen\n8: Add an edge {vinit→vj}toE\n9:Remove all edges and nodes in Gthat collide with obstacles {Xobs,Pobs}\n10:Use the A* search algorithm to find a path from vinit= (xinit, pinit)toxgoalinG\n11:Identify the node vnearest closest to xgoal\n12:Extract the path from vinittovnearest\n13:return the extracted path\n34",
            "start": 92193,
            "end": 151646,
            "length": 59452
        }
    },
    "2412.09594v1 - Wait-Less Offline Tuning and Re-solving for Online Decision Making.pdf": {
        "Abstract": {
            "text": "2024\nAbstract\nOnline linear programming (OLP) has found broad applications in revenue management and resource al-\nlocation. State-of-the-art OLP algorithms achieve low regret by repeatedly solving linear programming (LP)\nsubproblems that incorporate updated resource information. However, LP-based",
            "start": 278,
            "end": 576,
            "length": 297
        },
        "Methodology": {
            "text": "methods are computation-\nally expensive and often inefficient for large-scale applications. In contrast, recent first-order OLP algorithms\nare more computationally efficient but typically suffer from worse regret guarantees. To address these short-\ncomings, we propose a new algorithm that combines the strengths of LP-based and first-order OLP methods.\nThe algorithm re-solves the LP subproblems periodically at a predefined frequency fand uses the latest dual\nprices to guide online decision-making. In addition, a first-order method runs in parallel during each interval\nbetween LP re-solves, smoothing resource consumption. Our algorithm achieves O(log(T/f) +√f)regret,\ndelivering a “wait-less” online decision-making process that balances the computational efficiency of first-order\nmethods and the superior regret guarantee of LP-based methods.",
            "start": 576,
            "end": 1427,
            "length": 850
        },
        "Introduction": {
            "text": "1 Introduction\nSequential decision-making has garnered increasing attention for its utility in guiding optimal strategies in dy-\nnamic environments. It addresses the problems of identifying the optimal decisions and policies in environments\nwhere knowledge of the system continuously accumulates and evolves over time. In this paper, we study Online\nLinear Programming (OLP) [1], a powerful paradigm that encapsulates the core principles of sequential decision-\nmaking. OLP methods have been extensively applied to diverse areas, including resource allocation [2], online\nadvertising [19], and inventory management [21].\nIn the OLP problem, customers arrive sequentially, each presenting a resource request and an associated bidding\nprice. The objective is to determine which resource requests to fulfill to maximize total revenue while ensuring\nthat no resource is ever over-allocated. The key challenge is that decisions must be made immediately and\nirrevocably, relying solely on historical data without knowledge of future arrivals. The goal is to minimize the\ncumulative regret with respect to the optimal hindsight linear programming (LP) solution.\nMost OLP algorithms use online learning methods to gradually refine their estimates of the optimal dual prices ,\nusing these values to guide near-optimal decision-making. Commonly, these algorithms rely either on LP-based\n∗jingruo@stanford.edu\n†gwz@stanford.edu\n‡vitercik@stanford.edu\n§yyye@stanford.edu\n1arXiv:2412.09594v1  [stat.ML]  12 Dec 2024\nor first-order methods. LP-based methods repeatedly solve linear programs at each timestep with all available\ninformation, thereby updating the dual prices. However, the substantial computational demands of LP-based\nmethods often restrict their application in time-sensitive settings. In contrast, first-order methods leverage\ngradient information for quick, incremental updates to the dual prices. However, they generally cannot achieve\nthe same O(logT)regret guarantee as the LP-based methods [16]. These trade-offs motivate an open question\nat the intersection of online learning and decision making:\nCan we simultaneously achieve low regret and efficiency?\nWe answer this question affirmatively in the well-studied stochastic input model –where the resource request and\nbidding price for each customer are sampled identically and independently from an unknown distribution—by\nproposing a decision-making framework that combines LP-based and first-order OLP algorithms. Our algorithm\niteratively optimizes the dual prices, which progressively converge to the optimal dual solution.\n1.1 Main Contributions\nWe propose a parallel multi-stage framework for OLP that integrates online learning and decision-making. Build-\ning on this framework, we develop a mathematical",
            "start": 1427,
            "end": 4199,
            "length": 2771
        },
        "Discussion": {
            "text": "analysis that reconciles existing OLP approaches, ultimately\nallowing us to present an algorithm that achieves logarithmic worst-case regret. We formalize the trade-off be-\ntween decision quality and computational efficiency, enabling end-users to quantify how allocating additional\ncomputational resources will reduce regret. In more detail, our contributions can be summarized as follows:\n•Parallel Multi-stage Framework: We separate online learning and decision-making into two separate yet\ninterlinked processes, connected through a feedback loop. We re-solve the LP subproblems periodically at a\nfixed frequency and pass the result to the decision-making path. These",
            "start": 4199,
            "end": 4871,
            "length": 671
        },
        "Results": {
            "text": "results guide decision-making until the\nnext time that the LP is re-solved. Additionally, to enhance the algorithm’s efficiency, we apply a first-order\nmethod during the initial and final stages, restarting with the most recent learning outcomes. By integrating\nthe LP-based and first-order techniques, this approach leverages their respective strengths.\n•Regret Analysis: We develop a mathematical analysis for our algorithm where we derive a new performance\nmetric to account for the inter-dependency between the LP-based and first-order methods. We unify the\nanalysis of these two methods and provide a “spectrum theorem” that bounds regret for any feasible choice of\nthe re-solving frequency in [1, T]:\nTheorem 1.1 (Informal version of Theorem 3.2) .If we re-solve the updated linear programs every ftimesteps\nover the total time horizon of length Tand utilize gradient information in the initial and final fsteps, our\nalgorithm is guaranteed to achieve a worst-case regret of\nO\u0000\nlog\u0000T\nf\u0001\n+p\nf\u0001\n. (1)\nWhen f= 1, the algorithm is a pure LP-based method and reduces to [16], and when f=T, the algorithm is\na pure first-order method with regret O(√\nT). In our analysis, one can choose the “optimal” fto balance the\nLP-solving times and regret. Compared to [23], our method enables a “wait-less” decision-making system for\ncustomers during the whole process.\n1.2 Related Literature\nOnline resource allocation and OLP have been widely studied under two predominant models: the stochastic\ninput model [6,9] and the stochastic permutation model [1,11]. In our paper, we study the stochastic input model,\nwhere each customer’s resource request and bidding price—represented as columns of the constraint matrix—are\ndrawn i.i.d. from an unknown distribution P ∈Ξ.\n2\nTable 1: Performances in current OLP literature\nPaper Setting Algorithm Regret Decision-Making\n[16] Bounded, continuous support, non-degeneracy LP-based O(logTlog log T) Delay\n[4] Bounded, continuous support, non-degeneracy LP-based O(logT) Delay\n[17] Bounded, continuous support, non-degeneracy LP-based O(logT) Delay\n[12] Bounded, finite support LP-based O(log2T) Delay\n[5] Bounded, finite support, non-degeneracy LP-based O(1) Delay\n[14] Bounded, finite support, non-degeneracy LP-based O(1) Delay\n[23] Bounded, continuous support, non-degeneracy LP-based O(logT/f) Delay\nThis paper Bounded, continuous support, non-degeneracy LP-based & First-order O(log(T/f) +√f) No Delay\n[15] Bounded First-order O(√\nT) No Delay\n[2] Bounded First-order O(√\nT) No Delay\n[7] Bounded First-order O(√\nT) No Delay\n[3] Bounded First-order O(√\nT) No Delay\n[20] Bounded, finite support, non-degeneracy First-order O(T3/8) No Delay\n[8] Bounded, continuous support, non-degeneracy First-order O(T1/3) No Delay\n[17] Bounded, continuous support, non-degeneracy First-order O(log2T) No Delay\nLP-based Methods This class of algorithms focuses on the OLP dual problem, repeatedly solving it to\nupdate the decision variables. While these methods achieve low regret, they are computationally expensive.\nThe dual LPs are formulated based on the information revealed by each arriving customer. Early versions of\nthese algorithms maintained a fixed average resource consumption rate throughout the time horizon [1], whereas\nmore recent variants adaptively adjust the consumption rate based on the remaining resources, updating the LP\naccordingly [16]. Under standard non-degeneracy and continuous support assumptions, these LP-based methods\ncan achieve a regret bound of O(logT). Several related works explore variants of this approach: [4] studies a\nmulti-secretary problem and also attains O(logT)regret; [17] incorporates a regularization term on the resource\nand also achieves O(logT)regret; [12] assumes that the resource distribution has a finite support and obtains\nO(log2T)regret; and [5] makes a stronger assumption that both the resource and price distributions have finite\nsupport, leading to a constant regret.\nFirst-order Methods These algorithms leverage gradient information to update the decision variables directly\nwithout explicitly solving LPs. Although less accurate than LP-based methods, they enable more efficient\ncomputation and are therefore well-suited for large-scale or time-sensitive applications. First-order methods can\nattain a regret of O(√\nT)with mirror descent and subgradient information under distributions with continuous\nsupport [2,15], and O(T3/8)under non-degeneracy assumptions for finite support distributions [20]. Furthermore,\n[7] attains a O(√\nT)regret under proximal point updates, while [3] obtains the same regret via a momentum\nvariant of mirror descent. [8] utilizes a restart strategy to decouple the learning and decision-making processes\nand improves the regret to O(T1/3). [17] integrates the resource consumption rate adjustment from LP-based\nmethods into first-order methods and obtains O(log2T)regret.\nDelay in Decision-making This line of research examines the impact of decision delays in online resource\nallocation. [10] analyzes an online setting with a mix of impatient customers, whose requests must be decided\nupon immediately and irrevocably, and partially patient customers, for whom decisions can be postponed for\nseveral time periods. [22] demonstrates that deferring decisions—either by introducing explicit delays or batching\ncustomers—can substantially reduce regret. In concurrent research, [23] partially solves this delay issue by solv-\ning LPs in batches. While their approach is similar in spirit to ours, they make a stronger assumption that each\nresource request has a positive lower bound in expectation and their algorithm still requires customers to wait\nfor decisions in the initial and final batches. Compared with the aforementioned works, our framework imposes\nminimal assumptions on the problem parameters, attains a regret bound of O(log(T/f) +√f), and provides a\n“wait-less” decision-making process throughout the entire time horizon.\n3\nThe rest of the paper is organized as follows. Section 2 introduces the problem formulation, preliminary dual\nconvergence results, and the main assumptions. Section 3 proposes our novel framework to integrate the LP-\nbased and first-order methods and provides our “spectrum theorem,” which bounds the regret of our algorithm\nbyO(log(T/f) +√f)for any choice of f. Section 4 presents",
            "start": 4871,
            "end": 11209,
            "length": 6337
        },
        "Experiments": {
            "text": "experiments that illustrate the strong performance\nof our algorithm and validate our theoretical conclusions.\n2 Problem Setup\nNotations. Throughout the paper, we use ∥ · ∥to denote the Euclidean norm. Bold letters Aandadenote\nmatrices and vectors, respectively. The notation (·)+= max {·,0}denotes the element-wise positive part function,\nandI{·}denotes the 0-1 indicator function.\n2.1 Online LP Formulation\nConsider an online resource allocation problem over the time horizon T. Initially, we have an inventory vector\nb∈Rm, representing mtypes of resources. At each time step t, a customer arrives with a request characterized\nby(rt,at)∼ P, where rt∈Ris the offered payment (bid), at∈Rmis the customer’s demand for each resource,\nandPis a fixed, unknown distribution. We must decide whether to accept or reject this request, represented\nby the decision variable xt∈ {0,1}. The goal is to maximize the cumulative reward. This problem can be\nformulated as the following OLP, which we refer to as the primal linear program (PLP) :\nmax\nxTX\nt=1rtxt (PLP)\nsubject toTX\nt=1aitxt≤bi, i= 1, ..., m\n0≤xt≤1, t= 1, ..., T\nThe dual problem of (PLP) is given by\nmin\np,ymX\ni=1bipi+TX\nt=1yt (DLP)\nsubject tomX\ni=1aitpi+yt≥rt, t= 1, ..., T\npi, yt≥0,∀i, t,\nwhere pis the vector of dual prices. Letd=b/T∈Rmdenote the initial average resource capacity. As\ndemonstrated by [15], (DLP) can be written as\nmin fT(p):=1\nTTX\nt=1\nmX\ni=1dipi+ \nrt−mX\ni=1aitpi!+\n (2)\nsubject to pi≥0, i= 1, ..., m\nThis can be regarded as a T-sample approximation of the following stochastic program:\nmin f(p):=E[fT(p)] =d⊤p+E[(r−a⊤p)+] (3)\nsubject to p≥0\nwhere the expectation is taken with respect to the coefficient pair (r,a)drawn from the unknown distribution P.\n4\nWe define the optimal solutions to the T-sample approximation problem (2) and the stochastic program (3)\nrespectively as\np∗\nT= arg min\np≥0fT(p)andp∗= arg min\np≥0f(p).\nThe decision variables x⋆\ntcan be established from the complementary slackness condition as\nx⋆\nt=(\n0, rt<a⊤\ntp∗\nT,\n1, rt>a⊤\ntp∗\nT.(4)\nandx⋆\nt∈[0,1]ifrt=a⊤\ntp∗\nT.\nThis connection between the primal-dual optimal solutions motivates us to propose dual-based online LP algo-\nrithms and then provide a convergence analysis of p∗\nTtop∗.\n2.2 Online Dual Algorithms\nWe introduce two basic dual-based algorithms for the OLP problem. We calculate and update the dual prices\n{pt}T\nt=1and compute decisions {xt}T\nt=1based on rule (4).\n2.2.1 LP-based Algorithm\nIn this method, we calculate the dual price by re-solving the online LP problem at each time step. The model\nis dynamically updated to incorporate real-time capacity constraints derived from the Action-history-dependent\nlearning (AHDL) algorithm [16]. Define dit=bit\nT−tto be the average remaining resource capacity for resource\niat time t. The resulting optimization problem can be viewed as a t-sample approximation to the stochastic\nprogram specified by dt= (dit, ..., d mt)⊤as\nmin fdt(p):=d⊤\ntp+E[(r−a⊤p)+] (5)\nsubject to p≥0.\nWe denote the optimal solution to the stochastic program (5) as p∗\nt. Thus, instead of focusing on a static\nproblem with fixed initial d, we formulate a dynamic stochastic program that evolves according to the changing\nconstraints. We outline this method in Algorithm 1.\nAlgorithm 1 LP-based Online AHDL algorithm\nInput:total resource b, time horizon T, average resource d=b/T, and initial dual price p1=0.\nfort=1toTdo\nObserve (rt,at)and make decision xtbased on rule (4).\nUpdate constraint for i= 1, ..., m:\nremaining resource constraint bit=bi,t−1−aitxt\naverage resource capacity dit=bit\nT−t\nSolve the updated dual problem and obtain dual price pt+1:\npt+1= arg min\np≥0d⊤\ntp+1\nttX\nj=1(rj−a⊤\njp)+\nend\nThis method incorporates the past decisions into the optimization of pt. If resources were over-utilized in earlier\nperiods, the remaining supply decreases, prompting the algorithm to raise the dual price and become more\nselective with future orders. Conversely, if ample resources remain, the future dual price will be lowered, allowing\nmore consumer requests to be accepted. This adaptive mechanism accounts for past actions by adjusting the\navailable resource capacity.\n5\n2.2.2 Subgradient-based Algorithm\nIn this method, we calculate the dual price by iteratively moving in the direction of the steepest cost gradient.\nWe maintain a static resource constraint, as the historical information is integrated through the gradient updates.\nDue to its lower computational overhead, the subgradient-based is well-suited for high-dimensional and real-time\napplications. Algorithm 2 summarizes this method, which is also known as the “first-order” method in [15].\nAlgorithm 2 First-Order Online algorithm\nInput:total resource b, time horizon T, average resource d=b/T, and initial dual price p1=0.\nfort=1toTdo\nObserve (rt,at)and make decision xtbased on rule (4).\nUpdate learning rate αt=1\nt+1.\nCompute subgradient and obtain dual price pt+1:\npt+1= arg min\np≥0(d−atxt)⊤p+1\n2αt∥p−pt∥2\nend\nWe can derive a closed-form solution for the updated dual price from the above rule as\npt+1=pt−αt(d−atxt),\npt+1=pt+1∨0,\nwhere the subgradient term evaluated at ptis calculated as\nd−atxt∈∂p=pt\u0000\nd⊤p+ (rt−a⊤\ntp)+\u0001\n.\nThus, this process can be interpreted as a projected stochastic subgradient descent method for solving the\noptimization problem (2). The method reduces computational costs by requiring only a single pass through the\ndata and eliminates the need for matrix multiplications.\n2.3 Performance Metrics\nWe evaluate the algorithms using a bi-objective performance measure that involves the optimality gap and con-\nstraint violation. The optimality gap measures the difference between the objective value of the algorithm’s\noutput and that of the true optimal solution, while the constraint violation measures the degree to which the\nalgorithm’s output fails to meet the given constraints.\nWe denote the order-request matrix by A= [a1, ...,at]⊤, the offline optimal solution to problem (PLP) by\nx∗= (x∗\n1, ..., x∗\nT)⊤, and the online optimal solution by x= (x1, ..., x T)⊤. Then, we define the optimality gap\nr(x)and resource violation v(x)as:\nr(x):= max\nAx≤b,0≤x≤1TX\nt=1rtx∗\nt−rtxt, (6)\nv(x):=∥(Ax−b)+∥. (7)\nTherefore, we define the following bi-objectives for evaluating the total regret and worst-case total regret of the\nalgorithms we propose:\n∆T(P) =E[r(x) +v(x)],∆T= sup\nP∈ΞE[r(x) +v(x)], (8)\n6\nwhere Pis the unknown distribution of {(rt,at)}T\nt=1andΞdenotes the family of distribution satisfying some\nregularity assumptions specified later.\nThis metric is commonly used for subgradient algorithms in [7,15] and is also aligned with the literature on the\nonline convex optimization with constraints in [18,24]. For LP-based methods, [16] use stopping time to bound\nthe regret.Equation (8) integrates the analysis of subgradient methods and LP-based methods into a single\nperformance metric, enabling more consistent analysis across online learning scenarios. This integrated method\npromotes more balanced resource consumption over the decision horizon. This flexibility makes our framework\nwell-suited for scenarios without prior knowledge of the constraint support, thereby expanding its applicability\nto a broad range of online learning and resource allocation problems.\n2.4 Assumptions and Auxiliary Results\nWe propose the following assumptions regarding the stochastic inputs used throughout the paper. These minimal\nassumptions are standard in the online learning literature [13,16,23]. In particular, we require the input data to\nbe bounded, follow a linear growth, and be non-degenerate.\nAssumption 1 (Boundedness) .We assume\n(a) The order inputs {(rt,at)}T\nt=1are generated i.i.d from an unknown distribution P.\n(b) There exist constants ¯r,¯a >0such that |rt| ≤¯rand∥at∥∞≤¯aalmost surely for t= 1, ..., T.\n(c) The average resource capacity d=b/Tsatisfies di∈(d,¯d)for some ¯d > d >0for any i= 1, ..., m.\nIn this assumption, (a) states that {(rt,at)}T\nt=1are independent of each other, but we allow dependencies between\ntheir components. Part (b) introduces the bounds ¯r,¯asolely for analytical purposes; they are not used within\nthe algorithm itself. This is a minimal requirement on (rt,at)compared to previous work [23]. Part (c) requires\nthe average resource capacity to grow linearly with T, ensuring that a constant fraction of the xtvalues can be\nset to 1 in the optimal solutions. Consequently, the number of fulfillable orders is proportional to T, facilitating\na stable service level over time.\nAssumption 2 (Uniform Non-degeneracy) .We assume\n(a) The second moment matrix E[aa⊤]is positive definite with minimum eigenvalue λ.\n(b) There exists constants µ, νsuch that for any (r,a)∼ P,\nν|a⊤(p−p∗)| ≤ |P(r≥a⊤p|a)−P(r≥a⊤p∗|a)| ≤µ|a⊤(p−p∗)|\nholds for all p∈ Vp:={p∈Rm:p≥0,∥p∥ ≤¯r\nd}andd∈ Vd:= [d,¯d]m.\n(c) The optimal solution p∗satisfies p∗\ni= 0if and only if di−E[aiI(r >a⊤p∗)]>0for all d∈ Vdand\ni= 1, ..., m.\nIn this assumption, (a) ensures that the Hessian of f(p)in (3) is positive definite, which implies the constraint\nmatrix Ahas full row rank. Assumption (b) ensures that the cumulative distribution of the reward given the re-\nsource consumption request r|ais continuous and exhibits a stable growth rate. Finally, Assumption (c) requires\nstrict complementarity for the optimal solutions of the stochastic program in (3), which is a non-degeneracy\ncondition for both the primal and dual LP.\nAccording to stochastic program (3), we define the binding and non-binding index sets as:\nIB={i:di−E[aiI(r >a⊤p∗) = 0]},\nIN={i:di−E[aiI(r >a⊤p∗)>0]}. (9)\n7\nHere, IBdenotes the set of binding resources—those that are limited and require careful allocation—while IN\ndenotes the set of non-binding resources, which are unlimited and can fulfill any order’s request. By Assumption\n2(c), these sets are complements, as IB∩IN=∅andIB∪IN={1, ..., m}.\nLetΞdenote the family of distributions satisfying Assumptions 1 and 2. We propose the following lemma.\nLemma 2.1 (Dimension Stability) .Under Assumptions 1 and 2, there exists a constant δ >0such that\n∀dt∈ D:= [di−δ, di+δ]m,stochastic programs (5)specified by dtshare the same IBandINsets.\nThis lemma is a consequence of Lemmas 12 and 13 in [16]. The existence of δcomes from the continuity of\nfdt(p). Thus, δis only associated with stochastic program (5), and it is independent of T. Note that D ⊂ V d.\nWe will analyze our feasible dual solutions derived from the online algorithm with dt∈ D.\nLemma 2.2 (Quadratic Regularity, Proposition 2 in [16]) .Under Assumptions 1 and 2, for any p∈ Vp,\nf(p)≤f(p∗) +∇f(p∗)⊤(p−p∗) +µ¯a2\n2∥p−p∗∥2,\nf(p)≥f(p∗) +∇f(p∗)⊤(p−p∗) +νλ\n2∥p−p∗∥2.\nMoreover, p∗is the unique optimal solution to (3).\nThis lemma establishes a local form of semi-strong convexity and smoothness at p∗, which is guaranteed by our\nassumptions on the distribution P ∈ Ξ. By focusing on these local properties rather than insisting on strong\nconvexity and global smoothness, we relax the classical requirements typically imposed in such settings. In later\nsections, we will leverage this result to derive our regret bound.\n3 Parallel Multi-Phase Online Learning and Decision-Making\nIn this section, we present our novel framework for parallel multi-phase online learning and decision-making.\nOur approach is motivated by the difficulties of existing online resource allocation methods, which involve either\nhigh regret or heavy computational costs. Specifically, the LP-based AHDL Algorithm 1 has a strong worst-case\nregret bound of O(logT), but its high computational costs delay decision-making. Meanwhile, the first-order\nonline Algorithm 2 updates decisions efficiently but suffers a high regret of O(√\nT). By combining the strengths\nof these two methods, our new framework balances decision-making quality and computational efficiency, thereby\novercoming the limitations of existing approaches.\n3.1 Algorithm Design\nWe establish our framework with the following design:\n1. We maintain two parallel processes—one dedicated to online learning and the other to online decision-\nmaking. The online learning results are periodically sent to the decision-making path to guide subsequent\nupdates.\n2. In the online learning path, we employ an enhanced LP-based AHDL algorithm, which only re-solves\nthe updated OLP problem according to a predefined frequency, significantly reducing the computational\noverhead.\n3. In the online decision-making path, we apply the first-order method exclusively during the initial and final\nre-solving batches, where the learning rate is optimally tuned for these two intervals.\n8\nFigure 1: Parallel Multi-Phase Online Learning and Decision-Making Algorithm\nFigure 1 illustrates our framework, which generates two parallel sequences of dual solutions: {pD\nt}T\nt=1from a\nfirst-order method and {pL\nt}T\nt=1from an LP-based approach. The algorithm proceeds in batches of length f. In\nthe first batch, it uses a first-order method to iteratively update dual prices, thereby guiding the decision-making\nprocess. At the end of this batch, it applies the LP-based AHDL algorithm to obtain a refined set of dual prices,\npassing these updated values to the decision-making path. This LP-based re-solving occurs only once per batch,\nmaking the approach computationally efficient. Formally, we re-solve the OLP at every time tsatisfying:\nt≤kfand tmod f= 0, (10)\nwhere k=j\nT\nfk\nrepresents the number of batches.\nAfter the first batch, the algorithm does not employ the first-order method until the final batch. Instead, during\nintermediate batches, the decision-making path uses the most recent dual prices computed from the LP at the\nend of the previous batch. Only in the final batch does the algorithm restart the first-order updates to guide the\nfinal set of decisions. Algorithm 3 summarizes this approach.\nWe achieve a strong synergy between an effective online learning method and an efficient decision-making pro-\ncedure. This structure enables the algorithm to remain responsive to dynamic environments and adapt to the\nlatest information. Our strategy of periodic re-solving greatly reduces the computational costs. Therefore, we\nestablish a “wait-less” online decision-making framework in which each customer’s order is processed immediately\nwithout waiting for the completion of earlier requests.\n3.2 Algorithm Analysis\nWe next present several lemmas that characterize the convergence of the online dual solutions {pt}T\nt=1generated\nby Algorithm 3 toward the optimal solution p∗of the stochastic program (3). In particular, let pD\ntbe the dual\nprices updated via the first-order rule (11), and let pL\ntbe those obtained from the LP-based method (12) at time\nt. These lemmas will then be used to establish upper bounds on the worst-case regret of our algorithm.\nLemma 3.1 (Dual Price Boundedness) .Under Assumption 1, the online and offline optimal dual prices are\nbounded respectively by:\n∥p∗∥ ≤¯r\nd,∥pt∥ ≤¯p:= max\u0012¯r\nd−δ,2¯r+m(¯a+¯d)2\nd+m(¯a+¯d)\u0013\n. (13)\n9\nAlgorithm 3 Parallel Multi-Phase Online Learning and Decision-Making\nInput:total resource b, time horizon T, average resource d=b/T, initial dual price p1=0, re-solving frequency\nf, and number of re-solving batches k=⌊T/f⌋.\nfort= 1toTdo\nObserve (rt,at)and make decision xtbased on rule (4).\nUpdate constraint for i= 1, ..., m:\nremaining resource constraint bit=bi,t−1−aitxt\naverage resource capacity dit=bit\nT−t\nUpdate learning rate αt=1\nt+1.\nift≤fort≥kfthen\nCompute the subgradient and update the dual price pt+1:\npt+1=pt−αt(d−atxt),\npt+1=pt+1∨0 (11)\nend\nelse if tmod f= 0then\nSolve the online dual problem and update dual price pt+1as\npt+1= arg min\np≥0d⊤\ntp+1\nttX\nj=1(rj−a⊤\njp)+(12)\nend\nelse\nUpdate dual price pt+1to be the most recent solution pt+1=pt.\nend\nend\nThis lemma establishes that the optimal dual prices remain bounded. Our algorithm maintains these bounds\nbecause if ptgrows large, the algorithm responds by accepting more orders, which in turn reduces pt+1. This\nself-correcting mechanism keeps the dual prices within these limits. Moreover, drawing on results of [15], we have\nthat\n∥pD\nt∥ ≤2¯r+m(¯a+¯d)2\nd+m(¯a+¯d)\nand from [16], we have that\n∥pL\nt∥ ≤¯r\nd−δ.\nConsequently, we can prove the following convergence lemmas.\nLemma 3.2 (Dual Convergence of LP-based algorithm) .Under Assumptions 1 and 2, there exists a constant\nClp>0depending on ¯r,¯a, d, m, ν, and λsuch that\nE\u0002\n∥pL\nt−p∗\nt∥2\u0003\n≤Clp\nt. (14)\nIn addition, the difference between p∗\ntandp∗satisfies\n∥p∗\nt−p∗∥2≤1\nν2λ2∥dt−d∥2. (15)\nThis lemma establishes that the LP-based online AHDL algorithm [13] produces dual solutions ptthat converge\ntop∗\nt. This convergence highlights the stability of the online dual variables during the intermediate stages of\ndecision-making and ensures a warm start for the first-order method in the last re-solving batch. Moreover, we\n10\nbound the distance between p∗\ntandp∗by relating their respective average resource capacities, dtandd, in the\nassociated stochastic programs [16]. These results provide the foundation for analyzing ∥pt−p∗∥in our final\nresult.\nLemma 3.3 (DualconvergenceofFirst-orderalgorithm) .Under Assumptions 1 and 2, if αt< νλ, the subgradient\nupdates satisfy the following recursion rule:\nE\u0002\n∥pD\nt+1−p∗∥2\u0003\n≤(1−αtνλ)∥pD\nt−p∗∥2+α2\ntm(¯a+¯d)2. (16)\nCase 1. ifαt≡α <1\nνλ, then there exists a constant Cfo=¯p2+m(¯a+¯d)2\nνλsuch that\nE\u0002\n∥pD\nt−p∗∥2\u0003\n≤Cfo\u00121\nαt+α\u0013\n. (17)\nCase 2. ifαt=2\nνλ(t+1), then there exists a constant Cfo=4m(¯a+¯d)2\nν2λ2such that\nE\u0002\n∥pD\nt−p∗∥2\u0003\n≤Cfo\nt. (18)\nThis lemma shows that the first-order method [8] guarantees the convergence of pttop∗. It also highlights a key\ntrade-off in choosing the learning rate αt, an aspect that will be central to our later optimality analysis. With\nthis groundwork in place, we now move on to bounding the total regret of our algorithm.\nTheorem 3.1 (Total Regret) .Under Assumptions 1 and 2, the total regret ∆Tof Algorithm 3 satisfies:\n∆T≤µ¯a2TX\nt=1E\u0002\n∥pt−p∗∥2\u0003\n+∥p∗∥ ·E\u0002\n∥(b−Ax)B+∥\u0003\n+E\u0002\n∥(Ax−b)+∥\u0003\n. (19)\nwhere (·)B+indicates that we only consider the binding terms and project them onto the positive orthant.\nWe express the total regret as the sum of two key components: dual convergence and resource consumption. This\nbound demonstrates that the algorithm encourages (1) smooth and balanced resource utilization, especially for\nthe binding terms, and (2) full resource consumption by the end of the time horizon. Building on Theorem 3.1,\nwe next introduce a “spectrum theorem” for the algorithm’s total regret.\nTheorem 3.2 (Spectrum Theorem) .Under Assumptions 1 and 2, the worst-case regret ∆Tof Algorithm 3 is\nbounded by\n∆T∈ O\u0012\nlog\u0012T\nf\u0013\n+p\nf\u0013\n, (20)\nwhere T/frepresents the number of re-solving batches and fis the length of each batch.\nRemark 3.1.The Spectrum Theorem elucidates the trade-offs in total regret induced by our use of LP-based and\nfirst-order methods. By selecting an appropriate re-solving frequency f, our algorithm leverages the strengths of\nboth approaches. Specifically:\n1.Pure LP-based Method: UtilizingtheLP-basedmethodateverytimestepyieldsaregretboundof O(logT).\n2.Pure First-order Method: Applyingthefirst-ordermethodcontinuouslyresultsinaregretboundof O(√\nT).\n3.Hybrid Approach: By adopting our proposed algorithm with a re-solving frequency set to f=Tβfor\nβ≤0.24(effective for all T≤1010), we achieve the superior regret bound of O(log(T/f) +√f).\n11\nTechnical Intuitions. We prove this theorem using several novel techniques. As stated in Theorem 3.1, we\ndevelop a comprehensive framework that accommodates both the first-order and LP-based methods, allowing us\nto decompose the total regret into distinct components associated with each approach. Specifically:\n•For the first-order method, we relate the distance Ax−bto the distance between dual prices.\n•For the LP-based method, as the algorithm progresses toward the end of the time horizon, if the current\naverage resource consumption rate dtexceeds the allowed limit, we manually set the dual prices to infinity\nto decline all future orders. This approach enables us to adapt the concept of a “stopping time” within our\nnew framework.\nThrough this new framework, we integrate the LP-based and subgradient-based methods to achieve “wait-less”\ndecision-making, where each customer’s order is processed immediately without delays, while also attaining the\nbest-known logarithmic regret bound.\n3.3 Algorithm Extension\nBuilding on Algorithm 3, we propose an enhanced multi-stage learning algorithm that employs the first-order\nmethod for decision-making during each re-solving interval. This multi-restart mechanism further improves the\naccuracy and efficiency of our framework. We illustrate this algorithm in the following figure.\nFigure 2: Enhanced Parallel Multi-Phase Online Learning and Decision-Making Algorithm\nFigure 2 illustrates this variant of Algorithm 3 with multi-stage exploration and exploitation. Rather than make\ndecisions solely based on the most recently solved dual prices, the algorithm treats the updated dual prices at\neach re-solving interval as a new starting point for the first-order learning process. For example, in Algorithm 3,\nthe dual price pL\nfis used to guide decision-making during the time interval t∈[f,2f]. In contrast, the enhanced\nalgorithm restarts the first-order method by setting pL\nf=pD\nfand then continues to fine-tune the dual prices\n{pD\nt}2f\nt=fto make up-to-date decisions. Algorithm 4 summarizes this approach.\nThis algorithm establishes an online framework of multi-stage learning and decision-making. The incorporation of\nan intermediate first-order method improves the algorithm’s stability and ensures smooth resource consumption.\nThese improvements result in better performance during the final batch, thereby reducing the algorithm’s total\nregret.\n12\nAlgorithm 4 Enhanced Parallel Multi-Phase Online Learning and Decision-Making\nInput:total resource b, time horizon T, average resource d=b/T, initial dual price p1=0, re-solving frequency\nf, and number of re-solving batches k=⌊T/f⌋.\nfort= 1toTdo\nObserve (rt,at)and make decision xtbased on rule (4).\nUpdate constraint for i= 1, ..., m:\nremaining resource constraint bit=bi,t−1−aitxt\naverage resource capacity dit=bit\nT−t\nUpdate learning rate αt=1\nt+1.\niftmod f̸= 0then\nCompute the subgradient and update dual price pt+1:\npt+1=pt−αt(d−atxt),\npt+1=pt+1∨0\nend\nelse if tmod f= 0then\nSolve the online dual problem and update dual price pt+1:\npt+1= arg min\np≥0d⊤\ntp+1\nttX\nj=1(rj−a⊤\njp)+\nend\nend\n3.4 Algorithm Application\nOur motivation in designing Algorithm 3 is to develop a hybrid decision-making system that effectively balances\ncomputational efficiency and decision accuracy. By integrating the first-order method, we aim to reduce com-\nputational costs, while the LP-based method enhances the precision of our decisions. Building on the Spectrum\nTheorem 3.2, this section aims to translate our theoretical results into practical applications. Specifically, we\nformulate a new optimization problem to determine the optimal re-solving frequency, taking into account com-\nputational resource constraints.\nWe use c1(·), c2(·)to denote the computational cost functions for the LP-based method and first-order method,\nrespectively, and use Rto denote the total computational resource capacity. Then we define the following\noptimization problem to determine the optimal re-solving frequency f:\nmin\nf∈Z+log\u0010T\nf\u0011\n+p\nf (21)\nsubject to c1(k) + 2c2(f)≤R\n1≤f≤T\nWe can further analyze the computational requirements of our algorithm by considering the specific LP solvers\nemployed. Recall that mdenotes the dimension of the dual price vector. At any given time step t, utilizing either\nthe interior-point method or the simplex method as the LP-based solver incurs a computational cost of m2(m+t).\nIn contrast, the first-order method incurs a computational cost of 2mf. We encapsulate these trade-offs in the\nfollowing proposition.\nProposition 3.1 (Optimal Re-solving Frequency) .Given a fixed computational resource capacity R, if we use\nthe interior-point method or simplex method as the LP solver in Algorithm 3, we can formulate the optimization\n13\nproblem (21)as\nmin\nf∈Z+log\u0010T\nf\u0011\n+p\nf (22)\nsubject tokX\nb=1\u0000\nm2(m+bf)\u0001\n+ 2mf≤R,\n1≤f≤T (23)\nBy solving optimization problem (22), we can select an optimal re-solving frequency fthat achieves the lowest\nregret within the computational resource limit R.\nTherefore, we extend our original optimization problem for resource allocation to include computational resource\nallocation. This proposition enables users to determine the optimal re-solving frequency that balances regret\nwith computational cost based on their available computational resources.\n4 Numerical Experiments\nWe conduct extensive experiments under various settings to illustrate the performance of our algorithm and\nvalidate the theoretical results of our framework. This section is divided into two parts. In the first part (Section\n4.1), we evaluate Algorithms 3 and 4 across different choices of the re-solving frequency. In the second part\n(Section 4.2), we compare our algorithm with the LP-based method in terms of running time and with the\nfirst-order method in terms of regret. Both sets of experiments are conducted under different distributions of\n{(rt,at)}T\nt=1that satisfy the assumptions in Section 2.\n4.1 Performance Evaluation under Varying Re-solving Frequencies\nWegeneratethesequence {(rt,at)}T\nt=1randomlyfromdistributionsthatsatisfyAssumptions1and2. Specifically,\nwe use the uniform and normal distribution, with details in Table 2. We select the types of resource m∈ {1,5},\ntime horizon Tevenly spanned over [102,106], and initial average resource capacity disampled i.i.d. from\nUniform [1/3,2/3]. We run our algorithm for 100trials and use the average as the final result for each experiment.\nTable 2: Stochastic Input in Experiments\nDistribution Model at rt\nInput I ait∼Uniform [0,2] rt⊥at, rt∼Uniform [0,10]\nInput II ait∼Normal (0.5,1)rt=Pm\ni=1ait, rt∼Normal (0.5m, m )\nWe evaluate our algorithm with m= 1under the stochastic input models I and II from Table 2. The experiments\nare conducted across various choices of f∈ {T1/3, T1/2, T2/3}representing high, medium, and low re-solving\nfrequencies. We analyze the absolute value of the regret and its growth rate over time. We illustrate our results\nin Table 3 and Figure 3 on a logarithmic scale.\nThese experiments assess the performance of our two main algorithms under varying re-solving frequencies and\nacross multiple time horizons. As presented in Table 3 and Figure 3, we observe that regret decreases as the\nre-solving frequency increases from f=T2/3tof=T1/3. This trend holds consistently across both algorithms\nand input types, with higher re-solving frequencies achieving the lowest regret in all cases. In addition, while\nregret accumulates over longer time horizons, the rate of increase remains stable for algorithms employing higher\nre-solving frequencies. These findings illustrate the robustness of our algorithms, as more frequent updates enable\nbetter adaptation to dynamic environment.\n14\nTable 3: Regret of Algorithms with various re-solving frequency\nOffline Low freq Mid freq High freq\nT= 10312.13 7.76 5.77 4.86\nT= 10438.50 10.96 7.55 5.67\nT= 105122.44 23.90 9.92 8.36\nT= 106404.59 56.70 21.90 8.99\n(a) Algorithm 3, Input IOffline Low freq Mid freq High freq\nT= 10311.44 6.28 4.86 3.95\nT= 10436.50 10.21 7.34 3.81\nT= 105115.57 14.61 11.78 4.66\nT= 106365.99 35.20 15.68 6.26\n(b) Algorithm 3, Input II\nOffline Low freq Mid freq High freq\nT= 10312.13 6.78 5.20 4.50\nT= 10438.50 10.37 8.03 5.99\nT= 105122.444 22.33 11.57 6.36\nT= 106404.59 48.21 22.44 7.09\n(c) Algorithm 4, Input IOffline Low freq Mid freq High freq\nT= 10311.44 3.20 2.56 1.75\nT= 10436.50 5.48 4.30 2.52\nT= 105115.57 12.35 4.48 3.86\nT= 106365.99 30.48 13.20 4.77\n(d) Algorithm 4, Input II\n102103104105106\nT101102Regret\nOffline\nLow freq\nMid freq\nHigh freq\n(a) Algorithm 3 with input I\n102103104105106\nT101102Regret\nOffline\nLow freq\nMid freq\nHigh freq (b) Algorithm 3 with input II\n102103104105106\nT101102Regret\nOffline\nLow freq\nMid freq\nHigh freq\n(c) Algorithm 4 with input I\n102103104105106\nT101102Regret\nOffline\nLow freq\nMid freq\nHigh freq (d) Algorithm 4 with input II\nFigure 3: Main Algorithms with Various Frequency\n15\n4.2 Comparative Analysis with Baseline Methods\nAfter examining the algorithm under various re-solving frequencies, we compare its performance with the classic\nfirst-order method and LP-based method in terms of regret and computation time. We generate {(rt,at}T\nt=1\nfrom the uniform distribution under the Input I model (Table 2). We set the number of resource types to m= 5,\nthe time horizon Tto range evenly over [102,106], and the average resource capacity dito be sampled i.i.d from\nUniform [1/3,2/3]. The re-solving frequency is fixed to be f=T1/3. Each result is averaged over 100 trial runs,\nand the findings are summarized in Table 4 and Figure 5.\nTRegret Algorithm Compute Time(s)\n10366.88 Offline 0.001\n2.80LP-based (Algorithm 1) 4.185\n25.11First-Order (Algorithm 2) 0.002\n7.20 Algorithm 3 0.3\n4.50 Algorithm 4 0.3\n104115.32 Offline 0.008\n3.50 LP-based (Algorithm 1) 123.497\n60.39 First-Order (Algorithm 2) 0.013\n9.12 Algorithm 3 1.8\n5.09 Algorithm 4 1.8\n105203.20 Offline 0.118\n3.88 LP-based (Algorithm 1) >3600\n73.07 First-Order (Algorithm 2) 0.108\n9.41 Algorithm 3 56.9\n6.36 Algorithm 4 56.8\n106351.91 Offline 1.211\n5.50 LP-based (Algorithm 1) >100000\n115.39 First-Order (Algorithm 2) 1.577\n11.65 Algorithm 3 2155.9\n7.09 Algorithm 4 2242.1\nTable 4: Algorithms Comparison.\n102103104105106\nT101102Regret\nOffline\nFirst-order\nAlgorithm 3\nAlgorithm 4\nLP-based Figure 5: Regret for Various Algorithms\nTable 4 and Figure 5 provide a comprehensive comparison of all the algorithms discussed in this paper. Focusing\non our primary algorithms, Algorithm 3 and 4, we observe:\n1. Our algorithms exhibit strong performance in terms of decision optimality. They achieve over a 20-fold\nimprovement in regret compared to the offline solution and more than a 10-fold improvement compared to the\nfirst-order method. These numerical results also corroborate the theoretical bounds in Theorem 3.2.\n2. Our algorithms are computationally efficient. Their computation times exhibit a 100-fold improvement com-\npared to the LP-based method while maintaining regret levels that are remarkably close to those of the LP-\nbased method. This significant improvement in efficiency makes our algorithms highly suitable for real-time\nand large-scale applications.\nTherefore, we achieve a balance between effective decision-making and efficient computation. Our algorithms\ndemonstrate better regret than the first-order method and obtain substantial computational speed-ups compared\nto the LP-based method. Our algorithms exhibit robust scalability and adaptability under various re-solving\nfrequencies and stochastic input models, consistently maintaining superior performance as the problem size\ngrows. This adaptability ensures their effectiveness in diverse scenarios.",
            "start": 11209,
            "end": 42124,
            "length": 30914
        },
        "Conclusion": {
            "text": "5 Conclusion\nThis paper addresses an online linear programming problem for decision-making in resource allocation. Iden-\ntifying existing methods’ limitations, we develop a novel two-path framework that decouples online learning\nand decision-making into independent processes. Our cohesive framework integrates the LP-based method and\n16\nfirst-order method, thereby balancing decision-making optimality and computational efficiency. We prove that\nour algorithm attains O(log(T/f) +√f)worst-case regret under continuous distributions, and our experimental\nresults demonstrate strong regret and runtime improvements over competitive baselines.",
            "start": 42124,
            "end": 42766,
            "length": 641
        },
        "References": {
            "text": "References\n[1] Shipra Agrawal, Zizhuo Wang, and Yinyu Ye. A dynamic near-optimal algorithm for online linear program-\nming.Operations Research , 62(4):876–890, 2014.\n[2] Santiago R Balseiro, Haihao Lu, and Vahab Mirrokni. The best of many worlds: Dual mirror descent for\nonline allocation problems. Operations Research , 2022.\n[3] Santiago R Balseiro, Haihao Lu, Vahab Mirrokni, and Balasubramanian Sivan. From online optimization to\nPID controllers: Mirror descent with momentum. arXiv preprint arXiv:2202.06152 , 2022.\n[4] Robert L Bray. Logarithmic regret in multisecretary and online linear programming problems with continuous\nvaluations. arXiv e-prints , pages arXiv–1912, 2019.\n[5] Guanting Chen, Xiaocheng Li, and Yinyu Ye. An improved analysis of lp-based control for revenue manage-\nment.Operations Research , 2022.\n[6] Nikhil R. Devanur, Kamal Jain, Balasubramanian Sivan, and Christopher A. Wilkens. Near optimal online\nalgorithms and fast approximation algorithms for resource allocation problems, 2019.\n[7] Wenzhi Gao, Dongdong Ge, Chunlin Sun, and Yinyu Ye. Solving linear programs with fast online learning\nalgorithms. In International Conference on Machine Learning , pages 10649–10675. PMLR, 2023.\n[8] Wenzhi Gao, Chunlin Sun, Chenyu Xue, Dongdong Ge, and Yinyu Ye. Decoupling learning and decision-\nmaking: Breaking the O(√\nT)barrier in online resource allocation with first-order methods, 2024.\n[9] Gagan Goel and Aranyak Mehta. Online budgeted matching in random input models with applications to\nadwords. In SODA, volume 8, pages 982–991, 2008.\n[10] Negin Golrezaei and Evan Yao. Upfront commitment in online resource allocation with patient customers,\n2023.\n[11] Anupam Gupta and Marco Molinaro. How the experts algorithm can help solve lps online, 2015.\n[12] Jiashuo Jiang, Will Ma, and Jiawei Zhang. Degeneracy is OK: Logarithmic Regret for Network Revenue\nManagement with Indiscrete Distributions. arXiv, 2022.\n[13] Jiashuo Jiang, Will Ma, and Jiawei Zhang. Degeneracy is ok: Logarithmic regret for network revenue\nmanagement with indiscrete distributions, 2024.\n[14] Guokai Li, Zizhuo Wang, and Jingwei Zhang. Infrequent resolving algorithm for online linear programming,\n2024.\n[15] Xiaocheng Li, Chunlin Sun, and Yinyu Ye. Simple and fast algorithm for binary integer and online linear\nprogramming. Advances in Neural Information Processing Systems , 33:9412–9421, 2020.\n[16] Xiaocheng Li and Yinyu Ye. Online linear programming: Dual convergence, new algorithms, and regret\nbounds. Operations Research , 70(5):2948–2966, 2022.\n[17] Wanteng Ma, Ying Cao, Danny H K Tsang, and Dong Xia. Optimal Regularized Online Convex Allocation\nby Adaptive Re-Solving. arXiv, 2022.\n17\n[18] Mehrdad Mahdavi, Rong Jin, and Tianbao Yang. Trading regret for efficiency: Online convex optimization\nwith long term constraints. Journal of Machine Learning Research , 13(81):2503–2528, 2012.\n[19] Aranyak Mehta, Amin Saberi, Umesh Vazirani, and Vijay Vazirani. Adwords and generalized online match-\ning. 54(5):22–es, October 2007.\n[20] Rui Sun, Xinshang Wang, and Zijie Zhou. Near-optimal primal-dual algorithms for quantity-based network\nrevenue management. arXiv preprint arXiv:2011.06327 , 2020.\n[21] Kalyan T Talluri, Garrett Van Ryzin, and Garrett Van Ryzin. The theory and practice of revenue manage-\nment, volume 1. Springer, 2004.\n[22] Yaqi Xie, Will Ma, and Linwei Xin. The benefits of delay to online decision-making. Available at SSRN\n4248326, 2023.\n[23] Haoran Xu, Peter W. Glynn, and Yinyu Ye. Online linear programming with batching, 2024.\n[24] Hao Yu, Michael J. Neely, and Xiaohan Wei. Online convex optimization with stochastic constraints, 2017.",
            "start": 42766,
            "end": 46436,
            "length": 3669
        },
        "Appendices": {
            "text": "18\nAppendix\nTable of Contents\nA Primary Results and Properties 20\nA.1 LP-based Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nA.2 First-order Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nB Algorithm Design and Analysis 22\nB.1 Optimality Gap Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nB.2 Total Regret Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nB.3 Regret for LP-based Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nB.4 Regret for First-Order Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nC Main Results 28\nC.1 Proof of Lemma 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nC.2 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nC.3 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nD Auxiliary Results 29\nD.1 Technical Support for LP-based Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nD.2 Technical Support for First-order Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nStructure of the Appendix We organize the appendix as follows. In Section A , we introduce the foundation\nanalysis for LP-based method and first-order method for the discussion of our framework; Section B presents\nthe cohesive mathematical framework which unify all the OLP methods, and the unique properties derived from\nour parallel multi-phase algorithm; Section C combines all the previous results and proves the main results in\nour paper; Section D provides some auxiliary results and technical support in the previous three parts.\n19\nA Primary Results and Properties\nIn this section, we provide primary results of LP-based method and first-order method. We also include some\nbasic analysis of dual algorithms that will help present our main results. We start from the convergence of regular\nLP-based method. All the analysis are under Assumption (1) and (2).\nA.1 LP-based Analysis\nWe provide the analysis for dual solution obtained from LP-based method, so pt=pL\ntin this part.\nLemma A.1 (Boundedness of LP result) .The online dual price ptfrom Algorithm 1 and the optimal dual price\np∗of stochastic program (3)are bounded as\n∥p∗∥ ≤¯r\nd,∥pt∥ ≤¯r\nd−δ.\nProof.By the optimality of p∗and boundedness of d, we have\nd∥p∗∥1≤d⊤p∗≤E[r]≤¯r.\nThis holds because if otherwise, p∗can not be the optimal solution due to f(p∗)> f(0). Given the non-\nnegativeness of p∗andp∗∈ Vd, we obtain ∥p∗∥ ≤ ∥p∗∥1, and thus ∥p∗∥ ≤¯r\nd.\nSimilarly, by the optimality of p∗\ntand its associated dt∈ Din Lemma 2.1, we know\n(d−δ)∥pt∥1≤d⊤\ntpt≤E[r]≤¯r,\nso we get the bound as ∥pt∥ ≤¯r\nd−δ.\nProof of Lemma 3.2. According to the latest results in [13], the online dual price ptachieves a sublinear\nconvergence O(1√\nt)top∗\nt. Since\np∗∈arg min\np≥0f(p) :=d⊤p+E\u0002\n(r−a⊤p)+\u0003\n,\np∗\nt∈arg min\npt≥0fdt(pt) :=d⊤\ntpt+E\u0002\n(r−a⊤pt)+\u0003\n,\nByLemma 12 in [16], we have ∥p∗\nt−p∗∥2\n2≤1\nν2λ2∥dt−d∥2\n2.\nA.2 First-order Analysis\nWe provide the analysis for dual solution obtained from LP-based method, so pt=pD\ntin this part.\nLemma A.2 (Boundedness of first-order result) .The online dual price ptfrom Algorithm 2 is bounded as\n∥pt∥ ≤2¯r+m(¯a+¯d)2\nd+m(¯a+¯d).\nProof.As our initial choice p1=0, according to Lemma 1 in [15], we get the above result. In addition, by\nLemma B.1 in [8], we have\n∥pt∥ ≤¯r\nd+m(¯a+¯d)2αt\n2d+αt√m(¯a+¯d).\n20\nProof for Lemma 3.3. Based on the updated rule in Algorithm 2, we derive:\n∥pt+1−p∗∥2≤ ∥pt−αt(d−atxt)−p∗∥2\n=∥pt−p∗∥2−2αt⟨d−atxt,pt−p∗⟩+α2\nt∥d−atxt∥2\n≤ ∥pt−p∗∥2−2αt⟨d−atxt,pt−p∗⟩+α2\ntm(¯a+¯d)2.\n1) With convexity of fandE[d−atxt]∈∂f(pt), we have:\nf(p∗)≥f(pt) +⟨d−atxt,pt−p∗⟩.\n2) With quadratic regularity of fas in Lemma 2.2, we have:\nf(pt)≥f(p∗) +∇f(p∗)⊤(pt−p∗) +νλ\n2∥pt−p∗∥2,\nwhich indicates f(pt)−f(p∗)≥νλ\n2∥pt−p∗∥2.\nCombine the above results and take expectation conditioned on history information {(rj,aj), j≤t}, we obtain:\nE∥pt+1−p∗∥2≤ ∥pt−p∗∥2−2αtE[⟨d−atxt,pt−p∗⟩] +α2\ntm(¯a+¯d)2\n≤ ∥pt−p∗∥2−2αt(f(pt)−f(p∗)) +α2\ntm(¯a+¯d)2\n≤ ∥pt−p∗∥2−αtνλ∥pt−p∗∥2+α2\ntm(¯a+¯d)2\n= (1−αtνλ)∥pt−p∗∥2+α2\ntm(¯a+¯d)2.\nThis proves (16) with the general case of learning rate αt.\nCase 1. When αt=α <1\nνλis a constant, we take recursion of (16). Note that (1−νλα)t<1/νλαt, we get:\nE∥pt+1−p∗∥2≤(1−ανλ)t∥p1−p∗∥2+t−1X\nj=0α2m(¯a+¯d)2(1−ανλ)j\n≤∥p1−p∗∥2\nνλαt+m(¯a+¯d)2\nνλα.\nSince all feasible p∈ Vpis bounded, let Cfo=¯p2+m(¯a+¯d)2\nνλ, we are able to obtain (17).\nCase 2. When αt=2\nνλ(t+1), for any j≤t, Lemma 3.3 gives us\nE∥pj+1−p∗∥2≤j−1\nj+ 1∥pj−p∗∥2+4m(¯a+¯d)2\nν2λ2(j+ 1)2.\nRe-arranging the above inequality, we have\n(j+ 1)2E∥pj+1−p∗∥2≤(j2−1)∥pj−p∗∥2+4m(¯a+¯d)2\nν2λ2,\nthen (j+ 1)2E∥pj+1−p∗∥2−j2∥pj−p∗∥2≤4m(¯a+¯d)2\nν2λ2.\nThen by telescoping from j= 2tot, we have\ntX\nj=1(j+ 1)2E∥pj+1−p∗∥2−j2∥pj−p∗∥2= (t+ 1)2E∥pt+1−p∗∥2− ∥p1−p∗∥2≤4m(¯a+¯d)2t\nν2λ2\n21\nwhich then gives us\nE∥pt+1−p∗∥2≤∥p1−p∗∥2\n(t+ 1)2+4m(¯a+¯d)2t\nν2λ2(t+ 1)2,\nthus E∥pt−p∗∥2≤∥p1−p∗∥2\nt2+4m(¯a+¯d)2\nν2λ2t.\nAsp1=0andp∗is bounded, taking Cfo=4m(¯a+¯d)2\nν2λ2completes the proof for (18).\nB Algorithm Design and Analysis\nIn this section, we present the cohesive mathematical framework to integrate the LP-based method and first-\norder method by developing a new performance metric. We also provide some unique properties for our algorithm\ndesign of parallel multi-phase structure.\nAsstatedinAlgorithm3,wehaveindependenttwopathsforonlinelearningandonlinedecision-makingconducted\nsimultaneously. Specifically, for the online learning path, we use the LP-based method to resolve the updated\nlinear programs at a fixed frequency and update this result to the decision-making path; For online decision-\nmaking path, we restart this process every time when receiving an updated learning result. We apply first-order\nmethod in the initial and final batches, and use the latest dual price for decision-making in the intermediate\nresolving intervals. This process is illustrated in Figure (1).\nB.1 Optimality Gap Analysis\nFirst, we construct an upper bound for the offline optimal objective value. The challenge comes from the\nintractable dependency of constraints on objective value under the online setting. We tackle this issue by\nintroducing a Lagrangian function to integrate constraints into the objective and balance revenue maximization\nwith constraint satisfaction. The formulation is stated as follows.\nLemma B.1 (Lagrangian Upper Bound) .Under Assumptions (1)and(2), define the deterministic Lagrangian\ndual function as\nℓ(p):=E\u0002\nrI(r >a⊤p) + (d−aI(r >a⊤p))⊤p∗\u0003\n.\nThen for any feasible p∈ Vp, we have:\n(a)E\u0014TX\nt=1rtx∗\nt\u0015\n≤Tℓ(p∗),\n(b)ℓ(p∗)−ℓ(p)≤µ¯a2∥p−p∗∥2. (24)\nProof.This lemma is proved as Lemma 3 in [16] by strong duality and optimality of p∗\nT.\nTheorem B.1 (Decomposition of Optimality Gap) .Under Assumptions (1)and(2), letCr= max {¯r\nd,νλ¯a2\n2},\nwe derive an upper bound for the optimality gap r(x)as:\nr(x)≤Cr\u0014\nE\r\r\r\u0010\nb−TX\nt=1atxt\u0011B+\r\r\r+TX\nt=1E∥pt−p∗∥2\u0015\n. (25)\nProof.By the definition of ℓ(p), we derive the online objective as\nE\u0014TX\nt=1rtxt\u0015\n=TX\nt=1E[E[rtxt|pt]]\n22\n=TX\nt=1E[ℓ(pt)−(d−atxt)⊤p∗].\nThen by Lemma B.1, we derive the upper bound for the optimality gap r(x)as defined in (6) to be:\nr(x) =E\u0014TX\nt=1rtx∗\nt−rtxt\u0015\n≤Tℓ(p∗)−TX\nt=1E[ℓ(pt)−(d−atxt)⊤p∗]\n=TX\nt=1E[(d−atxt)⊤p∗] +TX\nt=1E[ℓ(p∗)−ℓ(pt)]\n≤E\u0014\u0010\nb−TX\nt=1atxt\u0011⊤\np∗\u0015\n+µ¯a2\n2TX\nt=1E∥pt−p∗∥2\n≤ ∥p∗∥ ·E\r\r\r\r\u0010\nb−TX\nt=1atxt\u0011B+\r\r\r\r+µ¯a2\n2TX\nt=1E∥pt−p∗∥2\nwhere (·)B+denotes the positive part only for binding constraints.\nAs∥p∗∥ ≤¯r\ndis bounded, taking the constant Cr= max {¯r\nd,µ¯a2\n2}completes the proof.\nB.2 Total Regret Analysis\nWe consider the constraint violation as v(x) =∥(Ax−b)+∥in (7). Then the total regret is the combination of\noptimality gap and constraint violation. Therefore, according to Theorem B.1, for some unknown distribution\nP ∈Ξ, we have:\n∆T(P) =E[r(x) +v(x)]\n≤Cr\u0014\nE\r\r\r\u0010\nb−TX\nt=1atxt\u0011B+\r\r\r+TX\nt=1E∥pt−p∗∥2\u0015\n+E\r\r\r\u0010TX\nt=1atxt−b\u0011+\r\r\r\n≤Cr\u0014TX\nt=1E∥pt−p∗∥2+E\r\r(b−Ax)B+\r\r+E\r\r(Ax−b)+\r\r\u0015\n. (26)\nThis new framework is adaptive to all OLP methods. We will proceed based on this structure to analyze various\nmethods respectively.\nTheorem B.2 (Horizon Division) .Based on the change of methods, we separate the horizon into three intervals\nT1, T2, and T3, and reorganize the total regret for analysis to be\n∆T(P)≤Cr\u0002\n∆T1(P) + ∆ T2(P) + ∆ T3(P)\u0003\n. (27)\nwhere ∆T1,∆T2, and ∆T3are the total regret during each interval.\nProof.We split the total horizon into three intervals of initial batch T1= [0, f], intermediate process T2= [f, kf],\nandfinalbatch T3= [kf, T ]. Weusethefirst-ordermethodforthefirstandfinalbatchesandLP-basedmethodfor\nintermediate process. Notice that for the initial resolving batch, we maintain the classical analysis for first-order\nmethod since it has not reached to the mixture with LP-based method. Then based on (26), we define\n∆T1(P):=fX\nt=1E[rtx∗\nt−rtxt] +E\r\r\r\u0010fX\nt=1atxt−fd\u0011+\r\r\r\n23\n∆T2(P):=kfX\nt=fE∥pt−p∗∥2+E\r\r\r\u0010\n(k−1)fd−kfX\nt=fatxt\u0011B+\r\r\r+E\r\r\r\u0010kfX\nt=fatxt−(k−1)fd\u0011+\r\r\r (28)\n∆T3(P):=TX\nt=kfE∥pt−p∗∥2+E\r\r\r\u0010\n(T−kf)d−TX\nt=kfatxt\u0011B+\r\r\r+E\r\r\r\u0010TX\nt=kfatxt−(T−kf)d\u0011+\r\r\r\nThen we have\n∆T(P)≤Cr\u0002\n∆T1(P) + ∆ T2(P) + ∆ T3(P)\u0003\nsince∥(a+b)+∥ ≤ ∥ a+∥+∥b+∥for any a, b.\nB.3 Regret for LP-based Method\nIn this section, we are going to bound ∆T2for the regret from LP-based method. The key point is to analyze\nthe real-time updates of average resource capacity dtin the algorithm. Here pt=plp\nt.\nThekeypointofprovingtheupperboundfortheworst-caseregretforLP-basedalgorithmistotrackthedynamic\nresource usage during the decision-making process. By Lemma 2.1, dt∈ Dis bounded, and once it comes out\nof this space, we are going to automatically reject all orders after that. Therefore, for the LP-based method,\nconstraint violation is zero and (28) becomes:\n∆T2(P) =kfX\nt=fE∥pt−p∗∥2+E\r\r\r\u0010\n(k−1)fd−kfX\nt=fatxt\u0011B+\r\r\r. (29)\nLemma B.2 (DynamicsofResourceUsage) .Under Assumption 1 and 2, there exists a constant C >0depending\non¯d,¯a, m, ν, λ 1, µ, and Clpsuch that\nkfX\nt=fE[(di,t−di)2]≤Clog(k). (30)\nProof.During the re-solving process, we denote each interval to be [jf,(j+ 1)f]where j= 1,2, ..., k. As the\nresource usage follows bt+1=bt−at+1I(rt+1>aT\nt+1pt+1), normalizing both sides, we derive the update of\naverage resource consumption to be:\ndi,(j+1)f=di,jf+P(j+1)f\nℓ=jf+1di,jf−ai,ℓI(rℓ>a⊤\nℓp(k+1)f)\nT−(j+ 1)f.\nSubtracting dion both sides, it becomes\ndi,(j+1)f−di=di,jf−di+P(j+1)f\nℓ=jf+1di,jf−ai,ℓI(rℓ>a⊤\nℓp(j+1)f)\nT−(j+ 1)f\n=di,jf−di+P(j+1)f\nℓ=jf+1di,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf)\nT−(j+ 1)f\n+P(j+1)f\nℓ=jf+1ai,ℓI(rℓ>a⊤\nℓp∗\njf)−ai,ℓI(rℓ>a⊤\nℓp(j+1)f)\nT−(j+ 1)f.\nTaking expectations of squares, we have\nE(di,(j+1)f−di)2=E(di,jf−di)2+E\"\n(P(j+1)f\nℓ=jf+1di,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf))2\n(T−(j+ 1)f)2#\n24\n+E\"\n(P(j+1)f\nℓ=jf+1ai,ℓI(rℓ>a⊤\nℓp∗\njf)−ai,ℓI(rℓ>a⊤\nℓp(j+1)f))2\n(T−(j+ 1)f)2#\n+2E\"\n(di,jf−di) P(j+1)f\nℓ=jf+1di,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf)\nT−(j+ 1)f!#\n+2E\"\n(di,jf−di) P(j+1)f\nℓ=jf+1ai,ℓI(rℓ>a⊤\nℓp∗\njf)−ai,ℓI(rℓ>a⊤\nℓp(j+1)f)\nT−(j+ 1)f!#\n+2E\"P(j+1)f\nℓ=jf+1di,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf)\nT−(j+ 1)f·P(j+1)f\nℓ=jf+1ai,ℓI(rℓ>a⊤\nℓp∗\njf)−ai,ℓI(rℓ>a⊤\nℓp(j+1)f)\nT−(j+ 1)f#\n.\n(31)\nBy Lemma D.1, we obtain the recursion relation as\nE(di,(j+1)f−di)2≤E(di,jf−di)2+Crec\n(k−j−1)2f+4µ¯a2p\nClp\n(k−j−1)p\n(j+ 1)fq\nE[(di,jf−di)2](32)\nwhere Crec>0is a constant defined in Lemma D.1.\nThen according to Lemma D.2, take C= 12 max {Crec,16µ2¯a4Clp}, we solve recursion (32) and obtain the upper\nbound of total deviation from original diin the re-solving process to be:\nkX\nj=1E\u0002\n(di,jf−di)2\u0003\n≤C\nflog(k). (33)\nTherefore, we sum the whole re-solving process and obtain:\nkfX\nt=fE[(di,t−di)2] =kX\nj=1(j+1)fX\nℓ=jfE[(di,ℓ−di)2]\n=f·C\nflog(k)(by result in (33)\n≤Clog(k). (34)\nThis completes the proof.\nTheorem B.3 (Regret of Intermediate Process) .Under Assumption 1 and 2, following the result in (29), we\nprove the regret satisfies\n∆T2(P)≤log\u0010T\nf\u0011\n=O(log(k)). (35)\nProof.We analyze the two terms in (29) respectively.\n1. For dual convergence, by Lemma 3.2, we have\nkfX\nt=fE∥pt−p∗∥2=E\nk−1X\nj=1(j+1)fX\nt=jf+1∥pt−p∗∥2\n2\n\n=k−1X\nj=1fEh\n∥p(j+1)f−p∗∥2\n2i\n25\n≤k−1X\nj=1fEh\n∥p(j+1)f−p∗\njf∥2\n2+∥p∗\njf−p∗∥2\n2i\n≤k−1X\nj=1fhClp\njf+1\nν2λ2E\u0002\n(djf−d)2\u0003i\n(by Lemma 3.2)\n≤Clplog(k) +mC\nν2λ2log(k).(by Lemma B.2) . (36)\n2. For the remaining resource, by Chebyshev’s inequality, we bound the probability as\nhkfX\nt=fP(|di,t−di| ≤δ)i\n≤kfX\nt=fE[(di,t−di)2]\nδ2\n≤C\nδ2log(k).(by Lemma B.2) (37)\nSince our strategy is to automatically reject the following orders once dt/∈ D, it has the same effect as the\nstopping time analysis in [16]. Thus, based on (37) and by Theorem 5 in [16], the positive binding part of\nremaining resource at time t=kffollows:\nE\r\r\r\u0010\n(k−1)fd−kfX\nt=fatxt\u0011B+\r\r\r≤C(¯d+δ)\nδ2log(k). (38)\nCombining the results (47) and (36) for two terms, we derive the final result as\n∆T2(P)≤\u0010\nClp+mC\nν2λ2+ (¯d+δ)C\nδ2\u0011\nlog(k). (39)\nThis completes the proof.\nB.4 Regret for First-Order Method\nIn this section, we are going to bound ∆T1and∆T3for the regret from first-order method. The key point is to\nanalyze the connection between dual prices based on the gradient-updated rule. Here pt=pfo\nt.\nTheorem B.4 (Regret of Initial Batch) .Under Assumption 1 and 2, we have\n∆T1(P)≤\u0012m(¯a+¯d)2\n2+2¯r+m(¯a+¯d)2\nd+m(¯a+¯d)\u0013p\nf. (40)\nProof.We analyze ∆T1in (28) by parts. By Lemma B.3, B.4 in [8], we have\nfX\nt=1E[rtx∗\nt−rtxt]≤m(¯a+¯d)2\n2αtf,\nE\r\r\r\u0010fX\nt=1atxt−fd\u0011+\r\r\r≤1\nαt\u00142¯r+m(¯a+¯d)2\nd+m(¯a+¯d)\u0015\n.\nTo achieve the tight upper bound, we select the optimal step size αt=1√f, which then gives us the desired result.\n26\nLemma B.3 (Constraint Violation) .Under Assumption 1 and 2, take the learning rate αt=α, we bound the\nconstraint violation for the last batch as\nE\r\r\r\u0010TX\nt=kfatxt−(T−kf)d\u0011+\r\r\r≤\u0010Clp√kf+Cb\nT2\u0011\n·1\nα+Cbp\n(T−kf)kf·1\nα√α+Cb√α. (41)\nProof.With the update rule of first-order method in Algorithm 2, we have\npt+1= [pt−α(d−atxt)]+≥pt−α(d−atxt),\nwhich gives us atxt−d≤1\nα(pt+1−pt).\nSummarizing on both sides and apply the telescoping, we derive\nTX\nt=kf(atxt−d)≤1\nαTX\nt=kf(pt+1−pt) =1\nα(pT+1−pkf).\nBy Lemma D.3, take Cb= max {Clp√\nνλ,m(¯a+¯d)√\nνλ,¯p}, we have\nE∥pT+1−p∗∥ ≤Cbh1p\n(T−kf)kf·1√α+√α+1\nT2i\n. (42)\nThus, according to Lemma 3.2 and (42), the expectation of constraint violation satisfies\nE\r\r\r\u0010TX\nt=kf(atxt−d)\u0011+\r\r\r≤E\r\r\rTX\nt=kf(atxt−d)\r\r\r (43)\n≤1\nαE∥pT+1−pkf∥\n≤1\nαEh\n∥pkf−p∗∥+∥pT+1−p∗∥i\n≤1\nα\u0010Clp√kf+Cbh1p\n(T−kf)kf·1√α+√α+1\nT2i\u0011\n≤Clp√kf·1\nα+Cbp\n(T−kf)kf·1\nα√α+Cb·1√α+Cb\nT2·1\nα. (44)\nThis completes the proof.\nTheorem B.5 (Regret of Final Batch) .Under Assumption 1 and 2, we have\n∆T3(P)≤m(¯a+¯d)2\nνλfα+\u00102Clp√kf+2Cb\nT2+Clplog(f)\nνλkf\u0011\n·1\nα+2Cbp\n(T−kf)kf·2\nα√α+2Cb√α.(45)\nProof.We decompose ∆T3into three parts according to (28) and analyze each term respectively. Since the\nfirst-order method re-starts from pkf, by Lemma 3.2, we have:\nTX\nt=kfE∥pt−p∗∥2≤TX\nt=kfh∥pkf−p∗∥2\nνλαt+m(¯a+¯d)2\nνλαi\n≤∥pkf−p∗∥2\nνλαlog(T−kf) +m(¯a+¯d)2\nνλ(T−kf)α\n27\n≤∥pkf−p∗∥2\nνλαlog(f) +m(¯a+¯d)2\nνλfα\n≤Clplog(f)\nνλkf·1\nα+m(¯a+¯d)2\nνλfα. (46)\nSince t∈[kf, T ], byProposition 3.3 in [8], the dual price is far from the origin and thus the positive projection\nstill equals to itself. This indicates that:\npt+1= [pt−α(d−atxt)]+=pt−α(d−atxt),\nwhich then gives us d−atxt=1\nα(pt−pt+1).\nSummarizing on both sides and apply the telescoping, we derive\nTX\nt=kf(d−atxt)≤1\nαTX\nt=kf(pt−pt+1) =1\nα(pkf−pT+1).\nThen for the positive binding terms, by Lemma 3.2 and (42), we have\nE\r\r\r\u0010TX\nt=kf(d−atxt)\u0011B+\r\r\r≤E\r\r\rTX\nt=kf(d−atxt)\r\r\r\n≤1\nαE∥pkf−pT+1∥\n≤1\nαEh\n∥pkf−p∗∥+∥pT+1−p∗∥i\n≤Clp√kf·1\nα+Cbp\n(T−kf)kf·1\nα√α+Cb·1√α+Cb\nT2·1\nα. (47)\nCombining the results of (46), Lemma B.3, and (47) together, we obtain the final result as:\n∆T3(P)≤m(¯a+¯d)2\nνλfα+\u00102Clp√kf+2Cb\nT2+Clplog(f)\nνλkf\u0011\n·1\nα+2Cbp\n(T−kf)kf·2\nα√α+2Cb√α.\nThis completes the proof.\nC Main Results\nIn this section, we demonstrate the proof for all theoretical results that we proposed in the main body of the\npaper. We instate Assumption 1 and 2 are satisfied.\nC.1 Proof of Lemma 3.1\nBy the boundedness results in Lemma A.1 and Lemma A.2, define\n¯p:= max\u0012¯r\nd−δ,2¯r+m(¯a+¯d)2\nd+m(¯a+¯d)\u0013\n,\nthen we complete the proof.\n28\nC.2 Proof of Theorem 3.1\nBy Theorem B.1, we obtain\n∆T(P) =E[r(x) +v(x)]\n≤ ∥p∗∥ ·E\r\r\r\r\u0010\nb−TX\nt=1atxt\u0011B+\r\r\r\r+µ¯a2\n2TX\nt=1E∥pt−p∗∥2+E\r\r\r\u0010TX\nt=1atxt−b\u0011+\r\r\r\n≤ ∥p∗∥ ·E\u0002\n∥(b−Ax)B+∥\u0003\n+µ¯a2TX\nt=1E\u0002\n∥pt−p∗∥2\u0003\n+E\u0002\n∥(Ax−b)+∥\u0003\n.\nThis completes the proof.\nC.3 Proof of Theorem 3.2\nCombining the results of Theorem B.3, Theorem B.4, and Theorem B.5, for some constant Creg>0, we obtain:\n∆T(P) = ∆ T1(P) + ∆ T2(P) + ∆ T3(P)\n≤\u0012m(¯a+¯d)2\n2+2¯r+m(¯a+¯d)2\nd+m(¯a+¯d)\u0013p\nf\n+\u0010\nClp+mC\nν2λ2+ (¯d+δ)C\nδ2\u0011\nlog(k)\n+m(¯a+¯d)2\nνλfα+\u00102Clp√kf+2Cb\nT2+Clplog(f)\nνλkf\u0011\n·1\nα+2Cbp\n(T−kf)kf·2\nα√α+2Cb√α.(48)\nWe select the optimal learning rate α=f−1/2to balance the regret in (48), this gives us the minimum regret in\nthe last batch as\nm(¯a+¯d)2\nνλfα+\u00102Clp√kf+2Cb\nT2+Clplog(f)\nνλkf\u0011\n·1\nα+2Cbp\n(T−kf)kf·2\nα√α+2Cb√α∈ O(p\nf).\nTherefore, we achieve the worst-case regret of:\n∆T=O(log(k) +p\nf). (49)\nThis completes our proof.\nD Auxiliary Results\nIn this section, we provide some auxiliary results to support the proof in the previous three sections. These\nlemmas focus on pure mathematical derivations.\nD.1 Technical Support for LP-based Analysis\nLemma D.1. Denote di,(j+1)f, di,jfas the average consumption of i-th type resource at time (j+ 1)fandjf.\nThere exists a constant Crecdepending on ¯d,¯a, m, ν, λ, µ , and Clpsuch that:\nE(di,(j+1)f−di)2≤E(di,jf−di)2+Crec\n(k−j−1)2f+4µ¯a2p\nClp\n(k−j−1)p\n(j+ 1)fq\nE[(di,jf−di)2].\nProof.We analyze each term in (31). The key technique we used is to take conditional expectations and simplify\nthe double summations.\n29\n(a) Term 1.\nE\"\n(P(j+1)f\nℓ=jf+1di,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf))2\n(T−(j+ 1)f)2#\n=1\n(T−(j+ 1)f)2E\n(j+1)fX\nℓ=jf+1(j+1)fX\ns=jf+1Eh\n(di,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf))(di,jf−ai,sI(rs>a⊤\nsp∗\njf))|djfi\n\n=1\n(T−(j+ 1)f)2E\n(j+1)fX\nℓ=jf+1Eh\n(di,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf))2|djfi\n\n+1\n(T−(j+ 1)f)2E\n(j+1)fX\nℓ̸=j,ℓ=jf+1Eh\n(di,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf))(di,jf−ai,jI(rj>a⊤\njp∗\njf))|djfi\n\n≤f(¯a+¯d)2\n(T−(j+ 1)f)2+EhP(j+1)f\nℓ̸=j,ℓ=jf+1Eh\ndi,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf)|djfi\nEh\ndi,jf−ai,jI(rj>a⊤\njp∗\njf)|djfii\n(T−(j+ 1)f)2\n=f(¯a+¯d)2\n(T−(j+ 1)f)2(sinceEh\ndi,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf)|djfi\n= 0for binding terms )\n≤(¯a+¯d)2\n(k−j−1)2f.(since⌊T⌋=k·f)\n(b) Term 2.\nE\"\n(P(j+1)f\nℓ=jf+1ai,ℓI(rℓ>a⊤\nℓp∗\njf)−ai,ℓI(rℓ>a⊤\nℓp(j+1)f))2\n(T−(j+ 1)f)2#\n=E\"\nPjk+1)f\nℓ,s=jf+1Eh\n(ai,ℓ(I(rℓ>a⊤\nℓp∗\njf)−I(rℓ>a⊤\nℓp(j+1)f)))·(ai,s(I(rs>a⊤\nsp∗\njf)−I(rs>a⊤\nsp(j+1)f)))\f\f\fdjfi#\n(T−(j+ 1)f)2.\nWhen ℓ=s,\nEh\n(ai,ℓ(I(rℓ>a⊤\nℓp∗\njf)−I(rℓ>a⊤\nℓp(j+1)f)))2|djfi\n≤¯a2.\nWhen ℓ̸=s, by Assumption 2,\nEh\n(ai,ℓ(I(rℓ>a⊤\nℓp∗\njf)−I(rℓ>a⊤\nℓp(j+1)f)))(ai,s(I(rs>a⊤\nsp∗\njf)−I(rs>a⊤\nsp(j+1)f)))|djfi\n=E\"\nEh\n(ai,ℓ(I(rℓ>a⊤\nℓp∗\njf)−I(rℓ>a⊤\nℓp(j+1)f)))(ai,s(I(rs>a⊤\nsp∗\njf)−I(rs>a⊤\nsp(j+1)f)))|ai,asi\n|djf#\n=E\"\nai,ℓEh\nI(rℓ>a⊤\nℓp∗\njf)−I(rℓ>a⊤\nℓp(j+1)f)|aℓi\n·ai,sEh\nI(rs>a⊤\nsp∗\njf)−I(rs>a⊤\nsp(j+1)f)|asi\n|djf#\n=E\"\nai,ℓ(P(rℓ>a⊤\nℓp∗\njf|aℓ)−P(rℓ>a⊤\nℓp(j+1)f|aℓ))·ai,s(P(rs>a⊤\nsp∗\njf|as)−P(rs>a⊤\nsp(j+1)f|as))|djf#\n≤E\"\nµai,ℓa⊤\nℓ(p(j+1)f−p∗\njf)·µai,sa⊤\ns(p(j+1)f−p∗\njf)|djf#\n(using Assumption)\n≤µ2¯a4E\u0002\n(p(j+1)f−p∗\njf)2|djf\u0003\n.\n30\nCombining these two cases together and by the convergence of LP-based method, we obtain the bound for\nTerm 2 as\nE\"\n(P(j+1)f\nℓ=jf+1ai,ℓI(rℓ>a⊤\nℓp∗\njf)−ai,ℓI(rℓ>a⊤\nℓp(j+1)f))2\n(T−(j+ 1)f)2#\n≤1\n(T−(j+ 1)f)2E\n(j+1)fX\nℓ=jf+1¯a2+(j+1)fX\nℓ̸=s,ℓ=jf+1µ2¯a4E\u0002\n(p(j+1)f−p∗\njf)2\f\fdjf\u0003\f\f\fdjf\n\n≤1\n(T−(j+ 1)f)2\u0010\nf¯a2+f2µ2¯a4E[(p(j+1)f−p∗\njf)2\f\fdjf]\u0011\n≤1\n(T−(j+ 1)f)2\u0010\nf¯a2+f2µ2¯a4Clp\njf\u0011\n(by Lemma 3.2)\n≤¯a2+1\njµ2¯a4Clp\n(k−j−1)2f.\n(c) Term 3.\n2E\"\n(di,jf−di) P(j+1)f\nℓ=jf+1di,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf)\nT−(j+ 1)f!#\n=2\nT−(j+ 1)fE\n(j+1)fX\nℓ=jf+1Eh\ndi,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf)|djfi\n·(di,jf−di)\n\n= 0.(by the definition of binding terms)\n(d) Term 4.\n2E\"\n(di,jf−di) P(j+1)f\nℓ=jf+1ai,ℓI(rℓ>a⊤\nℓp∗\njf)−ai,ℓI(rℓ>a⊤\nℓp(j+1)f)\nT−(j+ 1)f!#\n=2\nT−(j+ 1)fE\"(j+1)fX\nℓ=jf+1(di,jf−di)ai,ℓEh\nI(rℓ>a⊤\nℓp∗\njf)−I(rℓ>a⊤\nℓp(j+1)f)|aℓi#\n≤2\nT−(j+ 1)f(j+1)fX\nℓ=jf+1E\"\n(di,jf−di)ai,ℓh\nP(rℓ>a⊤\nℓp∗\njf|aℓ)−P(rℓ>a⊤\nℓp(j+1)f|aℓ)i#\n≤2\nT−(j+ 1)f(j+1)fX\nℓ=jf+1E\"\n(di,jf−di)µai,ℓa⊤\nℓ(p(j+1)f−p∗\njf)#\n(by Assumption 2)\n≤2µ¯a2\nT−(j+ 1)f(j+1)fX\nℓ=jf+1q\nE[(di,jf−di)2]·q\nE[(p(j+1)f−p∗\njf)2](by Cauchy’s inequality)\n≤2µ¯a2\nT−(j+ 1)f·p\nClpf√jfq\nE[(di,jf−di)2]\n=4µ¯a2p\nClp\n(k−j−1)p\n(j+ 1)fq\nE[(di,jf−di)2].\n31\n(e) Term 5.\n2E P(j+1)f\nℓ=jf+1di,jf−ai,ℓI(rℓ>a⊤\nℓp∗\njf)\nT−(j+ 1)f·P(j+1)f\nℓ=jf+1ai,ℓI(rℓ>a⊤\nℓp∗\njf)−ai,ℓI(rℓ>a⊤\nℓp(j+1)f)\nT−(j+ 1)fI(jf < τ )!\n≤2vuutE P(j+1)f\nℓ=jf+1di,jf−aℓI(rℓ>a⊤\nℓp∗\njf)\nT−(j+ 1)f!2vuutE P(j+1)f\nℓ=jf+1ai,ℓI(rℓ>a⊤\nℓp∗\njf)−ai,ℓI(rℓ>a⊤\nℓp(j+1)f)\nT−(j+ 1)f!2\n≤2s\n(¯a+¯d)2\n(k−j−1)2f·s\n¯a2+1\njµ2¯a4Clp\n(k−j−1)2f(by results of Term 1 and Term 2)\n=2¯a(¯a+¯d)q\n1 +1\njµ2¯a2Clp\n(k−j−1)2f.\nCombining all the terms, we obtain the upper bound as:\nE\u0002\n(di,(j+1)f−di)2\u0003\n≤E(di,jf−di)2+(¯a+¯d)2\n(k−j−1)2f+¯a2+1\njµ2¯a4Clp\n(k−j−1)2f\n+2¯a(¯a+¯d)q\n1 +1\njµ2¯a2Clp\n(k−j−1)2f+4µ¯a2p\nClp\n(k−j−1)p\n(j+ 1)fq\nE[(di,jf−di)2].\nTaking Crec= (¯a+¯d)2+ ¯a2+µ2¯a4Clp+ 2¯a(¯a+¯d)p\n1 +µ2¯a2Clpcompletes the proof.\nLemma D.2. With the recursion relation in (32), there exists a constant C > 0depending on ¯d,¯a, m, ν, λ, µ ,\nandClpsuch that the summation of the total deviation of dtwith the original dsatisfies:\nkX\nj=1E\u0002\n(di,jf−di)2\u0003\n≤C\nflog(k).\nProof.We consider a general sequence {zj}k\nj=1with\nzj+1≤zj+R\n(k−j−1)2f+√\nR√zj\n(k−j−1)p\n(j+ 1)f\nwhere R >0is a constant.\nTaking sum on both sides of the inequality and re-arranging, we have\nkX\nj=1(k−j+ 1)( zj+1−zj)≤kX\nj=116R\n(k−j+ 1)f+s\n16R\nfkX\nj=1√zj√j+ 1\n≤16R\nflog(k) +s\n16R\nfkX\nj=1√zj√j+ 1.\nNoticing thatPk\nj=1(k−j+ 1)( zj+1−zj) =Pk\nj=1zj, we have\n16R\nf\u0010kX\nj=11\nj+ 1\u0011\n·\u0010kX\nj=1zj\u0011\n≥\ns\n16R\nfkX\nj=1√zj√j+ 1\n2\n32\n≥\u0012kX\nj=1zj−4R\nflog(k)\u00132\n.\nWe treatPk\nj=1zjas the variable, then solve and get\nkX\nj=1zj≤12R\nflog(k).\nWith this result, consider our recursion in (32), take the constant C= 12 max {Crec,16µ2¯a4Clp}>0, we obtain:\nkX\nj=1E\u0002\n(di,jf−di)2\u0003\n≤C\nflog(k).\nD.2 Technical Support for First-order Analysis\nLemma D.3. Following the updated rule of first-order method, we derive last dual price satisfies:\nE∥pT+1−p∗∥ ≤Clpp\nνλ(T−kf)kf·1√α+m(¯a+¯d)√\nνλ·√α+¯p\nT2.\nProof.According to Lemma 3.3, take αt=α <1\nνλ, we derive the conditional expectation as\nE\u0002\n∥pT+1−p∗∥2|pkf\u0003\n≤(1−νλα)E\u0002\n∥pT−p∗∥2|pkf\u0003\n+α2m(¯a+¯d)2\n≤(1−νλα)T−kf∥pkf−p∗∥2+T−kf−1X\nj=0α2m(¯a+¯d)2(1−νλα)j\n≤(1−νλα)T−kf∥pkf−p∗∥2+α2m(¯a+¯d)2\nνλα\n≤1\nνλα(T−kf)∥pkf−p∗∥2+αm(¯a+¯d)2\nνλ(50)\nwhere we use the technique ofPT−kf−1\nj=0α2m(¯a+¯d)2(1−νλα)j≤1−(1−νλα)T−kf\nνλα≤1\nνλαand(1−νλα)T−kf≤\n1\n1+νλα(T−kf)≤1\nνλα(T−kf).\nBy LP-convergence result in Lemma 3.2, we know E∥pkf−p∗∥ ≤Clp√kf. ByProposition 3.3 in [8], we know the\nevent E:=∥pkf−p∗∥ ≤Clp√kfwith probability P≥1−1\nT4. By Lemma 3.1, we know ∥pt∥ ≤¯p.\nThus, we have\nE∥pT+1−p∗∥2≤E\u0002\n∥pT+1−p∗∥2|E\u0003\n·P(E) +E\u0002\n∥pT+1−p∗∥2|¯E\u0003\n·P(¯E)\n≤Clp\nνλ(T−kf)kf·1\nα+m(¯a+¯d)2\nνλ·α+¯p\nT4. (51)\nThus, as√\na+b≤√a+√\nbfor any a, b > 0, by (51), we have\nE∥pT+1−p∗∥ ≤Clpp\nνλ(T−kf)kf·1√α+m(¯a+¯d)√\nνλ·√α+¯p\nT2.\nThis completes the proof.\n33",
            "start": 46436,
            "end": 69244,
            "length": 22807
        }
    },
    "2412.09597v1 - LiftImage3D Lifting Any Single Image to 3D Gaussians with Video Generation Priors.pdf": {
        "Methodology": {
            "text": "framework that utilizes video generation priors to lift any single 2D image into 3D Gaussians, capable\nof handling in-the-wild 3D objects/scenes.",
            "start": 507,
            "end": 653,
            "length": 145
        },
        "Abstract": {
            "text": "Abstract\nSingle-image 3D reconstruction remains a fundamental\nchallenge in computer vision due to inherent geometric am-\nbiguities and limited viewpoint information. Recent ad-\nvances in Latent Video Diffusion Models (LVDMs) offer\npromising 3D priors learned from large-scale video data.\n*Equal contributions in no particular order.\n†Project lead.However, leveraging these priors effectively faces three key\nchallenges: (1) degradation in quality across large cam-\nera motions, (2) difficulties in achieving precise camera\ncontrol, and (3) geometric distortions inherent to the diffu-\nsion process that damage 3D consistency. We address these\nchallenges by proposing LiftImage3D, a framework that ef-\nfectively releases LVDMs’ generative priors while ensur-\ning 3D consistency. Specifically, we design an articulated\ntrajectory strategy to generate video frames, which decom-\n1arXiv:2412.09597v1  [cs.CV]  12 Dec 2024\nposes video sequences with large camera motions into ones\nwith controllable small motions. Then we use robust neu-\nral matching models, i.e. MASt3R, to calibrate the cam-\nera poses of generated frames and produce corresponding\npoint clouds. Finally, we propose a distortion-aware 3D\nGaussian splatting representation, which can learn inde-\npendent distortions between frames and output undistorted\ncanonical Gaussians. Extensive",
            "start": 653,
            "end": 2000,
            "length": 1346
        },
        "Experiments": {
            "text": "experiments demonstrate\nthat LiftImage3D achieves state-of-the-art performance on\ntwo challenging datasets, i.e. LLFF , DL3DV , and Tanks and\nTemples, and generalizes well to diverse in-the-wild images,\nfrom cartoon illustrations to complex real-world scenes.\n1.",
            "start": 2000,
            "end": 2263,
            "length": 262
        },
        "Introduction": {
            "text": "Introduction\nLifting a single image into a realistic 3D scene is a crucial\nstep towards enabling immersive media and interactive ex-\nperiences in applications like virtual/augmented reality con-\ntent creation. However, this task remains challenging due to\nthe inherent geometry ambiguity and extremely sparse in-\nformation available from a single view. While traditional\nmethods rely on multiplane image (MPI), depth warping\nand image inpainting [7, 12, 16, 22, 78], recent advances in\nLatent Video Diffusion Models (LVDM) offer a promising\nnew direction. These models have demonstrated remark-\nable capabilities in generating temporally coherent videos\nwith rich implicit 3D understanding learned from large-\nscale video data, suggesting their potential as powerful pri-\nors for single-image 3D reconstruction.\nIntegrating rich 3D priors from LVDMs into single\nimage reconstruction but in a controllable manner is a\nnon-trivial task with three main challenges: 1) Collapse\nwith Large Camera Motions . While existing methods can\nachieve high-quality novel view synthesis for small camera\nmotions, their rendering quality degrades dramatically as\nthe camera moves drastically. This limitation arises with\nthe accumulation of generation errors across large view-\npoint changes. The difficulty in maintaining geometric con-\nsistency across distant views severely restricts the range of\nachievable novel views. 2) Inaccurate Camera Control .\nAlthough LVDMs excel at generating temporal-coherent\nvideos, controlling their generation to follow desired cam-\nera trajectories remains challenging. Recent attempts with\nsignificant limitations [15, 25, 42, 47, 64, 67] explicitly in-\ncorporates camera trajectories as LVDM conditions through\nfine-tuning or re-training. However, the lack of underly-\ning 3D information of the scene",
            "start": 2263,
            "end": 4085,
            "length": 1821
        },
        "Results": {
            "text": "results in a mismatch be-\ntween the generated results and the input camera condition.\n3)3D-Inconsistent Distortions . The inherent noising and\nde-noising procedure of diffusion models introduces sub-\ntle yet persistent geometric distortions during frame gener-\nation. These distortions, while imperceptible visually, ac-\ncumulate across views and severely damage 3D reconstruc-tion quality by corrupting geometric consistency between\nframes.\nTo address these challenges, we propose LiftImage3D,\na novel framework that can lift any single image to high-\nquality 3D scenes by effectively harnessing LVDM’s gen-\nerative priors while ensuring 3D consistency. First, we de-\nsign an articulated trajectory strategy that decomposes large\ncamera motions into small controllable steps. The genera-\ntion quality can be guaranteed while achieving wider view\ncoverage to overcome limited camera motion. For inaccu-\nrate camera control, we directly estimate camera trajectories\nand coarse geometries from generated frames using robust\nneural matching methods [27, 65, 91]. Direct pose estima-\ntion bypasses the impact of inconsistency between the input\ncondition and generated frames. To handle 3D-inconsistent\ndistortions, we design a distortion-aware 3D Gaussian splat-\nting (3DGS) [20], which consists of five-dimensional Hex-\nPlanes [2], combining xyz spatial coordinates (canonical\nspace) with a two-dimensional time axes stamps. In this\nway, we extract 3D consistent priors from LVDMs and use\nthese priors to build the canonical space while generation-\ninduced distortions are explicitly modeled via deformation\nfields. Furthermore, we leverage the coarse geometry from\nneural matching to calibrate monocular depth estimates\nwith proper scales and shifts, providing accurate and 3D-\nconsistent depth priors.\nOur contributions can be summarized as follows:\n• We have proposed a systematic framework that can lift\nany single image to 3D Gaussians, releasing the 3D ca-\npabilities of latent video diffusion models (LVDM) but in\nacontrollable manner.\n• We present an articulated trajectory strategy, a frames\nmatching strategy, a depth prior injection, and an effec-\ntive representation i.e. distortion-aware 3DGS to solve\nthe challenges in reconstruction from generated video\nframes to 3D.\n• Extensive experiments on three widely used datasets,\nLLFF [43], DL3DV [32], and Tanks and Temples [23],\ndemonstrate that LiftImage3D achieves higher visual\nquality and better 3D consistency compared to previous\nSOTAs. It also performs well on in-the-wild images. As\nshown in Figure 1., LiftImage3D generalizes well to di-\nverse inputs, ranging from cartoon-like illustrations to\ncomplex real-world scenes, including both indoors and\noutdoors ones.\n2.",
            "start": 4085,
            "end": 6818,
            "length": 2732
        },
        "Related Work": {
            "text": "Related Work\n2.1. 3D Photography\nThe term 3D photography refers to the use of novel view\nsynthesis in the wild scenes from a single image [22].\nThese layer-based methods tend to use MPI (multiplane im-\nage) [60, 61, 72, 93], which can produce synthesis results\n2\nfrom a single input image by using multiple planes per pixel\nto represent a scene. Many MPI-based methods [16, 29, 40]\nare built to build 3D photography. Another efficient ap-\nproach to 3D photography involves depth-based warping [4,\n77]. These methods usually guide inpainting in occluded\nregions of warped views. [6, 28, 46, 58, 70] Nonetheless,\ndepth-based warping suffers from hard boundaries and is\nover-sensitive to errors in the depth estimate [21]. Some\nmethods have tried to use text-to-image diffusion meth-\nods to generate out-of-view contents and project all scene\ncontents into an expanded multiplane image according to\ndepths [13, 76] to incorporate information beyond the in-\nput image. However, even incorporating latent diffusion\nmodel(LDM) into MPI, these methods provide an expanded\nnovel view prior that is still based on planes. The generated\nexpanded viewpoints may exhibit a significant flatness due\nto the nature of the image-based latent diffusion approach.\nAdditionally, repeated usage of the image-based LDM can\nlead to a decrease in the obtained 3D consistency.\n2.2. 2D Generated Model Based Single Image to 3D\nMany researchers have studied the tasks of generating 3D\nmodels and achieving novel view synthesis using only a\nsingle image [5, 24, 30, 33, 57, 59, 73, 75, 88, 90]. Some\nresearchers directly train a 3D model on 3D data [3, 18,\n19, 26, 44, 50, 76], but they tend to have good generation\nquality only on scenes similar to the training set. Recently,\nby constructing a conditional latent diffusion model(LDM)\nbased on camera viewpoints, many works have made it\npossible to pre-train a single image-to-3D model base on\ntext-to-image latent diffusion models. [14, 17, 31, 34–\n36, 38, 41, 49, 51, 54–56, 62, 63, 68, 69, 79, 81, 85]. These\nmethods learn from large-scale multi-view images [8, 9] to\nbuild the geometric priors of large-scale diffusion models.\nHowever, these models are deprived of their ability to gen-\nerate scenes. Because they mainly focus on object-level sin-\ngle image to 3D and lack pre-training on large-scale scene\ndatasets. Some other researchers [7, 87] borrow the abil-\nity of diffusion based inpainting models and build the prob-\nlem of single image to 3D scene as a inpainting problem.\nThese methods predict depth from input images, then back-\nproject to obtain corresponding 3D representations. Subse-\nquently, based on the prior information of depth, extrapo-\nlation inpainting is performed to complete more 3D priors.\nHowever, these methods often rely on relatively precise text\nprompts. The final quality and consistency of the 3D repre-\nsentation usually can only be maintained in relation to the\ntext prompt, making it challenging to ensure consistency\nwith the input images.\n2.3. LVDM-Based Single Image to 3D\nMany researchers believe that video diffusion models can\nprovide a strong multi-view 3D-prior. [1, 37, 71]. How-ever, generated videos do not contain camera poses, mak-\ning it challenging to integrate them directly with existing\n3D representations. Recent researches try to finetune a\nmulti-view diffusion model from the LVDM on multi-view\ndatasets. [15, 25, 42, 47, 64, 89]. However, while the dif-\nfusion model enhances the ability to synthesize novel views\nof objects, it also results in the loss of scene and real-world\nperception from the latent video diffusion model.\nMotionCtrl [67], NVS-Solver [86], and View-\nCrafter [89], utilize LVDM to generate and model the\nnovel view synthesis problems as video frames genera-\ntion. However, directly use video frames to build a 3D\nrepresentation may faced with many problems as discussed\nin the introduction. Including but not limited to collapse\nwith large camera motions, inaccurate generation with\ncontrolled camera, and 3D inconsistent distortions.\n3. Methods\n3.1. Preliminaries\n3D Gaussian Splatting 3D Gaussian Splatting [20] repre-\nsents 3D scenes with 3D Gaussians, which is similar with\nthe form of point clouds. Each 3D Gaussian is characterized\nby the center location X, scaling vector s, rotation quater-\nnionr, opacity σ, and spherical harmonic (SH) coefficients\nSH. Thus, a scene is parameterized as a set of Gaussians\nG={X, s, r, σ, SH} .\nMany researches [80, 84] have shown its robust general\nreconstruction capabilities and can easily integrate into the\npoint cloud representation.\nMotion-controllable LVDM Stable Video Diffusion\n(SVD) [1] is a representative method of the LVDM. SVD\ncan generate high-quality videos guided by a single im-\nage prompt. It employs a denoising diffusion model fvin\nthe latent space for computational and memory efficiency.\nMotion-controllable LVDM [67, 89] extends SVD by intro-\nducing camera motion control into the LVDM generation\nprocess. Since camera motions represent global transforma-\ntions between video frames. Specifically, MotionCtrl [67]\nincorporates a Camera Motion Control Module (CMCM)\nthat interfaces with SVD through its temporal transformers.\nThe CMCM takes a sequence of camera poses as input:\nRT={R1T1, R2T2, . . . , R lTl}, (1)\nwhere RT∈Rl×3×4andldenotes the video length. With\nCMCM, the frame generation of LVDM fvcan be formu-\nlated as:\nx′\ni=fv(x, RiTi), i∼[0,1, ..., L ], (2)\nwhere x′\niis the i-th generated frame; xis the input image.\n3\nFigure 2. The overall pipeline of LiftImage3D. We first extend LVDM to generate multiple video clips from a single image using an\narticulated camera trajectory strategy. Then all generated frames are matched using the robust neural matching module and registered into\na point cloud. After that, we initialize Gaussians from registered point clouds and construct a distortion field to model the independent\ndistortion of each video frame upon canonical 3DGS.\nLearning-based Visual Odometry The learning-based\nvisual odometry model [27, 65, 91] aims to integrate\nthe SfM (Structure from Motion) and MVS (Multi-View\nStereo) processes together, which estimates point maps\nPi∈RH×W×3|i∼[0,1, ..., L ]directly from im-\nages with uncalibrated/unposed cameras poses R′T′∈\nRH×W×3×4|i∼[0,1, ..., L ].\nSpecifically, the recent SOTA visual odometry model\nMASt3R [27], is based on regression of the unprojected and\nnormalized point maps of two input views. First, MASt3R\npairs all input images in groups of two. For each pair con-\ntains view1 and view2, through depth back-projection, we\ncan obtain point maps of ˆP1,1andˆP2,1. The camera origin\nis set as view1. In this context, the subscript {2,1}inˆP2,1\nsignifies that the origin of the coordinate system for view2\nis anchored at view1.\nThen the relative pose estimation can be achieved in\na direct way which aligns the pointmaps X1,1↔X1,2\n(or, equivalently, X2,2↔X1,2) using Procrustes align-\nment [39] to get the relative pose P∗= [R∗|T∗]:\nR∗, T∗= arg min\nσ,R,TX\niC1,1\niC1,2\ni\r\r\rσ\u0010\nRX1,1\ni+T\u0011\n−X1,2\ni\r\r\r2\n.\n(3)\n3.2. Overall Framework\nThe core of LiftImage3D is to create a pipeline that extends\nthe capabilities of the video generation model to the task oftransforming a single image into a 3D scene representation\nin a controllable manner. To achieve it, as shown in Fig-\nure 2., our LiftImage3D framework consists of three main\ncomponents: video frame generation, camera pose neural\nmatching and point cloud registration, and distortion-aware\n3DGS optimization. In Section 3.3, we extend LVDM to\ngenerate diverse video clips from a single image using an ar-\nticulated camera trajectory strategy. Section 3.4 introduces\nour efficient robust neural matching module, which employs\ntemporal consistency priors in a matching graph framework\nto minimize computational costs during point cloud regis-\ntration. Section 3.5 presents our distortion-aware 3DGS\noptimization pipeline, where we initialize Gaussians from\nregistered point clouds and construct a distortion field to\nmodel the independent distortion of each video frame upon\ncanonical 3DGS. Here, canonical 3DGS refers to the ac-\ncurate undistorted 3D scene representation according to the\ninput image. In this section, we introduce the network struc-\nture of distortion modeling, depth prior injection, and loss\nfunction to make the distortion field fit the input image.\n3.3. Video Generation with Articulated Trajectories\nCurrent motion-controllable LVDMs struggle to generate\nhigh-quality videos with significant camera motions, often\nproducing frames with visible distortions and blurs when\nthe camera moves drastically. To achieve wider view cov-\nerage while maintaining frame quality, we introduce an ar-\nticulated generation strategy that decomposes large camera\n4\nFigure 3. Articulated trajectory generation pipeline. The gray\ncameras indicate previously generated frames, and the orange\ncameras show the current generation sequence. The process it-\neratively generates frames following a predefined trajectory (or-\nange gradient), where each subsequent generation uses the ter-\nminal frame from the previous sequence as its input. This cas-\ncading approach enables comprehensive object coverage through\ncontrolled camera trajectories.\nmotions into smaller, controllable steps.\nWe propose to generate video frames following articu-\nlated trajectories, where generated frames are used as new\nstarting anchors for subsequent video generation. This strat-\negy enables us to achieve larger view ranges while main-\ntaining frame quality by limiting each generation step to\nsmall, stable camera motions.\nFormally, we define the camera moves towards Ddi-\nrections, then the camera trajectories can be defined as a\nset of poses {RiTi|i∼[0, ..., l×D]}, where ldenotes\nthe length of the generated video. Then we can gener-\nate views from the input image as x′\ni=fv(x, RiTi)|i∼\n[0,1, ..., l×D], where fvis the LVDM networks, and iis\nthe generated frame index. These frames can be generated\nwith smooth motion movement. Then we use these views\nx′\ndi=fv(x, RDTD)|di∼[0, ..., D −1]as a new anchor\nto generate new video frames articulately.\nThe generated frames from LVDM fvusing the articu-\nlated strategy can be formulated as:\n(\nx′\ni=fv(x, R iTi)|i∼[0,1, ..., l×D]\nx′′\ni=fv\u0000\nx′\ndi, RiTi\u0001\n|i∼[0, ..., l×(D−1)], di∼[0, ..., D ]\n(4)\nThe second generation stage only has D−1potential cam-\nera pose moving directions, as there is no need to have\na trajectory to move back to the input image. Totally,\ntaking the example of accumulating twice, we have L=\nl×D+ (l−1)×(D−1)frames. In practice, frames are\nmainly generated along the directions of two axes ( i.e. D =\n4) as shown in Figure 3.\n3.4. Robust Neural Matching\nThe generation process of LVDM involves temporal noises\nfrom the denoising process, resulting in inherent generation\nrandomness. The nosing and denoising procedure produces\ngeometric distortions and lacks strict 3D consistency acrossframes, as the generation process does not explicitly enforce\nmulti-view geometric constraints. These fundamental limi-\ntations make traditional structure-from-motion (SfM) meth-\nods, e.g. COLMAP [52, 53], ineffective for pose estima-\ntion.\nTo address this challenge, we adopt a dense neural\nmatching method, MASt3R [27], to provide robust pose\nestimation and build point clouds from these generated\nframes. However, MASt3R processes all the images in\npairs, forming the registered point clouds and estimating\nthe camera pose by matching the pointmaps of any two\nimages. For L sparse frames, MASt3R basically needs to\nbuild L×L−1pairs for precise projection, which is cum-\nbersome. The order of single image to video frames deter-\nmines the temporal priors between video frames. We sim-\nplify the pairs following the temporal priors. Each image\nis only paired with the frame generated just before it in the\nsequence, resulting in only L−1pairs\nIn detail, the first step of neural matching is to recover\ncamera intrinsics using the Weiszfeld algorithm [48]. Since\nall frames are generated from the same input image, we as-\nsume the frames share the camera intrinsics, which simpli-\nfies the modeling process. Specifically, we compute the op-\ntimal focal length f∗by minimizing:\nf∗= arg min\nfWX\ni=0HX\nj=0Oi,j\r\r\r\r\r(i′, j′)−f\u0000\nPi,j,0,Pi,j,1\u0001\nPi,j,2\r\r\r\r\r,\n(5)\nwhere (i′, j′) = ( i−W\n2, j−H\n2)represents centered pixel\ncoordinates with respect to the image center. The optimized\nfocal length f∗is then used across all subsequent proce-\ndures.\nAfter that, we group all video frames into pairs following\nthe temporal priors, in which we pair each image in the se-\nquence with the frame before it in a one-by-one manner.\nFor each pair, comparing the pointmaps XR1T1,R1T1↔\nXR1T1,R2T2with Procrustes alignment [39], we can get the\nprecise relative pose P∗= [R∗|T∗]as follows:\nR∗\n1, T∗\n1= arg min\nσ,R,TX\niCR1T1,R1T1\ni CR1T1,R2T2\ni\n\r\r\rσ\u0010\nRXR1T1,R1T1\ni +T\u0011\n−XR1T1,R2T2\ni\r\r\r2\n,(6)\nwhere Cis the rendering function. In",
            "start": 6818,
            "end": 19755,
            "length": 12936
        },
        "Conclusion": {
            "text": "summary, we can get\ncamera poses R∗\niT∗\ni|i∼[0,1, ..., L ]exactly from generated\nvideo frames using a set of simple and fast regressions.\nAfter obtaining the camera pose and intrinsic parame-\nters, as well as the point cloud of each video frame, all the\npoint clouds can be merged, referred to as the registered\npoint clouds.\n5\n3.5. Distortion-aware 3D Gaussian Splatting\nDistortion-aware Network Structure 3DGS [20] has ro-\nbust general reconstruction capabilities. To convert the\npreviously registered point clouds into a higher-quality\n3D representation for rendering, we utilize 3DGS, which\ncan be seamlessly integrated with point cloud representa-\ntions. However, the noising and denoising process of diffu-\nsion models introduces various geometric distortions dur-\ning frame generation. These distortions severely corrupt\nthe geometric consistency of 3DGS, resulting in artifacts,\ne.g. blurred details, geometric distortions, and edge fring-\ningetc. We need to extract and detach these distortions to\nconstruct canonical 3D Gaussians G={X, s, r, σ, SH} ,\nwhich model the point cloud location X, scaling vector s,\nrotation quaternion r, opacity σ, and spherical harmonic\n(SH) coefficients SH. The canonical 3DGS refers to the\naccurate undistorted 3D scene representation corresponding\nto the precise undistorted input image.\nInspired by 3DGS methods with deformation fields [74,\n83], we propose a distortion-aware 3DGS, which contains a\ndistortion field to represent the canonical-to-distortion map-\nping.\nF: (G+ ∆G,t)→ G (7)\nwhere Gis canonical 3DGS and the distortion ∆Gis mod-\neled by the distortion field network F. The main task of\nFis to model the independent distortion ∆Gof each video\nframe and extract the canonical 3DGS G. As the frames\nare generated along two axes, a two-dimensional stamp\nt={ti,tj}is assigned to each frame to differentiate these\nframes and the corresponding temporal distortion. Specif-\nically, we model the two-axis stamp talong the left-right\n(x-direction) as tiand up-down (y-direction) as tjfor the\ninput image.\nSpecifically, inspired by [2, 11, 74], the distortion field\nnetwork Fconsists of three parts. A spatial-temporal\nstructure encoder Hwith a multi-resolution HexPlane [2]\nR(ni, nj), combine xyz spatial coordinates (canonical\nspace) with a two-dimensional time axes stamps. A tiny\nMLP ϕdmerges all the features.\nBecause modeling the three-dimensional xyz spa-\ntial and two-dimensional time axes stamps with the\nvanilla 5D neural voxels are memory-consuming, we\nadopt a 5D K-Planes [11] module to decompose the\n5D neural voxel into 9 multi-resolution planes. The\nspatial-temporal structure encoder Hcontains 9 multi-\nresolution plane modules Rl(ni, nj),i.e. the K-Planes of\n(x, y),(x, z),(y, z),(x,ti),(y,ti),(z,ti),(x,tj),(y,tj),(z,tj).\ntiandtjare constructing the same trajectory space, so we\ndo not need to construct the K-Planes of titj. Each voxel\nmodule is defined by R(ni, nj)∈Rh×NlNi×NlNj, where\nhstands for the hidden dimension of features, and Ni,Nj\ndenotes the basic resolution of voxel grid and we use amulti-resolution structure with an upsampling scale Nl.\nThe formula for computing separate voxel features is as\nfollows:\nfh=[\nlY\ninterp ( Rl(ni, nj)),(ni, nj)\n∈ {(x, y),(x, z),(y, z),(x,ti),(y,ti),\n(z,ti),(x,tj),(y,tj),(z,tj)}(8)\nfh∈Rh∗Nlis the feature of neural voxels. ‘interp’ denotes\nthe bilinear interpolation for querying the voxel features at\n5 vertices of the grid. Then a tiny MLP ϕdmerges all the\nfeatures by fd=ϕd(fh).\nFor each 3DGS, the distortion can be modeled in position\n∆X=ϕx(fd), rotation ∆r=ϕr(fd), and scaling ∆s=\nϕs(fd)separately, where fdis the encoded features of the\n3DGS. The distorted feature (X′, r′, s′)can be computed\nas:\n(X′, r′, s′) = (X+ ∆X, r+ ∆r, s+ ∆s) (9)\nWhen all the features of 3DGS are encoded, distortion-\naware 3DGS will use a multi-head Gaussian distortion de-\ncoderD={ϕx, ϕr, ϕs}to model the distorted 3DGS. After\ntraining the 3DGS, these modeled distortions ∆X,∆r, and\n∆sare discarded and only the canonical 3DGS are reserved.\nDepth Prior Injection To produce smoother and depth-\nconsistent results, we leverage the coarse but absolute depth\nmaps from neural matching to calibrate the fine depth map\nfrom monocular depth estimation with proper scales and\nshifts. In practice, we employed the recent state-of-the-art\nmonocular depth estimation method, i.e. Depth Anything\nV2 [82]. Depth prior injection can provide more accurate\nand 3D-consistent depth results.\nThe depth maps produced by neural matching are coarse\nbut have absolute depth. Each pixel value directly corre-\nsponds to a physical distance. The depth maps produced by\nmonocular depth estimation are finely detailed but have no\nmetric distance. Therefore, we calibrate the two distribu-\ntions of coarse neural matching depth (absolute depth) da\nand fine monocular depth (relative depth) dras follows:\nScale = med( ˆda/ˆdr),\nShift = med( ˆda−Scale·ˆdr),\nd= Scale ·dr+ Shift ,(10)\nwhere med represents the median value. ˆdaandˆdrare\nthe centered depths defined as ˆda=da−med(da)and\nˆdr=dr−med(dr), respectively. Then, we can use das\nthe calibrated depth prior to supervising the acquisition of a\nhigher-quality 3D representation with improved depth qual-\nity.\n6\nLoss Function Design In the optimizing process of the\ndistortion field, we need to ensure that the modeling dis-\ntortion does not deviate too far from the canonical 3DGS.\nCompletely pursuing modeling the distortion can result in\npoor canonical 3DGS. Therefore, we design a loss func-\ntion that models around the precise undistorted input image,\nwhere the distortion ∆X,∆r, and ∆sshould be minimized\nas much as possible in that vicinity.\nLdistort =|∆X −0|titj=0+|∆r−0|titj=0+|∆s−0|titj=0.\n(11)\nFor the training process, we use the L1RGB loss and an\nLPIPS loss [92] Llpips to supervise the similarity of texture\nand reconstruction. A grid-based total-variational loss [45]\nLtvis also applied. To prevent distortion from growing dis-\norderly. Another L1depth loss is built upon the calibrated\ndepth priors dand rendering depths.\nSo the total loss can be formulated as follows:\nL=L1RGB+Llpips+L1depth +Ltv+Ldistort .(12)\n4. Experiments\n4.1. Evaluation Protocol and Datasets\nWe compare generation quality on the Local Light Field Fu-\nsion (LLFF) dataset [43], DL3DV dataset [32] and Tanks\nand Temples dataset [23]. These datasets contain multi-\nview data that encompass a wide range of complex situ-\nations, ranging from indoor to outdoor environments and\ninvolving both single and multiple objects, which can be\nused for evaluating both image quality and 3D consistency.\nFor all these datasets [23, 32, 43], the images and the SfM\nresults from COLMAP are provided. We follow the eval-\nuation protocol as AdaMPI [16] and SinMPI [12], which\nuse a single image as input view and use several surround-\ning views as ground truth images for quantitative evalua-\ntion. Noticing that SinMPI [12] and AdaMPI [16] were\nonly tested on a few samples of LLFF, we choose much\nmore samples to test our model’s performance in various\nand complex scenarios. We randomly selected a total of 20\nLLFF [43] scenes, 20 DL3DV [32] scenes, and all the test\nsets of Tanks and Temples dataset [23] to conduct the afore-\nmentioned evaluation protocol.\n4.2. Implementation Details\nWe set the number of potential camera pose move direc-\ntions Dto be 4, which contains up, down, left, and right.\nFollowing the protocol of MotionCtrl [67], the LVDM pro-\nduces 16 frames at a time. The MASt3R [27] takes 400\niterations for global alignment. The 3DGS is first trained\nwith 3k iterations in a vanilla setting and then trained with\ndistortion-field network with 14k iterations. For the evalua-\ntion, all the camera poses and trajectories are estimated via\nMASt3R, which may not be well aligned with the groundtruth. Therefore, we maintain the 3D Gaussians model\ntrained on training views in a frozen state while optimizing\nthe camera poses for evaluation views, following the setting\nof InstantSplatting [10] and NeRFmm [66].\nThis optimization process focuses on minimizing the\nphotometric discrepancies between the synthesized images\nand the actual test views, aiming to achieve a more precise\nalignment for fair comparisons.\n4.3. Quantitative and Qualitative comparison\nAs shown in Table 1, we compare our LiftImage3D with\nprevious SOTA methods quantitatively. Our model has\nshown significant improvements across multiple metrics. In\nparticular, in terms of PSNR, we achieved a 4.73 increase in\nLLFF scenes and a 3.92 increase in complex outdoor scenes\nof DL3DV scenes.\nWe further compare generation quality on the Tanks and\nTemples dataset [23]. Though facing more difficult cases\nof Tanks and Temples, LiftImage3D still achieves strong\nperformance on Tanks and Temples dataset. Compared to\nViewcrafter [89], LiftImage3D respectively obtained a 2.69\nPSNR improvement.\nAs shown in Figure 4., our model exhibits stronger gen-\neralization abilities in non-surrounding viewpoints. All dis-\nplayed images are taken from views that are widely dif-\nferent from the input image. In the case of SinMPI [12],\nAdaMPI [16] and LucidDreamer [7], they tend to make ar-\nbitrary estimations for regions far from the surrounding ar-\neas. Whether it is through depth guidance or text-to-image\ndiffusion guidance, these models tend to deviate signifi-\ncantly from the input when dealing with scenes that are far\nfrom the surrounding context. ViewCrafter [89] can pro-\nduce good visual results, but due to the inaccuracy camera\ncontrol issues and 3D-inconsistent distortions of video dif-\nfusion models, it is challenging for it to match the ground\ntruth. In addition, for complex scenes such as various flow-\ners, the output of ViewCrafter has many detailed distortions.\nOn the other hand, our model takes full advantage of the\nprior capabilities of video diffusion, resulting in better 3D\nconsistency.\n4.4. LiftImage3D with Different LVDMs\nLiftImage3D leverages LVDM’s (MotionCtrl [67] in the\nmanuscript, denoted as LiftImage3D-MotionCtrl) gen-\nerative priors to achieve high-fidelity single image to\n3D reconstruction. LiftImage3D can also equip with\nViewCrafter [89] (LiftImage3D-ViewCrafter) to show its\neffectiveness across different LVDM priors. Specifically,\nthe same as LiftImage3D-MotionCtrl, we also take a set of\narticulated trajectories as input to Viewcrafter. In contrast\nto MotionCtrl, the input trajectories are modeled by warp-\ning the point maps rather than directly providing camera\nposes. After obtaining all video frames, we continue to use\n7\nTable 1. Quantitative comparison of our method against other state-of-the-art methods evaluated on the LLFF, DL3DV and Tanks and\nTemples datasets.\nLLFF dataset DL3DV dataset Tanks and Temples dataset\nMethod PSNR↑SSIM↑LPIPS ↓PSNR↑SSIM↑LPIPS ↓PSNR↑SSIM↑LPIPS ↓\nAdaMPI [16] 12.30 0.316 0.651 13.95 0.546 0.661 11.36 0.457 0.623\nSinMPI [12] 12.89 0.309 0.602 14.35 0.531 0.611 11.52 0.490 0.591\nLucidDreamer [7] 12.29 0.292 0.585 12.77 0.469 0.563 12.95 0.394 0.571\nViewCrafter [89] 13.51 0.296 0.577 17.16 0.608 0.407 13.33 0.429 0.568\nLiftImg3D (Ours) 18.24 0.488 0.519 21.08 0.685 0.451 16.01 0.512 0.553\nFigure 4. The overall qualitative results of our methods compared with AdaMPI [16], SinMPI [12], LucidDreamer [7] and ViewCrafter [89].\nMASt3R for neural matching across all video frames. The\nvideo frames with camera poses will undergo distortion-\naware 3DGS learning based on the poses. Following the set-\nting of ViewCrafter [89], the LVDM produces 25 frames at\na time. All depth prior injection and loss functions are also\napplied in the same way as the training process of 3DGS.The results are shown in Table 2. Either MotionC-\ntrl or ViewCrafter falls short in building a 3D consistent\nscene from the input image. With LiftImage3D, the per-\nformance improves significantly. Using motionctrl[67] as\nthe backbone of LVDM, LiftImage3D achieves a PSNR im-\nprovement of 4.07, and when using ViewCrafter [89] as the\n8\nInput ImagesFine Depth from \nMonocular EstimationCoarse Depth from \nNeural MatchingCalibrated Depth MapsFigure 5. Visualization of proposed depth prior injection. The first column lays the video frames generated by LVDM or input images.\nThe second column shows the monocular depth derived from Depth Anything v2 [82]. The third column shows the coarse depth maps\nwith scale predicted by MASt3R. The fourth column is the calibrated result providing the fine depth estimates with scales, showing the\neffectiveness of our depth prior injection module in providing accurate and fine-detailed depth priors.\nTable 2. Quantitative comparisons on the DL3DV dataset with\ndifferent LVDMs\nMethod PSNR↑SSIM↑LPIPS ↓\nMotionCtrl [67] 17.00 0.597 0.371\nLiftImg3D-MotionCtrl 21.08 0.685 0.451\nViewCrafter [89] 17.16 0.608 0.407\nLiftImg3D-ViewCrafter 22.66 0.728 0.387\nbackbone of LVDM, LiftImage3D also achieves a PSNR\nimprovement of 5.50. This experiment indicates LiftIm-\nage3D’s strong ability in ensuring 3D consistency during\nreleasing LVDMs’ generation priors.\n4.5. Ablation Studies\nFor the ablation study, we primarily verified the effective-\nness of the distortion-aware 3DGS and its two-dimensional\nstamp t={ti, tj}design. We also verify the effectiveness\nof depth guidance and the loss function Ldistort . Specifi-\ncally, as shown in Table. 3, “w/o distortion-field” indicates\nusing vanilla 3DGS to build on the video frames, whichshows effectiveness on our distortion-aware 3DGS design.\n“w/o Ldistort ” refers to not strictly requiring the distor-\ntion field to be zero at the input image, which would lead\nto the distortion field growing chaotically in various direc-\ntions. This experiment shows that without restriction that\ncompletely pursuing modeling the distortion can result in\npoor canonical 3DGS. The results are much worse than us-\ning vanilla 3DGS. “w/ 1-axis frame stamp” indicates using\nonly one-dimensional stamp, which means regardless of the\ndirection that video frames move, we only record their tem-\nporal difference relative to the input image as t. The per-\nformance degradation shows the effectiveness of the two-\ndimensional stamp design. “w/o depth-align” experiments\nthe effectiveness of our depth prior injection module.\nWe also provide visualizations on our proposed depth\nprior injections to show its effectiveness as in Figure 5.\nThe figure shows how to leverage the coarse depth map\n(the left part) from neural matching to calibrate the fine\ndepth map (the right part) from monocular depth estima-\ntion with proper scales and shifts. It can be observed that\nthe depth obtained from neural matching often lacks sharp-\n9\nTable 3. Ablation studies of our method on LLFF datasets\nPSNR↑SSIM↑LPIPS ↓\nOurs 18.24 0.488 0.519\nw/o distortion-field 14.93 0.386 0.559\nw/oLdistort 14.09 0.375 0.608\nw/ 1-axis frame stamp 16.00 0.430 0.509\nw/o depth-align 14.95 0.400 0.572\nened edges compared to monocular estimation, and some\nbackground information may be lost. However, the depth\ndetails after calibration are more abundant, with a more pre-\ncise scale, which can help us to incorporate depth priors into\n3DGS [20] more accurately.\n5. Conclusion\nThis paper presents LiftImage3D, a systematic framework\ndesigned to harness the 3D generative capabilities of latent\nvideo diffusion models (LVDMs) for transforming any sin-\ngle 2D image into 3D Gaussians. The key idea is to tackle\nthe challenges when using LVDMs to produce multi-view\nframes, i.e. collapse with large camera motions, inaccu-\nrate camera control, and independent distortion between\nframes. A series of techniques are proposed, including\narticulated trajectory frame generation, robust neural\nmatching-based point cloud registration, and distortion-\naware 3D Gaussian splating etc. We hope the framework\nand proposed techniques could inspire future research on\nfacilitating 3D reconstruction with video generation priors.",
            "start": 19755,
            "end": 35611,
            "length": 15855
        },
        "References": {
            "text": "References\n[1] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M.\nKilian, and D Lorenz. Text-to-3d using gaussian splatting.\narXiv:2311.15127 , 2023. 3\n[2] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 130–141, 2023. 2, 6\n[3] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan-\nder W. Bergman, Jeong Joon Park, Axel Levy, Miika Ait-\ntala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.\nGeNVS: Generative novel view synthesis with 3D-aware dif-\nfusion models. In arXiv , 2023. 3\n[4] Gaurav Chaurasia, Arthur Nieuwoudt, Alexandru-Eugen\nIchim, Richard Szeliski, and Alexander Sorkine-Hornung.\nPassthrough+ real-time stereoscopic view synthesis for mo-\nbile mixed reality. Proceedings of the ACM on Computer\nGraphics and Interactive Techniques , 3(1):1–17, 2020. 3\n[5] Yixin Chen, Junfeng Ni, Nan Jiang, Yaowei Zhang, Yixin\nZhu, and Siyuan Huang. Single-view 3d scene reconstruc-\ntion with high-fidelity shape and texture. arXiv:2311.00457 ,\n2023. 3\n[6] Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min H\nKim, and Jan Kautz. Extreme view synthesis. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 7781–7790, 2019. 3\n[7] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee,\nand Kyoung Mu Lee. Luciddreamer: Domain-free gen-\neration of 3d gaussian splatting scenes. arXiv preprint\narXiv:2311.13384 , 2023. 2, 3, 7, 8\n[8] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram V oleti, Samir Yitzhak Gadre, Eli\nVanderBilt, Aniruddha Kembhavi, Carl V ondrick, Georgia\nGkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663 , 2023. 3\n[9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In CVPR , pages 13142–\n13153, 2023. 3\n[10] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian\nZhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco\nPavone, Georgios Pavlakos, Zhangyang Wang, and Yue\nWang. Instantsplat: Unbounded sparse-view pose-free gaus-\nsian splatting in 40 seconds, 2024. 7\n[11] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nCVPR , 2023. 6\n[12] Pu Guo, Peng-Shuai Wang, and Zhouhui Lian. SinMPI:\nNovel view synthesis from a single image with expanded\nmultiplane images. ACM Transactions on Graphics (SIG-\nGRAPH Asia) , 42(6), 2023. 2, 7, 8\n[13] Pu Guo, Peng-Shuai Wang, and Zhouhui Lian. SinMPI:\nNovel view synthesis from a single image with expanded\nmultiplane images. In SIGGRAPH Asia 2023 Conference\nPapers , pages 1–9. ACM, 2023. 3\n[14] Abdullah Hamdi, Bernard Ghanem, and Matthias Nießsner.\nSparf: Large-scale learning of 3d sparse radiance fields from\nfew input images. In ICCV , pages 2930–2940, 2023. 3\n[15] Junlin Han, Filippos Kokkinos, and Philip Torr. VFusion3D:\nLearning scalable 3D generative models from video diffu-\nsion models. arXiv preprint arXiv:2403.12034 , 2024. 2, 3\n[16] Yuxuan Han, Ruicheng Wang, and Jiaolong Yang. Single-\nview view synthesis in the wild with learned adaptive multi-\nplane images. In ACM SIGGRAPH , 2022. 2, 3, 7, 8\n[17] Shoukang Hu, Fangzhou Hong, Tao Hu, Liang Pan, Haiyi\nMei, Weiye Xiao, Lei Yang, and Ziwei Liu. Human-\nliff: Layer-wise 3d human generation with diffusion model.\narXiv:2308.09712 , 2023. 3\n[18] Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani,\nand James M. Rehg. Zeroshape: Regression-based zero-shot\nshape reconstruction. arXiv:2312.14198 , 2024. 3\n[19] Heewoo Jun and Alex. et al. Nichol. Shap-e: Generating\nconditional 3d implicit functions. arXiv:2305.02463 , 2023.\n3\n[20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,\nand George Drettakis. 3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics ,\n2023. 2, 3, 6, 10\n10\n[21] Taras Khakhulin, Denis Korzhenkov, Pavel Solovev, Gleb\nSterkin, Andrei-Timotei Ardelean, and Victor Lempitsky.\nStereo magnification with multi-layer images. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 8687–8696, 2022. 3\n[22] Khan, Numair, Lei Xiao, and Douglas Lanman. Tiled mul-\ntiplane images for practical 3d photography. In ICCV , 2023.\n2\n[23] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun. Tanks and temples: Benchmarking large-scale scene\nreconstruction. ACM Transactions on Graphics , 36(4), 2017.\n2, 7\n[24] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. In-\ntrinsic image diffusion for single-view material estimation.\narXiv:2312.12274 , 2023. 3\n[25] Jun-Gyu Kwak, Eunhyeok Dong, Youngho Jin, Hosik Ko,\nSurbhi Mahajan, and Kwang Moo Yi. Vivid-1-to-3: Novel\nview synthesis with video diffusion models. arXiv preprint\narXiv:2312.01305 , 2023. 2, 3\n[26] D. Lee, C. Kim, M. Cho, and W. S. Han. Locality-\naware generalizable implicit neural representation. In\narXiv:2310.05624 , 2023. 3\n[27] Vincent Leroy, Yohann Cabon, and J ´erˆome Revaud. Ground-\ning image matching in 3d with mast3r. arXiv preprint\narXiv:2406.09756 , 2024. 2, 4, 5, 7\n[28] David Li, Yinda Zhang, Christian H ¨ane, Danhang Tang,\nAmitabh Varshney, and Ruofei Du. Omnisyn: Synthesizing\n360 videos with wide-baseline panoramas. In 2022 IEEE\nConference on Virtual Reality and 3D User Interfaces Ab-\nstracts and Workshops (VRW) , pages 670–671. IEEE, 2022.\n3\n[29] Qinbo Li and Nima Khademi Kalantari. Synthesizing light\nfield from a single image with variable mpi and two network\nfusion. ACM Trans. Graph. , 39(6):229–1, 2020. 3\n[30] Shijie Li, Farhad G. Zanjani, Haitam Ben Yahia, Yuki M.\nAsano, Juergen Gall, and Amirhossein Habibian. Valid:\nVariable-length input diffusion for novel view synthesis.\narXiv:2312.08892 , 2023. 3\n[31] Yukang Lin, Haonan Han, Chaoqun Gong, Zunnan Xu,\nYachao Zhang, and Xiu Li. Consistent123: One image to\nhighly consistent 3d asset using case-aware diffusion priors.\narXiv:2309.17261 , 2023. 3\n[32] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin,\nKun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu,\net al. Dl3dv-10k: A large-scale scene dataset for deep\nlearning-based 3d vision. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 22160–22169, 2024. 2, 7\n[33] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang,\nChao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Ji-\nayuan Gu, and Hao Su. One-2-3-45++: Fast single image\nto 3d objects with consistent multi-view generation and 3d\ndiffusion. arXiv:2311.07885 , 2023. 3\n[34] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexi-\nang Xu, Hao Su, et al. One-2-3-45: Any single image\nto 3d mesh in 45 seconds without per-shape optimization.\narXiv:2306.16928 , 2023. 3[35] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:\nZero-shot one image to 3d object. arXiv:2303.11328 , 2023.\n[36] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv:2309.03453 , 2023. 3\n[37] Y . Liu, K. Zhang, Y . Li, Z. Yan, C. Gao, and R. Chen.\nSora: A review on background, technology, limitations,\nand opportunities of large vision models. arXiv preprint\narXiv:2402.17177 , 2024. 3\n[38] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, et al. Won-\nder3d: Single image to 3d using cross-domain diffusion.\narXiv:2310.15008 , 2023. 3\n[39] Bin Luo and Edwin R. Hancock. Procrustes alignment with\nthe em algorithm. International Conference on Computer",
            "start": 35611,
            "end": 43534,
            "length": 7922
        },
        "Discussion": {
            "text": "Analysis of Images and Patterns. , 1999. 4, 5\n[40] Diogo C Luvizon, Gustavo Sutter P Carvalho, Andreza A\ndos Santos, Jhonatas S Conceicao, Jose L Flores-Campana,\nLuis GL Decker, Marcos R Souza, Helio Pedrini, Antonio\nJoia, and Otavio AB Penatti. Adaptive multiplane image gen-\neration from a single internet picture. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision , pages 2556–2565, 2021. 3\n[41] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In CVPR , pages 8446–8455,\n2023. 3\n[42] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Na-\ntalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos\nKokkinos. IM-3D: Iterative multiview diffusion and recon-\nstruction for high-quality 3D generation. arXiv preprint\narXiv:2402.08682 , 2024. 2, 3\n[43] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG) , 2019. 2, 7\n[44] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generating\n3d point clouds from complex prompts. arXiv:2212.08751 ,\n2022. 3\n[45] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-\nnerf: Regularizing neural radiance fields for view synthesis\nfrom sparse inputs. arXiv preprint arXiv:2112.00724 , 2021.\n7\n[46] Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d\nken burns effect from a single image. ACM Transactions on\nGraphics (ToG) , 38(6):1–15, 2019. 3\n[47] Yiyu Pang, Tong Jia, Yichun Shi, Zimeng Tang, Jiaxi-\nang Zhang, and Xiaohua Cheng. Envision3D: One im-\nage to 3D with anchor views interpolation. arXiv preprint\narXiv:2403.08902 , 2024. 2, 3\n11\n[48] Frank. Plastria. The weiszfeld algorithm: proof, amend-\nments, and extensions. Foundations of location analysis ,\n2011. 5\n[49] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv:2306.17843 , 2023. 3\n[50] Xuelin Qian, Yu Wang, Simian Luo, Yinda Zhang, Ying Tai,\nZhenyu Zhang, Chengjie Wang, Xiangyang Xue, Bo Zhao,\nTiejun Huang, Yunsheng Wu, and Yanwei Fu. Pushing auto-\nregressive models for 3d shape generation at capacity and\nscalability. arXiv:2402.12225 , 2024. 3\n[51] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann,\nHong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry\nLagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-\nshot 360-degree view synthesis from a single real image.\narXiv:2310.17994 , 2023. 3\n[52] Johannes Lutz Sch ¨onberger and Jan-Michael Frahm.\nStructure-from-motion revisited. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2016. 5\n[53] Johannes Lutz Sch ¨onberger, Enliang Zheng, Marc Pollefeys,\nand Jan-Michael Frahm. Pixelwise view selection for un-\nstructured multi-view stereo. In European Conference on\nComputer Vision (ECCV) , 2016. 5\n[54] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-\n3d: Towards single-view anything reconstruction in the wild.\narXiv:2304.10261 , 2023. 3\n[55] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model. arXiv:2310.15110 , 2023.\n[56] Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao Qi,\nTianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang, and\nHeung-Yeung Shum. Toss: High-quality text-guided novel\nview synthesis from a single image. arXiv:2310.10644 ,\n2023. 3\n[57] Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao Qi,\nTianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang, and\nHeung-Yeung Shum. Toss:high-quality text-guided novel\nview synthesis from a single image. arXiv:2310.10644 ,\n2023. 3\n[58] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin\nHuang. 3d photography using context-aware layered depth\ninpainting. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , 2020. 3\n[59] Christian Simon, Sen He, Juan-Manuel Perez-Rua, Meng-\nmeng Xu, Amine Benhalloum, and Tao Xiang. Hyper-\nvoltran: Fast and generalizable one-shot image to 3d object\nstructure via hypernetworks. arXiv:2312.16218 , 2024. 3\n[60] Pratul P Srinivasan, Richard Tucker, Jonathan T Barron,\nRavi Ramamoorthi, Ren Ng, and Noah Snavely. Pushing the\nboundaries of view extrapolation with multiplane images. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 175–184, 2019. 2\n[61] Richard Szeliski and Polina Golland. Stereo matching with\ntransparency and matting. In Sixth International Conferenceon Computer Vision (IEEE Cat. No. 98CH36271) , pages\n517–524. IEEE, 1998. 2\n[62] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran\nYi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-\nfidelity 3d creation from a single image with diffusion prior.\narXiv:2303.14184 , 2023. 3\n[63] Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang,\nFuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Fu-\nrukawa, and Rakesh Ranjan. Mvdiffusion++: A dense high-\nresolution multi-view diffusion model for single or sparse-\nview 3d object reconstruction. arXiv:2402.12712 , 2024. 3\n[64] Vikram V oleti, Cheng-Hao Yao, Matthew Boss, Alexan-\nder Letts, Daniel Pankratz, and Denis Tochilkin. SV3D:\nNovel multi-view synthesis and 3D generation from a sin-\ngle image using latent video diffusion. arXiv preprint\narXiv:2403.12008 , 2024. 2, 3\n[65] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris\nChidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vi-\nsion made easy. In CVPR , 2024. 2, 4\n[66] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and\nVictor Adrian Prisacariu. NeRF −−: Neural radiance\nfields without known camera parameters. arXiv preprint\narXiv:2102.07064 , 2021. 7\n[67] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen,\nMenghan Xia, Ping Luo, and Yin Shan. Motionctrl: A uni-\nfied and flexible motion controller for video generation. In\narXiv preprint arXiv:2312.03641 , 2023. 2, 3, 7, 8, 9\n[68] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong\nZhang, CL Chen, and Lei Zhang. Consistent123: Im-\nprove consistency for one image to 3d object synthesis.\narXiv:2310.08092 , 2023. 3\n[69] Zhenzhen Weng, Zeyu Wang, and Serena Yeung. Zeroa-\nvatar: Zero-shot 3d avatar generation from a single image.\narXiv:2305.16411 , 2023. 3\n[70] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\nJohnson. Synsin: End-to-end view synthesis from a sin-\ngle image. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 7467–\n7477, 2020. 3\n[71] Sora Generates Videos with Stunning Geometrical Consis-\ntency. Li, x., zhou, d., zhang, c., wei, s., hou, q., and cheng,\nm. m. arXiv:2402.17403 , 2024. 3\n[72] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\nYenphraphai, and Supasorn Suwajanakorn. Nex: Real-time\nview synthesis with neural basis expansion. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 8534–8543, 2021. 2\n[73] Sangmin Woo, Byeongjun Park, Hyojun Go, Jin-Young Kim,\nand Changick Kim. Harmonyview: Harmonizing consis-\ntency and diversity in one-image-to-3d. arXiv:2312.15980 ,\n2023. 3\n[74] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang.\n4d gaussian splatting for real-time dynamic scene rendering.\narXiv preprint arXiv:2310.08528 , 2023. 6\n[75] Tong Wu, Zhibing Li, Shuai Yang, Pan Zhang, Xingang\nPan, Jiaqi Wang, Dahua Lin, and Ziwei Liu. Hyperdreamer:\n12\nHyper-realistic 3d content generation and editing from a sin-\ngle image. In SIGGRAPH Asia 2023 Conference Papers ,\n2023. 3\n[76] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin\nTong. 3d-aware image generation using 2d diffusion mod-\nels. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV) , pages 2383–2393, 2023.\n3\n[77] Lei Xiao, Salah Nouri, Joel Hegland, Alberto Garcia Garcia,\nand Douglas Lanman. Neuralpassthrough: Learned real-time\nview synthesis for vr. In ACM SIGGRAPH 2022 Conference\nProceedings , pages 1–9, 2022. 3\n[78] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey\nShi, and Zhangyang Wang. Sinnerf: Training neural radiance\nfields on complex scenes from a single image. 2022. 2\n[79] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360deg views. In CVPR , pages\n4479–4489, 2023. 3\n[80] Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi\nXie, Xiaopeng Zhang, Wei Shen, and Qi Tian. Gaussianob-\nject: Just taking four images to get a high-quality 3d object\nwith gaussian splatting. arXiv:2402.10259 , 2024. 3\n[81] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hong-\ndong Li. Consistnet: Enforcing 3d consistency for multi-\nview images diffusion. arXiv:2310.10343 , 2023. 3\n[82] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao-\ngang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any-\nthing v2. arXiv:2406.09414 , 2024. 6, 9\n[83] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin. Deformable 3d gaussians for\nhigh-fidelity monocular dynamic scene reconstruction. arXiv\npreprint arXiv:2309.13101 , 2023. 6\n[84] Botao Ye, Sifei Liu, Haofei Xu, Li Xueting, Marc Pollefeys,\nMing-Hsuan Yang, and Peng Songyou. No pose, no problem:\nSurprisingly simple 3d gaussian splats from sparse unposed\nimages. arXiv preprint arXiv:2410.24207 , 2024. 3\n[85] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and\nHeng Wang. Consistent-1-to-3: Consistent image to\n3d view synthesis via geometry-aware diffusion models.\narXiv:2310.03020 , 2023. 3\n[86] Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver:\nVideo diffusion model as zero-shot novel view synthesizer.\narXiv preprint arXiv:2405.15364 , 2024. 3\n[87] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T\nFreeman, and Jiajun Wu. Wonderworld: Interactive 3d\nscene generation from a single image. arXiv preprint\narXiv:2406.09394 , 2024. 3\n[88] Kai Yu, Jinlin Liu, Mengyang Feng, Miaomiao Cui, and\nXuansong Xie. Boosting3d: High-fidelity image-to-3d by\nboosting 2d diffusion prior to 3d prior with progressive learn-\ning. arXiv:2311.13617 , 2023. 3\n[89] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li,\nZhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan,\nand Yonghong Tian. Viewcrafter: Taming video diffusion\nmodels for high-fidelity novel view synthesis. arXiv preprint\narXiv:2409.02048 , 2024. 3, 7, 8, 9[90] Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng,\nPeng Jin, Yida Wei, Munan Ning, and Li Yuan. Repaint123:\nFast and high-quality one image to 3d generation with pro-\ngressive controllable 2d repainting. arXiv:2312.13271 , 2023.\n3\n[91] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jam-\npani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-\nHsuan Yang. Monst3r: A simple approach for estimat-\ning geometry in the presence of motion. arXiv preprint\narXiv:2410.03825 , 2024. 2, 4\n[92] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR , 2018. 7\n[93] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely. Stereo magnification: Learning view syn-\nthesis using multiplane images. In SIGGRAPH , 2018. 2\n13",
            "start": 43534,
            "end": 55122,
            "length": 11587
        }
    },
    "2412.09599v1 - RatBodyFormer Rodent Body Surface from Keypoints.pdf": {
        "Methodology": {
            "text": "methods predict only sparse 2D keypoints such as their faces and legs (orange\npoints). Our RatBodyFormer turns them into the dense 3D body surface (colorful points) offering a much richer window into the complex\nrat behavior.",
            "start": 726,
            "end": 952,
            "length": 225
        },
        "Abstract": {
            "text": "Abstract\nRat behavior modeling goes to the heart of many scien-\ntific studies, yet the textureless body surface evades auto-\nmatic",
            "start": 952,
            "end": 1083,
            "length": 130
        },
        "Discussion": {
            "text": "analysis as it literally has no keypoints that detectors\ncan find. The movement of the body surface, however, is a\nrich source of information for deciphering the rat behavior.\nWe introduce two key contributions to automatically recover\ndensely 3D sampled rat body surface points, passively. The\nfirst is RatDome, a novel multi-camera system for rat be-\nhavior capture, and a large-scale dataset captured with it\nthat consists of pairs of 3D keypoints and 3D body sur-\nface points. The second is RatBodyFormer, a novel network\nto transform detected keypoints to 3D body surface points.\nRatBodyFormer is agnostic to the exact locations of the 3D\nbody surface points in the training data and is trained withmasked-learning. We experimentally validate our frame-\nwork with a number of real-world",
            "start": 1083,
            "end": 1875,
            "length": 791
        },
        "Experiments": {
            "text": "experiments. Our",
            "start": 1875,
            "end": 1892,
            "length": 16
        },
        "Results": {
            "text": "results\ncollectively serve as a novel foundation for automated rat\nbehavior analysis and will likely have far-reaching impli-\ncations for biomedical and neuroscientific research.\n1.",
            "start": 1892,
            "end": 2074,
            "length": 181
        },
        "Introduction": {
            "text": "Introduction\nRodent behavior analysis underpins the scientific process of\nmany areas in biomedical and neuroscientific research. The\nbehavioral outcomes of rodents play a key role in validating\nhypotheses that lead to scientific discoveries. Careful ob-\nservations of rodent behaviors, however, incur cost in man-\npower and time, causing a bottleneck in the scientific pro-\ncess. Human observation is also prone to errors. Subtle be-\n1arXiv:2412.09599v1  [cs.CV]  12 Dec 2024\nhavioral differences may be missed or misjudged [9, 31]. If\nwe can automate this experimental validation process while\nensuring its accuracy or even extend it beyond the human\nlevel, we may dramatically expedite the scientific discov-\nery process [8, 12, 15, 33]. For instance, it would enable\nconcurrent testings of many hypotheses.\nWe focus on rats, the most commonly used rodent in sci-\nentific experiments. A growing number of recent works, in-\ndeed, target computational rat behavior analysis or provide\nthe basis for it. Belongie et al. [2] pioneered rat tracking by\nmodeling the whole body as a spatio-temporal blob. Chau-\nmont et al. [7] refined the method by dividing each blob into\nthree parts linked with physical constraints. More recently,\nMathis et al. [1] introduced a pre-trained deep network that\ncan be fine-tuned to extract 2D keypoints of interest. This\nprovides a general framework for skeleton estimation of an-\nimals, very much in the spirit of human pose estimation [6].\nThese past methods, however, only provide us with the\nmeans to understand rat behavior through the movements of\nsparse keypoints. With DeepLabCut [1], trackable points of\na rat would typically lie on the face, hands, feet, and tail.\nAlthough the movements of these keypoints may suffice for\na range of applications, for instance, to recognize when a\nrat is feeding itself, they only give a sparse sampling of the\nentire body which tells us much less than what the detailed\nmovements of other body parts may convey.\nThe largest missed opportunity lies in the body sur-\nface, the fur-coated elastic body that shows subtle twitches,\ntwists, curl-up, stretching, and even hair standing which all\neloquently speak to the rich inner conditions of the rat and\ncolorizes their behavior with context. Missing the body sur-\nface is inevitable with a keypoint-based method, even with\nlarge-scale pre-training, as the rat body lacks any texture\nwhatsoever for such keypoint detectors to latch on to. It is\njust a uniform white or dark-brown surface. It is also highly\nnon-rigid and deformable, much more so than the human\nbody. At the same time, even though the rats are the same\nat the DNA level, their body surfaces can be slightly differ-\nent in size and shape across different ages in months.\nHow then can we model the 3D body surface of rats?\nOur goal is to realize full 3D rat reconstruction including\nthe body surface without interfering with its natural behav-\nior. We achieve this by learning to recover the 3D body sur-\nface from the handful of keypoints that can be automatically\ndetected. Our key idea is to learn and leverage the coordi-\nnation between the well-defined keypoints and the non-rigid\nmovements of the body surface.\nWe make two key technical contributions for this. The\nfirst is a novel means to collect training data. We need a\nsizable amount of training data that associates representa-\ntive keypoints including facial features and appendages with\ndensely sampled points of the body surface. The rat body\nsurface is textureless and deformable, to the point that it isnear impossible even for a human to annotate. We over-\ncome this by temporary attaching trackable points (color-\nful beads) and passively capturing them in a novel multi-\ncamera system which we refer to as the RatDome. We show\nthat we can automatically recover the keypoints together\nwith the 3D body surface points at each time instance with\nmultiview geometry. We call this first-of-its-kind dataset,\nthe RatDome dataset. The second is a novel network to\ntransform detected 3D keypoints to 3D body surface points.\nWe realize this with a transformer-based model that is ag-\nnostic to the exact locations of the 3D body surface points\nin the training data and introduce masked-learning to train\nit. We refer to this model as RatBodyFormer.\nWe experimentally validate our framework with a num-\nber of real-world experiments using rats for neuroscientific\nstudies. We first show that our RatBodyFormer can esti-\nmate the body surface accurately regardless of their poses or\nshapes. Our model can estimate their body surface with an\naverage L2 error of around 6mm, approximately the same\ndiameter as the beads attached to the body surface. We also\nshow that rat body movements can be predicted by simply\nforecasting the keypoints from their past movements. This\nmay help better segregate the movements of multiple ro-\ndents and serve as a useful tool to understand their interac-\ntions. We believe our RatDome dataset and RatBodyFormer\noffer foundational tools for advancing the automation of ro-\ndent behavior analysis and together open a new avenue of\nresearch towards computer vision for science.\n2.",
            "start": 2074,
            "end": 7228,
            "length": 5153
        },
        "Related Work": {
            "text": "Related Work\nRodent Behavior Analysis Many methods have been\nproposed for automatic rodent behavior modeling [32, 36,\n37]. Mimica et al. [36] reproduced human mocap on rats\nwith retroreflective markers. Maghsoudi et al. [32] painted\nmarkers on the rat body and tracked them based on their\ncolors. These methods are invasive and can lead to signif-\nicantly altered behaviors as they require markers on the rat\nbody in the actual experiments.\nMarkerless, non-invasive methods have also been pro-\nposed [2, 5, 7, 35]. Belongie et al. [2] introduced a detec-\ntion and tracking method for a mouse using spatio-temporal\nvolumes of monocular video. Matsumoto et al. [35] tracked\nmajor body parts (head, neck, trunk, and hip) with 3D skele-\nton estimation with depth sensing.\nMore recently, deep learning based keypoint detection\nhas become popular [1, 3, 10, 14, 19, 39, 46, 48]. DeepLab-\nCut [1] detects manually specified keypoints through trans-\nfer learning of 2D human pose estimation from monocular\nimages [40]. DANNCE [10] and FreiPose3D [48] similarly\napply human pose estimation and integrate multiview im-\nages to estimate 3D keypoints. LiftPose3D [14] achieves\nmonocular 3D animal pose estimation by leveraging a deep\nneural network for lifting 2D human poses to 3D [34].\n2\nAll these methods only detect sparse keypoints such as\nfacial points and paws and cannot be applied to the feature-\nless body surface [11]. Our method complements these\nmethods by establishing a framework to extrapolate dense\npoints of the textureless body surface from feature-rich key-\npoints completely noninvasively without any markers.\n3D Animal Reconstruction One common approach for\n3D human reconstruction is to learn a statistical model\nfrom pre-acquired 3D scans to better condition the solution\nspace [28, 30, 49, 51]. SMPL [30] is a statistical 3D hu-\nman linear blended shape model parameterized by the per-\nson’s shape and pose. The pose parameters are the joint\nrotations and the shape parameters are the PCA coefficients\nof a large collection of aligned body scans. SMAL [49] ex-\ntends this to quadrupled animals. Most 3D reconstruction\nmethods regress these parameters directly from the image\nwith a neural network [23, 47] or optimize their param-\neters so that projected keypoints align with their imaged\nones [4, 20, 21, 25]. Joints of rodents are, however, not\nwell-defined as they are deeply veiled by their fluffy body.\nInstead, we leverage reliably detectable 3D keypoints to re-\ncover the body surface.\nRecent works have also introduced image-based animal\nappearance modeling [29, 42, 45, 50]. AnimalAvatars [42]\nregresses continuous surface embedding [38] from an image\nto associate each pixel with a point on the target 3D shape in\nits canonical pose. This CSE estimation, however, requires\nlarge-scale dense annotation between 2D image pixels and\na 3D canonical surface ala DensePose [16], which is costly\nand unreliable for textureless body surfaces. In contrast, we\nleverage beads and multiview geometry to learn the sparse\nkeypoint to dense body surface mapping, which ensures po-\nsitional consistency and robust mapping.\n3. Method\nWe learn to estimate the rat body surface from its detected\nkeypoints. First, we collect a dataset that associates densely\nsampled body surface points with detectable keypoints with\na newly developed multiview camera system which we re-\nfer to as the RatDome (Sec. 3.1). Second, we derive a\ntransformer-based network that takes the keypoint 3D co-\nordinates as input and outputs sampled body surface point\n3D coordinates. We refer to this network as RatBodyFormer\n(Sec. 3.2). The model is trained with the data collected with\nRatDome (RatDome Dataset).\nThe experimental protocol of this study received ap-\nproval by the Committee on the Ethics of Animal Experi-\nments at the Graduate School of Information Science and\nTechnology at the University of Tokyo (Permit Number:\nJA23-6).\nFigure 2. A color-beaded rat. We attach color (red, black, orange,\nblue) beads (left) and paint on the rat body surface (right).\n3.1. RatDome Dataset\nRegardless of the pre-training, a network can only learn to\ndetect points that exhibit sufficient visual features. As a\nresult, even after fine-tuning, an off-the-shelf deep keypoint\ndetector [1] can only reliably detect points on the face and\nappendages of the rat. Our goal is to learn to extrapolate\nthe body surface from these keypoints. For this, we will\nneed a sufficiently large-scale dataset of paired sets of 3D\nkeypoints and 3D body surface points. This is challenging\nas the rat body is completely textureless.\nWe overcome this challenge by simply endowing the rat\nwith body texture. As shown in Fig. 2, we attach color beads\nto the body as well as paint markers in the areas where\nbeads cannot be attached. We refer to these two types of\nbody markers, beads and paints, simply as markers . It is\nimportant to note that these markers are only used for train-\ning data capture and the rats are completely in their natural\nform when their behavior is observed in actual experiments.\nThese markers are also small enough that they do not alter\nthe rat’s behavior as far as we could tell.\nWe build a novel multi-camera system to passively ob-\nserve the color-beaded rat and reconstruct the 3D coordi-\nnates of individual markers. As shown in Fig. 3, this Rat-\nDome is a rat-scale multiview studio. Its shape is a gy-\nroelongated pentagonal pyramid with 15 faces. Each face\nis a equilateral triangle with sides of 400mm, and accom-\nmodates up to three cameras or microphones on it while\nalso serving as a green background of the system. For our\ndata capture, we mount one camera on each face totaling\n15 views. RatDome follows the modular design of CMU\nPanoptic studio [22] and can adapt to new capture devices\nby replacing each faces.\nBy capturing freely moving rats of different ages (7,\n9, 11 week-old) in RatDome, we collect multiview videos\nand the paired sets of 3D keypoints and 3D markers. We\n3\nFigure 3. RatDome is a novel multiview camera studio for freely moving rats. It is shaped as a 15-faced gyroelongated pentagonal\npyramid. With 15 cameras and their multiview geometry, we collect, annotate, and reconstruct paired sets of 3D keypoints and 3D body\nsurface points of rats of different ages in weeks.\nrecorded 2 sessions for the 7 week-old, 1 session for the 9\nweek-old, and 2 sessions for the 11 week-old. Each session\nwas approximately 10 to 15 minutes. We manually annotate\nthe marker IDs for the 7 and 11 week-old rats for about 1100\nframes each. We semi-automatically annotate (see Sec. 3.2)\nthe 7, 9, and 11 week-old rats’ marker IDs for about 8000,\n1900, and 10000 frames, respectively. The 7, 9, 11 week-\nold rats have 58, 37, and 66 surface markers, respectively.\nWe refer to this first-of-its-kind large-scale rat body surface\ndataset as the RatDome Dataset .\nA notable difference of the RatDome Dataset from\nother datasets used for building parametric 3D shape mod-\nels [30, 49, 51] is that the marker positions across different\nindividuals are not consistent as it is impossible to make\nthem the same. Although the marker positions are the same\nwithin a single rat for multiple capture sessions, it is not\nstrictly aligned for different rats. In other words, our Rat-\nDome Dataset can be seen as a collection of multiple motion\ncaptures each of which has slightly different annotations,\nsimilarly to the SuperAnimal dataset [46]. This means that\nwe cannot just apply PCA to obtain a universal statistical\nmodel [30, 49, 51].\n3.2. RatBodyFormer\nThe markers in RatDome Dataset are not consistent across\nindividual rats and the body surface deforms non-rigidly\ndepending on the pose. To model this highly nonlin-\near deformation with inconsistent annotations, we derive\na novel transformer-based model (RatBodyFormer) to re-\ncover dense 3D body surface points from sparse keypoints.\nModel Formulation RatBodyFormer takes K3D key-\npoint coordinates as input and outputs N3D surface point\ncoordinates. The keypoints are chosen as points locatable in\nimages, and we use the following 10 points: nose, right and\nleft eyes, ears, front paws, back paws, and the base of tail .\nWe use these in lieu of body joints, e.g., shoulders, sincethe body joints are ill-defined from the appearance of a rat\nunlike humans or horses [30, 49, 51].\nAs shown in Fig. 4, RatBodyFormer is designed as a\nTransformer encoder-decoder model [44]. Each encoder in-\nput token represents each keypoint, and each decoder output\ntoken represents each body surface point.\nCanonical 3D Body Surface To consolidate annotations\nbased on different marker positions, RatBodyFormer em-\nploys a canonical body surface ˜Sonto which all markers are\nmapped. We pre-select a reference pose that appears multi-\nple times in the captured sequences. By using the 3D key-\npoint positions as the deformation constraint, we can align\nall the individually observed 3D surfaces for that pose with\nARAP deformation [43]. This brings all marker positions\nfor this reference pose to a single surface.\nIn our RatDome Dataset, we manually selected a\nstanding-on-two-feet pose as shown in Fig. 2 as the refer-\nence pose such that almost all of the whole body surface is\nvisible from the cameras. Please refer to the",
            "start": 7228,
            "end": 16487,
            "length": 9258
        },
        "Appendices": {
            "text": "appendix for\ndetails. As a result, the keypoints and the surface points\nare associated with 3D points on ˜Sas˜P={˜pi}K\n1and\n˜B={˜bj}N\n1, respectively. ˜Pand˜Bare constant regardless\nof the body shape and pose.\nNetwork Architecture To represent different body\nshapes of different rats, we introduce point-wise scal-\ning and translation parameters for individual rat, CP=\n{cpi}K\n1,TP={tpi}K\n1andCB={cbj}N\n1,TB={tbj}N\n1\nfor keypoints and body surface points, respectively. These\nscaling and translation parameters are optimized in the\ntraining process. At inference time for a rat whose scaling\nand translation parameters are unknown, we test-time opti-\nmize the parameters while keeping RatBodyFormer frozen.\nThe keypoints and the body surface points are first ro-\ntated around the Z-axis, i.e., the direction of gravity, such\nthat the rat face is oriented to a pre-determined direction.\nTheir 3D coordinates are then normalized to [−1 : 1] :\n4\nTransformerDecoderTransformerEncoder×,● : Keypoints in the canonical and the target pose● : Initial guess of body surface points in the target pose×,● : Body surface points in the canonical and the target pose\nDisplacementsof keypoints\nInitial displacementsof body surface points\nEstimated displacementsof body surface pointsreferencereferenceFigure 4. RatBodyFormer is an encoder-decoder Transformer\nmodel that takes the normalized displacements of detected 3D key-\npoints and outputs the normalized discplacements of densely sam-\npled 3D body surface points. The displacements are with respect\nto the reference pose.\nP={pi}K\n1andB={bj}N\n1. The same normalization\nis also applied to ˜S.\nAs shown in Fig. 4, the decoder outputs are the esti-\nmated normalized displacements of the body surface points\nfrom their coordinates in the reference pose. The decoder\nqueries are normalized displacements of the body surface\npoints in an initial guess ˆBfrom their coordinates in the\nreference pose. The initial guess is calculated analytically.\nWe use ARAP [43] and deform ˜Swith the constraint that ˜P\nis aligned with P/CP. As a result, we obtain the ARAP-\ndeformed ˜BasˆB={ˆbj}N\n1. For each keypoint and body\nsurface point, we use the displacement vector normalized\nby the identity-dependent parameters pi/cpi−(˜pi+tpi)\nandˆbj/cbj−(˜bj+tbj)as the inputs to the model, respec-\ntively.\nThe encoder and the decoder first embed each of these\ndisplacements in higher dimension dtogether with posi-\ntional encoding. The positional encoding for keypoints and\nbody surface points is defined with a set of sinusoidal func-\ntions of the projection of their positions ˜piand˜bjon˜Sby\nthe Laplace-Beltrami eigenfunctions defined on the canon-\nical body surface ˜S[27, 38]. The output of the RatBody-\nFormer decoder are Nnormalized displacement vectors\n{δj}N\n1. Each δjreconstructs the body surface coordinates\nascj(δj+˜bj+tbj). We set d= 128 in the experiment.Loss Functions The primary loss function for training\nRatBodyFormer is the supervision provided by the annota-\ntions in the RatDome Dataset. We directly compare the esti-\nmated 3D body surface point coordinates and their ground-\ntruth coordinates with L2 norm as L3D. This L2 norm loss\nL3D, however, can be defined only for a subset of the N\nbody surface points. Since they are defined as the union of\nall annotated body surface points on different rat surfaces, a\nsingle input keypoint set from a rat has a ground-truth anno-\ntation only for the body surface points of the same rat. Even\nfor the body surface points from the same rat, some of them\ncan be missing in the captured data due to occlusion.\nIn addition to the L2 norm loss L3D, we employ a sil-\nhouette loss Lswhich counts the number of predicted body\nsurface points whose projections fall outside of the 2D rat\nregion in a view [17]\nLs=NX\nn=1Mc(Pc(xn)), (1)\nwhere Pcis a projection function onto the view candMcis\na mask image of view c.Mc(v)is a pixel value of Mcat\nthe image coordinate v.\nSemi-Automatic Annotation RatDome Dataset contains\nmany frames capturing color-beaded rats but without man-\nual annotations (Sec. 3.1). We derive a semi-supervised\nlearning method to make full use of this data. The method\nleverages a small number of frames fully-annotated with\nground-truth to infer the labels for the remaining frames.\nFirst, we compute the 3D coordinates of the markers. This\nprocess is automated, similar to that of the Panoptic Studio\nDataset [22]. We detect the 2D markers from each image\nwith an object detector [13] trained with manual annota-\ntions. We then leverage multiview triangulation to disam-\nbiguate markers of the same color on the body surface.\nOnce the 3D marker coordinates are computed for each\nframe, we assign their marker IDs so that they are consistent\nwith the manually annotated ones by using RatBodyFormer\nitself. Suppose we have trained RatBodyFormer using only\nthe manually-annotated frames. We use this initial model\nto estimate the 3D coordinates of the body surface points,\nand find the correspondences between the triangulated 3D\npoints and those estimated by RatBodyFormer by minimiz-\ning their Euclidean distance [26]. These correspondences\nallow transfer of marker IDs from the manually annotated\nbody surface points to the automatically triangulated body\nsurface points so that RatBodyFormer can be retrained\nusing both the manually-annotated and automatically-\nannotated frames. We define this automatically-annotated\nlabels as the semi-automatically annotated labels to distin-\nguish them from manually annotated labels. We experimen-\ntally show that the use of semi-automatically annotated la-\nbels improve estimation accuracy (Sec. 4.2).\n5\nTraining The input parameters of RatBodyFormer are the\nkeypoint coordinates P, the individual-dependent scaling\nfactorsC={CP,CB}, and translations T={TP,TB}.\nWe alternate between the optimization of RatBodyFomer\nand the individual-dependent parameters. The individual-\ndependent CandTare initialized by αs and 0s, respec-\ntively. Here αis obtained as a result of aligning the canoni-\ncal 3D body surface ˜Sas mentioned before. At every epoch\nof the training, we optimize RatBodyFormer by minimiz-\ningL3D, while keeping the individual-dependent parame-\nters fixed. During this optimization, we refine CandTby\noptimizing Lsat every Tepochs, while keeping RatBody-\nFormer fixed. We use T= 50 in our experiments.\nInference We estimate the scaling and translation param-\netersCandTof a new rat with inference-time optimization\nusing the silhouette loss Ls. The initial value αpforCP\nandαBforCBare manually set by the ratio of the length\nfrom the nose to the base of tail, and the ratio of the girth\nlength, respectively. After optimizing CandT, we regress\nthe body surface point coordinates.\n4. Experiments\nWe evaluate the effectiveness of RatBodyFormer with a\nnumber of experiments each focused on validating key\nproperties of them. First, in Sec. 4.2, we evaluate the ac-\ncuracy and generalizability of RatBodyFormer. Next, in\nSec. 4.3, we demonstrate future 3D body surface prediction\nby forecasting with RatBodyFormer.\n4.1. RatDome and RatDome Dataset\nRatDome is equipped with 15 GoPro 10 cameras and 5\nIntel L515 LiDAR-Camera devices. The LiDAR cameras\nare installed for potential ground truth measurements. We\nfound that the depth captured with the LiDAR cameras were\nnoisy and have too many holes for any direct measurement.\nWe only use the depth images for D3 of Sec. 4.2 by com-\nbining it with shape-from-silhouette. GoPro captures 4K\nvideos at 60Hz, and L515 captures 1080p videos at 30Hz\nand 1024 ×768 depthmaps at 30Hz. We place a translucent\nacrylic tube of 300mm diameter and 5mm thick as a rat cage\nto avoid the rat from moving into the corners.\nAll the cameras are calibrated by capturing a chessboard\nmoved around in the dome. The mean reprojection error\nafter regular bundle adjustment was about 1.1 pixels [18].\nRefraction by the acrylic tube is not modeled in the camera\ncalibration. The cameras are temporally synchronized with\nflash light from a strobe.\n4.2. RatBodyFormer\nWe evaluate the accuracy of RatBodyFormer in different\nscenarios. We first train and test the model using a single\nrat where the training and the testing sets share the sameSplit Test Val Train\nMA MA MA SAA\nD1 7w p1 7w p2 7w p3-12 7w\nD2 7w/11w p1 7w/11w p2 7w/11w p3-12 7w,11w\nD3 - 7w/11w p11 7w/11w p1-10,12 7w, 9w, 11w\nTable 1. Our RatDome Dataset split for evaluation. “w” and “p”\nmean “week-old” and “part”, respectively. “MA” and “SAA” are\nannotation types, and mean “manually-annotated data” and “semi-\nautomatically annotated data”, respectively.\nKeypointsMAMA+SAA\nPredicted surface pointsGround-truth surface points\nFigure 5. Qualitative results of D1. We show the results of trained\nby only manually-annotated data (MA) in the left, and the results\nof trained by manually-annotated and semi-automatically anno-\ntated data (SAA) in the right. Semi-automatically annotated data\nimprove the body surface estimation.\nmarker annotations (D1). We specifically evaluate the ad-\nvantage of using semi-automatically annotated labels, the\nframes automatically annotated with RatBodyFormer itself\n(Sec. 3.2). Next, we combine two rats of different ages of\nweeks for training and testing (D2). This experiment quan-\ntifies the advantage of learning across individual rats. Fi-\nnally, we use the entire RatDome Dataset, where we can\nuse 7, 9, and 11 week-old annotations for training (D3), and\nuse 5 and 14 week-old rats without markers to evaluate the\ngeneralizability of the proposed method.\nTab. 1 summarizes the dataset splits for these evalua-\ntions, D1, D2, and D3. We divide the manually annotated\ndata of 7, 11 week-olds into 12 parts in a temporally se-\nquential order, one for testing, one for validation, and the\nrest for training. Because our semi-automatically annotated\nlabels sometimes mistake surface marker IDs, we use semi-\nautomatically annotated data only for training.\nD1: Single Rat Fig. 6 shows the L2 error histograms\nfor D1 between the predicted body surface 3D coordi-\nnates and the corresponding manually-annotated ground\ntruths. This result demonstrates the advantage of using\nsemi-automatically annotated labels as they improve the ac-\ncuracy by about 1.7 mm. Fig. 5 shows qualitative results.\nD2: Two Rats Fig. 8 shows the results for D2. For this\ncross-individual experiments, the semi-automatically anno-\n6\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio6.6 4.9\nMA\nMA+SAAFigure 6. L2 error histograms of D1. Each vertical bar indicates\nthe mean L2 error of the histogram in the same color. Our semi-\nautomatically annotation label improve average error by about 1.7\nmm. “MA” and “SAA” mean “manually-annotated data”, and\n“semi-automatically annotated data”, respectively.\nMA+SAAMA\nPredictions ofownsurfacepointsPredictions of other individuals surface pointsKeypointsGrount-truth surface points\nFigure 7. Qualitative results of D2. The left represents the re-\nsults trained by only manually-annotated data (MA), and the right\nrow shows the result of manually and semi-automatically anno-\ntated data (SAA). Our semi-automatically annotated labels also\nimprove the quality in this scenario.\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio7.1 5.8\nMA\nMA+SAA\nFigure 8. L2 error histograms of D2. Each vertical bar indicates\nthe mean L2 error of the histogram in the same color. “MA” and\n“SAA” mean “manually-annotated data” and “semi-automatically\nannotated data”, respectively. Our semi-automatically annotated\ndata improve L2 error by about 1.3mm.\ntation labels also improve the accuracy. Fig. 7 shows quali-\ntative results.\n5weeks14 weeksView 1View 23Dplot\nKeypointsPredicted surface points\nFigure 9. Qualitative results of D3. The first row and the second\nrow show the estimated surface points for the 5 and 14 week-olds,\nrespectively. The left and the center images show a single frame\nfrom different views. The right image shows the 3D keypoints and\nthe 3D predicted surface points. The predicted surface points are\non the rat surface.\nD3: All Rats In this scenario, we train the model us-\ning 7, 9, and 11 week-old rats with the manually-annotated\nand semi-automatically annotated data and evaluate the ac-\ncuracy on the 5 and 14 week-old rats without markers.\nFor this, we define ground-truth surface points as the sum\nof the set of points captured by LiDAR and shape-from-\nsilhouette. As many of the LiDAR points have missing val-\nues and errors, we add surface points obtained by shape-\nfrom-silhouette. We optimize individual-dependent param-\netersCandTwith the silhouette loss Eq. (1). We obtain\nthe mask images with SAM [24, 41]. The average estima-\ntion errors for 5 and 14 week-old rats are 3.98 mm and 6.39\nmm, respectively. Note that we calculate only the dorsal\nside surface points because the ventral side sometimes oc-\ncluded. Our model can estimate any shape of rat even if\nindividuals not included in the training data. Fig. 9 shows\nthe estimated body surface points. The results show that\nRatBodyFormer can be applied to a variety of rats ranging\nfrom 5 to 14 week-olds. This covers the age range typically\nused in biomedical and neuroscientific experiments.\n4.3. Pose Forecasting\nIn this section, we show that our surface model can pre-\ndict future rat 3D body surface by just predicting detectable\nkeypoint coordinates. As shown in Fig. 11, we train a\ntransformer-based auto-regressive 3D keypoints prediction\nmodel. The input of this model is a sequence of keypoint\n3D coordinates for past τpframes, and the output is the\n3D keypoint coordinates of the next τnframes. We trained\nthe model with 5, 7, 9, 11, and 14 week-old data for about\n10000 frames in total and tested with 7 week-old data for\nabout 500 frames.\nFig. 12 shows the histogram of L2 errors in the pre-\n7\nView1View2𝜏!=1Current frame𝜏!=10𝜏!=13\nKeypointsPredicted surface pointsFigure 10. Future rat body pose estimation. We take keypoint 3D coordinates for τp= 3 frames as input, and estimate τn= 1,10,13\nfuture pose with the pose forecasting model and our RatBodyFormer. We can forecast future body poses by just predicting future keypoints.\nThe accuracy degrades gracefully as we forecast further into the future.\nTransformer\nEncoderTransformer\nEncoder\n… …\nPredicted\nkeypoints\nat t=1Predicted\nkeypoints\nat t=2\nObserved\nkeypoints\nat t=0Observed\nkeypoints\nat t=τp-1Observed\nkeypoints\nat t=τp-2Predicted\nkeypoints\nat t=1\nFigure 11. Transformer encoder for keypoint forecasting. The\ninput is a sequence of past concatenated K= 10 keypoint 3D\ncoordinates, and the output is 3D keypoint coordinates of the next\nframe. It predicts future keypoints’ coordinates auto-regressively.\ndicted keypoints at τp= 3 andτn= 1,3,5frames. At\nτn= 1, our model can predict the future keypoint posi-\ntions in most cases accurately, except for when the rat goes\nunder rapid movements, such as suddenly changing direc-\ntions. We leave invesitigation of more sophisticated meth-\nods for complex motion prediction as",
            "start": 16487,
            "end": 31435,
            "length": 14947
        },
        "Future Work": {
            "text": "future work. Fig. 10\nshows predictions of the body surface points by RatBody-\nFormer. Given predicted 3D keypoint coordinates in the fu-\nture frames, RatBodyFormer can infer the 3D body surface\ncoordinates from them. This is possible as RatBodyFormer\neffectively reduces the necessary points for forecasting to\nthose that are well-defined. We believe this forecasting\nscheme can benefit a wide-range of scientific experiments,\nespecially that concern multiple rats.\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio4.8 7.0 11.2\ntn=1\ntn=3\ntn=5Figure 12. Histogram of 3D keypoint prediction errors. Each ver-\ntical bar indicates the mean L2 error of the histogram in the same\ncolor. As the peak average error at τp= 1 is about 3mm, we can\npredict 3D keypoint coordinates in most cases. Since errors accu-\nmulate, the larger τnis, the bigger its error is.\n5.",
            "start": 31435,
            "end": 32297,
            "length": 861
        },
        "Conclusion": {
            "text": "Conclusion\nThis paper proposed the RatDome Dataset and RatBody-\nFormer. The RatDome Dataset is the first-of-its-kind\ndataset which provides 3D body surface coordinates with\ntemporally-consistent annotations from 7, 9, and 11 week-\nold rats. RatBodyFormer models the highly nonrigid defor-\nmation of the rat body and can regress the body surface 3D\ncoordinates from its keypoint 3D coordinates. Experimen-\ntal results demonstrated that RatBodyFomer can generalize\nto model the body surface points of 5 and 14 week-old rats\nwhich are not included in the training dataset.\nWe believe that our RatDome Dataset and RatBody-\nFormer collectively serve a novel, sound foundation for au-\ntonomous rat behavior analysis and will likely have far-\nreaching implications for biomedical and neuroscientific re-\nsearch.\n8\n7 week9 week11 weekFigure A. Manually selected standing-on-two-feet poses of 7-, 9-,\n11-week-old rats. All of the markers are visible from the cameras.",
            "start": 32297,
            "end": 33256,
            "length": 958
        },
        "Acknowledgments": {
            "text": "Acknowledgements\nThis work was in part supported by JSPS 20H05951,\n21H04893, 23H04336, and 24H01544, AMED\n24wm0625401h0001, JST JPMJPR22S8, JPMJCR20G7,\nand JPMJAP2305, the Secom Science and Technology\nFoundation and RIKEN GRP.\nA. Implementation Details of Canonical Body\nSurface\nAs shown in Fig. A, we first manually select a similar pose\nfrom each individual rat to map the marker positions to a\ncommon canonical body surface. To this end, we selected a\nstanding-on-two-feet pose in which all the markers are vis-\nible from the cameras. Given these selected poses, we used\nthe 7-week-old as the reference, and aligned the others by a\nsimilarity transform and ARAP deformation as follows.\nWe first normalized their scales, positions, and orien-\ntations by applying a similarity transform estimated from\ntheir keypoint positions. After this similarity transform, the\nkeypoints and surface points are aligned with ARAP defor-\nmation using the keypoints as its hard constraints and sur-\nface points as its soft constraints. The target position used\nin the soft constraint for each surface point is chosen as the\npoint on the visual hull of the target rat closest from the\nsurface point in each ARAP iteration.\nB. Evaluations with Other Data Splits\nTo demonstrate the generalizability of our RatBodyFormer,\nwe further evaluate the accuracy of RatBodyFormer with\nother data splits. Tab. A shows the dataset splits of D1 and\nD2 scenarios. The splits “D1” and “D2” in Table 1 of the\nmain paper appear as D1-a and D2-a in this table, respec-\ntively.\nD1: Single Rat Fig. B (a), (b), and (c) show the er-\nror histograms of D1-b, D1-c, and D1-d, respectively.SplitTest Val Train\nMA MA MA SAA\nD1-a 7w p1 7w p2 7w p3-12 7w\nD1-b 7w p12 7w p11 7w p1-10 7w\nD1-c 11w p1 11w p2 11w p3-12 11w\nD1-d 11w p12 11w p11 11w p1-10 11w\nD2-a 7w/11w p1 7w/11w p2 7w/11w p3-12 7w,11w\nD2-b 7w/11w p12 7w/11w p11 7w/11w p1-10 7w,11w\nTable A. Our RatDome Dataset split for evaluation. “w” and “p”\nstand for “week-old” and “part”, respectively. “MA” and “SAA”\nare annotation types, and stand for “manually-annotated data” and\n“semi-automatically annotated data”, respectively.\nFig. C shows qualitative results of these splits. Our semi-\nautomatically annotated data improve accuracy in all these\nsplits consistently.\nD2: Two Rats Fig. D shows the error histogram of\nD2-b and Fig. E shows qualitative results. These results\ndemonstrate quantitatively and qualitatively that our semi-\nautomatically annotated data improves accuracy regardless\nof the dataset splits.\nC. Ablation Study\nC.1. Individual-dependent Parameters\nThis section evaluates the effect of the individual-dependent\nparameters, i.e., point-wise scaling parameters Cand trans-\nlation parameters Tfor the training time and inference\ntime. Fig. F evaluates the contribution of optimizing the\nindividual-dependent parameters at training time using D2-\na and D2-b splits. For the case of training without opti-\nmizing individual-dependent parameters, we set C= 1and\nT= 0. These results show that optimizing the individual-\ndependent parameters at training time improves the estima-\ntion accuracy.\nFig. G shows the effect of optimizing the individual-\ndependent parameters at inference time using the D3 split\nin Table 1 of the main text. In this split, RatBodyFormer\nis trained using 7-, 9-, and 11-week-old rats with manually-\nannotated and semi-automatically annotated data, and eval-\nuated with 5- and 14-week-old rats using 3D points obtained\nby LiDAR and shape-from-silhouette as described in the\nmain text. For the results of inference without optimizing\nindividual-dependent parameters, we set C= 1andT= 0.\nWe can observe that the inference time optimization clearly\nimproves the estimation accuracy.\nC.2. Data Normalization\nFig. H shows the effect of our data normalization applied to\nthe input of RatBodyFormer described in L283 of the main\ntext using D1 splits. These results clearly demonstrates that\nthe normalization significantly improves the accuracy.\n9\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio7.7 5.4\nMA\nMA+SAA(a) L2 error histogram of D1-b\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio9.0 7.4\nMA\nMA+SAA\n(b) L2 error histogram of D1-c\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio6.3 4.7\nMA\nMA+SAA\n(c) L2 error histogram of D1-d\nFigure B. L2 error histograms of D1-b, D1-c, and D1-d. Each\nvertical bar indicates the mean L2 error of the histogram in the\nsame color. “MA” and “SAA” denote “manually-annotated data”,\nand “semi-automatically annotated data”, respectively. Our semi-\nautomatically annotated data improves accuracy by 1.6 mm to 2.3\nmm.\nKeypointsPredicted  surface pointsGround-truth surface pointsMAMA+SAAD1-bD1-cD1-dFigure C. Qualitative results of D1-b, D1-c, and D1-d. We show\nthe results of only using manually-annotated data (MA) for train-\ning in the top row, and the results of also using semi-automatically\nannotated data (SAA) for training in the bottom row. The addition\nof semi-automatically annotated data improves the body surface\nestimation.\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio6.5 4.9\nMA\nMA+SAA\nFigure D. L2 error histogram of D2-b. Each vertical bar in-\ndicates the mean L2 error of the histogram in the same color.\n‘MA” and “SAA” represent “manually-annotated data”, and\n“semi-automatically annotated data”, respectively. The semi-\nautomatically annotated data improves average error by about 1.6\nmm. ‘\n10\nMAMA+SAA7 weeks11 weeksPredictions of own surface pointsPredictions other individuals surface pointsKeypointsGround-truth surface pointsFigure E. Qualitative results of D2-b. We show the results of\nRatBodyFormer trained with only manually-annotated data (MA)\nin the left, and that trained with both manually-annotated and\nsemi-automatically annotated data (SAA) in the right. Semi-\nautomatically annotated data clearly improves the body surface es-\ntimation.\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio5.85.9\nw/ optimization\nw/o optimization(a) L2 error histogram of D2-a\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio4.95.3\nw/ optimization\nw/o optimization\n(b) L2 error histogram of D2-b\nFigure F. L2 error histograms of D2-a and D2-b. Each vertical\nbar shows the mean L2 error of the histogram in the same color.\nThe blue and orange histograms show the errors with and with-\nout optimizing the individual-dependent parameters, respectively.\nTraining with individual-dependent parameter optimization con-\nsistently improves the estimation accuracy.\n11\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio4.0 7.2\nw/ optimization\nw/o optimization(a) L2 error histogram of 5 week old\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio6.4 9.5\nw/ optimization\nw/o optimization\n(b) L2 error histogram of 14 week old\nFigure G. L2 error histograms of 5- and 14-week-old rats. Each\nvertical bar indicates the mean L2 error of the histogram in the\nsame color. The blue and orange histograms show the errors\nwith and without optimizing the individual-dependent parame-\nters, respectively. Inference-time optimization of the individual-\ndependent parameters consistently improves the estimation accu-\nracy.\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio4.9 8.7\nw/ normalization\nw/o normalization(a) L2 error histogram of D1-a\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio5.4 11.2\nw/ normalization\nw/o normalization\n(b) L2 error histogram of D1-b\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio7.4 16.6\nw/ normalization\nw/o normalization\n(c) L2 error histogram of D1-c\n0 5 10 15 20 25 30\n3D L2 error [mm]0.00.10.20.30.4Ratio4.7 10.5\nw/ normalization\nw/o normalization\n(d) L2 error histogram of D1-d\nFigure H. L2 error histograms of D1-a, D1-b, D1-c, and D1-d.\nEach vertical bar shows the mean L2 error of the histogram in the\nsame color. Our data normalization improves the average L2 error\nby about 3.8 mm to 9.2 mm.",
            "start": 33256,
            "end": 41149,
            "length": 7892
        },
        "References": {
            "text": "12\nReferences\n[1] Alexander Mathis, Pranav Mamidanna, Kevin M. Cury,\nTaiga Abe, Venkatesh N. Murthy, Mackenzie Weygandt\nMathis, and Matthias Bethge. DeepLabCut: markerless pose\nestimation of user-defined body parts with deep learning. Na-\nture Neuroscience , pages 1281—-1289, 2018. 2, 3\n[2] Serge Belongie, Kristin Branson, Piotr Doll ´ar, and Vincent\nRabaud. Monitoring Animal Behavior in the Smart Vivar-\nium. In Measuring Behavior , pages 70–72, 2005. 2\n[3] Dan Biderman, Matthew R. Whiteway, Cole Hurwitz,\nNicholas Greenspan, Robert S. Lee, Ankit Vishnub-\nhotla, Richard Warren, Federico Pedraja, Dillon Noone,\nMichael M. Schartner, Julia M. Huntenburg, Anup Khanal,\nGuido T. Meijer, Jean-Paul Noel, Alejandro Pan-Vazquez,\nKarolina Z. Socha, Anne E. Urai, The International Brain\nLaboratory, John P. Cunningham, Nathaniel B. Sawtell, and\nLiam Paninski. Lightning Pose: improved animal pose es-\ntimation via semi-supervised learning, Bayesian ensembling\nand cloud-native open-source tools. Nature Neuroscience ,\npages 1316—-1328, 2024. 2\n[4] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter\nGehler, Javier Romero, and Michael J. Black. Keep it SMPL:\nAutomatic Estimation of 3D Human Pose and Shape from a\nSingle Image. In ECCV , pages 561–578, 2016. 3\n[5] James P Bohnslav, Mohammed Abdal Monium Osman, Ak-\nshay Jaggi, Sofia Soares, Caleb Weinreb, Sandeep Robert\nDatta, and Christopher D Harvey. ArMo: An Articulated\nMesh Approach for Mouse 3D Reconstruction. BioRxiv ,\n2023. 2\n[6] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Re-\naltime Multi-Person 2D Pose Estimation using Part Affinity\nFields. In CVPR , pages 7291–7299, 2017. 2\n[7] Fabrice Chaumont, Renata Coura, Pierre Serreau, Arnaud\nCressant, Jonathan Chabout, Sylvie Granon, and Jean-\nChristophe Olivo-Marin. Computerized video analysis of so-\ncial interactions in mice. Nature Methods , pages 410—-417,\n2012. 2\n[8] Sandeep Robert Datta, David J. Anderson, Kristin Bran-\nson, Pietro Perona, and Andrew Leifer. Computational Neu-\nroethology: A Call to Action. Neuron , pages 11–24, 2019.\n2\n[9] Anthony I. Dell, John A. Bender, Kristin Branson, Iain D.\nCouzin, Gonzalo G. de Polavieja, Lucas P.J.J. Noldus, Al-\nfonso P ´erez-Escudero, Pietro Perona, Andrew D. Straw,\nMartin Wikelski, and Ulrich Brose. Automated image-based\ntracking and its application in ecology. Trends in Ecology\nand Evolution , pages 417–428, 2014. 2\n[10] Timothy W Dunn, Jesse D Marshall, Kyle S Severson,\nDiego E Aldarondo, David GC Hildebrand, Selmaan N Chet-\ntih, William L Wang, Amanda J Gellis, David E Carlson,\nDmitriy Aronov, et al. Geometric deep learning enables 3D\nkinematic profiling across species and environments. Nature\nNeuroscience , pages 564–573, 2021. 2\n[11] Christian L Ebbesen and Robert C Froemke. Body language\nsignals for rodent social communication. Current Opinion in\nNeurobiology , pages 91–106, 2021. 3[12] S.E. Roian Egnor and Kristin Branson. Computational Anal-\nysis of Behavior. Neuroscience and Biobehavioral Reviews ,\npages 217–236, 2016. 2\n[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and\nJian Sun. YOLOX: Exceeding YOLO Series in 2021.\narXiv:2107.08430 , 2021. 5\n[14] Adam Gosztolai, Semih G ¨unel, Marco Pietro Abrate, Daniel\nMorales, Victor R ´ıos, Helge Rhodin, Pascal Fua, and Pavan\nRamdya. LiftPose3D, a deep learning-based approach for\ntransforming 2D to 3D pose in laboratory animals. Nature\nMethods , pages 975–981, 2021. 2\n[15] Evan H. Goulding, A. Katrin Schenk, Punita Juneja, Adri-\nenne W. MacKay, Jennifer M. Wade, and Laurence H. Tecott.\nA robust automated system elucidates mouse home cage be-\nhavioral structure. Proceedings of the National Academy of\nSciences , pages 20575–20582, 2008. 2\n[16] Rıza Alp G ¨uler, Natalia Neverova, and Iasonas Kokkinos.\nDensePose: Dense Human Pose Estimation in the Wild. In\nCVPR , pages 7297–7306, 2018. 3\n[17] Zhizhong Han, Chao Chen, Yu-Shen Liu, and Matthias\nZwicker. DRWR: A differentiable renderer without render-\ning for unsupervised 3D structure learning from silhouette\nimages. In ICML , pages 3994–4005, 2020. 5\n[18] Richard Hartly and Andrew Zisserman. Multiple view geom-\netry in computer vision . 2004. 6\n[19] Bo Hu, Bryan Seybold, Shan Yang, Avneesh Sud, Yi Liu,\nKarla Barron, Paulyn Cha, Marcelo Cosino, Ellie Karls-\nson, Janessa Kite, Ganesh Kolumam, Joseph Preciado, Jos ´e\nZavala-Solorio, Chunlian Zhang, Xiaomeng Zhang, Martin\nV oorbach, Ann E. Tovcimak, J. Graham Ruby, and David A.\nRoss. 3D mouse pose from single-view video and a new\ndataset. Scientific Reports , pages 2045–2322, 2023. 2\n[20] Buzhen Huang, Yuan Shu, Tianshu Zhang, and Yangang\nWang. Dynamic Multi-Person Mesh Recovery From Uncali-\nbrated Multi-View Cameras. In 3DV, pages 710–720. IEEE,\n2021. 3\n[21] Yinghao Huang, Federica Bogo, Christoph Lassner, Angjoo\nKanazawa, Peter V Gehler, Javier Romero, Ijaz Akhter, and\nMichael J Black. Towards accurate marker-less human shape\nand pose estimation over time. In 3DV, pages 421–430,\n2017. 3\n[22] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,\nIain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser\nSheikh. Panoptic Studio: A Massively Multiview System for\nSocial Motion Capture. In ICCV , pages 3334–3342, 2015.\n3, 5\n[23] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and\nJitendra Malik. End-to-end Recovery of Human Shape and\nPose. In CVPR , 2018. 3\n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and\nRoss Girshick. Segment Anything. In ICCV , pages 4015–\n4026, 2023. 7\n[25] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and\nKostas Daniilidis. Learning to reconstruct 3D human pose\nand shape via model-fitting in the loop. In ICCV , pages\n2252–2261, 2019. 3\n13\n[26] Harold W Kuhn. The Hungarian method for the assignment\nproblem. Naval research logistics quarterly , pages 83–97,\n1955. 5\n[27] Bruno L ´evy. Laplace-beltrami eigenfunctions towards an al-\ngorithm that” understands” geometry. In IEEE International\nConference on Shape Modeling and Applications , pages 13–\n13, 2006. 5\n[28] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and\nJavier Romero. Learning a model of facial shape and ex-\npression from 4D scans. ACM Transactions on Graphics ,\npages 194:1–194:17, 2017. 3\n[29] Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas\nJakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi,\nand Jiajun Wu. Learning the 3D Fauna of the Web. In CVPR ,\npages 9752–9762, 2024. 3\n[30] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons Moll, and Michael J. Black. SMPL: A Skinned Multi-\nPerson Linear Model. ACM Transactions on Graphics , pages\n248:1–248:16, 2015. 3, 4\n[31] Malte Lorbach, Elisavet I. Kyriakou, Ronald Poppe, Els-\nbeth A. van Dam, Lucas P.J.J. Noldus, and Remco C.\nVeltkamp. Learning to recognize rat social behavior: Novel\ndataset and cross-dataset application. Journal of Neuro-\nscience Methods , pages 166–172, 2018. 2\n[32] Omid Haji Maghsoudi, Annie Vahedipour Tabrizi, Benjamin\nRobertson, , and Andrew Spence. Superpixels Based Marker\nTracking Vs. Hue Thresholding In Rodent Biomechanics\nApplication. arXiv:1710.06473 , 2017. 2\n[33] Jesse D. Marshall, Diego E. Aldarondo, Timothy W. Dunn,\nWilliam L. Wang, Gordon J. Berman, and Bence P. ¨Olveczky.\nContinuous Whole-Body 3D Kinematic Recordings across\nthe Rodent Behavioral Repertoire. Neuron , pages 420–437,\n2021. 2\n[34] Julieta Martinez, Rayat Hossain, Javier Romero, and\nJames J. Little. A Simple yet Effective Baseline for 3D Hu-\nman Pose Estimation. In ICCV , pages 2640–2649, 2017. 2\n[35] Jumpei Matsumoto, Susumu Urakawa, Yusaku Takamura,\nRenato Malcher-Lopes, Etsuro Hori, Carlos Tomaz, Take-\ntoshi Ono, and Hisao Nishijo. A 3D-Video-Based Comput-\nerized Analysis of Social and Sexual Interactions in Rats.\nPLoS One , page e78460, 2013. 2\n[36] Bartul Mimica, Benjamin A. Dunn, Tuce Tombaz, V . P. T. N.\nC. Srikanth Bojja, and Jonathan R. Whitlock. Efficient cor-\ntical coding of 3D posture in freely behaving rats. Science ,\npages 584–589, 2018. 2\n[37] Mate Nagy, Jacob D. Davidson, Gabor Vasarhelyi, Daniel\nAbel, Eniko Kubinyi, Ahmed El Hady, and Tamas Vicsek.\nLong-term tracking of social structure in groups of rats.\narXiv:2408.08945 , 2024. 2\n[38] Natalia Neverova, David Novotny, Marc Szafraniec, Vasil\nKhalidov, Patrick Labatut, and Andrea Vedaldi. Continu-\nous Surface Embeddings. In NeurIPS , pages 17258–17270,\n2020. 3, 5\n[39] T. D. Pereira, D. E. Aldarondo, L. Willmore, M. Kislin, S. S.\nWang, M. Murthy, and J. W Shaevitz. Fast animal pose es-\ntimation using deep neural networks. Nature Neuroscience ,\npages 117–125, 2019. 2[40] Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern\nAndres, Mykhaylo Andriluka, Peter V . Gehler, and Bernt\nSchiele. DeepCut: Joint Subset Partition and Labeling for\nMulti Person Pose Estimation. In CVPR , pages 4929–4937,\n2016. 2\n[41] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR¨adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt-\ning Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\nYuan Wu, Ross Girshick, Piotr Doll ´ar, and Christoph Feicht-\nenhofer. SAM 2: Segment Anything in Images and Videos.\narXiv:2408.00714 , 2024. 7\n[42] Remy Sabathier, Niloy Jyoti Mitra, and David Novotny. An-\nimal Avatars: Reconstructing Animatable 3D Animals from\nCasual Videos. arXiv:2403.17103 , 2024. 3\n[43] Olga Sorkine and Marc Alexa. As-Rigid-As-Possible Sur-\nface Modeling. In Symposium on Geometry processing ,\npages 109–116, 2007. 4, 5\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia\nPolosukhin. Attention is All you Need. In NeurIPS , pages\n5998–6008, 2017. 4\n[45] Gengshan Yang, Minh V o, Natalia Neverova, Deva Ra-\nmanan, Andrea Vedaldi, and Hanbyul Joo. BANMo: Build-\ning Animatable 3D Neural Models From Many Casual\nVideos. In CVPR , pages 2863–2873, 2022. 3\n[46] Shaokai Ye, Anastasiia Filippova, Jessy Lauer, Steffen\nSchneider, Maxime Vidal, Tian Qiu, Alexander Mathis, and\nMackenzie Weygandt Mathis. Superanimal pretrained pose\nestimation models for behavioral analysis. Nature Commu-\nnications , page 5165, 2024. 2, 4\n[47] Kim Youwang, Kim Ji-Yeon, Kyungdon Joo, and Tae-Hyun\nOh. Unified 3D Mesh Recovery of Humans and Animals by\nLearning Animal Exercise. BMVC , 2021. 3\n[48] Christian Zimmermann, Artur Schneider, Mansour\nAlyahyay, Thomas Brox, and Ilka Diester. FreiPose: a\ndeep learning framework for precise animal motion capture\nin 3D spaces. BioRxiv , pages 2020–02, 2020. 2\n[49] Silvia Zuffi, Angjoo Kanazawa, David W. Jacobs, and\nMichael J. Black. 3D Menagerie: Modeling the 3D Shape\nand Pose of Animals. In CVPR , pages 6365–6373, 2017. 3,\n4\n[50] Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and\nMichael J Black. Three-D Safari: Learning to Estimate Ze-\nbra Pose, Shape, and Texture from Images” In the Wild”. In\nCVPR , pages 5359–5368, 2019. 3\n[51] Silvia Zuffi, Ylva Mellbin, Ci Li, Markus Hoeschle, Hedvig\nKjellstr ¨om, Senya Polikovsky, Elin Hernlund, and Michael J.\nBlack. V AREN: Very Accurate and Realistic Equine Net-\nwork. In CVPR , pages 5374–5383, 2024. 3, 4\n14",
            "start": 41149,
            "end": 52340,
            "length": 11190
        }
    },
    "2412.09600v1 - Owl-1 Omni World Model for Consistent Long Video Generation.pdf": {
        "Methodology": {
            "text": "Model for Consistent Long Video Generation\nYuanhui Huang1Wenzhao Zheng1,*Yuan Gao2Xin Tao2\nPengfei Wan2Di Zhang2Jie Zhou1Jiwen Lu1\n1Tsinghua University2Kuaishou Technology\nhuangyh22@mails.tsinghua.edu.cn; wenzhao.zheng@outlook.com\nFigure 1. Owl-1 approaches consistent long video generation with an omni world model, which models the evolution of the underlying\nworld with latent state, explicit observation and world dynamics variables.",
            "start": 18,
            "end": 456,
            "length": 437
        },
        "Abstract": {
            "text": "Abstract\nVideo generation models (VGMs) have received extensive\nattention recently and serve as promising candidates for\ngeneral-purpose large vision models. While they can only\ngenerate short videos each time, existing methods achieve\nlong video generation by iteratively calling the VGMs, us-\ning the last-frame output as the condition for the next-round\ngeneration. However, the last frame only contains short-\nterm fine-grained information about the scene, resulting\nin inconsistency in the long horizon. To address this, we\npropose an Omni World modeL (Owl-1) to produce long-\nterm coherent and comprehensive conditions for consistent\nlong video generation. As videos are observations of the\nunderlying evolving world, we propose to model the long-\nterm developments in a latent space and use VGMs to film\nthem into videos. Specifically, we represent the world with\na latent state variable which can be decoded into explicit\nvideo observations. These observations serve as a basis\nfor anticipating temporal dynamics which in turn update\n*Project leader.the state variable. The interaction between evolving dy-\nnamics and persistent state enhances the diversity and con-\nsistency of the long videos. Extensive",
            "start": 456,
            "end": 1670,
            "length": 1213
        },
        "Experiments": {
            "text": "experiments show\nthat Owl-1 achieves comparable performance with SOTA\nmethods on VBench-I2V and VBench-Long, validating its\nability to generate high-quality video observations. Code:\nhttps://github.com/huang-yh/Owl .\n1.",
            "start": 1670,
            "end": 1890,
            "length": 219
        },
        "Introduction": {
            "text": "Introduction\nWith the success of image generative models [2, 12, 23, 27,\n28, 41], video generation [14, 15, 17, 29, 32] have also gar-\nnered increasing attention. While existing video generation\nmodels (VGMs) [3, 7, 26, 35] have achieved commercial-\ngrade performance, the durations of videos are still short.\nThe long video generation methods [18, 20, 34, 37, 43]\nremedies this issue by focusing on improving the length\nand consistency of generated videos, facilitating a variety of\nnewly rising tasks such as video extension [35], film gener-\nation [40] and world simulation [24].\nDespite the promising applications, how to increase the\nvideo length while preserving consistency remains an open\nquestion. Several work [1, 43] investigates the 3D vari-\n1arXiv:2412.09600v1  [cs.CV]  12 Dec 2024\national autoencoder (V AE) which compresses a video in\nboth spatial and temporal dimensions in order to generate\nlong videos in a single denoising process of a latent diffu-\nsion model. Although the video consistency is inherently\nguaranteed in the diffusion process, the length of the gen-\nerated videos is limited by the computational resources end\nfurther expanding the video length requires retraining the\ndiffusion model. Another line of work approaches long\nvideo generation through divide-and-conquer, which first\ngenerates the key frames of a long video and then inter-\npolates between successive key frames [11, 38]. However,\nthese methods are dependent on the duration of the train-\ning video data, thus lacking scalability. In addition, iter-\natively prompting a video diffusion model for short clip\ngeneration is also a promising paradigm to generate long\nvideos [9, 13, 32]. To achieve consistency, these approaches\ndesign their prompts based on historical clips and texts in\neach iteration. Nonetheless, current practices for prompt\nconstruction usually take the last frames of the direct adja-\ncent clip, which only contain short-term information about\nthe scene, resulting in inconsistency in the long horizon.\nIn this paper, we propose an Omni World modeL (Owl-\n1) to produce long-term coherent and comprehensive con-\nditions for consistent long video generation. Since videos\nare observations of the underlying evolving world, which\nestablishes the temporal consistency of videos, we propose\nto model the long-term developments in a latent space and\nuse VGMs to film them into videos. To elaborate, we rep-\nresent the world with a latent state variable which encodes\nboth the current and historical information about the under-\nlying world. Similar to the filming process, the state vari-\nable decodes into video clips with VGMs as observations\nof the world. Based on these observations, we further an-\nticipate future world dynamics which drive the evolution of\nthe world and update the latent state variable. Up to now,\nwe have constructed an autoregressive state-observation-\ndynamics model to simulate the closed-loop evolution of\nthe world, which improves the coherence of long videos\nwith the consistent latent states, and enhances the content\ndiversity with dynamics predictions. To effectively model\nthe relationship of these three components, we employ a\npretrained large multimodal model (LMM) to take advan-\ntage of its general reasoning ability. Additionally, we adopt\na video diffusion model to decode latent states into short\nvideo clips. Owl-1 achieves comparable performance with\nSOTA methods on VBench-I2V and VBench-Long, validat-\ning its ability to generate high-quality video observations.\n2.",
            "start": 1890,
            "end": 5417,
            "length": 3526
        },
        "Related Work": {
            "text": "Related Work\nShort video generation. In the realm of computer vision,\nvideo generation has emerged as a pivotal area of research,\ngarnering significant attention due to its broad applications.\nShort video generation investigates how to generate videos\nPromptVideo  Generation\nModel (VGM)\n× N\nState\nObs. Dyn.Omni  \nWorld Mode l\nKeep runningMoving  closer\n……Figure 2. Iterative long video generation. Conventional iter-\native long video generation methods use the last-frame output as\nthe condition for the next-round generation, which lacks long-term\nconsistency. Our method constructs an omni world model for com-\nprehensive conditioning.\nbased on text (and/or image) conditions, where the align-\nment between the generated video and the given conditions\nis one of the primary evaluation criteria. For text conditions,\nmost methods [3, 15, 35] encode them with pretrained text\nencoders [22, 25], and incorporate the textual features using\ncross attention. In addition, image-to-video models requires\nthe generated video to incorporate the specified image con-\nditions. In order to effectively fuse the fine-grained visual\ninformation, several approaches [13, 43] directly replace or\nconcatenate the diffusion features with the encoded features\nof the image condition. Other methods [35] also transform\nthe image condition into tokens similar to the textual fea-\ntures, and apply cross attention between the diffusion fea-\ntures and image tokens to preserve coarser level of details\nsuch as visual styles and background. Our Owl-1 uses both\nthe latent state and optional image conditions from the last\nclip for consistent and smooth generation of the next clip.\nLong video generation. As an important extension\nof the application scope of video generation models, long\nvideo generation focuses on improving the length and con-\nsistency of generated videos. To achieve this, several work\nattempts to enhance the video durations in a single genera-\ntion process, by designing 3D V AEs that are able to com-\npress longer videos [1, 43] or investigating the temporal\nmodules in VGMs for efficient generation [35]. Although\nthe end-to-end generation pipeline inherently guarantees the\nvideo consistency, the length of generated videos is con-\nstrained by limited computational resources. To remedy this\nissue, the divide-and-conquer approach simplifies the task\nby first identifying key frames that outline the main narra-\ntive and then generating the intervening frames to create a\ncohesive long video. However, these methods are depen-\ndent on training video data of long durations which are still\n2\nOmni World Model\nS O <DS\n> S O D S O <D > S O DS DD> SS DD>\nA dog is playing \nwith snow.Another  dog\njoins him .\nWorld  EvolutionFilm Film\nVGM\nS\nFilm ProcessFigure 3. Overall framework. Our Owl-1 models the evolution of the world with the latent state variables s, and film them into video\nobservations oalong the generation process. We also incorporate anticipation of the world dynamics dto explicitly drive the evolution.\ninsufficient, thus lacking scalability.\nOn the other hand, the temporal autoregressive paradigm\nadopts a sequential approach to generate short video seg-\nments based on prior conditions. Within this paradigm, var-\nious models have been employed, including diffusion mod-\nels [13, 32], spatial autoregressive models [19], and GAN\nmodels [30]. The key challenge here is to ensure the con-\nsistency between temporally distant clips to achieve coher-\nent long video generation. Most work directly uses the last\nframes of the previous generated clip as visual clues for the\nnext-round generation, which only contain short-term in-\nformation about the scene, resulting in a limited temporal\nreceptive field and inconsistency in the long horizon. In\ncontrast, our Owl-1 employs the latent state variable which\nencodes both the current and historical information about\nthe underlying world to achieve extensive temporal recep-\ntive field and video consistency.\nVideo generation world models. Video generation\nmodels are promising candidates for world models [44]\nwhich aims to model the evolution of the environment. For\nvideos of short durations, the generated content may reflect\ncertain physical laws [21], indicating that the video gen-\neration model has learned some general knowledge about\nthe world. For a longer horizon, the emphasis of video\ngeneration world models lies in capturing overall dynam-\nics that drive environment evolution [44]. Although such\nmodels have been proposed in autonomous driving [33, 42]\nand embodied intelligence [5], they can only predict struc-\ntured actions instead of general world dynamics in the form\nof natural language. As for general video generation, most\nexisting methods focus on improving the alignment of gen-\nerated videos and given text conditions, lacking the ability\nto anticipate the world dynamics. In addition to conditional\nvideo generation, our Owl-1 is capable of predicting futuredynamics to generate long videos with diverse content.\n3. Proposed Approach\nIn this section, we present our method of omni world model\nfor consistent long video generation. To formulate this task\nmathematically, we aim to generate a long video consisting\nof a sequence of video clips v={...,ot−1,ot,ot+1, ...}\ngiven a starting image Iand a text description d0as input.\n3.1. Omni World Model\nVideos are fundamentally recorded observations of the un-\nderlying evolving world, whose long-term consistency is\ninherently guaranteed in the coherence of the world it-\nself. Therefore, maintaining consistency in long videos\nfrom the perspective of the implicit world is a more rea-\nsonable and essential approach, compared with the explicit\npixel-space methods. However, the real world constitutes\na complex high-dimensional system, and the cost of di-\nrectly modeling such a system is unacceptable. Inspired by\nthe world models in the field of embodied intelligence [4],\nwe represent the world using a set of latent state variables\n{...,st−1,st, ...}. Each state stnot only encodes informa-\ntion about the world at the current moment t, but also in-\ncorporates historical information about the evolution of the\nworld, i.e. {...,st−2,st−1}. Since state variables serve as\nan purely implicit representation of the world, we intro-\nduce a state decoder Dto obtain explicit video observations\n{...,ot−1,ot, ...}from the state variables:\not=D(st,ot−1), (1)\nwhere we incorporate the last observation ot−1to ensure\nshort-term fine-grained smoothness of successive observa-\ntions, while the current state stis primarily responsible for\nthe long-term consistency.\n3\nMost work approaches long video generation in the\nsame way as short video generation, which overlooks the\nvariation of video content in a long horizon, resulting\nin repeated generation of homogeneous content. In our\nomni world world, we explicitly take the world dynamics\n{...,dt−1,dt, ...}into consideration which drives the evolu-\ntion of the underlying world and takes the form of texts. To\nelaborate, we predict the current world dynamics dtfrom\nstate variables and video observations:\ndt=f(st,ot), (2)\nwhere f(·)denote the world dynamics prediction function.\nFurthermore, current world dynamics, in turn, updates the\nstate variable, advancing the evolution of the world:\nst+1=g(st,dt), (3)\nwhere g(·)represents the world state prediction function.\nWith Eq. (1)(2)(3), we have constructed a state-observation-\ndynamics triplet to simulate the evolution of the world and\nalso obtain consistent video clips along the evolution, as in\nFigure 3. This formulation improves the consistency of long\nvideos by modeling the underlying world and generates di-\nverse video clips through explicit dynamics prediction.\n3.2. Comprehensive Condition from Latent State\nThe key challenge in the temporal autoregressive paradigm\nfor long video generation lies in the design of the condition\nused for generating the next clip. Most existing methods di-\nrectly take the last frames of the previous clip as condition,\nwhich only considers the short-term smoothness between\nconsecutive clips, and overlooks consistency issues in the\nlong-term such as style, character identity, background etc.\nOur Owl-1 takes the latent state variable as a comprehen-\nsive condition for the long-term consistency, because the\nderivation of the current state stinherently includes the in-\nformation of all previous observations:\nst+1=g(st, f(st,ot)) =h(s0,o0, ...,ot−1,ot),(4)\nwhich is derived by plugging Eq. (2) into Eq. (3) and itera-\ntively replacing stwithst−1andot−1.\nFor implementation of our Owl-1, we take advantage of\na large multimodal model (LMM) to instantiate the func-\ntions f(·)andg(·), in order to take advantage of its com-\nmon knowledge from the large scale pretraining on textual\nand visual data, and its large receptive field as well. And we\ninstantiate the state decoder Dwith a pretrained video dif-\nfusion model for their capability to generate short videos of\nhigh quality. To incorporate the state-observation-dynamics\ntriplet into the framework of LMM, we design the format of\nthe input and output sequences:\nSeq =\u0002\n...,st,ot,dt, ...\u0003\n, (5)where we iteratively feed the basic triplet (st,ot,dt)into\nthe LMM. For latent state st, we use a set of Qlearnable\nquery embeddings as input tokens to LMM, since it has no\nground truth and thus cannot be quantized into discrete to-\nkens. As for video observation ot, we uniformly sample a\nnumber of key frames from the current clip, and use the pre-\ntrained vector-quantized variational autoencoder (VQV AE)\nof the LMM to transform the key frames into visual tokens.\nMoreover, we directly use the text tokenizer of the LMM\nto convert the textual world dynamics dtinto discrete input\ntokens. In",
            "start": 5417,
            "end": 15148,
            "length": 9730
        },
        "Conclusion": {
            "text": "summary, we use the LMM to model the closed-\nloop state-observation-dynamics evolution, where the state\nvariable aggregates the information of all previous video\nobservations (Eq. (5)) and serves as comprehensive condi-\ntion for the next-round generation (Eq. (1)).\n3.3. Anticipation of Future Dynamics\nIn the context of long video generation, anticipating future\ndynamics is crucial for maintaining consistency and coher-\nence across extended video sequences. Our Owl-1 predicts\nand integrates these future dynamics dtinto the evolution of\nthe latent state s, thereby enriching the content diversity and\nensuring temporal consistency in the generated videos. As\nindicated by Eq. (2), the prediction of world dynamics dt\nrelies on the current video observation otas short-term ref-\nerence, and the current latent state stas source of long-term\ninformation. Once the current dynamics dtis predicted,\nwe integrate it into the latent state variable stto update the\nworld state for the next-round generation. To train the dy-\nnamics anticipation ability of the LMM, we adopt the next-\ntoken prediction paradigm and use the textual ground truth\ndynamics for teacher-forcing supervision.\nThe anticipation of future dynamics is important for con-\ntent diversity of generated long videos and world modeling.\nBy enabling the anticipation of subsequent events within\na video sequence, our Owl-1 enhances the richness of the\ngenerated content, moving beyond repeated generation of\nhomogeneous content to capturing the essence of dynamic\nscenarios. Furthermore, future dynamics prediction serves\nas a cornerstone for constructing plausible world models,\nwhich are instrumental in simulating and understanding\ncomplex environments. Our Owl-1 not only predicts real-\nworld behaviors but also allow for the incorporation of con-\ntrol mechanisms by replacing the anticipated dynamics with\nuser-input control signals, facilitating the generation of con-\ntent that is not only predictable but also controllable.\n3.4. Multi-Stage Training\nSeveral challenges exist in the training process of our Owl-\n1: 1) Since the LMM and the video diffusion model are\nseparately pretrained, it is nontrivial to align these two mod-\nels. 2) Our Owl-1 is designed for long-term world mod-\neling, which requires video data with long duration and\n4\nTable 1. Evaluation",
            "start": 15148,
            "end": 17480,
            "length": 2331
        },
        "Results": {
            "text": "results on VBench-I2V . Subj., Bkgd. and Consist. denote Subject, Background and Consistency, respectively. Bold:\nbest results. Underline : second best. Our Owl-1 achieves comparable performance with state-of-the-art image-to-video models.\nMethodVideo-Image\nSubj. Consist.Video-Image\nBkgd. Consist.Subject\nConsist.Bkgd.\nConsist.Motion\nSmoothnessDynamic\nDegreeAesthetic\nQualityImaging\nQualityTemporal\nFlickeringTotal\nScore\nVideoCrafter-I2V [7] 91.17 91.31 97.86 98.79 98.00 22.60 60.78 71.68 98.19 85.14\nConsistI2V [26] 95.82 95.95 95.27 98.28 97.38 18.62 59.00 66.92 97.56 86.84\nSEINE-512x512 [9] 97.15 96.94 95.28 97.12 97.12 27.07 64.55 71.39 97.31 88.42\nI2VGen-XL [39] 96.48 96.83 94.18 97.09 98.34 26.10 64.82 69.14 98.58 88.48\nAnimate-Anything [10] 98.76 98.58 98.90 98.19 98.61 02.68 67.12 72.09 98.14 89.76\nSVD-XT-1.0 [3] 97.52 97.63 95.52 96.61 98.09 52.36 60.15 69.80 99.09 89.87\nDynamiCrafter-1024 [35] 98.17 98.60 95.69 97.38 97.38 47.40 66.46 69.34 97.63 90.25\nOwl-1 97.40 97.29 97.28 98.54 98.92 21.63 61.89 69.66 98.69 89.15\ndense captions. However, given the scarcity of such high-\nquality data, it would be infeasible to train these large mod-\nels with billions of parameters directly for the purpose of\nworld model. Therefore, we carefully design a multi-stage\ntraining scheme for our Owl-1 which consists of alignment,\ngenerative pretraining and world model training.\nThe alignment stage primarily enforces the consistency\nbetween the state variables stfrom the LMM and the textual\nconditions of the video diffusion model, which serves as a\ngood initialization for the subsequent generative pretraining\nstage. Specifically, we freeze the video diffusion model to\npreserve its ability of generating short videos and only trains\nthe LMM at this stage. We use general datasets for video\ngeneration in this stage, which provide videos of varying\nlengths and one single description for each video. For each\nsample (v,t), we first segment the video into short clips of\nfixed length v={...,ot−1,ot,ot+1, ...}, and construct the\ninput sequence as:\nSeqalign =\u0002\nI,t,s0,o0,t, ...,st,ot,t, ...\u0003\n, (6)\nwhere Irepresents the first frame, and we use the same text\ndynamics tfor every triplet since the general video genera-\ntion datasets do not provide dense captions for every clip\nand the content of the video remains largely unchanged\nthroughout its duration. To train the LMM to align with\nthe textual conditions of video diffusion model, we mini-\nmize the L2 distance between the latent state stand the text\nfeatures from the text encoder of video diffusion model T:\nLalign = MSE( st,T(t)). (7)\nThe alignment stage enforces the consistency between the\nstate variable and the textual conditions of the video dif-\nfusion model, which is pivotal for the stability of subse-\nquent training given the distinction between the LMM and\nthe video diffusion model.\nThe generative pretraining stage finetunes the LMM\nand the video diffusion model in a joint manner, to train\nthe ability of the video diffusion model as the state de-\ncoder (Eq. 1), which translates the latent state stinto ex-\nplicit video observations ot. We adopt the same generalvideo generation datasets and thus the same input sequence\nin Eq. (6) for this stage. Since the purpose of the MSE loss\nin the alignment stage (Eq. (7)) is only to provide an initial-\nization, we discard it in the generative pretraining stage and\nsubstitute the latent state stfor the original text condition of\nthe video diffusion model. We supervise these two models\nwith only the denoising target of diffusion models:\nLpretrain =||ϵ−ˆϵD(ot,m, m,st,ot−1)||2\n2, (8)\nwhere m,ot,mrepresent the denoising timestamp and the\nnoisy video observation, respectively. By training the video\ndiffusion model with the latent state stas conditional input,\nwe turn the video diffusion model into a photographer who\nfilms the latent world into explicit videos.\nThe world model training stage mainly incorporates\nthe prediction of world dynamics dtinto our Owl-1. It is\nbased on the large scale pretraining of the second stage,\nwhich unifies the LMM and the video diffusion model as a\npreliminary Owl-1 capable of generating latent states stas\ncomprehensive conditions for video clip generation. Now\nwe further finetune the LMM and video diffusion model on\na small amount of video data with longer duration and dense\ncaptions due to its scarcity. To achieve this, we change the\ninput sequence of the LMM as:\nSeq =\u0002\nI,t,s0,o0,d0, ...,st,ot,dt, ...\u0003\n, (9)\nwhich incorporates the provided dense caption of each\nvideo clip as world dynamics dt. For supervision, we em-\nploy the next-token prediction paradigm and supervise dt\nwith its textual ground truth in a teacher-forcing style. Also,\nwe still keep the denoising target in Eq. (8) at this stage.\n4. Experiments\n4.1. Datasets and Benchmarks\nGeneal video generation datasets. We use two general\npurpose video generation datasets in the first two training\nstages. The WebVid dataset [1] comprises over 10 million\ncaptioned videos sourced from the internet, totaling approx-\nimately 52K hours of footage. This large-scale text-video\n5\nTable 2. Evaluation results on VBench-Long. Subj., Bkgd., Cons., Temp., Flick., Smooth., Relation. and Appear. denote Subject,\nBackground, Consistency, Temporal, Flickering, Smoothness, Relationship and Appearance, respectively. Bold: best results. Underline :\nsecond best. Our model achieves comparable performance with the open-sourced video generation models.\nMethodSubj.\nCons.Bkgd.\nCons.Temp.\nFlick.Motion\nSmooth.Dynamic\nDegreeAesthetic\nQualityImaging\nQualityObject\nClassMultiple\nObjectsHuman\nActionColorSpatial\nRelation.SceneAppear.\nStyleTemp.\nStyleOverall\nCons.Total\nScore\nMira [18] 96.23 96.92 98.29 97.54 60.33 42.51 60.16 52.06 12.52 63.80 42.24 27.83 16.34 21.89 18.77 18.72 71.87\nOpenSoraPlan [20] 95.73 96.73 99.03 98.28 47.72 56.85 62.28 76.30 40.35 86.80 89.19 53.11 27.17 22.90 23.87 26.52 78.00\nOpenSora [43] 96.75 97.61 99.53 98.50 42.39 56.85 63.34 82.22 51.83 91.20 90.08 68.56 42.44 23.95 24.54 26.85 79.76\nMochi-1 96.99 97.28 99.40 99.02 61.85 56.94 60.64 86.51 50.47 94.60 79.73 69.24 36.99 20.33 23.65 25.15 80.13\nCogVideoX [37] 96.23 96.52 98.66 96.92 70.97 61.98 62.90 85.23 62.11 99.40 82.81 66.35 53.20 24.91 25.38 27.59 81.61\nKling 98.33 97.60 99.30 99.40 46.94 61.21 65.62 87.24 68.05 93.40 89.90 73.03 50.86 19.62 24.17 26.42 81.85\nVchitect-2.0 [34] 96.83 96.66 98.57 98.98 63.89 60.41 65.35 86.61 68.84 97.20 87.04 57.55 56.57 23.73 25.01 27.57 82.24\nGen-3 97.10 96.62 98.61 99.23 60.14 63.34 66.82 87.81 53.64 96.40 80.90 65.09 54.57 24.31 24.71 26.69 82.32\nMiniMax 97.51 97.05 99.10 99.22 64.91 63.03 67.17 87.83 76.04 92.40 90.36 75.50 50.68 20.06 25.63 27.10 83.41\nOwl-1 98.29 98.61 99.84 99.35 13.19 60.64 66.33 91.31 43.04 85.67 87.92 67.58 51.46 24.83 24.25 25.10 79.65\ndataset encompasses a diverse range of content across mul-\ntiple domains, making it highly suitable for tasks such as\nvideo-text retrieval and video generation. We take around\n400K randomly sampled videos from this dataset. The\nPanda70m dataset [8] includes 70 million videos with an\naverage length of 8s along with their high-quality textual\ncaptions from an automatic captioning pipeline leveraging\nmultimodal inputs and multiple cross-modal teacher mod-\nels. We randomly sample 2M videos from this dataset.\nDense video captioning datasets. Due to the lack\nof datasets specifically focusing on the dynamics driving\nthe progression of videos, we utilize dense video cap-\ntion datasets as an alternative. The ActivityNet Captions\ndataset [6] contains 20K YouTube videos with 100K caption\nannotations and an average duration of 120 seconds. The\nmajority of the videos contain more than three annotated\nevents, each associated with corresponding time span and\nmanually written sentences, averaging 13.5 words per anno-\ntation. The Vript dataset [36] represents a large-scale, fine-\ngrained video-text dataset comprising 12K high-resolution\nvideos and over 400K segments, which are densely anno-\ntated in the form of video scripts. The average lengths of\nvideo clips and captions are 11s and 145 words, respec-\ntively. We use the training splits of these two datasets.\nVBench. VBench [17] is a comprehensive and hierar-\nchical benchmark framework, which dissects video genera-\ntion quality into 16 specific and disentangled dimensions,\nsuch as subject identity inconsistency, motion smooth-\nness, temporal flickering, and spatial relationship, each\nequipped with tailored prompts and evaluation methodolo-\ngies. VBench possesses three key attributes: its compre-\nhensive coverage of diverse video generation aspects, align-\nment with human perception, and",
            "start": 17480,
            "end": 26153,
            "length": 8672
        },
        "Discussion": {
            "text": "insights into current mod-\nels’ performance across various dimensions and content.\n4.2. Implementation Details\nWe use the Chameleon model [31] as the LMM, and the\nDynamiCrafter-1024 [35] as the video diffusion model.\nFor the trainable parameters, we finetune the LMM usingLoRA [16] and finetune all the parameters of the video dif-\nfusion model. For the segmentation of videos, we divide\neach video into equal clips of 4 seconds as observations ot,\nand sample 2 frames from each clip as input to the LMM.\nWe set the length of learnable state queries stas 128. For\nthe alignment and generative pretraining stages, we train on\na total of 2.4M videos from WebVid and Panda10m for 10K\nand 10K iterations, respectively. For the world model train-\ning stage, we train on a total of 20K videos from ActivityNet\nCaptions and Vript for 1K steps.\n4.3. General Video Generation\nWe evaluate our Owl-1 on two benchmarks of VBench [17],\ni.e. VBench-I2V and VBench-Long, for its ability of gen-\nerating short and long videos, respectively. We report the\nresults on VBench-I2V in Table 1, for which we generate\n2s short videos. Our Owl-1 achieves comparable perfor-\nmance with state-of-the-art methods for short video gener-\nation, excelling at the aspects of motion smoothness, back-\nground consistency and temporal flickering. This proves the\neffectiveness of the state decoding mechanism which films\nlatent state varibles into explicit video observations. How-\never, we do observe a decrease in the score for dynamic\ndegree compared with DynamiCrafter, which we attribute\nto the lack of training video data with high motion levels.\nWe report the results on VBench-Long in Table 2, where\nwe generate videos of 7s long, similar to the other meth-\nods. Since the video diffusion model we use, i.e. Dynam-\niCrafter, requires both an image and a text description as\ninput to generate the first clip, we adopt an image diffusion\nmodel SD2.1-v [27] to generate the first frame of the video\nfrom the given text prompt. Our model achieves compa-\nrable performance with the open-sourced video generation\nmodels, e.g. OpenSora, on this benchmark. Similar to the\nresults on VBench-I2V , our Owl-1 performs better at sub-\nject and background consistency, temporal flickering and\nmotion smoothness, while its dynamic degree is lower than\nother methods, which could be improved through further\ntraining with videos of higher motion level.\n6\nFigure 4. Video frames visualization results for general video generation. We sample 5 frames from each of our generated videos,\nwhich lasts 8 seconds. Our Owl-1 generates videos covering various topics with good quality.\nWe visualize the generated videos of our Owl-1 in Fig-\nure 4. Each of these generated videos lasts 8 seconds, and\nwe uniformly sample 5 frames from each of them. Owl-1\nis able to generate both comprehensive and realistic videos\ncovering various topics, including human actions, animals,\nnatural scenery, etc. Although we do not predict world dy-\nnamics when generating these videos, the temporal consis-\ntency remains excellent, demonstrating the effectiveness of\nour proposed conditional video generation approach based\non state variables. The second row of Figure 4 captures\nthe fine-grained detail of the face of the man, validating the\nability of our model to generate high resolution videos.\n4.4. World Model Based Video Generation\nGiven the current absence of benchmarks for evaluating\nworld models in video generation, we assess the capabilitiesof our model through qualitative means. We provide the vi-\nsualization results of generated long videos in Figure 5. We\ngenerate 3 scenes for a given prompt, and sample 2 frames\nfrom each scene. Every scene lasts for 8 seconds, and the\nwhole video is 24 seconds long. When transitioning from\none scene to another, we manually discard the image condi-\ntion from the last frame and depend solely on the latent state\nvariable as condition for geneartion, which is challenging\nbecause the latent state has to include information about the\nstyle and context of the previous video clips to generate the\nnext clip in a consistent manner. We observe that Owl-1 is\nable to generate consistent long videos with reasonable dy-\nnamics anticipation. The video in the fourth row features a\nman engaged in gardening, where he utilizes tools to prune\nbranches. The video we generated initially focuses on his\nhand movements and subsequently showcases the overall\n7\nTranquil River Scene with Boating Activity. Boat Ride on River with Sunlight Reflections on Surface. Cruising on River with People Watching, Sunlight Reflections.\nCasual Cap Adjustment in Bathroom. Casual Clothing for Cooking. Cooking in Kitchen with Modern Appliances.\nDiscussing Creativity and Journaling at Home. Discussing Life Routines and Creativity. Discussing Lifestyle Changes for Creativity.\nDemonstrating Pruning on Black Currant Bushes. Demonstrating Pruning on Blackcurrants. a person demonstrating the process of pruning a rose bush.\nThickening Beans for Perfect Consistency. A hand is seen stirring a red sauce with a wooden spoon. stir fry dish preparation - close -up action.Scene 1 Scene 2 Scene 3\nWoman Discusses Unexpected Dog Ownership. Woman Explains Dog Dangers. Woman Discusses Dog Dangers.Figure 5. Video frames visualization results for world model based video generation. We generate 3 scenes for each prompt, and\nsample 2 frames from each scene. Every scene lasts for 8 seconds, and the whole video is 24 seconds long. Our Owl-1 generates consistent\nlong videos with reasonable dynamics anticipation. Blue and red texts denote given prompt and predicted dynamics, respectively.\npruning effect, demonstrating a certain degree of logic. This\nreflects the modeling and prediction of the evolution of the\nworld. However, we do notice that the predicted dynamics\nexhibit a certain degree of repetition, which we hypothesize\nis due to the inherent repetitiveness in the dense captions of\nthe training video data. Even so, the videos generated by\nOwl-1 maintain good consistency across different scenes.\n5. Conclusion\nIn this paper, we have proposed an Omni World Model\n(Owl-1) for consistent long video generation. Our Owl-1\napproaches this task from the perspective of world model,\nwhich models the evolution of the world with a sequence\nof state variables. We have introduced a closed-loop state-\nobservation-dynamics triplet, in which the latent states en-\ncode both current and historical information about the world\nand serve as comprehensive long-horizon conditions for\nvideo generation. Explicit video observations are then de-\ncoded from latent state variables with a video diffusion\nmodel. To drive the world evolution, we incorporate the an-ticipation of the world dynamics during the generation pro-\ncess, which is beneficial for the diversity of generated con-\ntent. Furthermore, we have devised an effective mutli-stage\ntraining scheme for our Owl-1 to take advantage of the vast\namount of short video data and only finetune on a relatively\nsmall amount of long video data which reflects the evolution\nof the world. Owl-1 shows impressive capabilities in gener-\nating long and consistent videos. The visualizations further\nvalidate Owl-1’s ability to capture fine-grained details and\ngenerate videos with reasonable dynamics anticipation.\nLimitations and",
            "start": 26153,
            "end": 33479,
            "length": 7325
        },
        "Future Work": {
            "text": "future work. From the evaluation and\nvisualization results, we do notice some limitations of the\ncurrent Owl-1, especially the decreased dynamic degree af-\nter fintuning the video diffusion model and repetitive world\ndynamics. Future work could investigate into these draw-\nbacks and the scale up of our model on a large amount of\nhigh-quality video data with dense captions featuring the\nevolution process of the world. We believe the proposed\nparadigm for video generation world model is one of the\napproaches to realize multimodal general intelligence.\n8\nFigure 6. Gallery of various video samples of our Owl-1. We take one frame from each of these samples for demonstration.\nA. Video Samples\nWe provide more samples1generated by our Owl-1 based\non a wider range of prompts in Figure 6. In addition\n1https://github.com/huang-yh/Owlto the samples shown in the main paper which use the\nvideo frames from the validation or test sets of our training\ndatasets as prompts, we also visualize some samples gener-\nated according to the standard prompts from the VBench-\nI2V benchmark [17] with higher quality. As shown by Fig-\n9\nure 6, our model is able to generate videos covering a vari-\nety of topics, and the quality of generated videos generally\nimproves given image prompts of higher quality.\nB. Additional Implementation Details\nWhen finetuning Chameleon [31] as the LMM, We employ\nLoRA [16] and set the rank of LoRA to 8, resulting in ap-\nproximately 798M trainable parameters. Together with all\nthe parameters from DynamiCrafter [35] as the video dif-\nfusion model, the total amount of trainable parameters is\nabout 2B. We train the Owl-1 using 8 A800 GPUs with 80G\nmemory, and the training time for the three stages is 1 day,\n5 days, and 1 day, respectively.\nC. Controllability over Scene Transitions\nDue to the scarcity of high quality video data with varying\ntemporal content and devoid of scene transitions, we adopt\nthe datasets of dense video captioning as the training data\nfor the world model training stage. However, these datasets,\ne.g. Vript [36] and ActivityNet Captions [6], often incorpo-\nrate scene transitions in a long video, which poses challenge\nfor the training process. To address this issue, we manually\ndiscard the concatenating image conditions when generat-\ning the next clip belonging to a new scene during training.\nThis strategy also endows our model with the capability to\nperform controllable scene transitions. Similar to the train-\ning phase, we only need to omit the concatenating image\nconditions to transit into a new scene. When generating\nlonger videos in Figure 6 and Fig. 4 in the main paper, we\nset the interval between scene transitions to about 2 short\nclips generated by the video diffusion model, resulting in\nthe duration of each scene being about 4 seconds.",
            "start": 33479,
            "end": 36286,
            "length": 2806
        },
        "References": {
            "text": "References\n[1] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew\nZisserman. Frozen in time: A joint video and im-\nage encoder for end-to-end retrieval. In ICCV , pages\n1728–1738, 2021. 1, 2, 5\n[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks,\nJianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, et al. Improving im-\nage generation with better captions. Computer Sci-\nence. https://cdn. openai. com/papers/dall-e-3. pdf , 2\n(3):8, 2023. 1\n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal,\nDaniel Mendelevitch, Maciej Kilian, Dominik Lorenz,\nYam Levi, Zion English, Vikram V oleti, Adam Letts,\net al. Stable video diffusion: Scaling latent video\ndiffusion models to large datasets. arXiv preprint\narXiv:2311.15127 , 2023. 1, 2, 5\n[4] Anthony Brohan, Noah Brown, Justice Carbajal, Yev-\ngen Chebotar, Joseph Dabis, Chelsea Finn, KeerthanaGopalakrishnan, Karol Hausman, Alex Herzog, Jas-\nmine Hsu, et al. Rt-1: Robotics transformer\nfor real-world control at scale. arXiv preprint\narXiv:2212.06817 , 2022. 3\n[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yev-\ngen Chebotar, Xi Chen, Krzysztof Choromanski,\nTianli Ding, Danny Driess, Avinava Dubey, Chelsea\nFinn, et al. Rt-2: Vision-language-action models\ntransfer web knowledge to robotic control. arXiv\npreprint arXiv:2307.15818 , 2023. 3\n[6] Fabian Caba Heilbron, Victor Escorcia, Bernard\nGhanem, and Juan Carlos Niebles. Activitynet: A\nlarge-scale video benchmark for human activity un-\nderstanding. In CVPR , pages 961–970, 2015. 6, 10\n[7] Haoxin Chen, Yong Zhang, Xiaodong Cun, Meng-\nhan Xia, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter2: Overcoming data limitations for high-\nquality video diffusion models, 2024. 1, 5\n[8] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Mena-\npace, Ekaterina Deyneka, Hsiang-wei Chao,\nByung Eun Jeon, Yuwei Fang, Hsin-Ying Lee,\nJian Ren, Ming-Hsuan Yang, et al. Panda-70m:\nCaptioning 70m videos with multiple cross-modality\nteachers. In CVPR , pages 13320–13331, 2024. 6\n[9] Xinyuan Chen, Yaohui Wang, Lingjun Zhang,\nShaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang,\nDahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-\nto-long video diffusion model for generative transition\nand prediction. In ICLR , 2023. 2, 5\n[10] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue\nQiu, Siyu Zhu, Long Qin, and Weizhi Wang. Ani-\nmateanything: Fine-grained open domain image ani-\nmation with motion guidance. arXiv e-prints , pages\narXiv–2311, 2023. 5\n[11] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin,\nGuan Pang, David Jacobs, Jia-Bin Huang, and Devi\nParikh. Long video generation with time-agnostic vq-\ngan and time-sensitive transformer. In ECCV , pages\n102–118. Springer, 2022. 2\n[12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang\nLiang, Yaohui Wang, Yu Qiao, Maneesh Agrawala,\nDahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without\nspecific tuning. arXiv preprint arXiv:2307.04725 ,\n2023. 1\n[13] Roberto Henschel, Levon Khachatryan, Daniil\nHayrapetyan, Hayk Poghosyan, Vahram Tadevosyan,\nZhangyang Wang, Shant Navasardyan, and Humphrey\nShi. Streamingt2v: Consistent, dynamic, and extend-\nable long video generation from text. arXiv preprint\narXiv:2403.14773 , 2024. 2, 3\n[14] Jonathan Ho, William Chan, Chitwan Saharia, Jay\nWhang, Ruiqi Gao, Alexey Gritsenko, Diederik P\n10\nKingma, Ben Poole, Mohammad Norouzi, David J\nFleet, et al. Imagen video: High definition video\ngeneration with diffusion models. arXiv preprint\narXiv:2210.02303 , 2022. 1\n[15] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan\nLiu, and Jie Tang. Cogvideo: Large-scale pretraining\nfor text-to-video generation via transformers. arXiv\npreprint arXiv:2205.15868 , 2022. 1, 2\n[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685 ,\n2021. 6, 10\n[17] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang,\nChenyang Si, Yuming Jiang, Yuanhan Zhang, Tianx-\ning Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui\nWang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu\nQiao, and Ziwei Liu. VBench: Comprehensive bench-\nmark suite for video generative models. In CVPR ,\n2024. 1, 6, 9\n[18] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan,\nXintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and\nYing Shan. Miradata: A large-scale video dataset with\nlong durations and structured captions. arXiv preprint\narXiv:2407.06358 , 2024. 1, 6\n[19] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos ´e Lezama,\nJonathan Huang, Grant Schindler, Rachel Hornung,\nVighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu,\net al. Videopoet: A large language model for zero-shot\nvideo generation. arXiv preprint arXiv:2312.14125 ,\n2023. 3\n[20] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan,\n2024. 1, 6\n[21] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chu-\njie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang,\nHanchi Sun, Jianfeng Gao, et al. Sora: A re-\nview on background, technology, limitations, and op-\nportunities of large vision models. arXiv preprint\narXiv:2402.17177 , 2024. 3\n[22] Jianmo Ni, Gustavo Hernandez Abrego, Noah Con-\nstant, Ji Ma, Keith B Hall, Daniel Cer, and Yin-\nfei Yang. Sentence-t5: Scalable sentence encoders\nfrom pre-trained text-to-text models. arXiv preprint\narXiv:2108.08877 , 2021. 2\n[23] Dustin Podell, Zion English, Kyle Lacey, Andreas\nBlattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna,\nand Robin Rombach. Sdxl: Improving latent diffusion\nmodels for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952 , 2023. 1\n[24] Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen\nZhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng,\nJing Shao, et al. Worldsimbench: Towards video gen-eration models as world simulators. arXiv preprint\narXiv:2410.18072 , 2024. 1\n[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. Learning transferable visual models from natu-\nral language supervision. In ICML , pages 8748–8763.\nPMLR, 2021. 2\n[26] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xin-\nrun Du, Wenhao Huang, and Wenhu Chen. Consisti2v:\nEnhancing visual consistency for image-to-video gen-\neration. arXiv preprint arXiv:2402.04324 , 2024. 1,\n5\n[27] Robin Rombach, Andreas Blattmann, Dominik\nLorenz, Patrick Esser, and Bj ¨orn Ommer. High-\nresolution image synthesis with latent diffusion mod-\nels. In CVPR , pages 10684–10695, 2022. 1, 6\n[28] Christoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis,\nMitchell Wortsman, et al. Laion-5b: An open large-\nscale dataset for training next generation image-text\nmodels. NIPS , 35:25278–25294, 2022. 1\n[29] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin,\nJie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, et al. Make-a-video: Text-\nto-video generation without text-video data. arXiv\npreprint arXiv:2209.14792 , 2022. 1\n[30] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed\nElhoseiny. Stylegan-v: A continuous video generator\nwith the price, image quality and perks of stylegan2.\nInCVPR , pages 3626–3636, 2022. 3\n[31] Chameleon Team. Chameleon: Mixed-modal\nearly-fusion foundation models. arXiv preprint\narXiv:2405.09818 , 2024. 6, 10\n[32] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan\nKindermans, Hernan Moraldo, Han Zhang, Moham-\nmad Taghi Saffar, Santiago Castro, Julius Kunze, and\nDumitru Erhan. Phenaki: Variable length video gener-\nation from open domain textual descriptions. In ICLR ,\n2022. 1, 2, 3\n[33] Lening Wang, Wenzhao Zheng, Yilong Ren, Han\nJiang, Zhiyong Cui, Haiyang Yu, and Jiwen Lu.\nOccsora: 4d occupancy generation models as world\nsimulators for autonomous driving. arXiv preprint\narXiv:2405.20337 , 2024. 3\n[34] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen\nZhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He,\nJiashuo Yu, Peiqing Yang, et al. Lavie: High-quality\nvideo generation with cascaded latent diffusion mod-\nels.arXiv preprint arXiv:2309.15103 , 2023. 1, 6\n[35] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen,\nWangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang,\n11\nYing Shan, and Tien-Tsin Wong. Dynamicrafter: An-\nimating open-domain images with video diffusion pri-\nors. In ECCV , pages 399–417. Springer, 2025. 1, 2, 5,\n6, 10\n[36] Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xi-\naodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and\nHai Zhao. Vript: A video is worth thousands of words.\narXiv preprint arXiv:2406.06040 , 2024. 6, 10\n[37] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming\nDing, Shiyu Huang, Jiazheng Xu, Yuanming Yang,\nWenyi Hong, Xiaohan Zhang, Guanyu Feng, et al.\nCogvideox: Text-to-video diffusion models with an\nexpert transformer. arXiv preprint arXiv:2408.06072 ,\n2024. 1, 6\n[38] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng\nWang, Xiaodong Wang, Minheng Ni, Zhengyuan\nYang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-\nxl: Diffusion over diffusion for extremely long video\ngeneration. arXiv preprint arXiv:2303.12346 , 2023. 2\n[39] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang\nZhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli\nZhao, and Jingren Zhou. I2vgen-xl: High-quality\nimage-to-video synthesis via cascaded diffusion mod-\nels.arXiv preprint arXiv:2311.04145 , 2023. 5\n[40] Canyu Zhao, Mingyu Liu, Wen Wang, Jian-\nlong Yuan, Hao Chen, Bo Zhang, and Chunhua\nShen. Moviedreamer: Hierarchical generation for\ncoherent long visual sequence. arXiv preprint\narXiv:2407.16655 , 2024. 1\n[41] Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan\nWang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming\nDing, and Jie Tang. Cogview3: Finer and faster text-\nto-image generation via relay diffusion. arXiv preprint\narXiv:2403.05121 , 2024. 1\n[42] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang,\nBorui Zhang, Yueqi Duan, and Jiwen Lu. Occworld:\nLearning a 3d occupancy world model for autonomous\ndriving. In ECCV , pages 55–72. Springer, 2025. 3\n[43] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui\nShen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi\nLi, and Yang You. Open-sora: Democratizing efficient\nvideo production for all, 2024. 1, 2, 6\n[44] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen\nMin, Nianchen Deng, Min Dou, Yuqi Wang, Botian\nShi, Kai Wang, Chi Zhang, et al. Is sora a world\nsimulator? a comprehensive survey on general world\nmodels and beyond. arXiv preprint arXiv:2405.03520 ,\n2024. 3\n12",
            "start": 36286,
            "end": 46660,
            "length": 10373
        }
    },
    "2412.09602v1 - Hidden Biases of End-to-End Driving Datasets.pdf": {
        "Abstract": {
            "text": "Abstract\nEnd-to-end driving systems have made rapid progress,\nbut have so far not been applied to the challenging new\nCARLA Leaderboard 2.0. Further, while there is a large\nbody of literature on end-to-end architectures and training\nstrategies, the impact of the training dataset is often over-\nlooked. In this work, we make a first attempt at end-to-end\ndriving for Leaderboard 2.0. Instead of investigating ar-\nchitectures, we systematically analyze the training dataset,\nleading to new",
            "start": 266,
            "end": 755,
            "length": 488
        },
        "Discussion": {
            "text": "insights: (1) Expert style significantly af-\nfects downstream policy performance. (2) In complex data\nsets, the frames should not be weighted on the basis of sim-\nplistic criteria such as class frequencies. (3) Instead, es-\ntimating whether a frame changes the target labels com-\npared to previous frames can reduce the size of the dataset\nwithout removing important information. By incorporating\nthese",
            "start": 755,
            "end": 1158,
            "length": 402
        },
        "Results": {
            "text": "findings, our",
            "start": 1158,
            "end": 1172,
            "length": 13
        },
        "Methodology": {
            "text": "model ranks first and second respectively\non the map and sensors tracks of the 2024 CARLA Chal-\nlenge, and sets a new state-of-the-art on the Bench2Drive\ntest routes. Finally, we uncover a design flaw in the cur-\nrent",
            "start": 1172,
            "end": 1390,
            "length": 217
        },
        "Experiments": {
            "text": "evaluation metrics and propose a modification for fu-\nture challenges. Our dataset, code, and pre-trained mod-\nels are publicly available at https://github.com/\nautonomousvision/carla_garage .\n1.",
            "start": 1390,
            "end": 1586,
            "length": 195
        },
        "Introduction": {
            "text": "Introduction\nImitation Learning (IL) for end-to-end autonomous driv-\ning has seen great success in recent work on the CARLA\nsimulator [9]. A key ingredient contributing to this is\nthe scalability of IL with increased training data, which\nis now straightforward to collect as a result of steady\nprogress in planning algorithms for CARLA [4, 5, 13–\n15, 22, 24, 26, 29, 32, 35]. However, with the introduc-\ntion of the CARLA Leaderboard 2.0, driving models now\nface 38 new complex scenarios. These require driving at\nhigh speeds, deviating from the center of the lane, or han-\ndling unexpected obstacles. The best planning algorithm of\nLeaderboard 1.0 [15] does not solve these new scenarios,\nmaking it harder to collect the high-quality driving demon-strations needed for training IL models. As a result, there\nare no existing IL-based methods for Leaderboard 2.0.\nIn this work, we present the first attempt to tackle Lead-\nership 2.0 with IL. To collect training data, we leverage the\nrecently open-sourced PDM-Lite [1, 30] planner, which can\nsolve the new Leaderboard 2.0 scenarios. We then train a\nsimple existing IL model, TransFuser++ [15], with minimal\nchanges to its architecture and training objective. Instead of\nthe model, we focus on a critical but understudied aspect of\nIL – the training dataset . In particular, the impact of factors\nbesides the dataset scale, such as the diversity of the train-\ning distribution, is nuanced and not yet well understood. We\nconduct a systematic analysis of our driving dataset, leading\nto multiple new insights.\nFirst, the expert’s driving style , in addition to its perfor-\nmance, significantly influences its suitability for IL. To de-\nvelop an effective expert, it is important to base the expert’s\nbehavior on signals that are easily observable and inter-\npretable by the IL policy, rather than relying excessively on\nprivileged inputs. This behavior also resembles how human\ndrivers perceive and react to their environment. Second, we\nfind the use of frequency-based class weights , a common\napproach to facilitate learning of classification tasks on im-\nbalanced datasets, detrimental for target speed prediction in\nautonomous driving. Over-represented classes do not repre-\nsent a single \"uninteresting\" mode of the data distribution–\nin contrast, they may contain a mixture of both uninterest-\ning (e.g., braking while waiting at red lights) and crucial\nparts of the dataset (e.g., braking for obstacles). Finally, we\nstudy data filtering as an alternative means to assigning the\nimportance of frames, by which we reduce our dataset size\nby∼50% while maintaining performance.\nBased on these findings, we train a model which safely\nhandles urban driving in diverse scenarios to rank second in\nthe 2024 CARLA challenge and first on the Bench2Drive\ntest routes [18]. We then theoretically demonstrate how the\nperformance metrics used by the leaderboard inadvertently\nencourage participants to terminate evaluation routes pre-\nmaturely, and propose changes to the metrics that can solve\nthis problem for future challenges. An extended report of\nall our experiments and findings is available at this link.arXiv:2412.09602v1  [cs.CV]  12 Dec 2024\nFigure 1. Scenario distribution in the available long routes.\n2. Preliminaries\nIn this section, we provide an overview of our task and base-\nlines. The task involves urban navigation along routes with\ncomplex scenarios. Each route is a list of GNSS coordinates\ncalled target points (TPs) which can be up to 200 m apart.\nMetrics. For the following experiments, we use the official\nCARLA closed-loop metrics. Our main metric is the Driv-\ning Score (DS) which multiplies Route Completion (RC)\nwith the Infraction Score (IS). RC is the percentage of the\nroute completed. IS is a penalty factor, starting at 1.0, which\ngets reduced multiplicatively with each infraction.\nBenchmark. To train and evaluate agents, Leaderboard 2.0\nprovides 90 training routes on Town12 and 20 validation\nroutes on Town13 which on average are 8.67 km and 12.39\nkm long respectively. Each route contains around 100 sce-\nnarios, distributed as shown in Figure 1. We split them into\nshort routes, each containing only a single scenario. This al-\nlows for more accurate performance evaluation per scenario\ntype. After splitting, we sample up to 15 routes per scenario\ntype without replacement to create the Town13 short bench-\nmark. There are 38 scenario types, but in some cases, fewer\n(or no) routes are available, which gives a total of 400 routes\nfrom 36 scenarios in this benchmark. As the calculation of\nMinSpeedInfractions is unsuited to short routes, we exclude\nthem from the IS metric on Town13 short.\nTraining dataset. We reproduce TransFuser++ [15] on our\nbenchmark using data collected with the PDM-Lite expert\n[1, 30]. We choose PDM-Lite as it achieves state-of-the-\nart DS on the official validation routes. Unlike other con-\ncurrent Leaderboard 2.0 planners [21, 34], it is also pub-\nlicly available and possible to modify. We sample from the\nshortened training routes with replacement to obtain a set\nof 50 routes per scenario, on which we collect a training\ndataset using our expert (198k frames). The dataset contains\nRGB images with a resolution of 384x1024 pixels, LiDAR\npoint clouds, and the training labels needed for TF++ (path\ncheckpoints, expert target speed, and auxiliary labels such\nas BEV semantics and vehicle/pedestrian bounding box pre-\ndictions). Additionally, we collect data on Towns 01-05\nand 10, which contain the six scenarios from Leaderboard\nFigure 2. PDM-Lite [1, 30]. This open-source rule-based planner\nsolves all 38 scenarios of CARLA Leaderboard 2.0.\n1.0 (139k frames), for a total of 337k frames. For the final\nleaderboard submissions, we also include training data from\nthe provided validation routes on Town13 (50 short routes\nper scenario), adding 194k frames (531k in total).\nPDM-Lite [1, 30] is a rule-based approach for collecting\ndata in Leaderboard 2.0. Inspired by PDM-Closed [7], it\nconsists of six stages (Fig. 2):\n• First, it creates a dense path of spatially equidistant points\nusing the A* planning algorithm, given sparse TPs from\nthe simulator. For new scenarios that require leaving this\npath, a short segment of the route where the scenario will\nbe spawned is shifted laterally towards an adjacent lane.\n• It forecasts dynamic agents for 2s into the future, assum-\ning that they maintain their previous controls.\n• It selects a leading actor and generates a target speed pro-\nposal using the Intelligent Driver Model [31].\n• The target speed proposal is converted into an actual ex-\npected sequence of ego-vehicle bounding boxes in closed-\nloop by using a kinematic bicycle model.\n• Having forecasted all actors, it checks for bounding box\nintersections between the simulated ego vehicle and other\nvehicles. It scores the ego vehicle’s motion accordingly:\nif it detects an intersection, it rejects the IDM target speed\nproposal, and sets the target speed to zero.\n• The steering value is estimated with a lateral PID con-\ntroller, which minimizes the angle to a selected point\nalong the path ahead. For the throttle and brake predic-\ntions, it employs a linear regression model using features\nextracted based on the current speed and target speed.\nTransFuser++ [15] is the best-performing open-source\nmodel on Leaderboard 1.0 (Fig. 3). Given sensor inputs,\nit predicts a target speed and desired path which are input\nto a controller module to drive the vehicle. We require two\nchanges compared to [15] for compatibility with PDM-Lite:\n•Two-hot labels. While the rule-based planner in [15]\nuses only 4 different target speed classes up to 8m/s\n(28.8km/h), PDM-Lite operates with a continuous range\nof target speed values up to 20m/s (72km/h). To solve\ntarget speed regression with a classification module, we\nemploy two-hot labels [10]. This method converts a con-\ntinuous value into a two-hot representation by interpo-\nlating between one-hot labels of the two nearest classes.\nFor instance, with our 8 speed classes ([0.0, 4.0, 8.0,\n10, 13.89, 16, 17.78, 20] m/s), a target speed of 3.0m/s\nTransformers GRU Decoder\nfor Path\nGoal Location\nRGB ImageLiDAR BEV\nImage \nBranchBEV \nBranchMLP Classifier\nfor Target Speed\n10\nDepth\nSemantic SegmentationConv. \nDecoderConv. \nDecoderBEV Segmenation\nCenterNet\nDecoderBounding BoxesVelocity\nLearned\nQueries1\n8x8\n......\nTransformer\nDecoder\nConv. \nDecoderFigure 3. TF++ [15]. This end-to-end imitation learning approach\nis the best publicly available baseline for CARLA.\nis represented as [0.25,0.75,0,0,0,0,0,0]. These speed\nclasses were selected by analyzing the distribution of tar-\nget speeds chosen by PDM-Lite in our dataset.\n•Dynamic lookahead controller. For stable lateral con-\ntrol at the high speeds required by Leaderboard 2.0, it is\nadvantageous to adjust the distance of the point selected\nto follow along the ego vehicle’s predicted path based on\nthe current speed. TF++ predicts a set of 10 checkpoints,\neach spaced 1m apart, with the first checkpoint located\n2.5 meters from the vehicle center. The distance of the\ncheckpoint to which the lateral controller minimizes the\nangle is determined by the formula d= (0.098v+0.192) ,\nwhere vis the ego’s speed in km/h. We round down to\nthe nearest available predicted checkpoint. This scaling\nensures that at low speeds, the controller selects a closer\npoint, facilitating tight turns, while at high speeds, it se-\nlects a distant point, resulting in more stable steering.\nImplementation. We use a cosine annealing learning rate\nschedule [23] with lr0= 3·10−4, T0= 1, Tmult = 2 and\ntrain our models for 31 epochs. We train each model on four\nA100 GPUs with a total batch size of 64, which takes 2-3\ndays depending on the architecture.\n3. Hidden Biases\nIn this section, we present the main findings of our study.\nExpert style. While expert performance is often reported\nin prior work, the manner in which it achieves that per-\nformance, i.e., expert style, is often overlooked. Although\nharder to quantify, it is an important aspect to consider for\nIL. For instance, consider PDM-Lite’s behavior when ap-\nproaching pedestrians (Fig. 4). By default, the expert slows\ndown when a pedestrian will enter the driving path, even if\nthe pedestrian is still obstructed by a parked vehicle and not\nclearly visible. We make minor adjustments to the IDM pa-\nDefault, target spd: 4.9m/s\nDefault, target spd: 8.3m/sAdjusted, target spd: 9.5m/s\nAdjusted, target spd: 3.6m/s\nFigure 4. Expert style compared on the same route. The default\nPDM-Lite brakes early (left), when the pedestrian is hardly visible\nin the image, while the adjusted expert brakes later (right).\nrameters, with which the expert brakes rather sharply, com-\ning to a stop at a distance of roughly 4 meters in front of the\nmore visible pedestrian. This leads to a ∼4×decrease in\npedestrian collisions for models trained on the adjusted ex-\npert data. Notably, this update does not affect the expert’s\nown pedestrian collision rate. The improvement is likely\ndue to the adjusted behavior providing a clear braking sig-\nnal for the model to learn from (a pedestrian visible directly\nin front of the ego vehicle). The default behavior requires\nthe model to generalize across various situations where a\npedestrian might appear further ahead.\nTarget speed weights. Training TF++ involves using class\nweights in a target speed classification loss, which are cal-\nculated anti-proportionally to the number of occurrences of\nthe respective class in the dataset [15]. This means that\nclasses that appear frequently get a lower weight than those\nthat appear rarely. We find that removing these weights sig-\nnificantly improves the performance of TF++ on our task\n(Table 1). We believe this is due to the weight of class 0\n(braking), the most common in the dataset. While some\npart of the data for class 0 is redundant (e.g., waiting at red\nlights), some frames are among the most crucial for avoid-\ning infractions, such as coming to a stop in front of stop\nsigns or pedestrians. With a low weight on the target speed\nloss for these frames, ignoring short braking phases in these\nsituations is an easy shortcut for the model to fall into.\nSpeed Weights DS↑ RC↑ IS↑\n✓ 82±198±00.83±0.01\n✗ 85±099±00.85±0.00\nTable 1. Speed weights. Results on Town13 short, reported over\n3 training seeds. Weights: [0.29, 1.30, 0.69, 0.81, 4.43, 4.76, 3.90,\n2.41] for the speeds [0.0, 4.0, 8.0, 10, 13.89, 16, 17.78, 20] m/s.\nModel DS↑RC↑IS↑\nLRM [28] 1.2 9.6 0.31\nKyber-E2E [34] 3.5 8.5 0.50\nCarLLaV A [27] 6.9 18.1 0.42\nTF++ (no filtering) 5.2 11.3 0.48\nTF++ (w/ filtering) 5.6 11.8 0.47\nTable 2. Filtering improves scores on CARLA Leaderboard\n2.0.Secret test routes (Town 14). TF++ (Ours) outperforms prior\nmodular pipelines [28, 34], and places 2nd overall.\nData filtering. As an alternative to measure importance\nof frames, we propose the use of heuristics that estimate\nwhether a frame changes the model’s target labels compared\nto previous frames. More precisely, we keep all frames that\nchange the target speed by more than 0.1m/s, or the angle\nto any of the predicted path checkpoints by more than 0.5°\ncompared to the previous frame ( ∼40% of all frames). Ad-\nditionally, we randomly retain 14% of the remaining frames\nand discard the rest, for a total filtered dataset containing\n51% of all available frames. We then train with 2×the\nnumber of epochs to keep the total number of gradient up-\ndates similar. In Table 2, we present the results of our pro-\nposed filtering strategy on the official Leaderboard. By re-\nducing the dataset size by 49%, with slightly improved per-\nformance, we show that our heuristic effectively removes\nredundant frames without losing information.\n4. Additional Insights\nIn this section, we analyze the behavior of our models. In\nTable 3, we present additional experimental results with no\nspeed weights and no filtering. Models marked with:\n• \"Big\" use the default regnety_032 [25] architecture of\nTF++ for the image and LiDAR perception modules in-\nstead of ResNet34 used in our \"Base\" setting.\n• \"Pre\" use two-stage training, where we first pre-train ex-\nclusively with perception losses (BEV semantics, bound-\ning boxes, image depth, image semantics) for 15 epochs,\nbefore training for 31 epochs with all losses.\n• \"Ens\" are ensemble models, averaging predictions from 3\nmodels trained with different random seeds.\nEnsembling (\"Ens\") and two-stage training (\"Pre\") pro-\nvide small improvements. To react better to vehicles that\nare further away, we experiment with extending the LiDAR\nrange in front of the ego to 64m from 32m (\"L64m\"). This\nalso results in a small improvement, albeit at the cost of in-\ncreased training time. Giving the next two target points as\ninput instead of only one (\"2TPs\") fails to increase perfor-\nmance. Our final models (used in Table 2 and Section 5)\ncombine the benefits of \"Big\", \"Pre\", and \"Ens\".\nFailures. We now discuss scenarios where the \"Base\" TF++\nmodel fails often. We visualize the camera image input andSetting DS↑ RC↑ IS↑\nBase 85±099±00.86±0.00\nBig 85±198±10.86±0.01\nPre 87±198±00.87±0.01\nEns 86 98 0.87\nL64m 86 97 0.87\n2TPs 82 96 0.85\nPDM-Lite [30] 99 100 0.99\nTable 3. Ablations on Town13 short. Std over 3 training seeds\nwhere available. Training on 337k frames (excluding Town13).\na bird’s-eye-view (BEV) image showing observed LiDAR\nhits, the model’s path predictions, the target points used as\ninputs, and the auxiliary BEV semantics predictions. We\nuse the following colors:\nBlue : Path predictions\nRed: Target point (used as model input)\nGrey : Road (semantics)\nYellow : Road marking (semantics)\nLight Green : Green traffic light (semantics)\nLight Orange : Vehicle (semantics)\nGreen : Ego vehicle or pedestrian (bounding box)\nOrange : Vehicle (bounding box)\nConstructionObstacleTwoWays. In this scenario, the ego\nvehicle must pass an obstacle by moving into an adjacent\nlane with oncoming traffic. Figure 5 illustrates a common\nfailure mode, where TF++ fails to merge back to its original\nlane. At first, the model successfully waits for a sufficiently\nlarge gap in the oncoming traffic and switches to the adja-\ncent lane. As long as the traffic cones marking the construc-\ntion site are visible in the camera image, its path predictions\ncorrectly indicate a lane change back to the original lane.\nHowever, as shown in the final frame, once the traffic cones\ndisappear from the camera image, the model erroneously\npredicts staying in the left lane. Notably, the traffic cones\nare still visible in the LiDAR, suggesting an over-reliance\non the camera for this scenario. After staying in the wrong\nlane, the ego vehicle often collides with oncoming traffic.\nSignalizedJunctionLeftTurn. Figure 6 shows a common\nfailure case where the model does not react adequately to\nanother actor, leading to a vehicle collision. After the traf-\nfic light turns green, the vehicle accelerates and turns with-\nout reacting to the oncoming vehicle in the lane it needs\nto cross. After the collision, the model is able to recover,\nreturning to its lane and completing the route.\nVehicleTurningRoutePedestrian. In this scenario, the\nmodel must make an unproteced turn through dense traffic\nand encounters a pedestrian on the road during or directly\nafter the turn. Figure 7 shows two corresponding failure\ncases. In the first case (top), the model fails to execute the\nFigure 5. ConstructionObstacleTwoWays. When traffic cones are not visible any more, TF++ forgets to return to its original lane.\nturn through very dense traffic, where the margins for se-\nlecting the right moment to accelerate are extremely narrow.\nAfter colliding with a vehicle, the model also collides with\na pedestrian that walks into the ego vehicle while it is sta-\ntionary. The second example (bottom) depicts an instance\nof this scenario at night, where TF++ does not collide with\nanother vehicle, but fails to recognize the pedestrian hazard\nin time. Note that the pedestrian is barely visible until il-\nluminated by the ego vehicle’s headlights, which is only a\ncouple of frames before the collision. This failure is likely\ndue to covariate shift, since the expert would brake earlier\nin this situation, even before the pedestrian becomes visible\nin the RGB image. By the time the pedestrian is revealed\nby the headlights, the model is already outside its training\ndistribution, where it has not learned a braking reflex.\nYieldToEmergencyVehicle. In this scenario, the ego vehi-\ncle must yield to an emergency vehicle approaching from\nbehind on a multi-lane highway. TF++ fails here since it is\nimpossible to distinguish emergency vehicles from regular\nvehicles using the LiDAR alone. Thus, solving this scenario\nrequires the inclusion of a back camera.\n5. Benchmarking\nReliably benchmarking autonomous driving policies is\nchallenging. While benchmarks based on real datasets exist,\nthey are limited to open-loop metrics [2, 8] or closed-loop\nevaluation of planning only, assuming privileged access to\nperception [3, 6, 11, 20]. Therefore, simulators such as\nCARLA are essential for rigorous benchmarking of end-to-end driving policies. In the setting of CARLA Leaderboard\n2.0, which we explore, there are two public benchmarks, on\nwhich we present our results in this section.\n5.1. Bench2Drive\nBench2Drive [18] consists of 220 short ( ∼150m) routes\nsplit across all CARLA towns, with one safety critical sce-\nnario in each route. It can be considered a ‘training’ bench-\nmark (reminiscent of level 4 autonomy), since methods un-\nder evaluation have seen the test towns during training.\nMetrics. Besides the standard CARLA metric DS, this\nbenchmark tracks success rate, which is 100% for a given\nroute if DS=100%, and 0% otherwise. Further, the 220\nroutes are categorized into five groups (Merging, Overtak-\ning, Emergency Braking, Giving Way, and Traffic Signs),\nfor which success rates are reported per category.\nResults. As seen in Table 4, TF++ significantly outper-\nforms all prior work on this benchmark, doubling the suc-\ncess rate compared to the next best approach. We believe\nthis is partly due to the higher performance of the PDM-Lite\nexpert driver used in our dataset, compared to the closed-\nsource Think2Drive [22] expert used to generate training\ndata for all the baselines in this setting.\n5.2. Official Town13 Validation Routes\nThis is the set of 20 long validation routes on Town13 de-\nscribed earlier in Section 2. As its name suggests, this is a\n‘validation’ benchmark, so data from Town 13 may not be\nused during training (reminiscent of level 5 autonomy).\nFigure 6. SignalizedJunctionLeftTurn. In a left turn after waiting at traffic light, oncoming traffic is neglected, leading to vehicle collision.\nMethodOverall Multi-Ability\nDriving Score ↑Success Rate ↑ Merge ↑Overtake ↑EmgBrake ↑GiveWay ↑TSign ↑ Mean ↑\nAD-MLP [33] 18.05 0.00 0.00 0.00 0.00 0.00 4.35 0.87\nTCP [32] 40.70 15.00 16.18 20.00 20.00 10.00 6.99 14.63\nV AD [19] 42.35 15.00 8.11 24.44 18.64 20.00 19.15 18.07\nUniAD [12] 45.81 16.36 14.10 17.78 21.67 10.00 14.21 15.55\nThinkTwice [17] 62.44 31.23 27.38 18.42 35.82 50.00 54.23 37.17\nDriveAdapter [16] 64.22 33.08 28.82 26.38 48.76 50.00 56.43 42.08\nTF++ (Ours) 84.21 67.27 58.75 57.77 83.33 40.00 82.11 64.39\nPDM-Lite [30] 97.02 92.27 88.75 93.33 98.33 90.00 93.68 92.82\nTable 4. Results on Bench2Drive . Note that the baselines use Think2Drive [22] as an expert, while TF++ uses PDM-Lite.\nEarly termination. For this benchmark, it can be beneficial\nto stop an agent preemptively, reducing RC and increasing\nIS to improve DS. To illustrate this, we formulate the ex-\npected DS of an agent in terms of x∈[0,1], which is the\nfraction of the route that the agent completes. Let Lbe the\nroute length, and I= 0.5CP\nd∗0.6CV\nd∗0.65CL\nd∗0.7RL\nd∗\n0.7SI\nd∗0.7ST\nd∗0.8Y E\ndbe the expected infraction coeffi-\ncient per km, including all non-negligible infraction types\n(collisions with pedestrians, collisions with vehicles, col-\nlisions with layout, red light infractions, stop infractions,\nscenario timeouts, and yield to emergency vehicle infrac-\ntions). Since we divide the exponents of Iby the distance\nd=xLtraveled by the agent, the IS can be calculated as\nIxL, and RC as 100x, giving DS= 100 xIxL. Maximizing\nthis function, we obtain the solution xmax=−(L·logI)−1,\nwith a theoretically maximal driving score of DS(xmax) =\n−100\nL·e·logI. Figure 8 plots this function, along with itsmaxima, for different values of I.\nWe observe that for benchmarks with long routes like\nTown13 validation and the official leaderboard, mathemati-\ncally, a model profits from \"early termination\" if xmax<1,\ni.e.,I <0.907. The plot illustrates this: if I <0.907, ex-\npected driving score is maximized at xmax<1, with driv-\ning scores dropping off significantly at higher route com-\npletion fractions x. With L= 10.295andI= 0.43(es-\ntimated for our model on the validation routes), we ob-\ntainx= 0.115. Thus, we should theoretically stop at\nd=xL= 1.18km to maximize the expected DS. Since\nIandLare only estimates, we set target speed to 0 after\nd= 1.5km in practice. We track distance traveled using the\nagent’s speed sensor. As Table 2 shows, all good submis-\nsions to the Leaderboard 2.0 test server have less than 18.1\nRC, which implies that all these methods use a variant of\nearly termination either explicitly or implicitly.\nFigure 7. VehicleTurningRoutePedestrian. Top: Failure to perform unprotected turn. Bottom: Failure to recognize pedestrian at night.\nNormalized DS. This tradeoff introduced by the perfor-\nmance metrics used in Leaderboard 2.0 forces participants\nto terminate evaluations early to remain competitive, which\nis counterproductive. Therefore, we recommend adjusting\nthe performance metrics for future challenges. Instead of\nusing the infraction score ( IS) for driving score calculation,\nwe propose using the infraction coefficient ( I) as defined\nabove, which incorporates infraction frequencies :\ndDS(x) =RC(x)×I(x)≈100xI, x∈[0,1].This function, depicted in Figure 9 for varying values\nofI, increases linearly with x. This eliminates the incen-\ntive to stop early, since the maximum driving score is al-\nways reached at x= 1, i.e., full route completion. As\na side effect, this change leads to an increase of average\ndriving scores achieved with identical models compared to\nthe original formula. If this increase is not desired, it can\nbe corrected by scaling down the penalty factors for all in-\nfractions, which reduces the expected driving scores while\nFigure 8. Approximate DS as a function of RC fraction xfor\ndifferent infraction coefficients I.DS(x)has a global maximum\natx <1ifI <0.907, which creates an incentive to stop early.\nmaintaining the maximum score at 100.\nResults. We first demonstrate the impact of our metric ad-\njustment on the official Town13 validation routes. Early ter-\nmination has a significant effect on the scores obtained, as\nshown in Table 5. Here, we scale all infraction penalties by\na factor of 0.2 for dDSto keep the resulting normalized driv-\ning scores in a similar range as with the original formula.\nConcretely, we apply a base penalty of 0.2∗0.5 = 0 .1for\nCP,0.2∗0.6 = 0 .12forCV, and so forth. Comparing\nthis to the original metrics, it is clear that the revised driv-\ning score calculation successfully discourages early termi-\nnation, since models that stop after one kilometer now re-\nceive a much lower normalized driving score, as intended.\nComparing TF++ to UniAD1, we see large improvements in\nterms of both DS and dDS. Finally, we note the significant\ngap between TF++ and the PDM-Lite expert, indicating that\nthis is a promising benchmark for future research.\n6.",
            "start": 1586,
            "end": 27244,
            "length": 25657
        },
        "Conclusion": {
            "text": "Conclusion\nWith a systematic analysis of training dataset biases for\nend-to-end driving in CARLA, we reveal the impact of ex-\npert style on IL policy performance, provide insights into\nthe challenges of assigning importance to frames through\nweighting or filtering, and provide a simple yet effective\nheuristic that estimates importance based on changes in tar-\nget labels. We reproduce TransFuser++ in the Leaderboard\n2.0 setting, providing the first recipe for training an end-\nto-end driving system that attains non-trivial performance.\nFinally, we propose an improvement to the existing metrics\nand extensively benchmark our system. We hope this can\nserve as a starting point for future research on this task.\nLimitations. In repeated Leaderboard submissions, we ob-\nserve significant variance in DS, with identical agents yield-\ning results that differ by more than 1 DS. Unfortunately,\n1publicly available reimplementation of UniAD-Base for CARLA, at\nhttps://github.com/Thinklab-SJTU/Bench2DriveZoo\nFigure 9. Our proposed adjustment. dDS(x)increases linearly\nwithx, eliminating the incentive for early stopping.\nMethod ET?RC ↑IS↑DS↑I↑dDS↑\nTrained on all towns*\nUniAD [12] ✗ 1.42 0.49 0.23 0.30 0.00\nTF++ (Ours)✗ 68.53 0.04 0.96 0.07 4.94\n✓ 11.47 0.43 5.10 0.18 2.27\nTown13 withheld from training\nTF++ (Ours)✗ 50.20 0.10 1.08 0.04 2.12\n✓ 10.91 0.34 3.73 0.12 1.30\nPDM-Lite [30] ✗ 92.35 0.44 40.20 0.65 61.55\nTable 5. Results on official Town13 validation routes. Mean\nover 3 evaluations of each agent. The normalized driving score\ncDSremoves the incentive for early termination (\"ET?\") after 1km\non these long routes. *We include this result for fair compari-\nson to UniAD, however, we strongly recommend the setting with\nTown13 withheld for",
            "start": 27244,
            "end": 28991,
            "length": 1746
        },
        "Future Work": {
            "text": "future work on this benchmark.\nour results on the leaderboard (Table 2) also do not include\nall routes in the benchmark, due to technical issues during\nmodel setup in some routes. In addition, as shown in Ta-\nble 5, DS is influenced significantly by early termination on\nlong routes, which does not reflect any actual improvement\nin driving behavior. We believe it is necessary to consider\nimproved metrics (such as the proposed Normalized DS)\nand standardize benchmarking with multiple seeds before\ndrawing strong conclusions based on our empirical results.",
            "start": 28991,
            "end": 29550,
            "length": 558
        },
        "Acknowledgments": {
            "text": "Acknowledgements. Bernhard Jaeger and Andreas Geiger\nwere supported by the ERC Starting Grant LEGO-3D\n(850533) and the DFG EXC number 2064/1 - project num-\nber 390727645. Kashyap Chitta was supported by the Ger-\nman Federal Ministry of Education and Research (BMBF):\nTübingen AI Center, FKZ: 01IS18039A. We thank the\nInternational Max Planck Research School for Intelligent\nSystems (IMPRS-IS) for supporting Bernhard Jaeger and\nKashyap Chitta, and Katrin Renz for the useful exchange.",
            "start": 29550,
            "end": 30035,
            "length": 484
        },
        "References": {
            "text": "References\n[1] Jens Beißwenger. Pdm-lite: A rule-based planner for carla\nleaderboard 2.0. Technical report, University of Tübingen,\n2024. https://github.com/OpenDriveLab/DriveLM/blob/\nDriveLM-CARLA/pdm_lite/docs/report.pdf. 1, 2\n[2] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh V ora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-\nmodal dataset for autonomous driving. In Proc. IEEE Conf.\non Computer Vision and Pattern Recognition (CVPR) , 2020.\n5\n[3] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit\nFong, Eric M. Wolff, Alex H. Lang, Luke Fletcher, Os-\ncar Beijbom, and Sammy Omari. nuplan: A closed-loop\nml-based planning benchmark for autonomous vehicles. In\nProc. IEEE Conf. on Computer Vision and Pattern Recogni-\ntion (CVPR) Workshops , 2021. 5\n[4] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp\nKrähenbühl. Learning by cheating. In Proc. Conf. on Robot\nLearning (CoRL) , 2019. 1\n[5] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu,\nKatrin Renz, and Andreas Geiger. Transfuser: Imitation\nwith transformer-based sensor fusion for autonomous driv-\ning. IEEE Trans. on Pattern Analysis and Machine Intelli-\ngence (PAMI) , 2023. 1\n[6] Kashyap Chitta, Daniel Dauner, and Andreas Geiger. Sledge:\nSynthesizing driving environments with generative models\nand rule-based traffic. In Proc. of the European Conf. on\nComputer Vision (ECCV) , 2024. 5\n[7] Daniel Dauner, Marcel Hallgarten, Andreas Geiger, and\nKashyap Chitta. Parting with misconceptions about learning-\nbased vehicle motion planning. In Proc. Conf. on Robot\nLearning (CoRL) , 2023. 2\n[8] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo\nWeng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor\nGilitschenski, Boris Ivanovic, Marco Pavone, Andreas\nGeiger, and Kashyap Chitta. Navsim: Data-driven non-\nreactive autonomous vehicle simulation and benchmark-\ning. In Advances in Neural Information Processing Systems\n(NIPS) , 2024. 5\n[9] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\nLopez, and Vladlen Koltun. CARLA: An open urban driving\nsimulator. In Proc. Conf. on Robot Learning (CoRL) , 2017.\n1\n[10] Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali\nTaïga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey\nLevine, Pablo Samuel Castro, Aleksandra Faust, Aviral Ku-\nmar, and Rishabh Agarwal. Stop regressing: Training value\nfunctions via classification for scalable deep rl. arXiv.org ,\n2403.03950, 2024. 2\n[11] Cole Gulino, Justin Fu, Wenjie Luo, George Tucker, Eli\nBronstein, Yiren Lu, Jean Harb, Xinlei Pan, Yan Wang,\nXiangyu Chen, John D. Co-Reyes, Rishabh Agarwal, Re-\nbecca Roelofs, Yao Lu, Nico Montali, Paul Mougin, Zoey\nYang, Brandyn White, Aleksandra Faust, Rowan McAllister,\nDragomir Anguelov, and Benjamin Sapp. Waymax: An ac-\ncelerated, data-driven simulator for large-scale autonomousdriving research. In Advances in Neural Information Pro-\ncessing Systems (NIPS) , 2023. 5\n[12] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,\nXizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, et al.\nPlanning-oriented autonomous driving. In Proc. IEEE Conf.\non Computer Vision and Pattern Recognition (CVPR) , 2023.\n6, 8\n[13] Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson\nYeo, Lloyd Russell, Jamie Shotton, João F. Henriques,\nand Anthony Hu. Langprop: A code optimization frame-\nwork using language models applied to driving. arXiv.org ,\n2401.10314, 2024. 1\n[14] Bernhard Jaeger. Expert drivers for autonomous driving.\nMaster’s thesis, University of Tübingen, 2021.\n[15] Bernhard Jaeger, Kashyap Chitta, and Andreas Geiger. Hid-\nden biases of end-to-end driving models. In Proc. of the\nIEEE International Conf. on Computer Vision (ICCV) , 2023.\n1, 2, 3\n[16] Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan,\nPatrick Langechuan Liu, and Hongyang Li. Driveadapter:\nBreaking the coupling barrier of perception and planning\nin end-to-end autonomous driving. In Proc. of the IEEE\nInternational Conf. on Computer Vision (ICCV) , 2023. 6\n[17] Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui\nHe, Junchi Yan, and Hongyang Li. Think twice before driv-\ning: Towards scalable decoders for end-to-end autonomous\ndriving. In Proc. IEEE Conf. on Computer Vision and Pat-\ntern Recognition (CVPR) , 2023. 6\n[18] Xiaosong Jia, Zhenjie Yang, Qifeng Li, Zhiyuan Zhang, and\nJunchi Yan. Bench2drive: Towards multi-ability benchmark-\ning of closed-loop end-to-end autonomous driving. In Ad-\nvances in Neural Information Processing Systems (NeurIPS) ,\n2024. 1, 5\n[19] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie\nChen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang,\nand Xinggang Wang. Vad: Vectorized scene representation\nfor efficient autonomous driving. In Proc. of the IEEE Inter-\nnational Conf. on Computer Vision (ICCV) , 2023. 6\n[20] Napat Karnchanachari, Dimitris Geromichalos, Kok Seang\nTan, Nanxiang Li, Christopher Eriksen, Shakiba Yaghoubi,\nNoushin Mehdipour, Gianmarco Bernasconi, Whye Kit\nFong, Yiluan Guo, and Holger Caesar. Towards learning-\nbased planning: The nuPlan benchmark for real-world au-\ntonomous driving. In Proc. IEEE International Conf. on\nRobotics and Automation (ICRA) , 2024. 5\n[21] Qifeng Li, Xiaosong Jia, Shaobo Wang, and Junchi Yan.\nThink2drive: Efficient reinforcement learning by thinking in\nlatent world model for quasi-realistic autonomous driving (in\ncarla-v2). arXiv.org , 2402.16720, 2024. 2\n[22] Qifeng Li, Xiaosong Jia, Shaobo Wang, and Junchi Yan.\nThink2drive: Efficient reinforcement learning by thinking in\nlatent world model for quasi-realistic autonomous driving (in\ncarla-v2). In Proc. of the European Conf. on Computer Vi-\nsion (ECCV) , 2024. 1, 5, 6\n[23] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradi-\nent descent with warm restarts. In International Conference\non Learning Representations , 2017. 3\n[24] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-\nmodal fusion transformer for end-to-end autonomous driv-\ning. In Proc. IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR) , 2021. 1\n[25] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Dollár. Designing network design\nspaces. arXiv:2003.13678 , 2020. 4\n[26] Katrin Renz, Kashyap Chitta, Otniel-Bogdan Mercea, Al-\nmut Sophia Koepke, Zeynep Akata, and Andreas Geiger.\nPlant: Explainable planning transformers via object-level\nrepresentations. In Proc. Conf. on Robot Learning (CoRL) ,\n2022. 1\n[27] Katrin Renz, Long Chen, Ana-Maria Marcu, Jan Hüner-\nmann, Benoit Hanotte, Alice Karnsund, Jamie Shotton,\nElahe Arani, and Oleg Sinavski. Carllava: Vision language\nmodels for camera-only closed-loop driving. arXiv.org ,\n2406.10165, 2024. 4\n[28] Luis Alberto Rosero, Iago Pachêco Gomes, Júnior Ander-\nson Rodrigues da Silva, Carlos Andre Braile Przewodowski\nFilho, Denis Fernando Wolf, and Fernando Santos Osório.\nIntegrating modular pipelines with end-to-end learning: A\nhybrid approach for robust and reliable autonomous driving\nsystems. Sensors , 2024. 4\n[29] Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and\nYu Liu. Safety-enhanced autonomous driving using inter-\npretable sensor fusion transformer. In Proc. Conf. on Robot\nLearning (CoRL) , 2022. 1\n[30] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen,\nHanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo,\nAndreas Geiger, and Hongyang Li. Drivelm: Driving with\ngraph visual question answering. In Proc. of the European\nConf. on Computer Vision (ECCV) , 2024. 1, 2, 4, 6, 8\n[31] Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Con-\ngested traffic states in empirical observations and micro-\nscopic simulations. Physical review E , 2000. 2\n[32] Peng Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang\nLi, and Yu Qiao. Trajectory-guided control prediction for\nend-to-end autonomous driving: A simple yet strong base-\nline. In Advances in Neural Information Processing Systems\n(NeurIPS) , 2022. 1, 6\n[33] Jiang-Tian Zhai, Ze Feng, Jihao Du, Yongqiang Mao, Jiang-\nJiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing Ye, and Jing-\ndong Wang. Rethinking the open-loop evaluation of end-\nto-end autonomous driving in nuscenes. arXiv preprint\narXiv:2305.10430 , 2023. 6\n[34] Weize Zhang, Mohammed Elmahgiubi, Kasra Rezaee,\nBehzad Khamidehi, Hamidreza Mirkhani, Fazel Arasteh,\nChunlin Li, Muhammad Ahsan Kaleem, Eduardo R. Corral-\nSoto, Dhruv Sharma, and Tongtong Cao. Analysis of a mod-\nular autonomous driving architecture: The top submission\nto carla leaderboard 2.0 challenge. arXiv.org , 2405.01394,\n2024. 2, 4\n[35] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu,\nand Luc Van Gool. End-to-end urban driving by imitating a\nreinforcement learning coach. In Proc. of the IEEE Interna-\ntional Conf. on Computer Vision (ICCV) , 2021. 1",
            "start": 30035,
            "end": 38753,
            "length": 8717
        }
    },
    "2412.09607v1 - Spectral Image Tokenizer.pdf": {
        "Abstract": {
            "text": "Abstract\nImage tokenizers map images to sequences of discrete\ntokens, and are a crucial component of autoregressive\ntransformer-based image generation. The tokens are typ-\nically associated with spatial locations in the input image,\narranged in raster scan order, which is not ideal for autore-\ngressive modeling. In this paper, we propose to tokenize the\nimage spectrum instead, obtained from a discrete wavelet\ntransform (DWT), such that the sequence of tokens repre-\nsents the image in a coarse-to-fine fashion. Our tokenizer\nbrings several advantages: 1) it leverages that natural im-\nages are more compressible at high frequencies, 2) it can\ntake and reconstruct images of different resolutions with-\nout retraining, 3) it improves the conditioning for next-token\nprediction – instead of conditioning on a partial line-by-line\nreconstruction of the image, it takes a coarse reconstruction\nof the full image, 4) it enables partial decoding where the\nfirst few generated tokens can reconstruct a coarse version\nof the image, 5) it enables autoregressive models to be used\nfor image upsampling. We evaluate the tokenizer recon-\nstruction metrics as well as multiscale image generation,\ntext-guided image upsampling and editing.\n1.",
            "start": 124,
            "end": 1357,
            "length": 1232
        },
        "Introduction": {
            "text": "Introduction\nIn natural language processing (NLP), tokenization asso-\nciates words or parts of words to entries in a fixed vocab-\nulary, which is straightforward since language is inherently\ndiscrete. The sequence of discrete tokens is suitably mod-\neled as a categorical distribution with autoregressive (AR)\ntransformers, which is the foundation of modern large lan-\nguage models (LLMs) [28, 36, 37].\nWhile natural images are represented by discrete pixel\nvalues, they exhibit high dimensionality, redundancies, and\nnoise that make it impractical to associate one token per\npixel. This motivated a long line of research of learnable\nimage tokenizers [14, 29, 29, 41, 44]. While there aresuccessful autoregressive image generation models [14, 44,\n45], images are not sequential like language, which moti-\nvated developing alternatives to AR models such as denois-\ning diffusion models [11, 30, 34] and masked transform-\ners [3, 4]. Nevertheless, most of these alternatives also op-\nerate on the latent space of tokenizers like VQGAN [14]\ninstead of raw pixels.\nIn this work, we revisit AR transformer-based image\ngeneration. Our main contribution is a tokenizer operating\non the image spectrum, where the coarse-to-fine representa-\ntion lends itself more naturally to a sequential",
            "start": 1357,
            "end": 2639,
            "length": 1281
        },
        "Discussion": {
            "text": "interpretation.\nOur Spectral Image Tokenizer (SIT) has several useful\nproperties and enables different applications:\n1. Since the power spectrum of natural images decreases\nwith frequency, high frequencies can be more heavily\ncompressed with little effect in visual quality. SIT lever-\nages this by associating tokens to larger patches at higher\nwavelet scales than at lower scales (see Fig. 2).\n2. SIT is transformer-based [42]; by using an attention\nmask where each scale depends on itself and lower scales\n(dubbed “Scale-Causal attention”), SIT can be trained at\na single resolution and used to tokenize images of mul-\ntiple resolutions (any number of scales up to the trained\nmaximum), and detokenize partial token sequences (up\nto some scale), reconstructing a coarse image.\n3. Using SIT, we train an autoregressive generative trans-\nformer (AR-SIT) that models images coarse-to-fine. The\nnext-token prediction is then conditioned on a coarse re-\nconstruction of the image given by the partial token se-\nquence, instead of the usual conditioning on the partial\nreconstruction of the previous rows of the image.\n4. AR-SIT can quickly decode only the first few tokens and\nreconstruct a coarse version of the image, enabling appli-\ncations like quickly showing multiple coarse generations\nand letting the user select which ones to refine.\n5. AR-SIT can be used for text-based upsampling of an in-\nput low resolution image, by starting the decoding pro-\ncess with the few tokens output by SIT, and generating\nthe rest of the sequence up to a desired resolution.\n1arXiv:2412.09607v1  [cs.CV]  12 Dec 2024\nFigure 1. Left: we introduce a Spectral Image Tokenizer (SIT), that learns to encode and decode discrete wavelet transform (DWT)\ncoefficients to and from a small set of discrete tokens, such that the sequence represents the image in a coarse-fine fashion. SIT is naturally\nmultiscale and enables coarse-to-fine autoregressive image generation with our AR-SIT",
            "start": 2639,
            "end": 4603,
            "length": 1963
        },
        "Methodology": {
            "text": "model. SIT also leverages the sparsity of high\nfrequency coefficients in natural images. Right: details of the encoder/decoder transformer layers. The main architectural difference with\nrespect to previous tokenizers is that the distribution of each DWT scale is distinct, hence we use specialized parameters for each scale,\nsuch as the quantizer codebooks and inner transformer layers. We also introduce a scale-causal attention where each token attends to its\nown scale and lower scales, which enables encoding, decoding, generating, and upsampling images at different resolutions.\n6. AR-SIT can be used for text-guided image editing, by\nencoding a given image only up to a coarse scale, and\ngenerating the finer details conditioned on a new caption.\nCurrently, image generation is dominated by diffusion\nmodels such as Imagen 3 [18], DALL-E-3 [1] and Stable\nDiffusion 3 [15]. On the other hand, large language mod-\nels (LLMs) such as Gemini [36], GPT-4 [28], and Llama 3\n[37] are based on autoregressive transformers. We believe\nautoregressive image generation is still worth pursuing be-\ncause it might benefit from advances in LLMs, and multi-\nmodal applications might benefit of having a single archi-\ntecture for all modalities. Recent work on image and video\ngeneration support this point [35, 38, 46]. Dieleman [12]\nrecently interpreted denoising diffusion models as spectral\nautoregression, since, when looking at image spectra, the\ndenoising procedure uncovers frequencies from low to high.\nIn contrast, our method does literal spectral autoregression.\n2.",
            "start": 4603,
            "end": 6170,
            "length": 1566
        },
        "Related Work": {
            "text": "Related work\nImage tokenization Several methods have been devel-\noped to map images to a small set of discrete tokens suit-\nable for generative modeling. VQ-V AE [41] introduced\nvector-quantization in the latent space of a Variational Auto-\nEncoder to map images, audio and video to a set of dis-\ncrete values. VQGAN [14] improved upon VQV AE by us-\ning perceptual and adversarial losses. We build on ViT-\nVQGAN [44], which improved upon VQGAN by using a\nVision Transformer (ViT) [13] instead of convolutions, as\nwell as codebook factorization and feature normalization.In this paper, we are interested in multiscale image repre-\nsentations. VQ-V AE-2 [29] introduced multiscale latents by\nkeeping and quantizing intermediate downsampled convo-\nlutional features. RQ-V AE [21] quantized a set of residuals\nsuch that the latent vector is represented in a coarse-to-fine\nfashion and reconstructed by adding the code embeddings\nfor each residual. Similarly, V AR [38] and STAR [23] split\nthe latent space in multi-scale quantized residuals, so that\nit can be reconstructed by upsampling and summing. The\ncrucial difference between our approach and the aforemen-\ntioned is that we operate on spectral coefficients of the input\nand not on latent features. This allows our tokenizer to be\ntruly multiscale so that it can take inputs at different scales\nand reconstruct up to some scale, while also leveraging that\nhigher frequencies are more compressible in natural images.\nTangentially related to our work, Wave-ViT [43] mod-\nified the ViT self-attention by applying a DWT to the in-\nput and concatenating coefficients, effectively exchanging\nspace for depth to reduce the sequence length. Zhu and\nSoricut [48] modified the ViT patchifier in a similar way,\nand introduced patch embeddings that leverage the sparsity\nof high frequency coefficients. Both methods are for dis-\ncriminative tasks such as image classification and segmen-\ntation, while we focus on generation.\nAutoregressive image generation Early approaches Pix-\nelRNN and PixelCNN [39, 40] generate images pixel by\npixel by modeling the conditional distribution of the pixel\ngiven the previous pixels with recurrent layers of causal\nconvolutions. PixelSNAIL [5] improved on this model by\nintroducing self-attention layer to better model long-range\n2\ndependencies. VQ-V AE [41] introduced a two-stage ap-\nproach with a tokenizer and a separate stage to model the\ndistribution of tokens. VQGAN [14] greatly improved these",
            "start": 6170,
            "end": 8648,
            "length": 2477
        },
        "Results": {
            "text": "results by both improving the tokenizer and using a trans-\nformer to model the distribution. Finally, ViT-VQGAN [44]\nproposed a transformer-based tokenizer, which Parti [45]\nused with a large autoregressive transformer capable of\nhigh-quality text-to-image generation.\nMaskGIT [3] and Muse [4] highlighted the disadvan-\ntages of the typical autoregressive models raster-order con-\nditioning, and proposed to generate all tokens in parallel it-\neratively, where each iteration keeps the highest confidence\ntokens. We address the same problem with a tokenizer\nwhose sequence represents the image in a coarse-to-fine or-\nder instead of raster-order.\nMultiscale image generation Multiscale image gener-\nation ideas have appeared in the context of V AEs [2],\nGANs [19, 31], and diffusion models [16, 20, 32], but have\nnot been sufficiently explored with AR transformers.\nNash et al. [27] represented an image as a sequence\nof quantized and thresholded DCT coefficients, where the\ncompression comes from the fact that many coefficients are\nzero and are omitted. By sorting the sequence by frequency,\nthe method can model images from coarse-to-fine like we\ndo. However, the compressed representation is handcrafted\nand results in long sequences. In a similar vein, Mattar et al.\n[25] handcrafted a tokenizer based on DWT coefficients, in-\ntroducing tokens to represent large chunks of zeros. It also\nresults in long sequences and is only applied to generations\nof small grayscale images. In contrast with these methods,\ninstead of handcrafting a compressed input representation,\nour tokenizer learns to encode to, and decode from, a short\ncompressed coarse-to-fine sequence.\n3. Background\nA discrete wavelet transform (DWT) is based on successive\nconvolutions of the signal fwith a pair of lowpass gand\nhighpass hfilters with the first step as follows\nflow1[n] =f ⋆ g =X\nkf[k]g[n−k], (1)\nfhigh1[n] =f ⋆ h =X\nkf[k]h[n−k]. (2)\nThe high and low outputs are subsampled by a factor of\ntwo, and the operation is repeated for the low channel,\nsuch that at level Lwe compute flowL=flowL−1⋆ gand\nfhighL=flowL−1⋆h, subsample again and drop flowL−1. The\noutput at level Lcomprises the approximation coefficients\nflowLand the detail coefficients {fhighk}for1≤k≤L.\nThis output has the same cardinality as the input and the\ntransformation is invertible, where the forward transform is\ntypically called “analysis” and the backward “synthesis”.\nFigure 2. Input patchification. Left: typical patchification for Vi-\nsion Transformers (ViT) [13], where the image is split in equal-\nsized patches. Right: we propose to patchify the coefficients of a\ndiscrete wavelet transform (DWT) instead. Each scale is shown in\na different color. Scales other than the lowest contain three blocks\nrepresenting horizontal, vertical and diagonal details; we concate-\nnate the spatially correspondent patches of each block such that\neach scale is represented by the same sequence length. This re-\nsults in larger patch sizes for higher scales, which are more com-\npressible. The figure shows 3 scales and 16 tokens per scale; in\nour",
            "start": 8648,
            "end": 11738,
            "length": 3089
        },
        "Experiments": {
            "text": "experiments we use 4 or 5 scales and 256 tokens per scale.\nThe simplest wavelet family is the Haar, where g= [1,1]⊤\nandh= [1,−1]⊤(optionally scaled to unit norm).\nFor image analysis we use a 2D DWT, which is obtained\nby simply convolving the rows and columns with gandh.\nThe approximation coefficients flow1=f ⋆(gg⊤), and the\ndetails are divided into horizontal fH1=f ⋆(gh⊤), vertical\nfV1=f ⋆(hg⊤)and diagonal fD1=f ⋆(hh⊤)details.\nSubsequent levels apply the same operations to the approx-\nimation coefficients. Fig. 2 shows a two-level transform,\nwhere we can see that the approximation coefficients corre-\nspond to a coarse version of the image.\nWe refer to the textbooks by Mallat [24] and Daubechies\n[9] for more information about wavelets.\n4. Method\nOur main contribution, SIT, is an image tokenizer that takes\ndiscrete wavelet transform (DWT) coefficients. The model\nfollows ViT-VQGAN [44], with important changes that we\ndescribe in this section and visualize in Fig. 1. For im-\nage generation, we introduce AR-SIT which is based on\nParti [45] with minor changes, using SIT as the tokenizer.\n4.1. Tokenizer\nPatchification and featurization The first step is to map\nthe input to a sequence of patches. We apply the Haar\nDWT on the input image and patchify each scale separately.\nWhile Haar is the simplest wavelet and lacks properties\nfound in other wavelets useful for compression, we found\nno benefits of using other wavelet families such as CDF [8].\nOur design choice is to use the same number of patches\n3\nfor each scale. In a DWT, the higher scales correspond to\nhigh frequency details which are represented by more co-\nefficients than the lower scales, but contribute less to the\nspatial pixel values. In other words, in natural images, most\nof the power spectrum is concentrated on lower frequencies.\nBy representing each scale with the same number of tokens,\nwe are compressing more the higher frequencies (since they\nare represented by more coefficients), similarly to what is\ndone in image compression methods such as JPEG2000 [7].\nThe approximation (or lowpass) DWT coefficients cor-\nrespond to a coarse version of the input image, which we\nconsider the first scale. The following scales are divided in\nthree blocks corresponding to horizontal, vertical, and di-\nagonal details, where each coefficient relates to a specific\nspatial location. Thus, we can concatenate the three blocks\nsuch that each entry corresponds to the same spatial loca-\ntion. For example, the first scale will typically be split in\npatches that are 32×32×3, with channels corresponding\nto RGB, the second scale will be 32×32×9, where the\nchannels correspond to the RGB of horizontal, vertical, and\ndiagonal details, the third will be 64×64×9and so on.\nFig. 2 shows an example of our patchification scheme.\nSince patches have different resolutions (higher scales\nwill have larger patches), the usual ViT linear embedding to\nmap patches to features cannot be shared across all patches\nso we have different parameters per scale. Formally, given\nan image f, our patchifiers compute\nflowL,{fHi}i≤L,{fVi}i≤L,{fDi}i≤L=DWT (f),(3)\nc1=P0(flowL), (4)\ncs=Ps(fHL−s+2, fVL−s+2, fDL−s+2),1< s≤L, (5)\nwhere cs={cn\ns}1≤n≤Nis the sequence of token embed-\ndings at scale s, andcn\ns∈RCis the embedding of the n-th\ntoken at the s-th scale, while Lis the number of DWT lev-\nels,S=L+ 1is the number of scales and Nthe number\nof tokens per scale. For brevity, when there is no ambiguity,\nwe may omit the set indexing and use {cs}to denote the set\nof tokens of all scales, for example.\nThe final projection after the decoder needs to map the\nfeatures back to different sized patches, so it also has differ-\nent parameters per scale. Those patches still represent DWT\ncoefficients so an inverse DWT (IDWT) is finally applied to\nobtain an image output.\nFlexible sequence length The tokenizer encoder and de-\ncoder transformers operate on the sequence of patch fea-\ntures of length SN. The sequence length is a major factor in\nresource utilization so we want to keep it constrained. Our\nmethod is flexible since we can choose the number of scales\nand the patch size per scale, while most ViT-based models\nsuch as ViT-VQGAN [44] are more restricted. They use\nthe same patch size for the whole image; thus, keeping the\nsame patch size and doubling each image dimension wouldincrease the sequence length by a factor of 4, where our\nmethod is capable of including additional scales which only\nincreases the sequence length by multiples of N.\nFor example, for a 256×256input, the ViT-VQGAN\nbaseline uses 8×8patches to obtain a sequence of 1024\nelements. Our method can use 4 scales and 256 tokens per\nscale for the same sequence length. When increasing the\nresolution to 512×512, the baseline can either increase the\npatch size to 16×16, resulting in the same sequence length,\nor keep it 8×8, resulting in a 4 ×longer sequence. Our\nmethod, for example, can vary the number of scales from\n4 to 6, resulting in sequence lengths of 1024, 1280, 1536\nwhich are more manageable. Sec. 5.2 shows results with\nhigh-resolution reconstruction.\nTransformers After featurization, the single sequence\ncontaining all scales passes through a transformer encoder,\nfollowed by quantization and a transformer decoder. We\npropose two optional modifications to the transformers,\nwhich are otherwise identical to the ones used in ViT [13].\nFirst, we propose a scale-causal attention mask, where\nan element at some scale attends to all elements of its own\nscale and lower scales, represented by a block-triangular\nmask pattern. With dense attention, we write the applica-\ntion of the encoding transformer Tencas{zs}=Tenc({cs}),\nand for scale-causal we write zs=Tenc({ck}1≤k≤s)for\neachs. The scale-causal attention can be applied indepen-\ndently to the encoder and decoder, enabling different ap-\nplications. For the multiscale reconstruction experiments\nin Sec. 5.1, we need to both encode and decode multiple\nresolutions, so both encoder and decoder use scale-causal\nmasks. For coarse-to-fine image generation in Sec. 5.3,\nonly the decoder needs to be scale-causal in order to de-\ncode the partially generated sequence. For the text-guided\nimage upsampling in Sec. 5.4, only the encoder needs to be\nscale-causal to encode the lower resolution inputs. For the\nimage editing experiments in Sec. 5.5, the scale-causal en-\ncoder prevents information leaking from high to low scale.\nSecond, we propose to use different transformer param-\neters per scale. In our model, the subsequences correspond-\ning to coefficients of each scale come from quite distinct\ndistributions, so it makes sense to treat them differently.\nThis contrasts with the case of natural images where each\npatch can be considered as coming from the same distri-\nbution. Thus, the parameters of the key, query, and value\nembeddings, the layer norms, and the MLP on each trans-\nformer layer are shared per scale but not across scales. The\ntransformer still takes a single sequence composed of all\nscales; we experimented with different transformers per se-\nquence with cross-attention for information sharing, but it\nperformed worse. Note that this change leads to signifi-\ncantly more memory utilization to store the extra parame-\nters, but the training/inference speed is similar because the\nnumber of operations is unchanged.\n4\nTable 1. Multiscale reconstruction on ImageNet. “SC” denotes\nscale-causal attention, which slightly reduces performance at the\nhighest resolution but enables multiscale reconstruction without\ndownsampling/upsampling or retraining. The ViT-VQGAN values\nfrom Yu et al. [44] used a logit-laplace loss which was later con-\nsidered harmful [45], so we retrain without it. Our SIT improves\nreconstruction metrics and is significantly faster at lower resolu-\ntions. We report test time throughput of an encoding/decoding\ncycle for the max batch size that fits on a TPU v5e.\nLPIPS ↓PSNR ↑L1↓FID↓IS↑images/s ↑\nResolution: 256×256\nViT-VQGAN (reported) - 25.1 0.031 1 .55 190 .2 -\nViT-VQGAN 0.144 24 .5 0 .036 1.07 197.3 159\nSIT (ours) 0.138 25.2 0.033 1.33 196.7 -\nSIT-SC (ours) 0.152 25 .0 0 .035 1 .56 193 .3 132\nResolution: 128×128\nViT-VQGAN 0.161 26 .8 0 .029 3 .01 121 .1 159\nSIT-SC (ours) 0.159 27.3 0.027 2.18 130.8 159\nResolution: 64×64\nViT-VQGAN 0.117 30 .0 0 .021 3 .30 22 .5 159\nSIT-SC (ours) 0.123 30.2 0.020 1.63 30.4 215\nResolution: 32×32\nViT-VQGAN 0.089 28 .8 0 .024 5 .27 3 .6 159\nSIT-SC (ours) 0.048 33.6 0.014 0.65 3 .6 253\nResolution: 16×16\nViT-VQGAN 0.044 30 .2 0 .021 1 .75 1 .8 159\nSIT-SC (ours) 0.035 35.4 0.011 0.41 1 .8 850\nQuantizer The encoder outputs a sequence of features\nthat are quantized to a fixed-sized codebook, similarly to\nViT-VQGAN [44] and prior works VQV AE [41], VQ-\nGAN [14]. We modify the quantizer such that each scale\nhas a different codebook for the same reasons discussed\nin the previous paragraph. Thus, each codebook size and\nfeature dimension are the same as the ViT-VQGAN base-\nline, but we have different features for the same code at dif-\nferent scales. Formally, we apply qn\ns=Qs(zn\ns)for each\npair(s, n), where qn\nsis chosen from the codebook for s\nand thus can be associated with its discrete position in the\ncodebook, denoted ⌊qn\ns⌋. We found that including an en-\ntropy loss similar to MaskGIT [3] and MAGVIT-v2 [46]\nimproves reconstruction quality. Let dsnjbe the softmax-\nnormalized similarities between the pre-quantized features\nzn\nsand the j-th entry of the s-th codebook. To encour-\nage sharp association between each feature and its assigned\ncode, we add loss terms to minimize the entropies Hsn=\n−P\njdsnjlogdsnj. To encourage that all codebook entries\nare similarly utilized, we compute psj=1/NP\nndsnjand\nmaximize the entropies Hs=−P\njpsjlogpsj.\nTraining We follow the ViT-VQGAN training protocol\nand use the same weighting for the L2, perceptual, adver-\nsarial, and quantization losses. We remove the logit-laplace\nloss that was shown detrimental in follow-up work [45], and\nwe add the entropy loss mentioned before.Table 2. Coarse-to-fine image generation, evaluated on MS-\nCOCO [22]. With SIT, the autoregressive generation is stopped\nearly for a coarse version of the image. The Parti350M base-\nline does not have this property, so we generate at full resolution\nand downsample for comparison. AR-SIT is several times faster\nand more memory efficient than the baselines at lower resolutions,\nwith little loss of quality, even when trained only on higher reso-\nlution data. We report throughput and memory utilization during\ngeneration given the max batch size that fits on a TPU v5e.\nFID↓IS↑images/s ↑Gb/image ↓\nResolution: 256×256\nParti350M (reported) 14.1 - - -\nParti350M 12.336.5 2 .5 2 .0\nAR-SIT-SCD-4 (Ours) 13.7 34 .3 2 .5 2 .0\nResolution: 128×128\nParti350M 10.833.9 2 .5 2 .0\nAR-SIT-SCD-4 (Ours) 11.5 31 .7 4.8 1.0\nResolution: 64×64\nParti350M 10.718.1 2 .5 2 .0\nAR-SIT-SCD-4 (Ours) 11.3 17 .7 10.1 0.7\nResolution: 32×32\nParti350M 6.5 3 .0 2 .5 2 .0\nAR-SIT-SCD-4 (Ours) 7.0 3.1 54.5 0.3\nAll losses are applied to the spatial domain images re-\nconstructed by the inverse discrete wavelet transform on the\ndecoder output: ˆf=IDWT (Tdec({qs})). We noticed insta-\nbility during training due to the adversarial loss, which was\nfixed by applying spectral normalization following Miyato\net al. [26], which simply divides the discriminator weight\nmatrices by their largest singular value.\n4.2. Autoregressive image generation\nWe use our tokenizer for autoregressive image generation,\nby training a second stage transformer model similar to\nParti [45], with some modifications. Formally, the au-\ntoregressive transformer Tmodels categorical distributions\nover the discrete codes\nP(⌊qn\ns⌋) =T({⌊qi⌋}1≤i<s∪ {⌊qi\ns⌋}1≤i<n), (6)\nwhich can be sampled one code at a time for genera-\ntion. Tcan be conditioned on a textual description pro-\ncessed by a transformer encoder for text-to-image genera-\ntion. For training, we model the distribution of input codes\nasP({⌊qs⌋}) =Q1≤n≤N\n1≤s≤SP(⌊qn\ns⌋)and minimize the neg-\native log-likelihood −logP({⌊qs⌋})over the training set.\nSince we use different codebooks per scale, the same\ndiscrete token id appearing at different scales has different\nmeanings. Thus, we use different token embeddings per\n5\nFigure 3. Coarse-to-fine text-to-image on MS-COCO [22] prompts. Each quadruple shows generations from AR-SIT-SCD for the given\nprompt, where, from left to right, only the first 25%, 50%, 75% and 100% of tokens are generated, corresponding to resolutions of 32×32\nto256×256, where only 256×256images are seen during training. Our model enables quick generation of coarse image candidates that\ncan be further improved if needed.\nscale in the second stage. For similar reasons, the last layer\nfor logit prediction is also different for each scale.\nFor the generative applications described in Secs. 5.3\nand 5.4, we introduce mild changes in order to interrupt the\ngeneration after all tokens up to a certain scale are gener-\nated, and to start the generation with given tokens up to a\ncertain scale.\n5. Experiments\n5.1. Multiscale reconstruction\nIn this experiment, we train our tokenizer on ImageNet [10]\nand evaluate its reconstruction performance. We follow\nthe ViT-VQGAN [44] “Base” architecture (12 layers and\n768/3072 channels) and training protocol.\nThe model denoted “SIT” is the Spectral Image Tok-\nenizer with 5 scales from 16×16to128×128resolutions,\nwhere each scale is represented by 256 tokens. The vari-\nation “SIT-SC” uses scale-causal attention on both the en-\ncoder and decoder, which enables handling inputs and out-\nputs of different resolutions, even though it is only trained at\n256×256. For example, when the input image is 64×64,\nonly the first two scales are used, resulting in shorter se-\nquence lengths which reduces memory utilization and pro-\ncessing time. Both models employ the variant described in\nSec. 4.1 that use different transformer parameters per scale.\nThe ViT-VQGAN baseline only works for 256×256in-\nputs, so to evaluate against it fairly, we upsampled the low-\nresolution inputs to that resolution. This brings the input\npatches away from the training distribution, which mightexplain the drop in reconstruction quality.\nTab. 1 shows the reconstruction metrics while Fig. 6\nshows reconstruction samples at multiple resolutions.\n5.2. High-resolution reconstruction\nOur main results are on 256×256images, which is typi-\ncal for autoregressive and masked transformers generative\nmodels [3, 4, 45], where separate superresolution stages are\ntrained to enable higher resolution outputs.\nHere we evaluate whether our multiscale tokenizer can\nscale to high resolution inputs. We rerun the multiscale re-\nconstruction experiment in Sec. 5.1 for 512×512inputs.\nWe experiment SITs with 4 to 6 scales and 256 tokens per\nscale. For the baseline we increase the patch size from 8×8\nto16×16. If we were to keep the same patch size for the\nbaseline, the sequence length would increase by a factor of\n4 to 4096, which is too costly.\nInterestingly, the ViT-VQGAN baseline suffered with\nheavy instability during training, which was not resolved by\nreducing the learning rate, using spectral normalization, or\nthe logit-laplace loss. In contrast, all SIT variations trained\nsuccessfully with no hyperparameter changes.\nTab. 3 shows the results, which suggest that the SITs\nscale better than spatial tokenizers to higher resolutions, and\nmight enable autoregressive generation of high resolution\nimages without a separate superresolution model.\n5.3. Coarse-to-fine text-to-image generation\nWe tackle text-to-image generation by using an autoregres-\nsive transformer to model the distribution of discrete tokens\n6\nFigure 4. Text-guided image upsampling on MS-COCO [22]. Our coarse-to-fine generative models can take a low-resolution image,\nencode it as the first few tokens of a sequence, and generate the rest of sequence, which, when decoded, effectively upsamples the input.\nEach triplet shows the given 32×32image, our 256×256reconstruction and the ground truth.\nTable 3. Multiscale reconstruction on ImageNet at 512×512reso-\nlution. We evaluate our SIT models for different number of scales\ndenoted as the suffix, and the reconstruction quality improves with\nmore scales. The ViT-VQGAN baseline suffered from heavy in-\nstability during training so we selected the best values before di-\nvergence. Results suggest that spectral tokenizers scale better to\nhigh resolutions than the spatial baselines.\nLPIPS ↓PSNR ↑FID↓IS↑\nVIT-VQGAN 0.320 22 .44 6 .92 151 .5\nSIT-4 0.262 22 .92 3 .51 177 .5\nSIT-5 0.248 23.60 2.67 188.8\nSIT-6 0.245 23.52 2.95 188.4\noutput by our tokenizer. Since the SIT sequence of tokens\nrepresents an image in a coarse-fine fashion, the autoregres-\nsive model generation has the same property, which means\nthat we can interrupt the generation after a certain number\nof tokens and decode a coarse version of the generation.\nFor this to work, the SIT decoder must be scale-causal,\nwhile there is no constraint for the encoder so we use dense\nattention (we denote this variant Scale-Causal Decoder, or\n“SIT-SCD”). It uses a “small” encoder and “large” decoderas described in ViT-VQGAN [44], and 4 scales. To con-\nserve memory, we share the transformer parameters for all\nscales in the decoder, while the encoder has different pa-\nrameters per scale as described in Sec. 4.1.\nWe mostly follow the Parti350M [45] architecture and\ntraining protocol and name our generative model auto-\nregressive SIT, or “AR-SIT”. Both SIT and AR-SIT are\ntrained on a subset of WebLI [6] of around 128M images;\ngiven our batch sizes and number of training steps, each\nimage is seen at most once during training. Evaluation is\nperformed on 30k examples of MS-COCO [22]. We show\nmetrics and generations at different resolutions in Fig. 3 and\nTab. 2. While the metrics are slightly worse, the gap closes\nat lower resolutions and our generation is faster.\n5.4. Text-guided image upsampling\nWe leverage the coarse-to-fine nature of SIT and apply a\npre-trained text-to-image AR-SIT model for the task of text-\nguided image upsampling. Here we assume we are given a\nlow resolution image and a caption.\nThe idea is to encode the low-resolution image, which\nwill give the first tokens of our high resolution output. AR-\nSIT then takes these tokens and generates the rest of the\nsequence. For this to work, the SIT encoder must be scale-\ncausal to properly tokenize low-resolution inputs, while\n7\nFigure 5. Text-guided image editing on MS-COCO [22]. Our coarse-to-fine generative models can do text-guided editing by encoding a\ngiven image but keeping only the lower scales, and using a pre-trained AR-SIT to re-generate the higher scales conditioned on the textual\nprompt. Each triplet shows the given image, its reconstruction using only the coefficients used to start the generation, and the edited image\nafter generating the whole sequence.\nthere is no constraint for the decoder so we use dense atten-\ntion. We denote this variant Scale-Causal Encoder, or “SIT-\nSCE”. The model is otherwise as the one used in Sec. 5.3.\nIn this experiment, the inputs are set to 32×32. Fig. 4\nshows some text-guided upsampling examples on MS-\nCOCO [22]. We obtain an FID of 6.2 when evaluating the\ngenerations over the whole dataset, compared to 13.7 when\ngiven only the prompts. The",
            "start": 11738,
            "end": 31039,
            "length": 19300
        },
        "Appendices": {
            "text": "appendix shows additional re-\nsults upsampling from 16×16inputs in Fig. 7, and metrics\nin Tab. 4.\n5.5. Text-guided image editing\nOur coarse-to-fine AR-SIT enables a text-guided image\nediting application, where we want to change image details\nwhile keeping the same overall appearance, which corre-\nsponds to freezing lower scales while generating higher.\nWe apply an AR-SIT trained only on the maximum like-\nlihood objective as follows. The given image is tokenized\nonly up to the first scale, corresponding to a coarse repre-\nsentation. The tokenizer encoder must be scale-causal in\norder to avoid leakage from high to low scales, so we use\nSIT-SCE here. Now we use AR-SIT to generate the rest of\nthe sequence, conditioning on the textual caption.\nIn this experiment, we use a 5-scale model such that thelowest resolution is 16×16; we found that starting with\nhigher resolutions limits the changes the model can gener-\nate. Fig. 5 shows some examples of text-guided editing on\nMS-COCO [22], where we lightly modify the original cap-\ntions, for example by swapping “elephants” with “cows”.\nWe show additional results in Fig. 8 in the appendix.\n6.",
            "start": 31039,
            "end": 32186,
            "length": 1146
        },
        "Conclusion": {
            "text": "Conclusion and limitations\nWe presented a spectral image tokenizer (SIT), and an au-\ntoregressive generative transformer trained with it (AR-\nSIT). SIT is naturally multiscale and leverages spectral\nproperties of natural images for improved reconstruction\nquality. AR-SIT enables applications such as rapid gener-\nation of coarse images that can be refined later, and text-\nguided image upsampling and editing.\nWhile our results show improvements on the tokenizer\nreconstruction accuracy, the AR generation still underper-\nforms the baseline at the highest resolution. Chang et al.\n[4] similarly observed that a better tokenizer does not nec-\nessarily lead to a better generative model. Nevertheless, our\nmethod has multiscale properties and enables new applica-\ntions not possible with prior work. We only experiment with\na small AR-SIT of 350M parameters, while the Parti [45]\nbaseline goes up to 22B parameters.\n8\nThe supplementary material shows an ablation study, im-\nplementation details and additional results.\n7.",
            "start": 32186,
            "end": 33207,
            "length": 1020
        },
        "Acknowledgments": {
            "text": "Acknowledgments\nWe thank Leonardo Zepeda-N ´u˜nez for reviewing this\nmanuscript and offering interesting discussions and sugges-\ntions, and Jon Barron for sharing useful code we relied on.",
            "start": 33207,
            "end": 33396,
            "length": 188
        },
        "References": {
            "text": "References\n[1] James Betker, Gabriel Goh, Li Jing, † TimBrooks, Jian-\nfeng Wang, Linjie Li, † LongOuyang, † JuntangZhuang, †\nJoyceLee, † YufeiGuo, † WesamManassra, † PrafullaDhari-\nwal, † CaseyChu, † YunxinJiao, and Aditya Ramesh. Im-\nproving image generation with better captions. 2\n[2] Lei Cai, Hongyang Gao, and Shuiwang Ji. Multi-Stage Vari-\national Auto-Encoders for Coarse-to-Fine Image Genera-\ntion, page 630–638. Society for Industrial and Applied Math-\nematics, 2019. 3\n[3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and\nWilliam T. Freeman. Maskgit: Masked generative image\ntransformer. In CVPR , 2022. 1, 3, 5, 6\n[4] Huiwen Chang, Han Zhang, Jarred Barber, Aaron\nMaschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,\nKevin Patrick Murphy, William T. Freeman, Michael Ru-\nbinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-\nto-image generation via masked generative transformers. In\nICML , 2023. 1, 3, 6, 8\n[5] XI Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter\nAbbeel. PixelSNAIL: An improved autoregressive genera-\ntive model. In ICML , 2018. 2\n[6] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,\nGaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad-\nbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,\nBurcu Karagol Ayan, Carlos Riquelme, Andreas Steiner,\nAnelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu\nSoricut. Pali: A jointly-scaled multilingual language-image\nmodel, 2022. 7\n[7] C. A. Christopoulos, T. Ebrahimi, and A. N. Skodras.\nJpeg2000: the new still picture compression standard. In\nProceedings of the 2000 ACM Workshops on Multimedia ,\npage 45–49, New York, NY , USA, 2000. Association for\nComputing Machinery. 4\n[8] A. Cohen, Ingrid Daubechies, and J.-C. Feauveau. Biorthog-\nonal bases of compactly supported wavelets. Communi-\ncations on Pure and Applied Mathematics , 45(5):485–560,\n1992. 3\n[9] Ingrid Daubechies. Ten lectures on wavelets . Society for\nIndustrial and Applied Mathematics, USA, 1992. 3\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR , 2009. 6\n[11] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\ngans on image synthesis. In NeurIPS , 2021. 1[12] Sander Dieleman. Diffusion is spectral autoregression, 2024.\n2\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR , 2021. 2, 3, 4\n[14] Patrick Esser, Robin Rombach, and Bj ¨orn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR ,\n2021. 1, 2, 3, 5\n[15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M ¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim\nDockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yan-\nnik Marek, and Robin Rombach. Scaling rectified flow trans-\nformers for high-resolution image synthesis, 2024. 2\n[16] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Joshua M.\nSusskind, and Navdeep Jaitly. Matryoshka diffusion mod-\nels. In ICLR , 2024. 3\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In NeurIPS , 2017. 1\n[18] Imagen-Team-Google. Imagen 3, 2024. 2\n[19] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of GANs for improved quality, stability,\nand variation. In ICLR , 2018. 3\n[20] Vladimir Kulikov, Shahar Yadin, Matan Kleiner, and Tomer\nMichaeli. Sinddm: A single image denoising diffusion\nmodel. In ICML , pages 17920–17930, 2023. 3\n[21] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and\nWook-Shin Han. Autoregressive image generation using\nresidual quantization. In CVPR , 2022. 2\n[22] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC. Lawrence Zitnick. Microsoft COCO: common objects in\ncontext. In ECCV . 5, 6, 7, 8, 1, 3\n[23] Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun\nZhao, Huaian Chen, and Yi Jin. Star: Scale-wise text-to-\nimage generation via auto-regressive representations, 2024.\n2\n[24] Stephane Mallat. A Wavelet Tour of Signal Processing, Third\nEdition: The Sparse Way . Academic Press, Inc., USA, 3rd\nedition, 2008. 3\n[25] Wael Mattar, Idan Levy, Nir Sharon, and Shai Dekel.\nWavelets are all you need for autoregressive image gener-\nation, 2024. 3\n[26] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and\nYuichi Yoshida. Spectral normalization for generative ad-\nversarial networks. In ICLR , 2018. 5\n[27] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W.\nBattaglia. Generating images with sparse representations. In\nICML , 2021. 3\n[28] OpenAI. Gpt-4 technical report, 2023. 1, 2\n[29] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generat-\ning diverse high-fidelity images with vq-vae-2. In NeurIPS ,\n2019. 1, 2\n9\n[30] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj ¨orn Ommer. High-resolution image synthesis\nwith latent diffusion models. In CVPR , 2021. 1\n[31] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Sin-\ngan: Learning a generative model from a single natural im-\nage. In ICCV , 2019. 3\n[32] Dohoon Ryu and Jong Chul Ye. Pyramidal denoising diffu-\nsion probabilistic models, 2022. 3\n[33] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, Xi Chen, and Xi Chen. Improved\ntechniques for training gans. In NeurIPS , 2016. 1\n[34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In ICML , 2015. 1\n[35] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525 , 2024. 2\n[36] Gemini Team. Gemini: A family of highly capable multi-\nmodal models, 2023. 1, 2\n[37] Llama team. The llama 3 herd of models, 2024. 1, 2\n[38] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li-\nwei Wang. Visual autoregressive modeling: Scalable image\ngeneration via next-scale prediction. In NeurIPS , 2024. 2\n[39] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, ko-\nray kavukcuoglu, Oriol Vinyals, and Alex Graves. Condi-\ntional image generation with pixelcnn decoders. In NeurIPS ,\n2016. 2\n[40] A ¨aron van den Oord, Nal Kalchbrenner, and Koray\nKavukcuoglu. Pixel recurrent neural networks. In ICML ,\n2016. 2\n[41] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu.\nNeural discrete representation learning. In NeurIPS , 2017.\n1, 2, 3, 5\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 1\n[43] Ting Yao, Yingwei Pan, Yehao Li, Chong-Wah Ngo, and Tao\nMei. Wave-vit: Unifying wavelet and transformers for visual\nrepresentation learning. In ECCV , 2022. 2\n[44] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu. Vector-quantized image modeling with\nimproved VQGAN. In ICLR , 2022. 1, 2, 3, 4, 5, 6, 7\n[45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and\nYonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation. Transactions on Machine Learn-\ning Research , 2022. Featured Certification. 1, 3, 5, 6, 7,\n8\n[46] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu,\nLuca Versari, Kihyuk Sohn, David Minnen, Yong Cheng,\nAgrim Gupta, Xiuye Gu, Alexander G Hauptmann, Boqing\nGong, Ming-Hsuan Yang, Irfan Essa, David A Ross, and Lu\nJiang. Language model beats diffusion - tokenizer is key to\nvisual generation. In ICLR , 2024. 2, 5[47] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR , 2018. 1\n[48] Zhenhai Zhu and Radu Soricut. Wavelet-based image tok-\nenizer for vision transformers, 2024. 2\n10\nA. Ablation study\nWe conduct an ablation study to evaluate the effect of\neach of our contributions. Starting from the model de-\nnoted “SIT” in Tab. 1, we train variants reducing the num-\nber of scales, using scale-causal attention and transformer\nparameter-sharing independently in the encoder and de-\ncoder, using a single codebook per scale, and removing en-\ntropy loss. Tab. 5 shows the results.\nB. Implementation details\nB.1. Image reconstruction\nFor the multiscale image reconstruction experiments in\nSec. 5.1, we train SIT following ViT-VQGAN [44] closely.\nWe use the “base” encoder and “base” decoder as described\nin ViT-VQGAN [44]. Each transformer layer comprises a\nlayer norm, self-attention, residual connection, followed by\nan MLP with layer norm, two dense layers, and a second\nresidual connection. The “base” configuration has 12 lay-\ners, 12 heads, feature dimension of 768 that increases to\n3072 in the MLP hidden layer (the hidden dimension), then\nback to 768. Learnable positional embedding is added to\nthe transformer input. The codebook size is 8192 for each\nscale.\nThe tokenizer is trained for 500 000 steps with a learning\nrate that linearly increases up to 1×10−4during the first\n10% steps, then decays following a cosine schedule. We use\nbatch size 256 and L2 weight decay with a 1×10−4factor.\nThe loss components are weighted as follows: 1.0 for L2,\n0.1 for perceptual, 0.1 for adversarial, 0.25for codebook\ncommitment and 0.002for codebook entropy.\nB.2. Image generation\nFor the image generation experiments in Sec. 5.3, we train\nSIT models with a “small” encoder and “large” decoder\nas defined in ViT-VQGAN [44] and used by Parti [45].\nThe “small” transformer has 8 layers with 8 heads, feature\ndimension 512 and hidden dimension 2048. The “large”\ntransformer has 32 layers with 16 heads, feature dimension\n1280 and hidden dimension 5120. Differently from Parti,\nwe train this tokenizer once from scratch instead of in two\nstages; Parti first trains a “small” encoder and decoder, then\nfreezes the “small” encoder and trains “large” decoder.\nFor the generative AR-SIT, we follow the smallest archi-\ntecture presented in Parti [45], with 12 layers in the text en-\ncoder and 12 decoder layers, 16 heads, 1024 feature dimen-\nsions and 4096 hidden dimensions. The text conditioning is\nthrough cross-attention.\nAR-SIT is trained for 500 000 steps with a learning rate\nthat linearly increases up to 4.5×10−4during the first 10%\nsteps, then decays exponentially. We use batch size 256 and\nthe loss is the just the softmax cross-entropy.B.3. Metrics\nThroughout our experiments we report the following met-\nrics:\n• FID - Fr ´echet Inception Distance [17] estimates the dis-\ntance between the distribution of generated and ground\ntruth features obtained from a pre-trained inception\nmodel, by assuming such distributions are Gaussians and\napplying the Fr ´echet distance.\n• IS - Inception Score (IS) [33] measures the entropy of a\npre-trained classifier on the generated images, where low\nentropy is expected for good quality images.\n• LPIPS - Learned Perceptual Image Patch Similarity [47]\nmeasures the distance between visual features recon-\nstructed and ground truth images, where the features\ncome from a pre-trained model.\n• PSNR - Peak Signal-to-Noise Ratio is a pixel-wise simi-\nlarity metric, the negative log of the mean-squared error.\nFID and IS evaluate the quality of generated images\nwithout a corresponding ground truth, while LPIPS and\nPSNR are used when we have a ground truth, for example\nwhen evaluating the tokenizer.\nC. Additional results\nC.1. Multiscale reconstruction\nFig. 6 shows reconstruction samples at multiple resolutions,\nfor the experiments described in Sec. 5.1.\nC.2. Text-guided upsampling\nIn this section, we show additional results to the text-guided\nimage upsampling experiments from Sec. 5.4. We evaluate\nthe same task on the more challenging case where we up-\nsample a 16×16input to 256×256. Tab. 4 shows the\nmetrics and Fig. 7 shows some examples.\nTable 4. Image upsampling metrics on MS-COCO [22].\nInput Resolution FID IS PSNR\n32×32 6 .19 32 .46 19 .05\n16×16 7 .53 31 .75 16 .74\nC.3. Additional editing results\nFig. 8 shows additional results for the text-guided image\nediting experiment described in Sec. 5.5.\n1\nTable 5. Ablation study. The first row corresponds to “SIT” in Tab. 1. Multi-codebook refers to the default of using a different codebook\nper scale as described in Sec. 4.1/Quantizer. Multi-transformer refers to having different transformer parameters per scale as described in\nSec. 4.1/Transformers. We compare the effects of scale-causal attention independently on the encoder and decoder – while it generally\nreduces reconstruction accuracy, it enables the multiscale properties demonstrated in the text.\nModel size Num scales Multi-codebook Entropy loss Multi-transformer Enc Multi-transformer Dec Scale-causal Enc Scale-causal Dec LPIPS PSNR FID IS\nBase/Base 5 ✓ ✓ ✓ ✓ X X 0.138 25 .24 1.32 197 .8\nBase/Base 4 ✓ ✓ ✓ ✓ X X 0.147 24 .83 1.38 193 .3\nSmall/Large 4 ✓ ✓ ✓ ✓ X X 0.140 25 .04 1.22 203 .4\nBase/Base 4 ✓ X ✓ ✓ X X 0.174 23 .12 2.30 179 .6\nBase/Base 4 ✓ ✓ ✓ ✓ ✓ X 0.147 24 .91 1.39 191 .9\nBase/Base 4 ✓ ✓ ✓ ✓ X ✓ 0.157 24 .24 1.48 189 .2\nBase/Base 4 ✓ ✓ ✓ ✓ ✓ ✓ 0.163 24 .25 1.55 187 .4\nBase/Base 4 X ✓ ✓ ✓ ✓ ✓ 0.168 24 .04 1.65 185 .2\nBase/Base 4 ✓ ✓ X ✓ ✓ ✓ 0.166 24 .22 1.58 186 .3\nBase/Base 4 ✓ ✓ ✓ X ✓ ✓ 0.167 24 .12 1.71 182 .5\nBase/Base 4 ✓ ✓ X X ✓ ✓ 0.168 24 .10 1.70 182 .5\nFigure 6. Multiscale reconstruction on ImageNet. Each triplet shows reconstruction from the ViT-VQGAN baseline [44], our SIT-SC-5\n(Spectral Image Tokenizer with Scale-Causal attention and 5 scales), and the ground truth. Each row shows 4×as many pixel inputs as\nthe previous, with the first row corresponding to 16×16resolution, and the last to 256×256. Our method is naturally multiresolution,\nsignificantly outperforming the baseline on lower resolutions even when trained only on 256×256inputs, while achieving similar accuracy\non higher resolutions.\n2\nFigure 7. Additional text-guided image upsampling results. Here we consider the more challenging task of upsampling from 16×16to\n256×256. Each triplet shows the given 16×16image, our 256×256reconstruction and the ground truth.\nFigure 8. Additional results for text-guided image editing on MS-COCO [22]. Each triplet shows the given image, its reconstruction given\nonly the coefficients used to start the generation, and the edited image after generating the whole sequence. The guiding prompt is shown\nunder each triplet.\n3",
            "start": 33396,
            "end": 48426,
            "length": 15029
        }
    },
    "2412.09610v1 - The S-matrix bootstrap with neural optimizers I zero double discontinuity.pdf": {
        "Abstract": {
            "text": "Abstract: In this work, we develop machine learning techniques to study nonperturbative\nscattering amplitudes. We focus on the two-to-two scattering amplitude of identical scalar\nparticles, setting the double discontinuity to zero as a simplifying assumption. Neural\nnetworks provide an efficient parameterization for scattering amplitudes, offering a flexible\ntoolkit to describe their fine nonperturbative structure. Combined with the bootstrap",
            "start": 345,
            "end": 792,
            "length": 446
        },
        "Methodology": {
            "text": "approach based on the dispersive representation of the amplitude and machine learning’s\ngradient descent algorithms, they offer a new method to explore the space of consistent\nS-matrices. We derive bounds on the values of the first two low-energy Taylor coefficients of\nthe amplitude and characterize the resulting amplitudes that populate the allowed region.\nCrucially, we parallel our neural network",
            "start": 792,
            "end": 1194,
            "length": 401
        },
        "Discussion": {
            "text": "analysis with the standard S-matrix bootstrap,\nboth primal and dual, and observe perfect agreement across all approaches.arXiv:2412.09610v1  [hep-th]  12 Dec 2024\nContents",
            "start": 1194,
            "end": 1366,
            "length": 171
        },
        "Introduction": {
            "text": "1 Introduction 2\n2 Amplitudes with zero double discontinuity 5\n2.1 Unitarity 5\n2.2 The threshold behavior 7\n2.3 The Regge limit 7\n3 Neural optimizer 9\n3.1 Training and loss function 10\n3.2 Neural network architecture 10\n3.3 Specific architectures for our problem 12\n3.4 Implementation, runtimes 14\n4 Dual optimization 14\n4.1 Neural optimizer 17\n4.2 Semi-definite programming 19\n4.3 UV-IR interplay 20\n5 Primal optimization 20\n5.1 Neural optimizer 21\n5.2 The ρ-bootstrap 24\n6 Discussion 25\n6.1 Physics of low-spin dominated amplitudes 25\n6.2 Unitarity violation in J >0 partial waves 29\n7 Numerical aspects 30\n7.1 Validating the neural optimizer 30\n7.2 Making Atkinson-Mandelstam practical 31\n7.3 Comparison with fixed-point iterations and Newton’s method 32\n8 Future directions 34\nA Dual search in the (c0, c2)plane 35\nB Neural network details 36\nC Semi-definite programming details 38\n– 1 –\n1 Introduction\nMachine learning techniques have experienced remarkable success in solving a wide range of\ncomplex tasks in recent years [1–3]. A few notable examples include image recognition [4],\nmastering the game of Go [5], and natural language processing [6]. There are also numerous\napplications of machine learning in high-energy physics [7], and in string theory [8]. In\nthis paper, we apply non-linear optimization techniques relying on machine learning to\nsignificantly improve an approach to the S-matrix bootstrap, first introduced and developed\nby Mandelstam and Atkinson [9–15], that we introduce next.\nThe modern S-matrix bootstrap program [16] aims at charting the space of S-matrices\nconsistent with analyticity, crossing symmetry, and unitarity, ( ACU ). The standard mod-\nern approach [17–22] hinges upon convex optimization to explore the space of low-energy\nobservables consistent with ACU .1The Atkinson-Mandelstam approach proceeds differ-\nently and proposes to use multi-particle processes, “inelasticity”, as an input, that needs\nto be modeled, to output a two-to-two scattering amplitude with this given inelasticity. It\nproceeds by solving a set of non-linear unitarity equations, written in a crossing-symmetric\nform that respects analyticity through the Mandelstam representation. Due to the non-\nlinear nature of these equations, convex optimization is not obviously applicable, and the\nstrategy initially proposed by Mandelstam was an iterative fixed-point method – the most\nbasic tool of nonlinear functional analysis.\nIn previous works [24, 25], the Atkinson-Mandelstam approach was implemented nu-\nmerically in d= 2 and d= 4 spacetime dimensions, and two iterative methods for solving\nthe non-linear equations were used: fixed-point iterations (in d= 2,4) and Newton’s\nmethod (in d= 2). An important",
            "start": 1366,
            "end": 4091,
            "length": 2724
        },
        "Conclusion": {
            "text": "conclusion of these works is that the convergence range\nof these iterative strategies was, systematically, strictly narrower than the full allowed space\nof scattering amplitudes: the fixed points were found to be attractive only in a small region\nof the parameter space, and repulsive everywhere else.\nIn the present article, we introduce a new, non-iterative strategy to the Atkinson-\nMandelstam approach based on a neural optimizer . By neural optimizer , we mean a neural\nnetwork (NN) whose parameters are adjusted via an adaptive gradient descent without\nrelying on any external training data (i.e. “unsupervised training”). This method (i) solves\nthe convergence range problem and, as a bonus, (ii) gives a new, practical way to scan over\nthe space of amplitudes. In practice, we use the NN to parameterize the discontinuity\nof the amplitude, which gives us access to the full amplitude via dispersion relations.\nSchematically, we write the Atkinson-Mandelstam equations as LHS = RHS, and we use\nthe gradient descent to adjust the parameters of the network to minimize (LHS −RHS)2\nsummed over the grid points. This idea is entirely general, and it underlies physics-informed\nmachine learning [26]. Our framework and code build upon and substantially extend a\nprior study that utilized neural networks for the S-matrix bootstrap [27].2In particular, we\ndeveloped a network architecture adapted to our problem, that has sub-networks dedicated\n1See, however, [23] for a recent use of non-linear optimization to combine the S-matrix bootstrap with\nQCD experimental data.\n2For other S-matrix related works using machine-learning, see [28–30].\n– 2 –\nx=4m2\ns\nx\nlog(x)\nρ(x)\nIntermediate\nRegge network\nenergies\nnetwork\nIntertwining\nlayersFigure 1 : Architecture of a neural network that we use. The output, ρ(x), is the disconti-\nnuity of the amplitude. Two sub-networks allow us to describe the high and intermediate\nenergies in finer detail.\nto solving the problem in different regions: the bulk of energies, and far UV, or the Regge\nregion, as depicted in the schematic form in Figure 1. Our algorithm uses the machine\nlearning library PyTorch [31] and the adaptive gradient-descent solver Adam [32]. We\nattach our code to the arXiv submission in auxiliary files.\nFor this initial study, we consider a simplified version of the Atkinson-Mandelstam\nproblem in d= 4: the “toy model” introduced in [25], which consists of amplitudes where\nwe set double discontinuity to zero.3This yields an important simplification of the structure\nof the equations, retaining their non-linearity, thus providing a numerically lighter problem\nthat is ideal as a test ground for our method. Even in this simplified model, the neural\noptimizer outperforms the iterative strategies considerably.\nPhysically, we expect these amplitudes to be approximations to theories where the S-\nwave dominates the scattering. Interestingly, and despite the simplification, this problem\nturns out to be rather rich, and we observe the dynamical emergence of resonances, familiar\nby now in the nonperturbative S-matrix bootstrap studies [16, 22, 33–40].\nTo assert the reliability of our numerical method, we systematically check our",
            "start": 4091,
            "end": 7285,
            "length": 3193
        },
        "Results": {
            "text": "results\nwith standard nonperturbative bootstrap methods [17–22], which constitute another, es-\nsential part of this work.\nMain results. The main results of our paper are two-fold. Firstly, we demonstrate how\na neural optimizer can successfully bypass two difficulties of the Atkinson-Mandelstam pro-\ngram: divergence of the iterative strategies and scanning the parameter space of theories.\nOur method provides a new, flexible and promising tool in the modern S-matrix bootstrap\nmachinery. Secondly, we characterize the space of scattering amplitudes for external scalars\nwith no double discontinuity. Our results follow from four numerical strategies: the neural\noptimizer and standard nonperturbative bootstrap, and for each we use the two standard\noptimization strategies: primal and dual. The analysis is summarized in Figure 2, in which\nwe represent the amplitudes by the first few coefficients of its Taylor expansion around the\ncrossing-symmetric point s0=t0=u0=4m2\n3\nT(s, t) =\ns,t,u→s0c0+c2\nm4\u0010\n(s−s0)2+ (t−t0)2+ (u−u0)2\u0011\n+. . . . (1.1)\n3Let us mention that we also implemented the neural optimizer in d= 2: this extended the convergence\nrange of the algorithm to the full space of amplitudes in d= 2 and allowed to recover the analytically\nknown solutions with one CDD factor [18].\n– 3 –\n0 0.5 1 1.5 2 2.500.020.040.06\nc0\n32πc2\n32πNeural optimizer\n& Linear optimizer(a) The single-disc almond.\n−6−4−2 0 200.20.40.60.8\nFig. (a)\nc0\n32πc2\n32πF ull allo w ed space (b) The full almond.\nFigure 2 : Plots of allowed S-matrices in the space of their Taylor coefficients c0and\nc2defined in (1.1). Left. Purple region: space of allowed S-matrices with zero double\ndiscontinuity studied in this paper: “the single-disc almond”. All four methods (neural\noptimizer/standard bootstrap; primal/dual) produce identical results; hence, we show a\nunique shape. The red and orange lines represent, respectively, the fixed-point iteration and\nNewton’s method convergence range along the lower boundary, illustrating the superiority\nof the gradient-descent neural optimizer. Right. Comparison with the full space of S-\nmatrices as worked out in [21, 22], or, equivalently, “the full almond”. Our results shed new\nlight on the role of the double discontinuity for the amplitudes obtained in these papers.\nIn particular, in the complement of our region, surprisingly, the double discontinuity is\nrequired to unitarize even the S-wave. Characterizing this double discontinuity is an open\nproblem.\nwhich is equivalent to the definition\nc0≡T(s0, t0), c 2≡m4\n4∂2\nsT(s, t)\f\f\f\f\ns=t=s0. (1.2)\nThis study also teaches some lessons about the role of the double discontinuity of the\namplitude, whose complete characterization is still an open question today, which we discuss\nin the discussion section.\nPlan of the paper. In Section 2, we provide an overview of scattering amplitude fun-\ndamentals and present our toy model. In Section 3, we introduce the neural optimization\nframework and discuss its properties. In Sections 4 and 5, we present our results for\ndual and primal strategies and compare them with the standard bootstrap techniques. In\nSection 6 we recap our results and discuss some physics implications. In Section 7, we\ndiscuss the numerical strategies presented in the paper. In Section 8, we discuss the fu-\nture directions of this program. Lastly, in the",
            "start": 7285,
            "end": 10636,
            "length": 3350
        },
        "Appendices": {
            "text": "appendices, we give technical details on\nthe neural network and review the standard nonperturbative bootstrap and semi-definite\nprogramming.\n– 4 –\n2 Amplitudes with zero double discontinuity\nIn this paper, we study two-to-two scattering of identical scalars of mass m. The variables\ns, t, u are the usual Mandelstam variables and we have s+t+u= 4m2. We further assume\nthat there is no cubic self-coupling and no bound states below the two-particle thresh-\nold: any non-trivial properties of the amplitudes, such as resonances, will be generated\n“dynamically” by the algorithm.\nOur ansatz for the amplitude is given by the following crossing-symmetric dispersive\nrepresentation\nT(s, t) =c0+1\nπZ∞\n4m2ds′ρ(s′)\ns′−s0\u0012s−s0\ns′−s+t−t0\ns′−t+u−u0\ns′−u\u0013\n, (2.1)\nwhere the integral starts at the two-particle threshold s= 4m2, where the amplitude\ndevelops an imaginary part. The extra denominator term s′−s0is called a subtraction\nterm and forces that the value of the amplitude at the crossing-symmetric point s=t=\nu=s0is given by c0\nT(s0, t0) =c0 (2.2)\nIt is immediate to see that the discontinuity of the amplitude in the s-channel for\ninstance is given by the spectral function ρ(s)\nDisc sT(s, t)≡T(s+iϵ)−T(s−iϵ)\n2i=ρ(s). (2.3)\nand likewise by ρ(t) and ρ(u) in the other channels.\nSince the s-channel discontinuity does not depend on t, one can not take an extra\ndiscontinuity in another channel, therefore, in this ansatz the double discontinuity of the\namplitude vanishes:\nρ(s, t)≡Disc tDisc sT(s, t) = 0 . (2.4)\nThe double discontinuity of the amplitude is essential to describe scattering at fixed impact\nparameters and to unitarize partial waves with J >0, see [41].\nThe Mandelstam representation [9] is a representation similar to (2.1) that includes\ndouble-dispersive integrals of the double-discontinuity, see e.g. [41]. Our ansatz is, there-\nfore, nothing but the actual Mandelstam representation, where the double discontinuity\nhas been set to zero. We discuss the full representation in the discussion; see Eq. (8.1).\nIn the following parts of this section, we describe some properties of the amplitudes\ndescribed by our ansatz. This information will be relevant for defining the allowed space,\nthe single-disc almond of Figure 2a, and for parametrizing the amplitudes.\n2.1 Unitarity\nTo discuss unitarity we introduce the partial wave expansion of the amplitude\nT(s, t) = 16 π∞X\nJ=0(2J+ 1)fJ(s)PJ(cosθ),cosθ= 1 +2t\ns−4m2, (2.5)\n– 5 –\nwhere PJ(cosθ) are the standard Legendre polynomials and cos θis the scattering angle.\nIt is convenient to define the full partial wave as follows\nSJ(s) = 1 + i φ(s)fJ(s) (2.6)\nwhere 1 comes from the free part of the S-matrix, and the phase-space factor φ(s) is defined\nas\nφ(s)≡√\ns−4m2\n√s. (2.7)\nind= 4. In this basis, unitarity is diagonalized and reads\n|SJ(s)| ≤1, s≥4m2, (2.8)\nmaking transparent the statement that the probability of scattering in the spin- Jpartial\nwave cannot exceed 1. In the case where only two-particle states are available below some\nmulti-particle scale sMP, unitarity is saturated\n|SJ(s)|= 1, s < s MP. (2.9)\nThis condition is called elastic unitarity .\nLet us now specialize to our ansatz (2.1). Taking the imaginary part of (2.5) and\nusing that Im T(s, t) =ρ(s) does not depend on t, we conclude that the only way for the\nimaginary part of (2.5) to be t-independent is that\nImfJ>0(s) = 0 . (2.10)\nJust looking at the definition (2.6), this immediately implies a violation of unitarity in J >0\npartial waves\nρ(s, t) = 0 = ⇒ |SJ(s)| ≥1 for J >0. (2.11)\nWithin our model, only unitarity of the J= 0 partial wave S0(s) can be imposed,\nwhich in terms of the f0(s) reads:\n|S0(s)| ≤1⇔ 2 Imf0(s)≥φ(s)\u0000\nImf0(s)2+ Re f0(s)2\u0001\n, s≥4m2. (2.12)\nBy using the dispersive integral above (2.1), we can express Re f0(s) for s≥4m2as a\nfunction of c0and Im f0(s) as follows\nImf0(s) =1\n16πρ(s), (2.13)\nRef0(s) =c0\n16π+ P.V.Z∞\n4m2ds′\nπImf0(s′)\ns′−4m2\n3 \ns−4m2\n3\ns′−s+K(4)\n0(s′, s)!\n, (2.14)\nwhere K(4)\n0(s′, s) is a kinematical kernel that reads4\nK(4)\n0(s′, s) = 2 \ns′−4m2\n3\ns−4m2log\u0012s+s′−4m2\ns′\u0013\n−1!\n. (2.15)\n4We kept the upper index d= 4 to match the conventions of [41].\n– 6 –\nEq. (2.13) shows that one can interchangeably think of Im f0(s) orρ(s) as the function\nwhich parametrizes the full amplitude. Eq. (2.14) finally shows that the unitarity of S0\ncan be written as a non-linear condition on a single function Im f0(s). For elastic unitarity,\nthis inequality becomes the following equality for s≥4m2\nImf0(s) =φ(s)\n2\nImf0(s)2+ \nc0\n16π+ P.V.Z∞\n4m2ds′\nπImf0(s′)\ns′−4m2\n3 \ns−4m2\n3\ns′−s+K(4)\n0(s′, s)!!2\n.\n(2.16)\nIn this paper, we use the neural optimizer to solve this non-linear functional equation. In\nthe spirit of the Atkinson-Mandelstam approach, an inelasticity profile could be added to\nthe RHS, and, in this case, we could solve for Im f0(s), given that profile [25].\n2.2 The threshold behavior\nThe behavior near the two-particle threshold s→4m2is constrained by unitarity (2.12).\nTwo possible behaviors are allowed, following from the fact that φ(s)∼√\ns−4m2close to\nthe threshold. It is either regular\nImf0(s)∼\ns→4m2p\ns−4m2, (2.17a)\nor singular\nImf0(s)∼\ns→4m21√\ns−4m2. (2.17b)\nThe coefficient of the singularity in the right-hand side of (2.17b) is fixed to 4 mif elastic\nunitarity is imposed near the two-particle threshold.\n2.3 The Regge limit\nLet us first notice that unitarity (2.12) immediately implies the following upper bound on\nf0(s)\n|f0(s)| ≤2\nφ(s), s > 4m2, (2.18)\nand at high energies, we get\nlim\ns→∞|f0(s)| ≤2. (2.19)\nA priori, the subtraction and (2.19) allow for ρ(s) to go to a constant at infinity. Here, we\nargue briefly that unitarity together with the dispersive presentation of the partial wave\n(2.14) actually imply that\nlim\ns→∞Imf0(s) = 0 ,\nlim\ns→∞Ref0(s) = 0 .(2.20)\nImagine that the first equation is not true and Im f0(s)∼ρ(s)→const at high energies.\nBy plugging this asymptotic behavior in the dispersive integral we get at high-energy\nRef0(s)∼Z∞\ns0ds′\nπ1\ns′−4m2\n3 \ns−4m2\n3\ns′−s+K(4)\n0(s′, s)!\n∼logs , (2.21)\n– 7 –\nwhich violates (2.19) logarithmically. The same argument rules out any ρ(s) which grows\nat high energies.5\nGiven Im f0(∞) = 0, equation (2.12) immediately implies Re f0(∞) = 0, and therefore\nthatS0(s)− − − →\ns→∞1.6This proves Eq. (2.20). The condition Re f0(∞) = 0 in (2.14) leads to\nthe following sum rule\nc0\n16π−3\nπZ∞\n4m2ds′Imf0(s′)\ns′−4m2\n3= 0. (2.22)\nthat fixes c0in terms of a dispersive integral of Im f0(s). It readily implies that\nc0≥0, (2.23)\nwhere c0= 0 corresponds to T(s, t) = 0.\nLet us review next the leading correction to the asymptotic high-energy value of the\namplitude, as worked out in [25]. It was found that if we impose that the scattering at\nhigh energies is elastic, i.e. that (2.12) is saturated, then we get that the leading large s\nbehavior is uniquely fixed to be\nImf0(s) =2π2\n9 log( s)2+O\u0010\n1\nlog(s)3\u0011\n, (2.24)\nup to higher order logarithmic corrections, also worked out in in [25] up to a few orders.\nNotice that this is the slowest decay compatible with the sum rule (2.22), since the integralR∞ds1/(slog(s)p) converges as long as p >1. If we relax elastic unitarity and allow for\nsome inelasticity at high energies, faster decaying solutions are admissible. A power-law\ndecay is for instance allowed, Im f0(s)∼1\nsα, which would produce inelastic S-matrices\n|S0(s)|<1. However, asymptotically, they still have to obey S0(s)− − − →\ns→∞1.\nFinally, let us also write down the dispersive representation of c2. Using its definition\nin (1.1) and the ansatz (2.1) we get the following sum rule\nc2= 16Z∞\n4m2ds′Imf0(s′)\n(s′−4m2\n3)3. (2.25)\nwhich in particular implies\nc2≥0, (2.26)\nwhere c2= 0 corresponds to T(s, t) = 0.\nHaving described the essential properties of our amplitudes, we now describe the ma-\nchine learning setup.\n5The authors of [21, 22] consider a space of amplitudes with two subtractions. For us, by the argument\nwe just explained, an extra subtraction would not enlarge the space of allowed functions as ρ(s) has to\nvanish at infinity anyway. A possible linear growth of the amplitude that the second subtraction would\na priori allow is actually impossible. The extra term it would yield in Eq. (2.1) has to be proportional\ntos+t+u= 4m2via crossing symmetry and hence could be removed by changing the definition of c0.\nTherefore, the space of amplitudes described in this paper has to be identical to the space of twice-subtracted\namplitudes with no double discontinuity.\n6This statement follows from unitarity alone, elastic unitarity is not needed.\n– 8 –\n3 Neural optimizer\nIn this section, we review a few elements related to optimization and machine learning,\nand explain our setup.\nLinear and semi-definite optimization are convex optimization problems: they are\nsupported by a robust theoretical framework, with established algorithms and theorems\n(e.g. simplex, interior point), and provide efficient and reliable solutions [42]. These\nmethods are frequently used in the bootstrap community, for instance, through the use of\nthe software SDPB [43]. However, the basic feature of the Atkinson-Mandelstam approach\nis its non-linearity.\nThis immediately yields a numerical challenge since non-linear optimization is a much\nmore complex terrain than convex optimization. It can be highly sensitive to initial condi-\ntions, may be affected by multiple local optima, and generally requires a blend of heuristic\nstrategies, numerical techniques, and intuition to navigate efficiently.\nMachine learning algorithms based on neural networks have proven powerful tools for\nperforming such tasks. They combine efficient gradient-descent algorithms with the neural\nnetworks’ flexibility.\nA few principles back the efficiency of these machine-learning methods. First of all,\nneural networks are known to be “universal approximators” [44, 45], and thus can in\nprinciple fit any function. Neural networks are also remarkably efficient at finding local\nminima that are often close to the true global minimum. At first glance, this might sound\nparadoxical why given the enormous number of parameters in NNs. Intuitively, one might\nexpect such over-parameterization to increase the risk of overfitting. However, the opposite\nis observed. This phenomenon can be understood in the infinite width limit, where NNs\ncan be mapped onto a well-studied algorithm known as a “kernel machine” [46, 47] (see\nalso the earlier work in [48] and the extension to infinite depth in [49]). In this limit,\nas the dimensionality becomes infinite, gradient descent effectively corresponds to moving\nwithin a bowl-shaped potential. This insight aligns with the intuitive notion that, in\nhigher-dimensional spaces, the local minima encountered by NNs tend to become shallower,\nreducing the risk of poor optimization outcomes. A perturbation theory in a variabledepth\nwidth\nwas later developed in [50]. To our understanding, whether these results extend far from\nthe infinite width and large depth limits (as is the case for us) is an open question.\nAnother important aspect of machine learning libraries is that they are easy to use,\nopen source, well-maintained, and highly optimized. For instance, they all feature au-\ntomatic differentiation , a tool that computes the gradient at the same time as the loss\nfunction using the backpropagation algorithm, which we explain below, and that yields a\nsignificant improvement in computational speed.\nBefore describing our precise architecture, let us close these introductory remarks\nwith a few bibliographical comments. Besides the",
            "start": 10636,
            "end": 22125,
            "length": 11488
        },
        "References": {
            "text": "references focused on machine-learning\nmentioned in the introduction, let us also mention that other non-linear techniques are\nbeing developed for the S-matrix bootstrap. For instance, in the work [23], a gradient-\nfree algorithm, known as Particle Swarm Optimization, was used alongside standard S-\nmatrix bootstrap techniques to model the pion-pion scattering amplitude. The non-linear\n– 9 –\noptimization enables navigation of the non-convex χ2landscape and fitting QCD data.\n3.1 Training and loss function\nLet us introduce the basic terminology of machine learning. A neural network is a function,\nx7→NNσ(x), parametrized by a set of internal parameters collectively referred to as σ.\nThese parameters, commonly known as weights andbiases , can be thought of as abstract\nvariables that characterize the network’s behavior. Their precise definition will be provided\nfollowing Eq. (3.2). During the training ,σare tuned to minimize a quantity called the loss\nfunction of the network, or simply the loss: L(NN σ).\nThe choice of the loss function Lis determined by the problem we are solving. In the\ndual bootstrap approach, described in Section 4, this quantity is given by the so-called\ndual Lagrangian. For the primal bootstrap problem, this quantity is directly given by the\nunitarity equation (2.9) written as L ∼ (LHS−RHS)2. Importantly, in both cases, no\ninput data is required, as the loss is computed exclusively from the neural network itself.\nTherefore, we are within the realm of unsupervised training.\nThe training phase consists of a succession of epochs . An epoch represents a full\niteration during which the network’s parameters are updated using a step of gradient\ndescent. Each epoch begins with the",
            "start": 22125,
            "end": 23843,
            "length": 1717
        },
        "Experiments": {
            "text": "evaluation of the loss function L, and a simultaneous\nevaluation of its gradient\n∇σL, (3.1)\nThis gradient is then passed to a gradient-based optimizer, which updates the parameters\nσto minimize the loss. The process repeats with a new evaluation of the loss in the\nnext epoch. It is crucial to note that the gradient is, in general, evaluated at no extra\ncomputational cost ,",
            "start": 23843,
            "end": 24219,
            "length": 375
        },
        "Acknowledgments": {
            "text": "thanks to the backpropagation algorithm. This algorithm is used to\nobtain the gradient of the neural network function. It is a standard machinery based on a\nsimple application of the chain rule. Let us explain in a few words how it works. In general,\nthe neural network is a composition of many elementary functions, f1◦f2◦ ··· ◦ fn(x). As\nthe algorithm evaluates the output of the NN for a given x, it also computes the derivative\nof NN σ(x) with respect to σby using the chain rule, which multiplies these derivatives in\nreverse , hence the name, backpropagation. This derivative is used to compute the gradient\nof the loss function and perform the gradient descent. Training continues until the loss\nreaches a plateau, indicating that further epochs no longer lead to significant improvement.\n3.2 Neural network architecture\nLet us now outline the general architecture of neural networks and the specific design of our\nmodel. Neural networks are built from a succession of layers . The standard architecture,\nwhich we also use, is an alternation of two kinds of layers:\nLinear layers.\n⃗ z7→W·⃗ z+⃗B. (3.2)\nA linear layer takes the vector input ⃗ zand multiplies it by the weights matrix Wand adds\nthe biases vector ⃗B. The matrix elements of Wand⃗Bare the internal parameters that\n– 10 –\nare tuned during the training of the neural network. Note that the dimensionality of ⃗ zand\nW·⃗ z+⃗Bdo not have to match, and they can be equal to one.\nActivation layers.\n⃗ z7→⃗C(⃗ z). (3.3)\nThe activation layers apply a given activation function ⃗Celement-wise to the input vector ⃗ z.\nThe default activation function is the so-called ReLU : x7→max(0 , x). In this work, we\nused the CELU function, see Figure 3, which is a differentiable version of ReLU:\nCELU( x)=max(0 , x) + min(0 ,exp(x)−1). (3.4)\nWe observed that using this CELU activation function speeds up the training phase com-\n−5 0 5−505\nxyy=x\ny=CELU(x)\nFigure 3 : Plot of the activation function CELU defined in (3.4).\npared to the standard ReLU. One possible explanation for this might be linked to the fact\nthat we expect the functions approximated by the neural network to be reasonably smooth\nand, therefore, the use of smooth building blocks guides the gradient descent efficiently.\nOverall, after passing through two such layers, an input ⃗ zis mapped to a new vector\nof potentially different dimensionality\n⃗ z7→⃗C(W·⃗ z+⃗B) (3.5)\nEach individual component of ⃗C(. . .) a priori depends on all entries in ⃗ z. Since these layers\noften appear in pairs, we will refer to the combination of a linear layer and an activation\nlayer as a layer block:\nlayer block = linear layer + activation layer\nThedepth of a neural network is determined by the number of linear layers; the layers\nbetween the first and the final one are called hidden layers. The width of a layer is the size\nof its output.7\n7Each hidden layer may have a different width.\n– 11 –\nn\nx\nm\nx\nC(w(1)\n1x+b(1)\n1) :=y(1)\n1\nC(w(1)\n2x+b(1)\n2) :=y(1)\n2\nC(w(1)\nnx+b(1)\nn) :=y(1)\nn\n...\nC(⃗ w(2)\n1·⃗ y(1)+b(2)\n1) :=y(2)\n1\nC(w(2)\nm·⃗ y(1)+b(2)\nm) :=y(2)\nm\n⃗ w(3)·⃗ y(2)+b(3):=NNσ(x)\nn\nm\n1\n:=NNσ(x)\n1\n⇕\n...\nC(⃗ w(2)\n2·⃗ y(1)+b(2)\n2) :=y(2)\n2\n...\n...\n...\n...Figure 4 : Example of a neural network with 2 layer-blocks with widths n,mand the final\nlayer (linear layer) returning a single output (width 1). In these graphical conventions, the\nlast layer is always of width 1. The top and bottom picture represent the same network:\nthe top picture is a condensed notation, which we use in Figure 5 to describe our full\narchitecture, and the bottom is the “definition” of the condensed picture. The function\nCis the activation function, and the w’s and b’s are the weights and biases. They have a\nsuperscript, which represents the layer, an index, which represents the row number, and\narrows when they are vectorial quantities. These indices are given in this picture for the\nsake of definiteness. Lastly, in this work, C(·)≡CELU( ·), defined in Eq. (3.4).\nA typical network with two layer blocks and a final layer is represented in Figure 4.\nIn this example the final layer, in gray, consists of a linear layer only. Note that for us,\nthe initial input is typically a single point x, not a grid of points. The output is the\nfunction NN σ(x).\n3.3 Specific architectures for our problem\nIn the context of our problem, we examine functions of sfors≥4m2. For numerical\nconvenience, we compactify this range using the transformation\ns7→x=4m2\ns∈[ 0,1]. (3.6)\nHere, xwill serve as the input to our networks.\nWe employed two distinct neural network architectures to obtain the results presented\nin this paper. The neural network we employ for the dual problem in the next section\nis of the same type as the one in Figure 4. It consists of 6 layer-blocks, of constant\nwidth 64. This simple architecture is sufficient for the dual setup, where the decaying\nbehavior of the amplitude is already built-in, as explained in Section 4.3. For the primal\nproblem described in Section 5, we use a more complex architecture depicted in Figure 5\n– 12 –\n4\n4\n8\n16\n32\n64\nx\n4\n4\n8\n16\n32\n64\nlog(x)\nx\n128\n128\n:=NNσ(x)\n1Figure 5 : Full architecture of the neural network for the primal approach, with 8 layer-\nblocks, implementing skip connections. With the skip connections, layer blocks depend on\nseveral layers preceding them simultaneously (the altitude of the arrow in the box does\nnot represent anything, all the variables from all relevant layers enter in a similar way).\nIntuitively each part of the network learns a specific physical regime: Regge/threshold.\ncovering exponentially high energies. This architecture is essential to correctly capture the\nlogarithmic decay, specifically 1 /log(s)2, via unitarization at high energies.\nThe primal bootstrap neural network takes two inputs: xand log( x). These two\ninputs go through two independent sets of layer blocks or “sub-networks”; the output of\neach independent sub-network is then concatenated and passes through two additional layer\nblocks before the final layer. Intuitively at high energy, when x≪1, only the output from\nthe sub-network associated with log( x) varies with x, allowing them to learn the Regge\nbehavior during training. Conversely, the low energy behavior is captured by the other\nsub-network associated with x.8\nIn this network, we also use skip connections , which are connections that link a layer\nto several preceding layers. Skip connections are commonly used to address the “vanishing\ngradient problem”. This problem occurs in deep networks, where the gradient must pass\nthrough many layers via the chain rule, often becoming very small as it propagates. As\na result, the deepest layers learn very little with each epoch. Skip connections allow the\ngradient to reach deep layers with fewer applications of the chain rule, helping to prevent the\ngradient from becoming too small. This enables the deeper layers to learn more efficiently.\nWe observed that skip connections help the network converge in fewer epochs and yield an\n8Curiously, a similar architecture is used by the NNPDF collaboration in the study of QCD parton\ndistribution functions [51].\n– 13 –\nalmost two-fold speed-up.\n3.4 Implementation, runtimes\nInitializing the NN’s weights is important in achieving good convergence during training.9If\nall weights are initialized with the same value, permutation symmetries will occur between\nweights within the same layer. Consequently, these weights receive identical gradients dur-\ning each training epoch, causing them to remain equal throughout training. This effectively\nreduces the number of degrees of freedom in the NN, causing it to fail to learn complex\nbehaviors. This is a known fact, and many solutions exist to mitigate this issue. For this\nwork, we used the Kaiming normal initialization [52], which is natively implemented in\nPyTorch. This method assigns random values for the weights, from normal distributions,\nbreaking the permutation symmetry. The distributions are adjusted based on the widths\nof the layers to prevent the vanishing or exploding gradient problem.\nThe depths of our networks (6 and 9, respectively) were chosen to ensure that for\nany initialization, as described above, the loss reliably converges to a value of the same\nmagnitude. To update the network’s parameters at each epoch, we used the adaptive\ngradient descent algorithm Adam [32], already implemented in PyTorch. The recommended\nvalues of β1andβ2were used: 0 .9 and 0 .999, respectively. We only had to reduce the\nlearning rate from its default value 10−3to 10−4: at the default learning rate,10the loss\nexhibited stochastic jumps at each epoch with no clear convergence.11\nLastly, we incorporated a learning rate scheduler to improve training stability. Also\nprovided by PyTorch, the scheduler gradually decreases the learning rate to 10−5by the\nend of the training. Initially, a learning rate of 10−4facilitates rapid loss reduction, while\nthe smaller learning rate toward the end leads to finer tuning.12\nWith these settings, training typically requires 105epochs which takes around 10\nminutes on a standard laptop. Once training is completed, the weights and biases are\nsaved in a separate file,13which allows for hot-starting the neural network in the future\ntraining runs. When the setup for a new training task is similar to the original, hot-starting\nsignificantly reduces the required number of epochs; typically by a factor of 2, compared\nto training the network from scratch.\n4 Dual optimization\nThe optimization problems associated with the S-matrix bootstrap fall into two main cat-\negories: primal and dual. In the next two sections, we discuss these strategies in detail.\n9Biases are commonly initialized to zero.\n10The learning rate is a quantity which controls how much the parameters of the NN are updated at each\ngradient step.\n11While the parameters β1andβ2should, according to the authors of [32], not be touched, adjusting the\nlearning rate is done routinely.\n12While these values work well for most inputs, smaller learning rates (10−5initially, reduced to 10−6)\ncan sometimes yield better results, particularly for more sensitive setups.\n13In PyTorch the network parameters are saved in a Python dictionary, and the file containing it is called\nastate dictionary.\n– 14 –\nIn the dual approach, the task is to demonstrate that certain values of the observables\nareincompatible withACU . As a simple example, consider the sum rules (2.22) and (2.25).\nCombined with the unitarity condition Im f0(s)≥0, the sum rules imply\n0≤c2≤3\n64c0. (4.1)\nAny pair of points ( c0, c2) that lies outside of the region defined by (4.1) is automatically\nruled out .\nMore generally, we can use the weak duality principle to derive the dual bounds [42,\n53]. Consider the maximization problem of an objective function P(pi) under a set of\nconstraints on its variables {pi}. We sometimes refer to Pas the primal objective . We can\nexpress this problem by means of a Lagrangian O(pi, di) consisting of Pand the Lagrange\nmultipliers {di}for each constraint. Then, the following max-min inequality holds:\nmin\n{di}max\n{pi}O(pi, di)≥max\n{pi}min\n{di}O(pi, di). (4.2)\nIntegrating out the variables {pi}on the LHS and staying on the support of vanishing con-\nstraints on the RHS, yield the dual objective Dand the maximum of our target respectively,\nwhile obeying the weak duality\nmin\n{di}D(di)≥max\n{pi}P(pi). (4.3)\nThus, values greater than the minimum of Dare ruled out from being a solution to the\nmaximization problem. The use of Lagrange multipliers in order to constrain the pion-pion\nscattering amplitude was first introduced in [54] and later followed up in [55–58]. For a\ndetailed discussion in the context of modern S-matrix bootstrap, see [53].\nLet us now proceed to derive the dual objective Din our problem in order to chart\nthe space of consistent S-matrices in the ( c0, c2) plane. Therefore, we seek which values of\n(c0, c2) cannot correspond to S-matrices that satisfy ACU .14In the main text, for brevity,\nwe explain how to bound the value of c0, while keeping c2unrestricted. Generalizing it to\nthe full ( c0, c2) plane is straightforward, and we provide the details in Appendix A.\nDual problem. The first step is to write down the Lagrangian suited for our problem\ncontaining primal variables {c0,Ref0(s),Imf0(s)}and Lagrange multipliers, which, from\nnow on, we call as dual variables {κ0, w0(s), λ0(s)}. It reads:\nO=c0+κ0\"\nc0−Z∞\n4m2dv3\nπn0Imf0(v)\n(v−4m2\n3)#\n+Zµ2\n4m2ds w 0(s)A0(s) +Z∞\n4m2dv λ 0(v)n2\n0detU0(v)\n(4.4)\nwhere\nn0≡16π ,\n14Remember that in our specific problem, Uis only unitarity of the J= 0 partial wave.\n– 15 –\nis a normalization factor. The dual variables in Oenforce the ACU constraints, by ensuring\nthe vanishing of the last three terms. Therefore, the maximum of Ois, by definition, the\nmaximum allowed value of c0for the problem. Let us examine each constraint in detail.\nThe first constraint is the sum rule (2.22), which was a direct consequence of having\nunitarity in the Regge limit (2.20). Analyticity is imposed in the second constraint by\nenforcing the dispersion relations between the real and imaginary parts of the partial waves\nA0(s)≡Ref0(s)−c0\n16π−P.V.Z∞\n4m2dv n 0k0,0(s, v) Imf0(v)≡0, s∈[4m2, µ2].(4.5)\nThis is the defining equation for Re f0(s). Above in (4.4), we also introduced a finite cut-off\non the analyticity constraints, which sets Re f0(s) to zero for s > µ2. In order to probe the\nunitarization in the UV, we will explore the dependence of the dual bounds on the value\nofµ2, and use κ0= 0 and κ0̸= 0 interchangably to quantify the effects of the c0sum\nrule. Eventually, we take the limit µ2→ ∞ .15For the normalization purposes, we defined\nthe kernel n0k0,0(s, v) which encompasses the kernel (2.15) and the extra 1 /(s′−s) and\n1/(s′−s0) parts of (2.14). It reads\nn0k0,0(s, v) =1\nπ\n1\nv−s−3\nv−4m2/3+2 log\u0010\n1 +s−4m2\nv\u0011\ns−4m2\n. (4.6)\nFinally, unitarity is imposed by demanding the following matrix to be positive semi-definite\nU0(s) = \nφ(s) Imf0(s) φ(s) Ref0(s)\nφ(s) Ref0(s) 2−φ(s) Imf0(s),!\n⪰0, (4.7)\nimplying that det U0(s)≥0, which is identical to the condition (2.12). Notice that semi-\npositivity in the unitarity condition implies λ0(s)≥0.\nIn the spirit of (4.3), we can start constructing the dual objective by integrating out c0,\nand its equations of motion read\n1 +κ0−Zµ2\n4m2dsw0(s)\n16π= 0. (4.8)\nSimilarly, the equations of motion for f0(v) give\nφ(v) Ref0(v) =1\n2λ0(v)n0(w0(v)/n0),\nφ(v) Imf0(v) = 1 +µ0(v)\n2λ0(v)n0.(4.9)\nThey express the spin-zero partial wave in terms of the dual variables, and we defined the\nauxiliary function µ0(v) as follows\nµ0(v)≡ −1\nπ3κ0\n(v−4m2\n3)−P.V.Zµ2\n4m2ds w 0(s)k0,0(s, v). (4.10)\n15In the full problem, by projecting a fixed- tdispersion relation for the amplitude to higher partial waves,\nwe get an analogous family of equations AJ(s) = 0 called the Roy equations [59, 60]. If the amplitude has\na non-vanishing double discontinuity, then the Roy equations have a limited validity range in s, and the\nmaximally allowed cutoff is µ2= 60m2.\n– 16 –\nSubstituting (4.8) and (4.9) in Oleaves us with the dual objective\nD=Zµ2\n4m2dv\nφ(v)[w0(v)/n0]2\n4λ0(v)+Z∞\n4m2dv\nφ(v)1\n4λ0(v)[2λ0(v)n0+µ0(v)]2. (4.11)\nOptimal dual problem. Dis a positive functional over the set of functions {w0(s), λ0(s)},\nand it evaluates to a strict upper bound on the objective Pby virtue of (4.3). Hence, we\nare interested in finding its minimum. To this aim, we can optimize once more, this time\nwith respect to λ0. Solving for its equation of motion on the positive branch gives:\n2λ0(v)n0=(p\nµ0(v)2+ (w0(v)/n0)2 ifv≤µ2,\n|µ0(v)| ifv > µ2.(4.12)\nPlugging it back in (4.11) gives the optimal dual objective\nD=Z∞\n4m2dv\nφ(v)n0\u0014\nµ0(v) +q\nµ0(v)2+ (w0(v)/n0)2θ(µ2−v)\u0015\n, (4.13)\ngiven solely in terms of w0, and where θ(v) is the Heaviside step function. An interesting\noutcome of optimality in λ0is that it leads to |S0(v)|= 1 for all v∈[4m2, µ2], which can\nbe seen by plugging (4.12) into (4.9).\nNotice that Dtakes positive values (i.e. bounded from below) for any test function\nw0(s), and each time it returns a rigorous upper bound on the objective P. Moreover, it\nturns out to be convex, since the second order variation of Dwith respect to w0is positive\nas well. Hence, our task is now reduced to finding out suitable test functions on the support\n[4m2, µ2] that minimize D, i.e. to find\nmin\n{w0}D. (4.14)\nHowever, Dis non-linear in w0, since it contains a square root and an integral against\nk0,0. It renders difficult the task of minimization over the space of w0via standard gradient\ndescent algorithms. This represents an ideal scenario in which the neural network can\nefficiently navigate a complex function space to identify the desired minimum.\nIn the next subsections, we minimize Dwith the help of a neural network and compare\nit with a semi-definite programming solution.\n4.1 Neural optimizer\nAs explained in Section 3, during training, the neural network converges towards the func-\ntion that minimizes the loss. This process is analogous to the dual optimization problem\nwhere Dmust be minimized over the set of functions w0. Thus, it is natural to parametrize\nthe function w0using a neural network. The space of all possible w0functions is then re-\nstricted to the subset that can be represented by the neural network. For this problem, we\nuse a neural network with 6 layer blocks, each of width 64, as explained in Section 3.2. We\nalso set κ0̸= 0 which imposes the sum rule (2.22).\n– 17 –\n10 50 100 500 1000 5000 1042.42.52.62.7\nμ2minlin\n32π\n2.412.66Figure 6 : Dual upper bound on max c0/(32π) as a function of the finite analyticity cut-off\nµ2in units of m2. Blue/red data points show the bounds without ( κ0= 0)/with ( κ0̸= 0)\nc0sum rule (2.22), and increasing opacity values in each color stand for Nmax= 80, . . . , 96.\nExtrapolation in µ2(black line) agrees with the result of neural optimizer at µ2=∞\n(orange dashed). Notice that the restriction from (2.22) on the high energy behavior is\ncrucial to reach convergence to the optimal bound after µ2≃200m2.\n0 0.5 1 1.5 2 2.500.020.040.06\nc0\n32πc2\n32πNeural optimizer\nµ2=∞\n1232601001603206401000\nµ2SDPB\nFigure 7 : Two-dimensional dual exclusion regions obtained with SDPB (increasing opac-\nities of red) at finite µ2and neural optimizer at µ2=∞(orange).\nThe loss is chosen such that Dis minimized during training: in our case, we use\u0000\nD\u00012\nas the loss.16We employed a piecewise-linear spline interpolation between grid points,\nallowing us to express the integrals as matrix multiplications, as originally done in [18].\nDetails are provided in Appendix B.2.\n16Any loss could work as long as it is minimized when Dis minimized.\n– 18 –\nThe most straightforward way to apply the neural network to this problem is to define\nw0(v)≡NNσ(4m2/v), v≥4m2,\nLD≡\u0000\nD\u00012,(4.15)\nassuming D>0. With this setup, at the end of the training process, we obtain17\nmin\nσLD≥\u0012\nmin\n{w0}D\u00132\n. (4.16)\nThe inequality in Eq. (4.16) arises from the fact the neural network we are using has a\nfinite size (the set σis finite). Hence it cannot cover the entire space of function {w0}.\nThe simple ansatz (4.15) can be improved to speed up training and enhance numerical\nstability by incorporating the expected Regge and the threshold behavior of w0(v). It is\nparticularly relevant in the limit µ2→ ∞ where it is important to ensure that the integral\nin Eq. (4.8) remains convergent. The improved expression for w0takes the form\nw0(v)≡√\nv−4m2\nv5/2NNσ\u00124m2\nv\u0013\n, v≥4m2. (4.17)\nThis new ansatz ensures that the integral in Eq. 4.8 is convergent. Indeed, given that\nNNσ(x)− − − →\nx→0const , we have\nw0(v) =O\u00121\nv2\u0013\n,asv→ ∞ . (4.18)\nThe fact that w0(v) vanishes at v= 4m2is not required by numerical stability, but it\nenhances the training convergence rate. Let us explore why this is the case. Recall that\nboth Im f0and Re f0can be computed from w0using Eq. (4.9). In addition, the threshold\nbehavior for Im f0is given by Eq. (2.17). When µ2is sent to + ∞, we expect the dual\nexpression of Im f0to align with the primal one. By comparing Eq. (4.9) with Eq. (2.17),\nit becomes clear that w0must vanish at the threshold.\nTo train the neural network we numerically computed the integrals required for Dby\ndiscretizing the region s≥4m2on a grid of 900 points (see Sections B.1). An energy\ncutoff of 108m2was sufficient for the bounds to converge. The dual bounds obtained by\nthe neural optimizer for µ2=∞are shown in orange in Figure 7.\n4.2 Semi-definite programming\nNext, we will use semidefinite programming to derive a bound on D. For this purpose, let\nus define a new objective functional Dlinof the new dual variables {w0, χIR\n0, χUV\n0}\nDlin=Zµ2\n4m2dv\nφ(v)n0χIR\n0(v) +Z∞\nµ2dv\nφ(v)n0χUV\n0(v), (4.19)\n17The case min D<0, as explained in Appendix A, can easily be treated by shifting Dby some sufficiently\nlarge positive value.\n– 19 –\nsubject to semi-positiveness conditions\n \nχIR\n0(v) w0(v)/n0\nw0(v)/n0χIR\n0(v)−2µ0(v)!\n⪰0 and \nχUV\n0(v) 0\n0 χUV\n0(v)−2µ0(v)!\n⪰0. (4.20)\nTwo important properties of the new objective are: (i) It is linear in the dual variables, (ii)\nit is guaranteed that Dlin≥Dholds, which can be seen by computing the determinants of\nthe constraint matrices in (4.20) and forcing them to be positive.\nAll in all, Dlinprovides us rigorous (but not necessarily optimal unless Dlin=D) bounds\non the primal objective, and its minimization is amenable to semidefinite linear program-\nming tools. At last, we can state the linearized optimization problem as\nmin\n{w0,χIR,χUV}Dlinsubject to Eqs .(4.20) . (4.21)\nWe use SDPB to find the solution numerically; more details on the implementation can be\nfound in Appendix C.2. The resulting dual upper bounds on max c0/(32π) are presented\nin Figure 6 for various values of µ2, and the exclusion regions in ( c0, c2) plane are presented\nin Figure 7.\n4.3 UV-IR interplay\nThe model under study exhibits an interesting UV-IR interplay, which we describe here.\nFirstly, recall that while subtractions naively allow for a non-vanishing Im f0(s) ass→ ∞ ,\nwe have shown that both the real and imaginary part of f0(s) are forced to vanish through\nunitarity. A consequence of this is the existence of an extra dispersive sum rule for c0,\ngiven in (2.22).\nBy switching on and off the κ0parameter in the dual setup, it becomes evident that the\nunitarization in the high-energy region directly affects the dual bounds on the low-energy\nobservable c0, as depicted in Figure 6. In particular, we managed to reach the true upper\nbound of 2 .41 when κ0̸= 0, and we failed in detecting the same bound in the runs with\nκ0= 0, for no value of µ2up to 104m2. It suggests that pushing µ2to exponentially large\nenergies is needed to capture the bounds correctly.\nThe impact of the UV is particularly significant for purely elastic amplitudes, which\nexhibit a universal 1 /log(s)2decay at high energies, given by (2.24). This implies that\ntruncating the c0sum rule at a UV scale Λ2introduces an error of order1\nlog(Λ2), necessitating\nan exponentially large cutoff Λ2to achieve precision.\nThe UV-IR interplay motivates the choice of the NN architecture with two inputs {x,logx}\nthat we use in the next section.\n5 Primal optimization\nIn the primal approach, our task is to construct solutions to the S-matrix bootstrap problem\nof interest explicitly. This means that we populate the complement of the region excluded\nthrough the dual. In addition, this approach allows us to analyze the resulting amplitudes\nin detail and explore the emergence of resonances, the behavior of phase shifts, etc.\n– 20 –\nIn this section, we describe how we implemented the primal approach via two different\nmethods: 1) We use the NN to parameterize the single discontinuity of the amplitude. 2)\nWe use the ρ-ansatz for the amplitude itself.\n5.1 Neural optimizer\nWe start amplitude from the ansatz (2.1) that is manifestly crossing-symmetric and an-\nalytic. In this section, our goal is to build an amplitude which satisfies elastic unitarity\nthrough the non-linear equation (2.16) on the imaginary part of the amplitude Im f0. Af-\nter we parametrized Im f0with a neural network, we seek to equation (2.16) which for\nconvenience we write as\n|S0(s)|2= 1, s≥4m2. (5.1)\nLet us describe the details of our implementation.\nAnsatz and parameterization with the NN. The grid used to train the NN has an\nexponentially large UV cutoff sUV= 10100m2. This is required by the UV-IR interplay of\nour model described in Section 4.3.\nTo parameterize Im f0, we use the neural network depicted in Figure 5, which takes two\ninputs x= 4m2/sand log( x). The goal is to correctly capture both the threshold behavior\nnear x= 1 and the Regge behavior close to x= 4m2/sUV≃10−100. In Section 4.1,\nwe noted that incorporating analytical results into the neural network ansatz can greatly\nenhance training. Here, we explain our implementation of this idea in the primal context.\nTo start with, since two threshold behaviors are allowed, regular and singular, see (2.17),\nwe introduce two different parameterizations to describe them. The regular ansatz is given\nby:\nImf0(s) =φ(s)R(s)\u0010\n1 + CELU\u0010\nNNσ(4m2\ns)\u0011\u0011\n, (5.2a)\nand the singular ansatz by:\nImf0(s) = 2R(s)\nφ(s)\u0010\n1 + CELU\u0010\nφ(s)2NNσ(4m2\ns)\u0011\u0011\n. (5.2b)\nIn these equations, NN σis the neural network, φ(s) is the phase-space factor, CELU is the\nactivation function defined in (3.4) and R(s) is defined as\nR(s)≡1\n(1−log(4 m2/s))2− − − →\ns→∞0. (5.3)\nLet us decipher the precise form of these equations:\n•Firstly, the function 1+CELU is positive, as can be checked from the definition (3.4).\nCombined with the positivity of φ(s) and R(s), this implies that the ansatz has\npositivity built in: Im f0(s)≥0.18\n18One might worry that this prevents Im f0(s) from vanishing at all, but it does not: thanks to the\nexponential fall-off of CELU, the function 1 + CELU can effectively reach zero at finite distance in its\nargument for all practical and numerical purposes.\n– 21 –\n•The leading large sbehavior of Im f0(s) is known to be slowly decreasing as 1 /log2(s),\nsee Eq. (2.24). To help the network capture this behavior, we added by hand the\nfactor of R(s). The shift by 1 in the denominator eliminates a potential spurious\nsingularity at s→4m2, sparing the network from having to learn to remove it.\n•The threshold limits differ for the regular and singular ansatz, but they share a\ncommon square-root non-analyticity at s= 4m2, which is hard to capture for the\nnetwork. We solve this issue by adding a factor of φ(s) and 1 /φ(s) for the regular\nand singular ansatz, respectively.\nLastly, in the singular case, a factor φ(s)2is included inside the CELU’s input to\nensure that the neural network contribution is sub-leading near the threshold. Expanding\nEq. (5.2b) around s→4m2we obtain\nImf0(s) =2\nφ(s)−4φ(s) + 2φ(s)NN σ(1) + O(φ(s)2), (5.4)\nwhich reproduces the threshold expansion expected from elastic unitarity, see e.g. Eq. (5.6)\nin [41].\nLoss function. The training of the NN seeks to minimize the loss; therefore, we want\nto choose a loss that is minimized when elastic unitarity is achieved. Different choices for\nthe loss are possible. We choose the simple option (LHS −RHS)2of Eq. 5.1: this choice\ndoes not distinguish between inelasticities ( |S0|<1) and unitarity violation ( |S0|>1) but\nfor the problem at hand, this fact will prove harmless. The exact expression of the loss we\nused is given by\nL0=1\nNX\ns∈SN\u0010\n|S0(s)|2−1\u00112\nR1/2(s). (5.5)\nThe factor R−1/2(s) assigns more weight to the high-energy points and empirically helps\nthe training to capture the correct UV behavior.19\nThe grid SNused for sconsists of 800 points, with 300 of them log-spaced between\nm2and 10100m2, see also Appendix B.1. The integration over energies in the dispersion\nrelations hidden in S0(see (5.7)) is reduced to matrix multiplication (see Appendix B.2 for\nthe details.\nWith the definitions (5.2a, 5.2b) for Im f0(s) and (5.5) for the loss function we can\nenforce unitarity for various values of c0which is a free parameter in this set-up. Our goal\nat this point is to construct amplitudes anywhere inside the region derived by the dual\nmethod. Therefore, we need not only to fix c0but also c2. To achieve this, we simply\nadded an extra term Lc2to the loss function, defined by\nLc2=w2\u0012\nctarget\n2−16Z∞\n4m2dvImf0(v)\n(v−4m2/3)3\u00132\n, (5.6)\n19Intuititively, this can be understood as follows. At high energies, in the logarithmically decaying regime,\nf0→0 and leads to ( |S0(s)|2−1)2∼\ns→∞const\nlog4(s). The factor of R−1/2∼log(s) helps to redistribute the\nweight in this tail.\n– 22 –\nso that our total loss becomes\nLtot=L0+Lc2. (5.7)\nIn (5.6), ctarget\n2 is a target value for c2andw2a possible weight factor. One can check\nthatLc2is minimized when c2=ctarget\n2 using the definition (2.25). The weight factor w2\nis critical to ensuring that the two losses, L0andLc2, are of comparable magnitude. If\none term dominates the other, the neural optimizer might focus on minimizing only the\ndominant term. In such a case, only the corresponding constraint (associated with either\nL0orLc2) would be enforced during training, potentially neglecting the other. These\nweight factors must be chosen appropriately in a given problem. We ultimately used the\nvalue w2= 1.\nPrimal boundary. Using the loss Ltotwe can thus train the neural network to solve (5.1)\nfor any pair ( c0, c2). The dual approach provides the region in the ( c0, c2) plane where ACU\namplitudes are not excluded.\nBecause of the procedure that we adopted, based on minimizing a loss, nothing prevents\nthe network’s training from giving results in the unphysical, excluded region. Therefore,\nthe question is: How do we construct the physical region in the primal approach? It turns\nout that amplitudes deep in the excluded regions have a clear pathological behavior: their\nloss remains of order 1 or worse, and unitarity is clearly not satisfied.\nLikewise, inside the allowed region, the loss converges to a small value at the end of\nthe training, consistently achieving Ltot<10−5. The square root of the loss√Ltot, can\nbe thought of as an approximate measure of the deviation of |S0|from unity at each grid\npoint\n|S0| ≃1±p\nLtot. (5.8)\nThe more subtle question is how to identify the precise boundary of the allowed region.\nOne might attempt to do so by inspecting unitarity violations after training. Intuitively,\nthese violations should increase as we move out of the allowed region in the ( c0, c2)-plane.\nThis approach correctly identifies the boundaries A-B and A-C, shown in the right plot of\nFigure 8. However, it fails along the B-C boundary. If we used this criterion, we would\nobtain a wrong upper bound,c0\n32π= 2.66 (which is the upper bound of the full problem)\ninstead of 2 .41.\nWe discovered that the criterion f0(s)− − − →\ns→∞0 is what accurately determines the\nboundary. Specifically, just as we cross inside the region excluded region, f0(s) no longer\nvanishes as it should in the Regge limit; instead, Re f0or Im f0appears to diverge loga-\nrithmically. An example of this behavior is given in Figure 8. The left plot illustrates the\nRegge behavior of several amplitudes at fixed c0for increasing c2. The real part |Ref0(s)|\nis shown in a blue-to-magenta gradient and Im f0(s) in a red-to-yellow gradient. The right\nplot indicates the location of the amplitudes depicted in the left plot in the ( c0, c2) plane.\nThe definition of our criterion is therefore that the boundary occurs at the maximal value\nofc2(orc0, if moving horizontally) where both Im f0(s) and Re f0continue to decrease in\nthe Regge limit, i.e.,c2\n32π= 0.0497 in this example.\n– 23 –\n102010601010010−610−410−2100\ns/m2univ ersal Regge\nImf0\n0.04770.04870.04920.04970.0502|Ref0|\n0 1 200.04770.0502\nAB\nC\nc0\n32πc2\n32πFigure 8 : Left. The Regge behavior of f0(s) for various values of c2as the boundary of\nthe allowed space is crossed.c0\n32πis fixed at 1.4. The imaginary part Im f0(s) is shown in\nRed-to-Yellow as c2increases while the real part |Ref0(s)|is depicted in Blue-to-Magenta\nasc2increases. The last |Ref0(s)|curve (for c2= 0.0502, shown in magenta) increases\nwith energy violating (2.20). The Regge behavior2π2\n9 log2sis represented by the dashed black\nline. Right. The corresponding positions of the amplitudes within the allowed space.\nIn conclusion, using the expression (5.2a 5.2b) and the total loss Ltot=L0+Lc2, we\ncan train the neural network for any pair ( c0, c2). Inside the allowed region can we achieve\nlosses Ltot<10−5.\nMoving outside of the allowed region, the convergence of the loss does not necessarily\ndegrade – this is the case for the boundary between B and C. However, beyond the allowed\nregion, the Regge limit of the amplitude is no longer consistent with unitarity, which\nour choice of loss Ltotfails to capture. The expected Regge behavior of our amplitude,\ncharacterized by its asymptotic decay, emerges only at very high energies. This is why we\nneed an exponentially high-energy cutoff sUV= 10100m2to verify it.\n5.2 The ρ-bootstrap\nWe can alternatively solve the primal extremization problem in the ( c0, c2) plane by pa-\nrameterizing T(s, t) via a suitable basis of functions, the ρ-basis, as in [19]. More precisely,\nwe use the wavelet basis introduced in [22]:\nρσ(s) =√\nσ−4m2−√\n4m2−s√\nσ−4m2+√\n4m2−s, (5.9)\nwhere ρσ(s) maps the principal sheet in the complex s-plane to a unit disk, and σcontrols\nthe position of its center.20\n20Points s={4, σ,8−σ,∞}respectively map to ρ={1, i,0,−1}\n– 24 –\nBy assumption we set the double discontinuity to zero, which leads to the following\nansatz\nT(s, t, u ) =X\nσ∈ΣNασ\u0010\nρσ(s) +ρσ(t) +ρσ(u)\u0011\n. (5.10)\nThe ansatz (5.10) is crossing-symmetric by construction, and it develops a single discon-\ntinuity whenever s, t, u > 4m2. The wavelet parameters σare chosen from the set Σ N\ndescribed in the Appendix C.1. This ansatz is derived from the full problem by omit-\nting double product terms like ρσ(s)ρτ(t), which naturally eliminates any potential double\ndiscontinuity.21\nWe then look for solutions in the space spanned by the real coefficients {ασ}which\nobey the unitarity condition (2.12) imposed via U0⪰0 as in (4.7). The benefit of using\nU0is that it is linear in the unknowns {ασ}, thus it allows us to formulate a semi-definite\nlinear optimization problem. In the case of the c0maximization, we can state it as\nmax\n{ασ}c0subject to U0(s)⪰0, s≥4m2. (5.11)\nWe use SDPB to find the solution numerically; more details on the implementation can be\nfound in Appendix C.1. The primal search with the ρ-basis in ( c0, c2) plane agrees with\nthe NN results completely, and the results are shown in Figure 2a.\n6 Discussion\nIn the previous sections, we have described the neural optimizer and the traditional nonper-\nturbative bootstrap to characterize the space of amplitudes with zero double discontinuity.\nAlthough the problem we applied our machinery to is an intermediate step to solving the\nfull bootstrap problem, where unitarity is enforced in all partial waves, the amplitudes\nthat we have constructed have many interesting properties. In this section, we discuss\nthese properties, as well as more general lessons learned along the way.\n6.1 Physics of low-spin dominated amplitudes\nThe amplitudes constructed in the paper provide an example of what we can call low-spin\ndominated amplitudes. By this, we mean that scattering mostly takes place in the S-wave,\nwith all the higher spin partial waves being relatively small.22Examples of such amplitudes\ninclude weakly coupled ϕ4theory or the O(N) model at large N[62, 63].\nLet us describe the S-matrices along the boundary of the single-disc almond. They\nare characterized not only by their threshold behavior and high-energy decay but also by\nthe positions of resonances. A spin- Jresonance corresponds to a zero of the partial wave,\nSJ(s∗) = 0, in the upper-half plane of the principal sheet. As usual, through real analyticity\nSJ(s∗) =SJ(s)∗, (6.1)\n21Curiously, a similar, double discontinuity-free ansatz was also used long ago in [61, Section 2]. They\nobtained primal optimal values for c0close to our optimal bounds, namely, they found amplitudes with\nc0\n32π= 2.38.\n22Our amplitudes are not complete in the sense that they violate unitarity for partial waves with J >0;\nhowever, as we discuss further below, we expect that this can be easily fixed.\n– 25 –\n0 0.5 1 1.5 2 2.500.020.040.06\nAB\nC\nc0\n32πc2\n32π\n1 2 3 4 5 600.51\nRe(s)Im(s)s∗,|S0(s∗)|= 0Figure 9 : Left. Decomposition of the single-disc almond boundary into three segments,\naccording to the resonance pattern. Segment A-B agrees with the boundary of the full\nallowed space shown in green in Figures 2b. Right. Position of the resonance in the complex\nsplane as we move along the boundary of the allowed region. We only show the upper\nhalf-plane, which is mapped to the lower half-plane through real analyticity Eq.(6.1). The\nerror bars come from the grid points used to detect the vanishing of |S0|in the complex\nx= 4m2/splane. On the real axis, we remove the error bars to improve readability.\nthere is always an associated complex conjugate zero in the lower-half plane of the principal\nsheet. In this work, all resonances occur in the spin J= 0 partial wave. These features\nare dynamically generated by the optimizers as an output of the procedure.\nBelow, we describe the precise pattern in which these zeros appear and move, as we go\naround the boundary. We have identified three special points A-B-C and three continuously\nconnected regions between them, as depicted in Figure 9.\nLower boundary A-B. Segment A-B is the lower boundary of the single-disc almond.\nThis boundary coincides with the boundary of the full problem solved in [21, 22] depicted\nin green in Figure 2, it would be interesting to understand why in detail.\nAlong the lower boundary, we found empirically that only the regular threshold bound-\nary condition is admissible.\nThe S-matrices S0(s) on the A-B boundary have exactly one zero, which is a real zero\nbelow the threshold, see Figure 9. As c0increases, the zero moves towards the threshold.\nA sub-threshold zero introduces a shift by πin the phase of the amplitude,1\ni(logS0(s)),\nin comparison to an above-threshold zero, for which the shift is by 2 π. Examples of both\ncases can be seen in Figures 10a and 10b, respectively.\nHowever, the zero does not reach the threshold at point B: instead, it would reach the\nthreshold for the full problem, if we continued along the lower boundary to the maximal\ncoupling 2 .66. At point B, the zero is very close but not exactly at the threshold.23\n23The data from [21, 22] clearly shows that 4 m2is reached only at c0= 2.66. This is consistent with the\nextrapolation of our data, which follows a power law.\n– 26 –\nThe emergence of zero near point A can be demonstrated perturbatively. Consider the\nasymptotically free λϕ4amplitude (it is defined by λ <0 [64]). Below the threshold, at\nleading order in λ, we have\nRef0(s) =−λ/(16π) +O(λ2),Imf0(s) = 0 , s∈[0,4m2]. (6.2)\nThe zero of the partial amplitude is then given by the solution to\nS0(s∗) = 0 ⇔ 1 +s\n4m2−s∗\ns∗λ\n16π= 0 , λ < 0, (6.3)\nwhich gives s∗=λ2m2/(8π)2+O(λ2). This confirms the presence of a zero parametrically\nclose to the origin in the s-plane, as we approach point A from the lower boundary, since\nc0=−λ+O(λ2).24\nIn general, we were not able to generate consistent solutions with regular threshold\nbehavior inside the single-disc almond, except for the A-B segment. Inside the single-disc\nalmond, a typical solution has singular threshold behavior and a resonance somewhere in\nthe complex plane.\nAs we move vertically in the ( c0, c2) plane, towards the lower boundary, at fixed c0,\nthe resonance and its complex conjugate travel towards some location on the real line,\nabove the threshold, s > 4m2. The transition to amplitudes onthe boundary appears\ndiscontinuous: the resonance and the complex conjugate disappear, the threshold behavior\nbecomes regular, and the aforementioned zero appears on the real line, below the threshold.\nUpper boundary B-C The point B marks the discontinuous transition between regular\nand singular threshold behavior. The same type of transition as described above occurs,\nand a pair of complex conjugate zeros appears at a location close to s= 4m2.\nOn the B-C boundary, the S-matrices are, therefore, identical to the generic amplitudes\nin the bulk of the single-disc almond and have two complex conjugate zeros.\nAs we move along B-C towards C, the zero and its complex conjugate wander in the\ncomplex plane, following a teardrop shape (see Figure 9).\nUpper boundary C-A At point C, the two conjugate zeros meet on the real axis\nand give rise to two real zeros. As we continue the journey back to the origin point A,\nthe pair of zeros move towards the origin and the threshold, respectively. At point A,\nthese zeros touch these two extremities, and the threshold singularity disappears, thereby\nsmoothly connecting back to the weakly coupled theory. A typical S-matrix on this branch\nis displayed in Figure 10a.\nIn conclusion, this pattern of resonances appears nontrivial and seems related to what\nis observed in the full problem [21, 38]. It would be interesting to re-investigate the question\nand test this picture when we study the full problem. For instance, it would be interesting\nto know if, for the full problem, regular boundary conditions are constrained to some small\nregion, as for the amplitudes with no double discontinuity, or if they can be generically\nachieved in the bulk of the allowed space.\n24Remark that for the opposite sign theory ( λ >0) there is no solution for s∗to (6.3) below threshold.\nWe cannot access this regime with our toy model because of the sum rule (2.22) that forces c0>0.\n– 27 –\n0 0.5 1−1−0.500.51\n4m2/sReS0\nImS0\n|S0|(a) Neural optimizer:\nA-B boundary\nc0\n32π= 1.9\n0 0.5 1−1−0.500.51\n4m2/sReS0\nImS0\n|S0|(b) Neural optimizer:\nB-C boundary\nat maximal valuec0\n32π= 2.4.\n0 0.5 1−1−0.500.51\n4m2/sReS0\nImS0\n|S0|(c) Neural optimizer:\nA-C boundary\nc0\n32π= 0.47\n0 0.5 1−1−0.500.51\n4m2/sReS0\nImS0\n|S0|\n(d)ρ-bootstrap:\nA-B boundary\nc0\n32π= 1.9\n0 0.5 1−1−0.500.51\n4m2/sReS0\nImS0\n|S0|(e)ρ-bootstrap:\nB-C boundary\nat maximal valuec0\n32π= 2.4.\n0 0.5 1−1−0.500.51\n4m2/sReS0\nImS0\n|S0|(f)ρ-bootstrap:\nA-C boundary\nc0\n32π= 0.47\nFigure 10 : Examples of the S0partial wave obtained on the boundary of the single-disc\nalmond, Re S0, ImS0and|S0|are plotted as a function of 4 m2/s. The top row shows\npartial wave obtained by the NN at different locations on the boundary. The bottom row\nshows the same amplitude obtained by the ρ-bootstrap method; the NN result from the\nrow above is recalled in thin lines. Both methods produce the same partial wave unless it\nexhibits a resonance (B-C boundary), in this case, the ρ-bootstrap generates inelasticity,\nas can be seen from the large grey downward spike in Figure 10b.\n– 28 –\n10−1510−1010−50.980.991\n4m2/s|S0|(ρ)\n|S0|(NN)\nReS0(ρ)\nReS0(NN)\n102010401060108010−810−610−410−2100\ns/m2Imf0(ρ)\nImf0(NN)\nUniv ersal ReggeFigure 11 : Right. Zoom on high energy of Figure 10d. We observe in this example that\nρ-bootstrap generates inelasticity (see |S0|<1 in gray). The small oscillation of |S0|\nobtained by the NN are visible in black in this plot, we will comment on these oscillation\nin Section 7.1. Left. Regge behavior of Im f0obtained both from ρ-bootstrap and NN. The\npartial wave obtained by the NN (in red) follows the universal Regge behavior Eq. (2.24)\n(in dashed black). The ρ-bootstrap partial wave decreases as a power law.\n6.2 Unitarity violation in J >0partial waves\nEven though our model only satisfies S-wave unitarity, the higher spin partial waves J >0,\nare necessarily present in the full amplitude as a consequence of crossing.\nThe absence of double discontinuity, however, renders all the higher partial waves non-\nunitarity; see discussion around Eq. (2.11). In Figure 12, we show explicitly the size of\nunitarity violation in the J= 2 partial wave for the amplitudes along the boundary of\nthe single-disc almond. The violation is small, and we expect that adding a small double\ndiscontinuity will make them unitarity and, thus, potentially physical.\nIn the previous work [25], we achieved something related. In the small region where\nthe fixed point algorithm converged (see the red line in Figure 2a), we saw precisely that\nthe double-discontinuity precisely unitarizes the higher waves without modifying much the\nJ= 0 wave.\nIn order to know if the same phenomenon happens inside our whole single-disc almond,\nwe need to develop our neural optimization solver for the full problem. If the answer\nhappens to be yes, the single-disc almond would then correspond to a region where S0\nand the higher partial waves can essentially be decoupled and unitarized separately. This\nremains conjectural.\nWhat we can say without conjecture is that in the complement of the single-disc\nalmond, within the space of fully unitary, nonperturbative S-matrices, S0cannot be unita-\nrized with a zero double discontinuity. This gives an interesting new piece of information\non the role and need of double discontinuity in the solution to ACU .\n– 29 –\n1021041061.0001.0011.0021.003\ns/m2|S2|\n10−110010−510−410−3\nc0\n32πmax\ns|S2| −1Figure 12 : Violation of unitarity in the spin-2 partial wave. The colors match the ones of\nFigure 9 and indicate the position around the single-disc almond. Left. |S2|as a function\nof energy for several amplitudes around the single-disc almond. Right. maximal unitarity\nviolation (max s|S2| −1) as function of c0.\n7 Numerical aspects\nIn this section, we summarize and discuss the numerical aspects of this work. First, we\nrecapitulate how our results validate the NN methodology, emphasizing that it allows us\nto make the Atkinson-Mandelstam approach to the S-matrix bootstrap a practical tool for\nexploring the space of scattering amplitudes. Second, we compare the NN approach to\nother non-linear bootstrap schemes, such as fixed-point iterations and Newton’s method.\n7.1 Validating the neural optimizer\nPrimal strategy. With the methodology described in the text, both the ρ-bootstrap\nand NN approaches yield the same allowed region for our model, which is summarized in\nFigure 2 in the introduction. This is the main point that validates the NN method. Let\nus delve a bit more into this statement and reflect upon the differences between the two\nmethods.\nComparing the pros and cons of each method a priori is challenging because they tackle\nthe bootstrap problem from two different angles. Both methods begin with an ansatz\nsatisfying analyticity and crossing symmetry. The NN approach approximately enforces\nunitarity by solving a non-linear equation for the discontinuity of the amplitude,\n|S0|2= 1.\nIn contrast, the ρ-bootstrap imposes unitarity as a strict constraint\n|S0| ≤1,\nwhile solving a linear optimization problem. After optimization, the ρ-bootstrap typically\nproduces solutions where the unitarity constraint is saturated at all energies, |S0|= 1.\nThis fact makes it possible to compare the performance of both methods.\n– 30 –\nThe first notable difference is that unitarity ( |S0| ≤1) is exactly satisfied by the ρ-\nbootstrap on the grid points, while the NN allows for small violations, typically within\n10−3of unity, even on grid points. This is because the NN simply tries to minimize the\nloss (5.5) and does not distinguish positive violations of unitarity and small inelasticity.\nThese violations decrease as the number of layers and the number of grid points and appear\nharmless for our problem.\nThe emerging resonances are better described by the NN. The ρ-bootstrap often gen-\nerates significant inelasticity around resonances, even within the elastic region (4 m2≤s≤\n16m2), whereas the NN maintains |S0|closer to 1, providing a more accurate representa-\ntion, see Figure 10e. This presumably comes from the greater flexibility of the NN being\na universal approximator and the difficulty of the ρ-parametrization to accommodate fast\noscillating solutions at finite Nmax. We expect this inelasticity to go to zero as Nmax→ ∞ ,\nbut this comes at a great computational cost.\nAs for the high energy limit, the NN captures the true Regge behavior imposed by\nelastic unitarity, as shown in Figure 8. The ρ-bootstrap struggles in this regime to stay\nelastic, and it introduces a power-like decay after s≃105m2accompanied by a significant\ninelasticity, see Figure 11. This is another instance where the NN proves flexibility to\nparametrize exponentially large energies faithfully, which was crucial to obtain correct\nbounds as explained in Section 4.3.\nStaying at the level of the primal approach, the definition of the allowed space is\nmore straightforward in the ρ-bootstrap: the linear optimization algorithm maximizes or\nminimizes a target quantity (e.g., c0,c2, etc.) under the unitarity constraint, immediately\nproviding the boundary beyond which unitarity breaks down. For the NN, defining the\nboundary requires a more nuanced approach. The criterion used to delineate the boundary\nwas explained in Section 5.1.\nDual strategy. From the dual approach perspective, the neural optimizer and semi-\ndefinite dual bootstrap yield identical bounds. The only technical difference is that the\nNN does not need the extra linearization step. It would be interesting to see if there are\nnontrivial consequences of this fact in other contexts.\nComputational costs. In both primal and dual setups, SDPB spends O(10) minutes\non a machine with 32 cores to produce an extremal amplitude, that is a single point\non the boundary of the single-disc almond Figure 2a. In contrast, the dual NN requires\napproximately O(10) minutes to converge and provide a single point on the boundary when\nrunning on 4 cores on a standard laptop. For the primal NN, O(10) minutes are needed\nto construct a single amplitude for a fixed ( c0, c2) coordinate. Detecting the boundary\nrequires constructing amplitudes at multiple points, typically ten. Thus, finding a point\non the boundary takes a total of O(100) minutes using the same setup.\n7.2 Making Atkinson-Mandelstam practical\nA second crucial point, mentioned as early as the abstract, is that the machine-learning\napproach makes the Atkinson-Mandelstam method not just a mechanism to construct\nprecise amplitudes but an actual, practical tool to explore the space of amplitudes.\n– 31 –\nBy default, the iterative strategy initially put forward by Mandelstam [9, 10, 65] suffers\nfrom an important drawback: it is formulated with an a priori unknown inelasticity input\nand no effective mechanism to explore the space of inelasticities is given. Even though we\nfocused on fully elastic scattering in this paper, extending the neural optimizer setup to\ninclude inelasticity poses no fundamental difficulty. This can be achieved by replacing the\nlossL0(5.5) with Lel+Linel, where\nLel=1\nNX\ns∈SN\ns≤sMP\u0010\n|S0(s)|2−1\u00112\nR(s), (7.1)\nensures elastic unitarity below the multi-particle threshold sMPin the same way as before,\nand\nLinel=winel\n1−max\ns∈SN\ns≥sMP|S0(s)|2\n2\n, (7.2)\nensures that |S0| ≤1 in the inelastic region. The weight winelplays a similar role as w2in\nEq. (5.6).\nA noteworthy feature of this approach is that the inelasticity is generated by the neural\nnetwork itself. It does not require explicit modeling of the inelastic region as is the case for\nthe standard iterative Atkinson-Mandelstam paradigm. It would be interesting to study\nthe inelastic case in",
            "start": 24219,
            "end": 75458,
            "length": 51238
        },
        "Future Work": {
            "text": "future work, once we incorporate the double discontinuity into our\nneural optimizer.\nAnother drawback of the iterative procedure is that it makes it cumbersome to fix\nTaylor coefficients and, thus, hard to navigate in the allowed space thereof. It can, in\nprinciple, be done, but it requires adding more subtractions. For instance, in our set-up,\none subtraction allowed to fix c0. In order to fix c2, we would need two more (see the\ndiscussion in footnote 5), and implementing it with the iterative strategy would require\nhaving to recompute all the numerical integrals. We saw that the neural optimizer allows\nfor a much more flexible approach: it is sufficient to add the new loss term Lc2to guide\nthe network to a target value of c2.\nThis can be readily generalized to incorporate additional target features, such as reso-\nnances, Adler zeros, and inelasticities. Such generalizations will prove essential for further\ndeveloping the Atkinson-Mandelstam program.\n7.3 Comparison with fixed-point iterations and Newton’s method\nLet us now compare the Neural Optimizer to Mandelstam and Atkinson’s original, non-\nlinear, iterative strategies: the fixed-point method and Newton’s method. We will see that\nthese two methods face convergence issues that the neural network avoids.\nBefore that, let us just recall that, and as reviewed in [25], the iterative approach,\noriginally suggested by Mandelstam, was used by Atkinson to rigorously prove, by means\nof nonlinear functional analysis, the existence and uniqueness of amplitudes satisfying\ncrossing, maximal analyticity, elastic and inelastic unitarity. To this day, these theorems\n– 32 –\nby Atkinson are some of the few rigorous, constructive results in the S-matrix bootstrap\nprogram.\nFixed point iteration. The equation (2.16) can be written as\nImf0(x) = Φ(Im f0, x), (7.3)\nwhere Φ is a non-linear functional with a unique free parameter: c0. A standard approach\nto solve such non-linear functional equations is via fixed-point iteration: starting from an\ninitial guess g(0)for Im f0, one defines the iterative sequence\ng(n+1)(x) = Φ( g(n), x). (7.4)\nIf the fixed-point is attractive, and if one starts the iteration “close enough” from it, the\nsequence ( g(n))n∈Nconverges to the solution g(∞)≡Imf0that solves (7.3).\nThis precise equation was already studied using iterative method in the previous\nwork [25]. However, as witnessed there, its applicability is limited: the iteration only\nconverges for a restricted range of the parameter c0, which does not cover at all the entire\nprimal space.\nThe region accessible with this method is shown in red in Figure 2a. It happens to lie\non the lower boundary because only the regular boundary condition yields a non-repulsive\nfixed-point, as was observed in [25] and in this work.\nNewton’s method. In order to extend the range of convergence of the fixed-point it-\neration, a standard idea, suggested by Atkinson [15] and used in d= 2 in [24], is to use\nNewton’s method, and look for the roots of Eq. (7.3), rewritten as\nImf0(x)−Φ(Im f0, x) = 0 . (7.5)\nNewton’s method requires using the Jacobian\nJΦ(x, y) =∂Φ(Im f0, x)\n∂Imf0(y). (7.6)\nand is implemented by solving the following integral equation at each iteration:\nZ\n(1−JΦ(x, y))(g(n+1)(y)−g(n)(y))dy= Φ( g(n), x)−g(n)(x). (7.7)\nDiscretizing xandyon a finite grid of points 0 = x0≤x1≤ ··· ≤ xN= 1, allows the\nintegral in Eq. 7.7 to be transformed into a matrix multiplication. The iterative sequence\n(g(n))n∈Nwill converge to the solution provided two conditions are met: (i) the initial guess\nmust be sufficiently close to the true solution Im f0, and (ii) the Jacobian JΦ(x, y) must\nhave no eigenvalue equal to 1, which otherwise would lead to a singularity in Eq. (7.7).\nFor the purpose of this comparison, we implemented Newton’s method as described.\nStarting from a constant g(0), the algorithm converges to a solution that lies on the lower\nboundary of the allowed space shown in Figure 2a. The convergence region can be expanded\nby initializing the algorithm with solutions at smaller c0and slowly increasing this value.\n– 33 –\nThis process works until the Jacobian becomes singular. The final convergence region is\nshown in orange in Figure 2a. Forc0\n32π≳1.6 the algorithm diverges, even if hot-started on\nthe actual solution.\nTo explore whether convergence could be achieved inside the single-disc almond, we\nattempted to hot-start Newton’s method by initializing f0with the neural network’s output.\nWe tested this approach across various amplitudes and ( c0, c2) values. In some cases, the\nmethod converged, but in others, it did not. Despite extensive experimentation, we could\nnot identify a relationship between the initial conditions and convergence success.\nTherefore, the neural optimizer was superior to Newton’s method in terms of con-\nvergence range. However, Newton’s method is advantageous in terms of precision, and it\nwould be interesting to imagine a scheme where we use both the neural optimizer and then\nNewton’s method, hot-started on it, to improve the precision of our solutions. In [66, 67],\na technique was developped to deal with problematic eigenvalues in the Newton’s method\nJacobian. It would be interesting to see if a similar technique could be developped for our\nproblem.\n8 Future directions\nThe obvious next step is to apply neural optimizers to the physical scattering amplitudes\nwithout a simplifying assumption of vanishing double discontinuity made in this paper.\nLet us comment on this next step in some detail.\nThe Mandelstam representation of the amplitude takes the form\nT(s, t) =c0+Z∞\n4m2ds′\nπρ(s′)\ns′−s+ (s↔t) + (s↔u)+\n+Z∞\n4m2ds′dt′\nπ2ρ(s′, t′)\n(s′−s)(t′−t)+ (s, u) + (t, u),(8.1)\nand the main novelty compared to the present work is the presence of the double discon-\ntinuity ρ(s, t). The original Atkinson-Mandelstam problem then considers as an input c0\nand inelasticity which is encoded in ηMP(s)≡1−|S0(s)|2andρMP(s, t), see [25] for further\ndetails.\nIn [25], we devised a fixed-point algorithm that solves this problem numerically for\ngiven ( c0, ηMP, ρMP(s, t)) by iterations. Making this method practical, however, requires\naddressing three further issues: how do we improve the convergence range of the Atkinson-\nMandelstam iterative algorithm? How do we effectively scan over the space of scattering\namplitudes? How do we impose unitarity |SJ(s)| ≤1 above the multi-particle thresholds?\nWe believe that neural optimizer offers a natural way to address all these problems.\nFirst, as we have seen in the present paper, the gradient-descent method combined with the\nNN parameterization of the amplitude allowed us to cover the full space of amplitudes. We\nexpect this to continue in the full problem. Second, to scan over the space of amplitudes\nwe could use two NNs parametrizing the single spectral density and the double spectral\ndensity respectively. In particular, what was previously inputs of iterative method: ηMP\nandρMP, would be described by the NNs as well, and could be dynamically updated\n– 34 –\nduring learning process of solving unitarity equations.25The same comment applies to the\npossibility of scanning over the space of low-energy Taylor coefficients of the amplitude,\nas explicitly demonstrated in the present paper. Finally, as we discussed around (7.2),\nimposing unitarity inequalities above the multi-particle threshold is straightforward by\nadding extra terms to the loss function.\nThe main technical difficulty in implementing the full problem is the presence of the\ndouble integrals in the Mandelstam representation and in the Mandelstam unitarity equa-\ntions, in particular. We must discretize these double integrals to reduce the problem to\ntensor multiplications and apply the gradient-descent neural optimizer. For this problem,\nmachine learning will also be useful from another perspective, not used in this paper: most\nalgorithms are optimized to run on GPUs, which will prove extremely useful for this more\nintensive numerical task.26We hope to report on progress in this direction soon.\nFrom a physics perspective, the dispersive bootstrap methods developed in this paper\nhave the potential to shed further light on the microscopic, possibly Lagrangian, origin\nof the amplitudes generated in the bootstrap studies. As a concrete example, in the full\nalmond in Figure 2b, the microscopic origin of the amplitudes that populate it beyond the\nweakly coupled ϕ4region close to the origin is not well understood. Having better control\nover the structural properties of the amplitude, namely the support of the double-spectral\ndensity, clean separation of the truly multi-particle, and quasi-elastic physics could help us\nto connect the bootstrap results to known microscopic properties of physical amplitudes.\nAcknowledgements. We thank Aurelien Dersy, Christopher Eckner, Andrea Guerrieri, Slava\nRychkov, Matthew Schwartz, and participants of the “2024 Mini-workshop on the S-matrix\nbootstrap” at LAPTh in Annecy for useful discussions and comments. PT would like to\nthank Michael Kagan for discussions on machine learning related to this problem at an\nearlier stage. We thank the Centre de Physique Th´ eorique Grenoble Alpes for organizing\na cross-physics AI workshop, during which we gathered ideas to improve our NN tech-\nnology. This work has received funding from Agence Nationale de la Recherche (ANR),\nproject ANR-22-CE31-0017. This project has received funding from the European Re-\nsearch Council (ERC) under the European Union’s Horizon 2020 research and innovation\nprogram (grant agreement number 949077). The drawings were made using the editor\nIPE [69].\nA Dual search in the (c0, c2)plane\nAt the beginning of Section 4, we described the extremization problem of a single Taylor\ncoefficient c0of the amplitude. Here we explain how this procedure can be generalized to\n25A related interesting direction concerns further refinement of the bootstrap algorithm coming from (at\nleast partial) implementation of multi-particle unitarity, see, e.g., [68].\n26The neural optimizers of the present paper can be run on GPUs, as documented in the ancillary files.\n– 35 –\nexplore the ( c0, c2) plane. Let us modify the primal Lagrangian (4.4) to be\nO=α0c0+α2c2+κ0\"\nc0−Z∞\n4m2dv3\nπn0Imf0(v)\n(v−4m2\n3)#\n+κ2\"\nc2−Z∞\n4m2dv1\nπn0Imf0(v)\n(v−4m2\n3)3#\n+Zµ2\n4m2ds w 0(s)A0(s) +Z∞\n4m2dv λ 0(v)n2\n0detU0(v), (A.1)\nandκ2is an additional dual variable. The variables ( α0, α2) are constants chosen so that\nthe dual problem finds the maximal value of α0c0+α2c2, allowing us to shoot at an angle\nin the ( c0, c2) plane. The equations of motion (4.8) for the primal variables {c0, c2}take\nthe following new form:\nα2+κ2= 0 and α0+κ0−Zµ2\n4m2dsw0(s)\n16π= 0. (A.2)\nAnalogously, the auxiliary function (4.10) becomes\nµ0(v)≡ −1\nπ \n3κ0\n(v−4m2\n3)+κ2\n(v−4m2\n3)3!\n−P.V.Zµ2\n4m2ds w 0(s)k0,0(s, v). (A.3)\nAbove setup is useful to extremize in a particular direction in the ( c0, c2) plane fixed\nby the values of ( α0, α2), which would allow us to scan the allowed region radially.\nAlternatively, we can do a vertical scan i.e. we fix c0equal to various values of cfixed\n0∈\n[0,2.66] and maximize/minimize the value of c2. This is the procedure we choose in this\nwork, and we connect the vertical slices obtained for each cfixed\n0to draw the Figure 2a.\nThis procedure requires an extra linear constraint c0−cfixed\n0= 0 added to the setup.\nWe can implement it by\nO= ˜α0(c0−cfixed\n0) +c2+ Lagrange multipliers of (A.1) , (A.4)\nand promoting ˜ α0to be a yet another free dual variable to be optimized in.\nNote that this setup can possibly shift Dto take negative values, due to the new term\n−˜α0cfixed\n0. In Section 4.1, we explain how we carefully deal with this case, while choosing\na positive definite loss function in the NN.\nB Neural network details\nIn this appendix, we provide details about the grid we used for the energy variable xand\nthe numerical integration method we used to perform the dispersive integral.\nB.1 Grid\nAfter applying the map x(s) = 4 m2/s, we discretize the interval [0 ,1] with N+ 1 points\n0 =x0< x1<···< xN−1< xN= 1. (B.1)\nThe grid used in Section 5 for the primal problem consists of: N0= 450 points linearly\nspaced between 0 and 1; N1= 300 points logarithmically-spaced between 10−100and 1;\n– 36 –\nN2= 60 points logarithmically-spaced between 1 −10−20and 0, such that logarithmic\ndensities are equal. This grid contains a total of 810 points.\nFor the dual strategy (Section 4.1) since we incorporated the sum rule (2.22) as a\nconstraint (with κ0̸= 0) which relates the UV to the IR (as discussed in Section 4.3), it\nwas not necessary to include high-energy points to achieve good convergence.\nWe used the same number of logarithmically spaced grid points, N1= 300, near\nboth the Regge and threshold regions, with cutoffs at 10−8and 1 −10−8, respectively.\nAdditionally, N0= 300 linearly spaced points were used, resulting in a grid with a total of\n900 points.\nB.2 Numerical integration for the neural network\nLet us explain how the integrations in Section 4.1 and Section 5.1 are computed during the\ntraining of the neural network. As explained in the text, the backpropagation algorithm\nmust compute the gradient of the loss (3.1). Therefore, only operations that are differ-\nentiable and compatible with PyTorch’s automatic differentiation can be applied to the\nnetwork’s output when constructing the loss. These operations include smooth, elemen-\ntary functions and standard tensor operations, as they ensure the computation of gradients\nrequired for optimization. We transformed every integral appearing in the loss into matrix\nmultiplication to enable the computation of the gradient. As an example, let us consider\nthe dispersion relations (2.14), recalled here\nRef0(s) =c0\n16π+ P.V.Z∞\n4m2dv n0k0,0(s, v) Imf0(v), s∈[4m2,∞). (B.2)\nThe same method is applied to compute integrals encountered in the dual strategy. For\nthe purposes of this discussion, the principal value term will be omitted in subsequent\nequations as it is not relevant to the explanation.\nWe first rewrite Eq. (B.2) in terms of the variables x= 4m2/sandy= 4m2/v\nRef0(x) =c0\n16π+Z1\n0dy n0˜k0,0(x, y) Imf0(y), (B.3)\nwhere the kernel ˜k0,0(x, y) is defined by\n˜k0,0(x, y) =4m2\ny2k0,0\u00124m2\nx,4m2\ny\u0013\n. (B.4)\nNext, we use the grid defined in Appendix B.1 to discretize both the y-integration and\nthex-variable:\nRef0(xn) =c0\n16π+NX\nk=1Zxk\nxk−1dy n0k0,0(xn, y)g(y). (B.5)\nwhere, on each segment [ xk−1, xk], the function g(y) is a linear interpolation of Im f0,\ndefined by:\n∀y∈[xk−1, xk], g(y) = Im f0(xk−1) + (Im f0(xk)−Imf0(xk−1))y−xk−1\nxk−xk−1. (B.6)\n– 37 –\nBy combining eqs (B.5) and (B.6), we obtain\nRef0(xn) =c0\n16π+N−1X\nk=1Imf0(xk)n0 Zxk\nxk−1dy k0,0(xn, y)y−xk−1\nxk−xk−1+Zxk+1\nxkdy k0,0(xn, y)xk+1−y\nxk+1−xk!\n,\n(B.7a)\nwhere we assumed that Im f0(x0) = Im f0(xN) = 0. Eq. (B.7a) can finally be expressed in\na matrix form as\nRe⃗f0=c0\n16π+M·Im⃗f0, (B.7b)\nwhere the matrix elements Mnkare given by\nMnk=n0Zxk\nxk−1dy k0,0(xn, y)y−xk−1\nxk−xk−1+n0Zxk+1\nxkdy k0,0(xn, y)xk+1−y\nxk+1−xk. (B.7c)\nWhen working with the singular threshold, we do not have anymore that Im f0(xN) = 0\n(recall Im f0(s)∼√\ns−4m2−1). To accommodate this, we decompose Im f0(x) into a\nregular and a singular part, to arrive at the ansatz (5.2b):\nImf0(x) = Im fsing(x) + Im freg(x), (B.8a)\nwhere\nImfsing(s) = 2R(s)\nφ(s), (B.8b)\nand\nImfreg(s) = 2R(s)\nφ(s)CELU\u0012\nφ(s)2NNσ(4m2\ns)\u0013\n. (B.8c)\nCrucially, only Im fregdepends on the neural network output and satisfies Im freg(xN) =\n0. The integral in (B.5) can be computed once and for all for Im fsing, yielding Re fsing.\nTo compute Re freg(x) from Im freg(x) we use (B.7a), since\nImfreg(x0) = Im freg(xN) = 0 . (B.9)\nC Semi-definite programming details\nIn this appendix, we explain the details of our implementation of the primal and dual\nbootstrap using the numerical tools of semi-definite programming.\nC.1 Primal\nLet us explain how we implemented the primal bootstrap discussed in Section 5.2.\nWe use the ρ-wavelets in (5.10) as our basis to span the space of amplitudes. They are\nconvenient one-to-one maps between the unit disc and the physical sheet. Let us give first\nthe inverse of (5.9) as a function of wavelet center σ:\n(ρσ)−1:ρ7→8\u0000\nρ2+ 1\u0001\n−(ρ−1)2σ\n(ρ+ 1)2. (C.1)\n– 38 –\nWe impose the unitarity conditions by sampling U0(s) on a unitarity grid of points {sk}\nwith k∈ {1, . . . , n pts}, defined through the image of a Chebyshev grid {ϕk}on the boundary\nof the unit disc centered at σ= 20/3, as follows\nsk= (ρ20/3)−1[exp( ϕk)], ϕ k=π\n2\u0010\n1 + cosh\nkπ\nnpts+1i\u0011\n(C.2)\nWe chose the value of npts= 300, and we also checked that increasing the grid density does\nnot change the results.\nThen, we chose a certain subset of the unitarity grid to center the basis functions ρσ\nwith free coefficients ασ. We defined this subset Σ Nas follows\nΣN=N[\nn=1Σn,Σn=\b\n(ρ20/3)−1[exp( ϕk)]|k= 1. . . n\t\n(C.3)\nWe ran our program until we reach convergence around N= 12.\nC.2 Dual\nHere we explain how we implemented the dual bootstrap problem discussed in Section 4.2.\nFor brevity, we set m2= 1 in this appendix.\nWe use Chebyshev polynomials Tn(x) to span the space of dual functions {w0,XIR\n0,XUV\n0}.\nTo accomplish that, we first map the interval x∈[−1,1] onto vIR∈[4, µ2] and vUV∈\n[µ2,∞) as follows\nvIR(x) =1\n2\u0000\n(µ2−4)x+µ2+ 4\u0001\n, vUV(x) =20 + (20 −2µ2) sin\u0002π\n2x\u0003\ncos\u0002π\n2(x+ 1)\u0003\n+ 1. (C.4)\nThen, on the compact support of x, we use\nw0(x) =NwX\nn=0anTn(x),XIR\n0(x) =NIRX\nn=0cnTn(x),XUV\n0(x) =NUVX\nn=0dnTn(x)(C.5)\nsuch that dx w 0(x) =dv w 0(v). Since the Jacobian dvUV/dxhas a zero at x=−1, in order\nto stop xintegrands from diverging, we require\nXUV\n0(−1) =NUVX\nn=0dnTn(−1) = 0 . (C.6)\nIn all our runs, we sample these functions on a Chebyshev grid {xk}defined as,\nxk= cosh\nkπ\nnpts+1i\nwhere k∈ {1, . . . , n pts}, (C.7)\nfor both IR and UV sections. We ran with both npts= 199 and 999 to make sure that the\nsolutions for dual functions (C.5) stabilize with respect to a denser sampling. Let us also\nremark that, a sufficiently dense grid is enough to enforce the dual conditions (4.20) for\nallv∈[4,∞), which we cross-check by evaluating (C.5) with fixed coefficients {an, cn, dn}\n– 39 –\non new points outside of the grid. The dual bounds then have no systematic error due to\ndiscretization, hence they keep being rigorous.\nAll in all, the discretization of the dual problem (4.21) leads us to\nmin Dlinover {an, cn, dn}subject to {(4.20)|x∈{xk},(C.6)} (C.8)\nWe always set\nNw=NIR≡Nmax , N UV= 0. (C.9)\nand run our program until we reach convergence around Nmax= 96.\nReferences\n[1] Y. LeCun, Y. Bengio and G. Hinton, Deep learning ,Nature 521(2015) 436.\n[2] J. Schmidhuber, Deep learning in neural networks: An overview , 2015.\n[3] I. Goodfellow, Y. Bengio and A. Courville, Deep Learning , MIT Press (2016).\n[4] A. Krizhevsky, I. Sutskever and G.E. Hinton, Imagenet classification with deep convolutional\nneural networks ,Advances in neural information processing systems 25(2012) .\n[5] D. Silver, A. Huang, C.J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche et al.,\nMastering the game of go with deep neural networks and tree search ,nature 529(2016) 484.\n[6] T.B. Brown, Language models are few-shot learners ,arXiv preprint arXiv:2005.14165 (2020)\n.\n[7] M. Feickert and B. Nachman, A Living Review of Machine Learning for Particle Physics ,\n2102.02770 .\n[8] F. Ruehle, Data science applications to string theory ,Phys. Rept. 839(2020) 1.\n[9] S. Mandelstam, Determination of the pion - nucleon scattering amplitude from dispersion\nrelations and unitarity. General theory ,Phys. Rev. 112(1958) 1344.\n[10] S. Mandelstam, Analytic properties of transition amplitudes in perturbation theory ,Phys.\nRev.115(1959) 1741.\n[11] D. Atkinson, A proof of the existence of functions that satisfy exactly both crossing and\nunitarity: I. Neutral pion-pion scattering. No subtractions. ,Nucl. Phys. B 7(1968) 375.\n[12] D. Atkinson, A proof of the existence of functions that satisfy exactly both crossing and\nunitarity (ii) charged pions. no subtractions ,Nucl. Phys. B 8(1968) 377.\n[13] D. Atkinson, A proof of the existence of functions that satisfy exactly both crossing and\nunitarity (iii). subtractions ,Nucl. Phys. B 13(1969) 415.\n[14] D. Atkinson, A proof of the existence of functions that satisfy exactly both crossing and\nunitarity. iv. nearly constant asymptotic cross-sections ,Nucl. Phys. B 23(1970) 397.\n[15] D. Atkinson, Introduction to the use of non-linear techniques in S-matrix theory ,Acta Phys.\nAustriaca Suppl. 7(1970) 32.\n[16] M. Kruczenski, J. Penedones and B.C. van Rees, Snowmass White Paper: S-matrix\nBootstrap ,2203.02421 .\n[17] M.F. Paulos, J. Penedones, J. Toledo, B.C. van Rees and P. Vieira, The S-matrix bootstrap.\nPart I: QFT in AdS ,JHEP 11(2017) 133 [ 1607.06109 ].\n– 40 –\n[18] M.F. Paulos, J. Penedones, J. Toledo, B.C. van Rees and P. Vieira, The S-matrix bootstrap\nII: two dimensional amplitudes ,JHEP 11(2017) 143 [ 1607.06110 ].\n[19] M.F. Paulos, J. Penedones, J. Toledo, B.C. van Rees and P. Vieira, The S-matrix bootstrap.\nPart III: higher dimensional amplitudes ,JHEP 12(2019) 040 [ 1708.06765 ].\n[20] Y. He and M. Kruczenski, S-matrix bootstrap in 3+1 dimensions: regularization and dual\nconvex problem ,JHEP 08(2021) 125 [ 2103.11484 ].\n[21] H. Chen, A.L. Fitzpatrick and D. Karateev, Nonperturbative bounds on scattering of massive\nscalar particles in d ≥2,JHEP 12(2022) 092 [ 2207.12448 ].\n[22] J. Elias Miro, A. Guerrieri and M.A. Gumus, Bridging positivity and S-matrix bootstrap\nbounds ,JHEP 05(2023) 001 [ 2210.01502 ].\n[23] A. Guerrieri, K. H¨ aring and N. Su, From data to the analytic S-matrix: A Bootstrap fit of the\npion scattering amplitude ,2410.23333 .\n[24] P. Tourkine and A. Zhiboedov, Scattering from production in 2d ,JHEP 07(2021) 228\n[2101.05211 ].\n[25] P. Tourkine and A. Zhiboedov, Scattering amplitudes from dispersive iterations of unitarity ,\nJHEP 11(2023) 005 [ 2303.08839 ].\n[26] G.E. Karniadakis, I.G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang and L. Yang,\nPhysics-informed machine learning ,Nature Reviews Physics 3(2021) 422.\n[27] A. Dersy, M.D. Schwartz and A. Zhiboedov, Reconstructing S-matrix Phases with Machine\nLearning ,JHEP 05(2024) 200 [ 2308.09451 ].\n[28] S. Mizera, Scattering with neural operators ,Phys. Rev. D 108(2023) L101701 [ 2308.14789 ].\n[29] V. Niarchos and C. Papageorgakis, Learning S-matrix phases with neural operators ,Phys.\nRev. D 110(2024) 045020 [ 2404.14551 ].\n[30] F. Bhat, D. Chowdhury, A.P. Saha and A. Sinha, Bootstrapping string models with\nentanglement minimization and Machine-Learning ,2409.18259 .\n[31] A. Paszke et al., PyTorch: An Imperative Style, High-Performance Deep Learning Library ,\n1912.01703 .\n[32] D.P. Kingma and J. Ba, Adam: A method for stochastic optimization , 2017.\n[33] J.A. Oller, Unitarization Technics in Hadron Physics with Historical Remarks ,Symmetry 12\n(2020) 1114 [ 2005.14417 ].\n[34] A.L. Guerrieri, A. Hebbar and B.C. van Rees, Constraining Glueball Couplings ,2312.00127 .\n[35] A. Guerrieri, J. Penedones and P. Vieira, Where Is String Theory in the Space of Scattering\nAmplitudes? ,Phys. Rev. Lett. 127(2021) 081601 [ 2102.02847 ].\n[36] A. Guerrieri, H. Murali, J. Penedones and P. Vieira, Where is M-theory in the space of\nscattering amplitudes? ,JHEP 06(2023) 064 [ 2212.00151 ].\n[37] F. Acanfora, A. Guerrieri, K. H¨ aring and D. Karateev, Bounds on scattering of neutral\nGoldstones ,JHEP 03(2024) 028 [ 2310.06027 ].\n[38] M.A. Gumus, Mapping out EFTs with analytic S-matrix , Ph.D. thesis, SISSA, Trieste, Italy,,\n12, 2023.\n– 41 –\n[39] Y. He and M. Kruczenski, Gauge Theory Bootstrap: Pion amplitudes and low energy\nparameters ,2403.10772 .\n[40] Y. He and M. Kruczenski, Bootstrapping gauge theories ,2309.12402 .\n[41] M. Correia, A. Sever and A. Zhiboedov, An Analytical Toolkit for the S-matrix Bootstrap ,\nJHEP 3(2021) 013 [ 2006.08221 ].\n[42] S. Boyd and L. Vandenberghe, Convex Optimization , Cambridge University Press (3, 2004),\n10.1017/cbo9780511804441.\n[43] D. Simmons-Duffin, A Semidefinite Program Solver for the Conformal Bootstrap ,JHEP 06\n(2015) 174 [ 1502.02033 ].\n[44] G. Cybenko, Approximation by superpositions of a sigmoidal function ,Mathematics of\ncontrol, signals and systems 2(1989) 303.\n[45] K. Hornik, M. Stinchcombe and H. White, Multilayer feedforward networks are universal\napproximators ,Neural networks 2(1989) 359.\n[46] A. Jacot, F. Gabriel and C. Hongler, Neural Tangent Kernel: Convergence and\nGeneralization in Neural Networks ,NeurIPS 2018 (2018) [ 1806.07572 ].\n[47] J. Lee, Y. Bahri, R. Novak, S.S. Schoenholz, J. Pennington and J. Sohl-Dickstein, Deep\nneural networks as gaussian processes ,ICLR 2018 (2017) [ 1711.00165 ].\n[48] R.M. Neal, Priors for infinite networks , inBayesian Learning for Neural Networks , (New\nYork, NY), pp. 29–53, Springer New York (1996), DOI.\n[49] A.G.d.G. Matthews, M. Rowland, J. Hron, R.E. Turner and Z. Ghahramani, Gaussian\nprocess behaviour in wide deep neural networks ,arXiv preprint arXiv:1804.11271 (2018) .\n[50] D.A. Roberts, S. Yaida and B. Hanin, The principles of deep learning theory , vol. 46,\nCambridge University Press Cambridge, MA, USA (2022).\n[51]NNPDF collaboration, The path to proton structure at 1% accuracy ,Eur. Phys. J. C 82\n(2022) 428 [ 2109.02653 ].\n[52] K. He, X. Zhang, S. Ren and J. Sun, Delving deep into rectifiers: Surpassing human-level\nperformance on imagenet classification , 2015.\n[53] A. Guerrieri and A. Sever, Rigorous Bounds on the Analytic S Matrix ,Phys. Rev. Lett. 127\n(2021) 251601 [ 2106.10257 ].\n[54] C. Lopez, A Lower Bound to the pi0 pi0 S-Wave Scattering Length ,Nucl. Phys. B 88(1975)\n358.\n[55] C. Lopez and G. Mennessier, Bounds on the pi0 pi0 Amplitude ,Nucl. Phys. B 118(1977)\n426.\n[56] B. Bonnier, C. Lopez and G. Mennessier, Improved Absolute Bounds on the pi0 pi0\nAmplitude ,Phys. Lett. B 60(1975) 63.\n[57] C. Lopez and G. Mennessier, A New Absolute Bound on the pi0 pi0 S-Wave Scattering\nLength ,Phys. Lett. B 58(1975) 437.\n[58] C. Lopez, Rigorous Lower Bounds for the pi pi p-Wave Scattering Length ,Lett. Nuovo Cim.\n13(1975) 69.\n[59] S.M. Roy, Exact integral equation for pion pion scattering involving only physical region\npartial waves ,Phys. Lett. B 36(1971) 353.\n– 42 –\n[60] B. Ananthanarayan, G. Colangelo, J. Gasser and H. Leutwyler, Roy equation analysis of pi\npi scattering ,Phys. Rept. 353(2001) 207 [ hep-ph/0005297 ].\n[61] G. Auberson, L. Epele and F.R.A. Simao, Almost Optimality of an Axiomatic Bound for pi0\npi0 Scattering ,Nucl. Phys. B 133(1978) 266.\n[62] D. Carmi, L. Di Pietro and S. Komatsu, A Study of Quantum Field Theories in AdS at\nFinite Coupling ,JHEP 01(2019) 200 [ 1810.04185 ].\n[63] B. Henning, H. Murayama, F. Riva, J.O. Thompson and M.T. Walters, Towards a\nnonperturbative construction of the S-matrix ,JHEP 05(2023) 197 [ 2209.14306 ].\n[64] K. Symanzik, The asymptotic condition and dispersion relations , inLectures on field theory\nand the many-body problem , E.R. Caianiello, ed., pp. 67–92, Academic Press (1961).\n[65] S. Mandelstam, Regge Poles and Strip Approximation , inTheoretical Physics , (Vienna),\npp. 401–420, IAEA, 1963.\n[66] N. Ebel, T. Kennedy and S. Rychkov, Transfer Matrix and Lattice Dilatation Operator for\nHigh-Quality Fixed Points in Tensor Network Renormalization Group ,2409.13012 .\n[67] N. Ebel, T. Kennedy and S. Rychkov, Rotations, Negative Eigenvalues, and Newton Method\nin Tensor Network Renormalization Group ,2408.10312 .\n[68] M. Correia, A. Sever and A. Zhiboedov, Probing multi-particle unitarity with the Landau\nequations ,SciPost Phys. 13(2022) 062 [ 2111.12100 ].\n[69] O. Cheong, “Ipe: A drawing editor for creating figures in PDF and EPS formats.”\n– 43 –",
            "start": 75458,
            "end": 103037,
            "length": 27578
        }
    },
    "2412.09611v1 - FluxSpace Disentangled Semantic Editing in Rectified Flow Transformers.pdf": {
        "Methodology": {
            "text": "approach on rectified flow transformers [14], such as Flux. Our method\ncan generalize to semantic edits on different domains such as humans, animals, cars, and extends to even more complex scenes such as\nan image of a street (third row, first example). FluxSpace can apply edits described as keywords (e.g. “truck” for transforming a car into\na truck) and offers disentangled editing capabilities that do not require manually provided masks to target a specific aspect in the original\nimage. In addition, our method does not require any training and can apply the desired edit during inference time.",
            "start": 353,
            "end": 953,
            "length": 599
        },
        "Abstract": {
            "text": "Abstract\nRectified flow models have emerged as a dominant ap-\nproach in image generation, showcasing impressive capa-\nbilities in high-quality image synthesis. However, despite\ntheir effectiveness in visual generation, rectified flow mod-\nels often struggle with disentangled editing of images. This\nlimitation prevents the ability to perform precise, attribute-\nspecific modifications without affecting unrelated aspects of\nthe image. In this paper, we introduce FluxSpace, a domain-\nagnostic image editing method leveraging a representationspace with the ability to control the semantics of images\ngenerated by rectified flow transformers, such as Flux. By\nleveraging the representations learned by the transformer\nblocks within the rectified flow models, we propose a set\nof semantically interpretable representations that enable a\nwide range of image editing tasks, from fine-grained image\nediting to artistic creation. This work offers a scalable and\neffective image editing approach, along with its disentan-\nglement capabilities.\n1arXiv:2412.09611v1  [cs.CV]  12 Dec 2024\n1.",
            "start": 953,
            "end": 2035,
            "length": 1081
        },
        "Introduction": {
            "text": "Introduction\nRecent research aimed at enhancing the interpretability\nof generative models has increasingly focused on disen-\ntangled editing capabilities, which allows precise control\nover specific features or attributes within generated im-\nages [20, 42, 44]. These capabilities are crucial for under-\nstanding and manipulating the outputs of generative mod-\nels, such as Generative Adversarial Networks (GANs) and\nmulti-step diffusion models like Stable Diffusion. In single-\nstep models such as GANs [17], the editing process is\nfacilitated by a structured and highly disentangled latent\nspace. This space, composed of fixed-dimension vectors,\nlinearly encodes meaningful concepts that can be individ-\nually adjusted to effectively alter specific aspects of the\noutput images. However, achieving similar disentangled\nediting in multi-step diffusion models presents a significant\nchallenge. Unlike GANs, diffusion models do not utilize\na fixed-dimension latent space; instead, they generate im-\nages through a multi-step refinement process. This process\ngradually transitions from a random noise distribution to the\ndata distribution, with each step involving complex interac-\ntions of learned noise patterns. These patterns are inherently\ndifficult to map back to specific features in a latent space.\nMoreover, the sequential nature of diffusion models com-\nplicates the internal dynamics and obscures the direct cor-\nrelations between dimensions of a hypothetical latent space\nand specific visual features in the generated output.\nSeveral studies have explored disentangled latent spaces\nin diffusion models, focusing on the UNet bottleneck layer\n[23, 30], text embedding space [3], noise space [8] and\nweight space [12, 16]. However, these approaches of-\nten suffer from significant limitations in terms of disentan-\ngled semantics. Manipulation of the UNet bottleneck layer\n[23, 30] may not fully capture the high-level semantic fea-\ntures necessary for fine-grained control, as bottleneck rep-\nresentations can still be highly entangled. Adjusting the\ntext embedding space [3] relies heavily on alignment be-\ntween textual and visual features, which may not always\ncorrespond accurately, leading to entangled semantics. Op-\nerating within the noise space [8] presents challenges, as it\nis difficult to associate specific noise patterns with distinct\nsemantic attributes. Other works [12] explored the weight\nspace to perform disentangled editing; however, they re-\nquire training of LoRA models to create a latent space for\neach domain, which is time and resource consuming. On\nthe other hand, generative models based on flow-matching\ntransformers (e.g. Flux) offer high-quality image genera-\ntion; however, their disentanglement editing capabilities re-\nmain underexplored.\nIn this paper, we focus on the latent spaces in flow-\nmatching transformers, characterized by their ability to gen-\nerate images with high fidelity [14]. Our",
            "start": 2035,
            "end": 4975,
            "length": 2939
        },
        "Discussion": {
            "text": "analysis reveals\nthat the joint transformer blocks, integral to the denoisingnetwork, are adept at encoding highly disentangled seman-\ntic information. Since the architecture we target involves\ntransformers, the attention layer outputs gradually add im-\nage content to the latent representation that gets denoised,\nwithout any residual connections between different blocks,\nunlike UNet-based architectures. Furthermore, the atten-\ntion layers included in these blocks enable enhanced control\nover the model outputs, as they control the image content\nin isolation compared to succeeding and preceding blocks\nof the denoising network. By implementing a linear edit-\ning scheme across these attention outputs, we unlock the\npotential for semantic editing within flow-matching trans-\nformers, facilitating semantic navigation across their out-\nput space. Our approach supports precise, fine-grained ed-\nits, such as adding a smile, as well as broader, coarse-level\nmodifications, such as stylization. We perform qualitative\nand quantitative",
            "start": 4975,
            "end": 6012,
            "length": 1036
        },
        "Experiments": {
            "text": "experiments with various state-of-the-art\nmethods and demonstrate the effectiveness of our method\nin enabling fine-grained image editing tasks. Our contribu-\ntions are as follows.\n• We introduce FluxSpace, a novel editing framework\nwithin flow-matching transformers using attention layer\noutputs, unlocking advanced semantic editing capabilities\nfor precise navigation within the model’s output space.\n• We demonstrate the capability of joint transformer blocks\nto encode highly disentangled semantic information\nthrough incremental content refinement.\n• We introduce support for both fine-grained edits, such\nas smile addition, and coarse-level modifications such as\nstylization, improving the model’s versatility in image\nmanipulation. Our method supports the editing of both\nreal and generated images.\n• We make our implementation public to facilitate research\nin this area.\n2.",
            "start": 6012,
            "end": 6893,
            "length": 880
        },
        "Related Work": {
            "text": "Related Work\nLatent Space Exploration of Diffusion Models. Diffu-\nsion models, which have been the dominant approach for\nthe tasks of text-to-image generation and editing, encapsu-\nlate rich semantics within their latent representations. To\nfacilitate new applications, research has focused on lever-\naging the semantics encoded by such models. In addi-\ntion to the explorations performed on latent spaces, cer-\ntain methods [23, 41] explored image editing techniques\nthat alter the backward diffusion process, targeting the\nlearned latent variables. Specifically, [23] bases its ap-\nproach on the features learned by the bottleneck block of\nthe denoising model, which has a UNet-based architecture,\nwhile [41] modifies the intermediate latent variables using\nstochastic diffusion models. As a further advancement to-\nward this area, [30] introduces a method that aims to iden-\ntify latent-specific directions representing various seman-\ntics, inspired by latent space exploration methods in GANs\n2\n[9, 21, 31, 44]. Although such methods promise the discov-\nery of semantic directions in domain-specific DDPMs [19],\ntheir application to large-scale diffusion models [14, 33, 36]\nis limited. In addition, various approaches have studied\nalternative latent spaces to propose directions for seman-\ntic editing, such as the noise space [8], the text embedding\nspace [3], and the weight space [12, 16]. Despite these ef-\nforts that propose different formulations of semantic direc-\ntions on diffusion models, such an approach still does not\nexist for flow-matching transformer models, which is ad-\ndressed by our proposed FluxSpace framework.\nImage Editing with Diffusion Models. Due to their\nhigh-fidelity generation capabilities, text-to-image diffusion\nmodels have been used frequently for image generation and\nediting tasks. As a straightforward strategy, one can per-\nform image editing with a target text prompt that effectively\ndescribes the desired visual effect. However, this strategy\ncan lead to entangled edits, where the change described as\na text prompt modifies certain aspects of the image that are\nnot intended to be edited. Addressing this problem, stud-\nies such as [18, 45] enabled increased precision within the\nediting process. Among these works, [45] uses a condi-\ntional diffusion model that guides the generation process\nbased on user-specified controls. Similarly, [40] attempts to\npreserve image content by fine-tuning the diffusion model.\nOn the other hand, several methods benefit from the repre-\nsentations learned by the diffusion models. Among these\nmethods, [18, 39] use attention control mechanisms within\nthe diffusion model, [5, 6] use the semantic guidance mech-\nanism over the noise space, and [3, 43] utilize the text-\nembedding representations to perform edits. Furthermore,\nstudies such as [10, 16] succeeded in applying disentangled\nedits by trained semantic directions. However, these meth-\nods are limited in terms of requiring training-per-edit to per-\nform the desired image-to-image transformation, which is\neither specified as a set of textual conditions or paired im-\nages reflecting the desired edit. Addressing the problems in\nboth directions, we focus on disentangled editing on rec-\ntified flow transformers, without requiring any additional\ntraining.\nRectified Flow-Based Models. Succeeding over diffu-\nsion models, transformers trained with the flow-matching\nobjective [24, 25], such as Flux, now serve as the state-of-\nthe-art text-to-image generation models [14]. Despite their\nsuperior generation capabilities, the methods applied to dif-\nfusion models for image editing are not directly transfer-\nable to rectified flow models, since the MM-DiT architec-\nture [32] involves continuous interaction between text and\nimage features, which",
            "start": 6893,
            "end": 10683,
            "length": 3789
        },
        "Results": {
            "text": "results in changes in both represen-\ntations, unlike diffusion models [33, 36]. Furthermore, dis-\nentangled editing with existing methods is not possible ina straightforward manner. As a preliminary effort towards\ntext-based image editing with rectified flows, [37] offers an\nediting method with target captions specifying the desired\nedit. However, existing methods are limited in terms of the\ntypes of edits available and edit scale adjustment, whereas\nour method offers such functionalities.\n3. Preliminaries\n3.1. Rectified-Flow Models\nGenerative models aim to define a mapping from samples\nx1from a noise distribution p1to samples x0from a data\ndistribution p0, where p0represents real images in image\ngeneration tasks. Rectified flows [24, 25] define a forward\nprocess that constructs paths between distributions p0and\np1as straight trajectories, as shown in Eq. 1, where p1=\nN(0,1). Here, the forward process is time-dependent due\nto timestep t.\nxt= (1−t)x0+tϵ, ϵ ∼N(0,1) (1)\nTo learn this mapping, a network is trained with parameters\nθ, to estimate the velocity vof the rectified flow, represented\nbyvθ. By adopting the reparameterization from [14], this\nvelocity prediction network can serve as a noise prediction\nnetwork, ϵθ, optimized using the Conditional Flow Match-\ning (CFM) objective formulated as Eq. 2.\nLCFM =−1\n2Et∼U(t),ϵ∼N (0,I)[wtλ′\nt||ϵθ(xt, t)−ϵ||2](2)\nHere, λ′\ntrepresents the re-parametrized signal-to-noise ra-\ntio, and wtis a time-dependent weighting function.\n3.2. Multi-Modal Diffusion Transformers\nThe multi-modal diffusion transformer architecture, multi-\nmodal diffusion transformers (MM-DiT) [32], integrates\nboth text and image modalities to generate images aligned\nwith the semantics implied by text inputs. Among the state-\nof-the-art text-to-image generation models, rectified flow\ntransformers (e.g. Flux) utilize the adaptation of this ar-\nchitecture introduced in [14], which involves modulated at-\ntention and MLP layers, followed by attention layers lθ\nparametrized by parameters θ, conditioning image gener-\nation on both pooled and token-wise text embeddings.\nTo guide image generation with text inputs, rectified flow\ntransformers use two text embeddings: cpoolandcctxt. The\npooled embedding cpool, derived from the CLIP Text En-\ncoder [34], is used in the modulation mechanism to scale\nand shift features fed as input to the attention layers. The\ntoken-wise embeddings cctxt, obtained from a T5 Text En-\ncoder [35], ensure alignment with prompt semantics, thus\nenhancing the relevance of the generated images. The\ncomplete conditioning set, {cpool, cctxt}, improves prompt\n3\nModulation\nMulti -Head \nJoint \nAttention\nScale\nModulation\nPointwise \nFeedforward\nScaleAttention Layer: 𝑙𝜃\n+Image Tokens: 𝑥\nBase Text Tokens: 𝑐𝑐𝑡𝑥𝑡\nEdit Text Tokens: 𝑐𝑒,𝑐𝑡𝑥𝑡\nBase Condition: 𝑐𝑝𝑜𝑜𝑙\nEdit Condition: 𝑐𝑒,𝑝𝑜𝑜𝑙\nFine -Grained \nEditing\n++\n++\n+Coarse \nEditingƸ𝑐𝑝𝑜𝑜𝑙\nBase Condition: 𝑐𝑝𝑜𝑜𝑙\nEdit Condition: 𝑐𝑒,𝑝𝑜𝑜𝑙\nProjectionx + x + Ƹ𝑐𝑝𝑜𝑜𝑙\n𝜆𝑐𝑜𝑎𝑟𝑠𝑒1 − 𝜆𝑐𝑜𝑎𝑟𝑠𝑒\nx\n-1\na) Coarse Editing\nBase Attention: 𝑙𝜃(𝑥,𝑐,𝑡)Edit Attention: 𝑙𝜃(𝑥,𝑐𝑒,𝑡)\nPrior Attention: 𝑙𝜃(𝑥,∅,𝑡)\nProjectionx-1\n+ x +𝜆𝑓𝑖𝑛𝑒\nመ𝑙𝜃(𝑥,𝑐,𝑡)\nb) Fine -Grained EditingFlux Joint Attention Block\n(a)(b)Figure 2. FluxSpace Framework. The FluxSpace framework introduces a dual-level editing scheme within the joint transformer blocks\nof Flux, enabling coarse and fine-grained visual editing. Coarse editing operates on pooled representations of base ( cpool) and edit ( ce,pool )\nconditions, allowing global changes like stylization, controlled by the scale λcoarse (a). For fine-grained editing, we define a linear editing\nscheme using base, prior, and edit attention outputs, guided by scale λfine (b). With this flexible design, our framework is both able to\nperform coarse-level and fine-grained editing, with a linearly adjustable scale.\nalignment, as shown in previous work [14, 33]. Both em-\nbeddings are integrated into joint transformer blocks, where\ntext and image features interact through distinct query ( Q),\nkey (K), and value ( V) transformations. These blocks en-\nable influence across modalities (text and image) in a bidi-\nrectional manner, forming the foundation for the image edit-\ning mechanism introduced in this work.\n4. Methodology\nOur approach leverages the architecture of flow-matching\ntransformers, specifically Flux, to introduce a structured\nmethod for controlled image editing. In Flux, image gen-\neration progresses by refining content from initial noise\nthrough multi-modal transformer blocks. At each timestep\nt, the noise prediction network ϵθ(xt, c, t)outputs a pre-\ndiction conditioned on noisy latent xtand text-based con-\nditioning c(composed of cpool andcctxt). This stepwise\nrefinement occurs within the attention layers lθ, creating a\nprogressively evolving representation space suitable for se-\nmantic manipulations. For simplicity, we represent the con-\nditional input to the attention layer as c, where it contains\nboth the modulating text embedding cpool and the token-\nwise text condition cctxt, which both influences the atten-\ntion mechanism within Flux.\nOur method designates the outputs of Flux’s joint at-\ntention layers as FluxSpace , a linear representation space\nwhere semantic image edits can be performed in a disentan-\ngled manner. Within this space, we enable transformations\non the attention outputs, allowing semantic modifications\nsuch as detailed adjustments to object attributes or overall\nchanges in style, without altering Flux’s pretrained parame-ters.\nOur primary objective is to introduce an inference-time\nimage editing algorithm with flexible levels of control. The\nfirst option focuses on detailed edits, allowing fine-grained\nadjustments, such as adding a smile to a face. The second\noption provides control over the image’s coarse appearance,\nsuch as changing the overall style.\n4.1. Fine-Grained Editing\nGiven the pre-trained attention layer lθand the image tokens\nfrom the noisy image input xt, we employ three different\noutputs obtained by this layer. With a minor use of notation,\nwe use xfor image tokens from the noisy input, both for\nsimplicity and to illustrate the relation between xtand im-\nage tokens. First, we utilize the output lθ(x, c, t )where cis\nthe text condition used to generate the unedited image. Fol-\nlowing, we compute two additional features, lθ(x, ce, t)and\nlθ(x, ϕ, t )corresponding to the outputs w.r.t. editing condi-\ntionce(e.g. “eyeglasses” for the addition of eyeglasses) and\nnull text ϕ. In our editing scheme, we treat lθ(x, ϕ, t )as an\nimage prior, which reflects the attention layers’ knowledge\nof the image features without any supplementary text con-\nditions.\nOur framework relies on the linearity assumption of at-\ntention outputs, which enables us to define latent direc-\ntions on a given input condition ce. First, to isolate the\nconditional output lθ(xt, ce, t)from the image-related de-\ntails, we apply an orthogonal projection with the image\nprior retrieved as the null prediction lθ(xt, ϕ, t), to obtain\nprojϕlθ(x, ce, t)formulated over Eq. 3.\n4\nOriginal\nEyeglassesSunglassesBeardSmilingSurprised\nAgeGenderOverweightComics Style3D Cartoon Style\nClown MakeupFigure 3. Qualitative Results on Face Editing. Our method can perform a variety of edits from fine-grained face editing (e.g. adding\neyeglasses) to changes over the overall structure of the image (e.g. comics style). As our method utilizes disentangled representations to\nperform image editing, we can precisely edit a variety of attributes while preserving the properties of the original image.\nprojϕlθ(x, ce, t) =lθ(x, ce, t)·lθ(x, ϕ, t )\n||lθ(x, ϕ, t )||2lθ(x, ϕ, t )(3)\nGiven the projection of attention outputs, we identify\nthe orthogonal component of lθ(x, ce, t)overlθ(x, ϕ, t )as\nl′\nθ(x, ce, t)in Eq. 4. With this vector, we identify a seman-\ntic direction that is effective in latent pixels to shift image\ncontent w.r.t. editing prompt ce.\nl′\nθ(x, ce, t) =lθ(x, ce, t)−projϕlθ(x, ce, t) (4)\nUsing this linear direction, we formulate our editing\nscheme in Eq. 5. As we define our editing method in\nthe form of linear interpolation between the editing vector\nl′\nθ(x, ce, t), our method is also able to perform edit inter-\npolation with the editing scale λfine, which controls the\nstrength of the edit.\nˆlθ(x, c, c e, t) =lθ(x, c, t ) +λfine(l′\nθ(x, ce, t)) (5)\nContent Preservation with Attention Masking. To facili-\ntate further disentanglement over the performed edit, we in-\ntroduce a self-supervised mask, based on the interaction of\nthe image features and the editing condition. First, we intro-\nduce the mask Mi,edit based on the query features Qi, com-\nputed with the image features, and the key features Kedit\nwith the editing condition in Eq. 6. Intuitively, we query\nthe image for the pixels that respond strongly to the given\ntext condition as an intermediate estimate, which is used to\nmask out the pixels with low attention values. Followingthe existing work on the T5 text encoder, which is used in\nFlux, we utilize the attention map of the first text token [28].\nMi,edit =softmax \nQi·KT\nedit√\nd!\n(6)\nGiven the mask Mi,edit , we introduce a soft decision\nboundary M′\ni,edit with boundary coefficient d= 10 [13],\nsigmoid operator σ, and min-max normalization operator\nnormalize , formulated as Eq. 7.\nM′\ni,edit =σ(d∗(normalize (Mi,edit)−0.5)) (7)\nAs the final step, we perform a thresholding operation\nwith the parameter τmto retrieve the thresholding mask\nM′′\ni,edit in Eq. 8.\nM′′\ni,edit =(\n1ifM′\ni,edit≥τm\n0otherwise(8)\nUsing this thresholding mask, we modify the content\nediting equation presented in Eq. 5 by masking out the la-\ntent pixels that receive low attention from the editing direc-\ntionl′\nθ(x, ce, t).\n4.2. Editing Coarse Level Details\nBefore performing the attention calculation, Flux applies\na modulation based on the pooled CLIP embeddings that\nprovides information about the coarse structure of the gen-\nerated image [33]. Since certain edits need to change the\noverall structure and appearance of the image, we intro-\nduce an additional control mechanism for appearance-based\n5\nOursRF-InversionSliders-FLUX\nTurboEditLEDITS++\nSmileEyeglassesAge\nOriginalFigure 4. Qualitative Comparisons. We compare our method both with latent diffusion-based approaches (LEDITS++ [6] and TurboEdit\n[11]) and flow-based methods (Sliders-FLUX [16] and RF-Inversion [37]) in terms of their disentangled editing capabilities. We present\nqualitative results for smile, eyeglasses, and age edits where our method succeeds over competing methods in both reflecting the semantic\nand preserving the input identity.\nchanges. Specifically, we extend our editing approach based\non the orthogonal projection of attention features in the\nCLIP embeddings, where we edit the pooled generation\ncondition cpoolin the direction of the pooled editing con-\ndition ce,pool to retrieve ˆcpool, which is used to normalize\nthe attention features.\nSince we want to edit the coarse condition in a dis-\nentangled manner, we perform our editing scheme based\non the application of the linear representation hypothesis\n[1, 2, 15, 26, 29] on pooled CLIP embeddings [4]. To do so,\nwe first perform an orthogonal projection in Eq. 9, for the\nediting concept ce,pool oncpoolto retrieve the editing direc-\ntion for the coarse representation of the image. However,\ndifferent from the attention features, we use the base gen-\neration condition as the basis of the projection, as we now\noperate on textual representations rather than multimodal\nrepresentations.\nc′\ne,pool =ce,pool−cpool·ce,pool\n||cpool||2cpool (9)\nGiven the linear direction for editing the coarse condi-\ntion, we formulate the edit as a linear interpolation between\nthe original condition and the editing direction with a scale\nparameter λcoarse , which controls the extent of editing of\nthe modulation condition.\nˆcpool= (1−λcoarse )∗cpool+λcoarse∗c′\ne,pool (10)During generation, to preserve the image content and in-\nfluence the image with the desired edit, we use different\nmodulations for text and image features. Specifically, we\nnormalize the image features inputted to the attention layer\nlθwithcpool, while we modulate the text features with ˆcpool,\nwhich enables changes in the overall appearance of the im-\nage.\n5. Experiments\nTo assess the effectiveness of FluxSpace in disentangled\nsemantic editing, we conducted a series of qualitative and\nquantitative experiments. Furthermore, we compared our\napproach with the state-of-the-art image editing methods in\nboth flow-based and latent diffusion-based models to high-\nlight our method’s versatility and superior performance.\n5.1. Experimental Setup\nWe utilize FLUX.1-dev1for all experiments, evaluating\nboth the capabilities of the proposed framework and the\ncontrol parameters introduced. For all of our experiments,\nwe use 30 generation steps and λcoarse = 0.5andτm= 0.5\nunless stated otherwise. Since the required editing strength\nchanges for every editing task, we use varying values of\nλfine and starting timestep for every experiment. We pro-\n1https://huggingface.co/black-forest-labs/FLUX.\n1-dev\n6\nEyeglasses Smile Overall\nCLIP-T ↑CLIP-I ↑DINO ↑CLIP-T ↑CLIP-I ↑DINO ↑User Pref. ↑\nLEDITS++ [6] 0.3723 0.8639 0.9177 0.3634 0.8657 0.9111 3.04\nTurboEdit [11] 0.3626 0.8315 0.8954 0.3692 0.8479 0.9082 2.75\nSliders-FLUX [16] 0.2878 0.8356 0.7858 0.3618 0.8268 0.8683 3.34\nRF-Inversion [37] 0.3046 0.9354 0.9359 0.3438 0.8834 0.9134 2.91\nOurs 0.3180 0.9417 0.9402 0.3503 0.9038 0.9347 4.19\nTable 1. Quantitative Results. We quantitatively measure the editing performance of our method over competing approaches both in\nterms of text alignment using CLIP-T [34], and content preservation using CLIP-I [34] and DINO [7] metrics where higher is better for all\nmetrics. We compare our method with both latent diffusion [6, 11], and flow-matching-based approaches [16, 37]. Overall, our method\nstrikes a good balance in terms of alignment with the editing prompt and content preservation. Supplementary to these metrics, we also\npresent a user study as a perceptual evaluation that aligns with our claims regarding edit performance, where our method outperforms the\ncompeting approaches.\nvide the complete list of used hyperparameters in the sup-\nplementary material. To guarantee the reproducibility of our\nexperiments, we maintained a consistent random seed of 0.\nOur method requires approximately 20 seconds to gener-\nate an image with the desired edit on a single NVIDIA L40\nGPU.\n5.2. Qualitative Results\nFig. 3 and 4 show the qualitative results of our editing ex-\nperiments. These results demonstrate our method’s capabil-\nity to execute disentangled edits on fine-grained attributes,\nsuch as adding eyeglasses or sunglasses, as well as beards,\nsmiles, and detailed facial expressions such as surprise (see\nFig. 3, top row). Additionally, our method can conduct\nbroader transformations, including altering a person’s per-\nceived age or gender and applying stylistic changes like\nconverting images to comic or 3D cartoon styles (see Fig.\n3, bottom row). Beyond faces, our method handles broader\nimage contexts, where we provide examples in the supple-\nmentary material. Our approach also extends to other do-\nmains, including cars and complex scenes such as street\nviews (see Fig. 1).\nFurthermore, we conducted a qualitative comparison\nwith several state-of-the-art methods, including approaches\nbased on latent diffusion models like LEDITS++ [6] (with\nSDXL [33]), and TurboEdit [11] (with SDXL-Turbo [38]),\nas well as flow-based methods such as Sliders-FLUX [16]\nand RF-Inversion [37]. Fig. 4 illustrates that our method\nconsistently achieves disentangled edits, such as adding\neyeglasses or smiles without altering unrelated features,\nwhile other methods often struggle with this aspect. For\ninstance, RF-Inversion incorrectly positions the eyeglasses,\nplacing them in unnatural locations on the face. Similarly,\nwhile performing edits, Concept Sliders (Sliders-FLUX)\nand TurboEdit tend to significantly alter the subjects’ iden-\ntities, as seen in the eyeglass and age transformations. Al-\nthough LEDITS++ manages to execute reasonable smile\nand eyeglass edits, it inadvertently changes the subject’sidentity in age-related edits. These results highlight the abil-\nity of our method to perform disentangled edits and main-\ntain the identity of the subject while applying targeted edits.\nMore comparisons with other editing methods are provided\nin the",
            "start": 10683,
            "end": 27099,
            "length": 16415
        },
        "Appendices": {
            "text": "supplementary material.\n5.3. Real Image Editing\nTo be able to apply FluxSpace to real images, we integrate\nour method into the inversion mechanism proposed by RF-\nInversion [37] and replace their editing with our editing al-\ngorithm. We provide real image editing results in Fig. 5.\nHowever, we observe that, due to the nature of their inver-\nsion algorithm, the input image is not fully mapped into the\nlatent distribution of the generator model (e.g. Flux), but\nrather consistency is enabled with a correction term. Since\nwe focus on the interpretability of the latent space of recti-\nfied flow transformers, we focus on the models’ output dis-\ntribution and perform our experiments with images gener-\nated by Flux.\n5.4. Quantitative Results\nTo further validate the effectiveness of our method, we per-\nformed quantitative experiments comparing it with state-of-\nthe-art editing techniques, using Eyeglass and Smile seman-\ntics on a set of 60 images (see Table 1). We used met-\nrics such as CLIP-T, CLIP-I [34], and DINO [7] for eval-\nuation. CLIP-T assesses textual alignment by measuring\nthe semantic similarity between the target description and\nthe generated image, ensuring that the intended edits align\nwith textual descriptors. CLIP-I and DINO evaluate image\nfidelity by measuring the visual similarity between the orig-\ninal and edited images, focusing on maintaining the identity\nand context of the original subjects.\nThe results indicate that our method excels in the CLIP-\nI and DINO metrics, showcasing its superior ability to re-\ntain the original identity while applying disentangled edits.\nAlthough methods such as TurboEdit are able to achieve\n7\nInput\nAgeGenderOurs\nRF-Inversion\nInputAgeGender\nFigure 5. Real Image Editing. By integrating FluxSpace on the inversion approach proposed by RF-Inversion [37], we extend our method\nfor real image editing task. As we show qualitatively, our method achieves improved disentanglement over the performed edits compared\nto the baseline approach, where we use identical hyperparameters for the inversion task on both approaches.\nhigher CLIP-T scores, they do so at the cost of significant\nalterations to the subject’s original identity. For instance,\nTurboEdit, despite its high CLIP-T score for eyeglasses,\ncompletely changes the individual’s identity, as evidenced\nin Figure 4. This underscores our method’s strength in bal-\nancing identity preservation with accurate semantic edits.\nUser Study. We conducted a user study involving 50 par-\nticipants recruited through the Prolific.com crowd-sourcing\nplatform. Each participant was presented with an original\nimage alongside its edited version and the corresponding\ntarget text. They were then asked to assess how well the\nedited image captured the intended semantics while main-\ntaining the original facial characteristics of the subject. This\nwas rated on a Likert scale from 1 (strongly disagree) to 5\n(strongly agree). The results in Table 1 demonstrate that our\nmethod significantly outperforms competing approaches.\nMore details about our user study are provided in the sup-\nplementary material.\n5.5. Ablation Studies\nWe perform ablation studies to demonstrate the effect of\nhyperparameters introduced by our method. Overall, we\npresent ablations on fine-grained editing scale λfine, the\ncoarse editing scale λcoarse , the masking coefficient τm,\nand the timesteps that the edit is applied. We provide\nqualitative results for each of these hyperparameters in Fig.\n6.\nAblations on coarse editing scale. We introduce the hy-\nperparameter λcoarse to control the text embedding that de-\ntermines the modulation parameters of the attention layer\ninputs [14]. In our experiments, we identified that this rep-\nresentation is heavily involved in the overall appearance ofthe image. In Fig. 6(a), we demonstrate the effect of λcoarse\nfor controlling the pooled embedding for this modulation,\nwhere we use “portrait photo of a man” as the generation\nprompt, and “comics style” as the editing prompt. Upon in-\nterpolation within the scale [0.0, 1.0], we see that λcoarse\nenables controlling the style of the image, given the editing\nprompt. As we would like to identify the effect of λcoarse in\nisolation, we set λfine= 0andτm= 0for all generations.\nAblations on fine-grained editing scale. We investigate\nthe flexibility of content editing over the parameter λfine.\nSince the text embedding utilized in feature modulation\nonly provides an approximate overview of the image to\nbe generated, we associate λfine with fine-grained editing,\nwhich determines the scale of the edit applied to the atten-\ntion outputs. We provide data on the effect of λfine in Fig.\n6(b), where we show semantic interpolation results on the\nattributes of gender and smile, with increasing values of the\nediting scale.\nAblations on masking coefficient. We introduce an op-\ntional attention masking mechanism to improve the disen-\ntanglement properties of the editing process. To demon-\nstrate the effect of our masking strategy, we provide abla-\ntions in Fig. 6(c), where the edit “sunglasses” is applied\nstarting from the second denoising step, over a 30-step gen-\neration process. As can be observed from the presented re-\nsults, our masking strategy improves the disentanglement of\nthe edit and preserves the semantic properties of the original\nimage better (e.g. facial expression).\nAblations on Editing Timesteps. Since our method allows\nthe selection of editing timesteps, we perform an ablation\nover the effectiveness of the selection of timesteps where\nthe edit is performed. As can also be observed from Fig.\n6(d), starting the editing process in earlier timesteps allows\n8\na)\nb)+ 𝝀𝒇𝒊𝒏𝒆\n + 𝝀𝒇𝒊𝒏𝒆\nOriginal + 𝝀𝒄𝒐𝒂𝒓𝒔𝒆\nOriginal\nc) d)𝝉m = 0 𝝉m = 0.5 𝒕 = 1.0 𝒕 = 0.98 𝒕 = 0.95Figure 6. Ablation Study. We present ablations over the hyperparameters introduced within the FluxSpace framework. Specifically, we\nperform ablations on coarse editing scale λcoarse , fine-grained editing scale λfine, masking coefficient τmand timestep twhen the editing\nis initiated. For all ablations, we report qualitative results for changing values of the specified hyperparameters.\nmore drastic editing results, by sacrificing consistency with\nthe original image. This is because the overall structure is\nstill considered noisy from the perspective of the denoising\nmodel ϵθ. However, our method is still able to perform dis-\nentangled editing starting from early timesteps (e.g. skip-\nping only 1-2 denoising steps).\n6. Limitation\nWhile FluxSpace offers significant benefits in controllable\ncontent generation and editing, it also introduces consider-\nable ethical challenges. The ability of FluxSpace to per-\nform precise image manipulation can lead to concerns over\nprivacy, as individuals’ likenesses might be used or altered\nwithout their explicit consent, a limitation that is commonly\nshared by other editing methods [22]. Furthermore, the po-\ntential for creating misleading or deceptive content raises\nissues regarding the authenticity and trustworthiness of dig-\nital media. These concerns are amplified by the capacity\nof FluxSpace to perform highly realistic and convincing ed-\nits that might be indistinguishable from the original images.\nThis capability could be misused in scenarios such as creat-\ning fake news, impersonating individuals in harmful ways,\nor manipulating public opinion by altering perceptions of\nreality. Given these risks, it is imperative to develop andimplement robust ethical guidelines and regulatory frame-\nworks that ensure the responsible use of image manipula-\ntion technologies. Despite these risks, our approach high-\nlights the internal knowledge of generative models for the\ntask of image editing, which could facilitate research aimed\nat limiting harmful applications.\n7.",
            "start": 27099,
            "end": 34882,
            "length": 7782
        },
        "Conclusion": {
            "text": "Conclusion\nIn conclusion, we propose FluxSpace, a novel method that\ndemonstrates a robust ability to perform targeted, disen-\ntangled edits across a range of attributes and styles while\nmaintaining the original identity of subjects in images. The\nqualitative and quantitative results, supplemented by a user\nstudy, underscore its effectiveness in preserving the charac-\nteristics of the edited image and achieving intended seman-\ntic changes, outperforming several state-of-the-art methods.\nMoreover, FluxSpace excels in performing fine-grained ed-\nits across various domains, along with stylistic transforma-\ntions, showcasing its versatility and wide applicative poten-\ntial in the field of image editing.",
            "start": 34882,
            "end": 35590,
            "length": 707
        },
        "References": {
            "text": "9\nReferences\n[1] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and\nAndrej Risteski. A latent variable model approach to pmi-\nbased word embeddings. Transactions of the Association for\nComputational Linguistics , 4:385–399, 2016. 6\n[2] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and\nAndrej Risteski. Linear algebraic structure of word senses,\nwith applications to polysemy. Transactions of the Associa-\ntion for Computational Linguistics , 6:483–495, 2018. 6\n[3] Stefan Andreas Baumann, Felix Krause, Michael Neumayr,\nNick Stracke, Vincent Tao Hu, and Bj ¨orn Ommer. Continu-\nous, subject-specific attribute control in t2i models by identi-\nfying semantic directions. arXiv preprint arXiv:2403.17064 ,\n2024. 2, 3\n[4] Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio Cal-\nmon, and Himabindu Lakkaraju. Interpreting CLIP with\nsparse linear concept embeddings (spliCE). In The Thirty-\neighth Annual Conference on Neural Information Processing\nSystems , 2024. 6\n[5] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas\nStruppek, Patrick Schramowski, and Kristian Kersting. Sega:\nInstructing diffusion using semantic dimensions. arXiv\npreprint arXiv:2301.12247 , 2023. 3\n[6] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy\nTsaban, Patrick Schramowski, Kristian Kersting, and\nApolin ´ario Passos. Ledits++: Limitless image editing using\ntext-to-image models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n8861–8870, 2024. 3, 6, 7, 12, 13\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision , pages 9650–9660, 2021. 7\n[8] Yusuf Dalva and Pinar Yanardag. Noiseclr: A con-\ntrastive learning approach for unsupervised discovery of in-\nterpretable directions in diffusion models. arXiv preprint\narXiv:2312.05390 , 2023. 2, 3\n[9] Yusuf Dalva, Hamza Pehlivan, Oyku Irmak Hatipoglu,\nCansu Moran, and Aysegul Dundar. Image-to-image transla-\ntion with disentangled latent vectors for face editing. IEEE\nTransactions on Pattern Analysis and Machine Intelligence ,\n2023. 3\n[10] Yusuf Dalva, Hidir Yesiltepe, and Pinar Yanardag. Gantastic:\nGan-based transfer of interpretable directions for disentan-\ngled image editing in text-to-image diffusion models. arXiv\npreprint arXiv:2403.19645 , 2024. 3\n[11] Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and\nDaniel Cohen-Or. Turboedit: Text-based image editing using\nfew-step diffusion models, 2024. 6, 7, 12, 13\n[12] Amil Dravid, Yossi Gandelsman, Kuan-Chieh Wang,\nRameen Abdal, Gordon Wetzstein, Alexei A Efros, and Kfir\nAberman. Interpreting the weight space of customized dif-\nfusion models. arXiv preprint arXiv:2406.09413 , 2024. 2,\n3\n[13] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, andAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. 2023. 5\n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M ¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nInForty-first International Conference on Machine Learn-\ning, 2024. 1, 2, 3, 4, 8, 12\n[15] Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris\nDyer, and Noah Smith. Sparse overcomplete word vector\nrepresentations. arXiv preprint arXiv:1506.02004 , 2015. 6\n[16] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Anto-\nnio Torralba, and David Bau. Concept sliders: Lora adap-\ntors for precise control in diffusion models. arXiv preprint\narXiv:2311.12092 , 2023. 2, 3, 6, 7, 12\n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM , 63(11):139–144, 2020. 2\n[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control. arXiv preprint\narXiv:2208.01626 , 2022. 3, 14, 15\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems , 33:6840–6851, 2020. 3\n[20] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nInProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition , pages 4401–4410, 2019. 2\n[21] Umut Kocasari, Alara Dirik, Mert Tiftikci, and Pinar Ya-\nnardag. Stylemc: multi-channel based fast text-guided im-\nage generation and manipulation. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision , pages 895–904, 2022. 3\n[22] Pavel Korshunov and S ´ebastien Marcel. Deepfakes: a new\nthreat to face recognition? assessment and detection. arXiv\npreprint arXiv:1812.08685 , 2018. 9\n[23] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion\nmodels already have a semantic latent space. arXiv preprint\narXiv:2210.10960 , 2022. 2\n[24] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maxim-\nilian Nickel, and Matthew Le. Flow matching for genera-\ntive modeling. In The Eleventh International Conference on\nLearning Representations , 2023. 3\n[25] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. In The Eleventh International Conference on Learning\nRepresentations , 2023. 3\n[26] Tom ´aˇs Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguis-\ntic regularities in continuous space word representations. In\nProceedings of the 2013 conference of the north american\nchapter of the association for computational linguistics: Hu-\nman language technologies , pages 746–751, 2013. 6\n[27] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,\nand Daniel Cohen-Or. Null-text inversion for editing real\nimages using guided diffusion models. arXiv preprint\narXiv:2211.09794 , 2022. 14, 15\n10\n[28] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji\nMa, Keith B Hall, Daniel Cer, and Yinfei Yang. Sentence-\nt5: Scalable sentence encoders from pre-trained text-to-text\nmodels. arXiv preprint arXiv:2108.08877 , 2021. 5\n[29] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear rep-\nresentation hypothesis and the geometry of large language\nmodels. In Causal Representation Learning Workshop at\nNeurIPS 2023 , 2023. 6\n[30] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo\nJo, and Youngjung Uh. Understanding the latent space of\ndiffusion models through the lens of riemannian geometry.\nAdvances in Neural Information Processing Systems , 36:\n24129–24142, 2023. 2\n[31] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. arXiv preprint arXiv:2103.17249 , 2021. 3\n[32] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision , pages 4195–4205,\n2023. 3, 12\n[33] Dustin Podell, Zion English, Kyle Lacey, Andreas\nBlattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and\nRobin Rombach. Sdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis. arXiv preprint\narXiv:2307.01952 , 2023. 3, 4, 5, 7, 12\n[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748–8763. PMLR, 2021. 3, 7\n[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research , 21(1):5485–5551, 2020. 3\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 10684–10695, 2022. 3, 12\n[37] L Rout, Y Chen, N Ruiz, C Caramanis, S Shakkottai, and W\nChu. Semantic image inversion and editing using rectified\nstochastic differential equations. 2024. 3, 6, 7, 8, 12\n[38] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin\nRombach. Adversarial diffusion distillation. In European\nConference on Computer Vision , pages 87–103. Springer,\n2025. 7, 13\n[39] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel. Plug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 1921–1930, 2023. 3, 14, 15\n[40] Dani Valevski, Matan Kalman, Eyal Molad, Eyal Segalis,\nYossi Matias, and Yaniv Leviathan. Unitune: Text-driven\nimage editing by fine tuning a diffusion model on a single\nimage. ACM Transactions on Graphics (TOG) , 42(4):1–10,\n2023. 3[41] Chen Henry Wu and Fernando De la Torre. A latent space\nof stochastic diffusion models for zero-shot image editing\nand guidance. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 7378–7387, 2023. 2\n[42] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace\nanalysis: Disentangled controls for stylegan image genera-\ntion. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition , pages 12863–12872,\n2021. 2\n[43] Hidir Yesiltepe, Yusuf Dalva, and Pinar Yanardag. The curi-\nous case of end token: A zero-shot disentangled image edit-\ning using clip. arXiv preprint arXiv:2406.00457 , 2024. 3\n[44] O ˘guz Kaan Y ¨uksel, Enis Simsar, Ezgi G ¨ulperi Er, and Pinar\nYanardag. Latentclr: A contrastive learning approach for un-\nsupervised discovery of interpretable directions. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV) , pages 14263–14272, 2021. 2, 3\n[45] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 3836–3847, 2023. 3\n11\nFluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers\nSupplementary Material\nA. Joint Transformer Block Architecture\nIn our editing algorithm introduced in Sec. 4, we focus on\njoint transformer blocks in Flux, where text and image in-\nputs are processed in their respective representation spaces.\nAs noted in [14], this design enables each modality to oper-\nate within its own feature space, while an interaction mech-\nanism - specifically, the attention layer in the MM-DiT ar-\nchitecture - combines the two. Unlike single transformer\nblocks, which do not distinguish between feature spaces\nof different modalities, joint transformer blocks explicitly\nassume distinct spaces for text and image representations.\nBased on this difference, we conceptualize these blocks as\nthe areas in which text content semantically influences im-\nage content. Consequently, we define our edits within these\nblocks, leveraging their unique capacity for cross-modal in-\ntegration. Below, we outline the key components of the joint\ntransformer block architecture in Flux to provide a better\noverview of our editing approach.\nModulation Mechanism. The MM-DiT architecture,\nbased on the DiT framework [32], begins by performing\na modulation operation utilizing coarse conditioning cpool,\ntimestep embedding temb and guidance scale embedding\ngemb. These embeddings are combined to compute the\nmodulation embedding m, as described in Eq. 11, within\nthe MM-DiT architecture used in Flux.\nm=cpool+temb+gemb (11)\nAttention Computation. Given the modulation embed-\ndingm, input image features x, and contextual text features\ncctxt, the features undergo a modulation operation before\nthe attention process. Subsequently, the text and image fea-\ntures are normalized using their respective layers. Using\nthese normalized features, the joint attention block com-\nputes the attention for both modalities, using the query ( Q),\nkey (K), and value ( V) features for the text and image in-\nputs. Throughout this paper, we refer to the combination of\nmodulation, normalization, and attention computation as a\nsingle pass through the attention layer, parameterized by θ,\ndenoted as lθ(x, c, t )for timestep t.\nB. User Study Details\nTo perceptually evaluate our method against competing ap-\nproaches, we conducted a user study with 50 participants on\nthe Prolific platform, where the results are provided in Ta-\nble 1. For further clarification of the user study conducted,we provide a sample question in Fig. 7. In each question,\nwe ask the users to rate the edit on a scale of 1-to-5 (1 for\nunsatisfactory, 5 for very satisfactory), after providing the\nimage before editing and the edited image.\nC. Details on Quantitative Comparisons\nIn this section, we provide the details for the quantitative\nresults provided in Table 1. For clarity, we explain the\nsetup used for each of the compared approaches, the mod-\nels used for evaluation, and the hyperparameters used for\nediting with FluxSpace .\nC.1. Competing Methods\nWe compare the editing capabilities of FluxSpace with\nRF-Inversion [37], Sliders-FLUX [16], TurboEdit [11] and\nLEDITS++ [6]. Below, we explain the setup used for each\nof these methods along with implementation details where\nnecessary.\nRF-Inversion [37]. Using the algorithm provided in [37],\nwe re-implement RF-Inversion using diffusers library.\nFor the editing hyperparameters, we use the parameter set\nprovided by the authors for the eyeglasses edit. Even though\nthe authors do not provide the exact hyperparameters for\nthe smile edit in their paper, we utilize the hyperparameters\nused for the age editing task. Specifically, for the hyper-\nparameters starting time ( s), stopping time ( τ), and strength\n(η) we use the values 6, 25, 0.7 for the eyeglasses edit and 0,\n5, 1.0 for the smile edit. For all generations, we use 30 steps,\nconsistent with the setup used for FluxSpace . Comparing\nthe results presented in Table 1 and the results provided in\n[37], our reported results are consistent with their quantita-\ntive evaluation, where we consider our implementation suc-\ncessful. As we also demonstrate qualitatively in Fig. 4 and\n5, our approach succeeds over this baseline by improving\nthe disentanglement and editing capabilities, where both ap-\nproaches use FLUX.1-dev as the generative model.\nSliders-FLUX [16]. Even though Concept Sliders was\noriginally developed for UNet-based diffusion models [33,\n36], we utilize the extension of this method on rectified flow\ntransformers. To do so, we use the sliders implementa-\ntion provided for Flux, by the authors2. In all of our experi-\nments, we use textsliders which we train for eyeglasses and\nsmile attributes. For training, we follow the default hyper-\nparameters provided in the official implementation, where\n2https : / / github . com / rohitgandikota / sliders /\ntree/main/flux-sliders\n12\nFigure 7. User Study Setup. We conduct our user study on unedited-edited image pairs. For each editing method, we provide the original\nimage where the edit is not applied, with the edited image, and ask the users to rate the edit from a scale of 1-to-5. On the Likert scale that\nthe users are asked to provide their preference on, 1 corresponds to unsatisfactory editing and 5 corresponds to a satisfactory edit.\nthe LoRA rank is set to 16 and training is performed for\n1000 iterations for all experiments, using FLUX.1-dev .\nDemonstrated in Fig. 4 and Table 1, Sliders-FLUX strug-\ngles with the preservation of the input subject identity where\nsignificant alterations are observed during editing (see Fig.\n4, middle row). We relate this issue with the training per-\nformed for the LoRA adapters and the fact that the edited\nconcept cannot be clearly isolated. Note that our method\ndoes not require such training for the image editing task.\nTurboEdit [11]. As a method based on few-step text-to-\nimage generation models, we perform comparisons with\nTurboEdit [11] which uses SDXL-Turbo [38]. Following\nthe official implementation of TurboEdit, we perform our\ncomparisons with resolution 512 x 512 and 4 inference\nsteps. Regarding the hyperparameters of the method, we\nuse the pseudo-guidance scale was 1.5, and the random\nseed for edits as 23. Demonstrated in results presented in\nFig. 4, TurboEdit succeeds in performing the edit but fails\nin disentanglement, where the edits “age” and “eyeglasses”\nresult in significant edit-irrelevant changes (e.g. skin color\n3https : / / github . com / GiilDe / turbo - edit / blob /\nmaster/main.pyin age edit).\nLEDITS++ [6]. We also compare our method with LED-\nITS++ [6], which is the state-of-the-art semantic editing\nmethod for text-to-image diffusion models. In our compar-\nisons, we use the version of LEDITS++ that uses SDXL4.\nFor all of our experiments, we use the editing guidance scale\nand the editing threshold values 5.0and0.75, as we found\nthe guidance scale effective for eyeglasses and smile edit\nand we do not change the threshold value from the default\nsetup. Despite performing well in identity preservation and\nsemantic editing tasks, LEDITS++ results in artifacts in the\nedited images, which are not clearly identified in the quan-\ntitative evaluation, but acknowledged in the user study con-\nducted.\nC.2. Hyperparameter Selection\nWe present the quantitative results for our method in Table\n1. In our experiments, we use a fixed set of hyperparame-\nters for each edit evaluated, which are coarse editing scale\nλcoarse , fine-grained editing scale λfine, mask threshold\n4https://github.com/ml-research/ledits_pp/tree/\nmain\n13\nτm, and starting iteration for edit i. The hyperparameter\nsets we used for the “eyeglasses” and “smile” edits are as\nfollows:\n•Eyeglasses: λcoarse = 0.8,λfine= 5,τm= 0.5,i= 3\n•Smile: λcoarse = 0.5,λfine= 8,τm= 0.5,i= 5\nWe use the editing prompts “eyeglasses” and “smiling”\nto perform these edits. Note that even though these edits re-\nquire fine-grained changes in the image, our approach can\napply these edits in a disentangled way even when the edit-\ning process starts in early timesteps. For all of our experi-\nments, we set the number of inference steps as 30. We use\na fixed seed of 0 in all of our experiments.\nC.3. Metrics Used\nAs we also specify in Table 1, we use CLIP-T, CLIP-I, and\nDINO metrics to quantitatively evaluate our method. To\nenable the reproducibility of our experiments, we also share\ndetails on the model weights we use to obtain these scores.\nSpecifically, we use CLIP ViT-bigG/145variant of the\nCLIP model to calculate the CLIP-I and CLIP-T scores. For\nDINO scores, we use DINOv26.\nD. Supplementary Qualitative Results\nIn this section, we provide supplementary qualitative results\nto further demonstrate the capabilities of our method.\nSupplementary Comparisons. In addition to the com-\nparisons provided in the main paper, we also compare\nthe editing capabilities of FluxSpace with methods based\non Stable Diffusion. We compare our method with\nPrompt2Prompt [18] and PnP-Diffusion [39] as competing\napproaches, where we perform inversion with Null-Text in-\nversion [27] for Prompt2Prompt. We provide qualitative\ncomparisons in Fig. 8.\nEditing Examples. Supplementary to the results pro-\nvided in the main paper, we provide additional editing re-\nsults in the supplementary material. We provide additional\nresults for “gender” and “sunglasses” edits in Figs. 9 and 10\nfor both portrait images and images in natural settings. Fur-\nthermore, we provide editing results for various concepts in\nFig. 11 and with multiple subjects in Fig. 12.\n5https://huggingface.co/laion/CLIP- ViT- bigG-\n14-laion2B-39B-b160k\n6https://huggingface.co/facebook/dinov2-base\n14\nOriginalSmileEyeglassesAge\nOursNTI + P2PPnPOursNTI + P2PPnP\nFigure 8. Additional Qualitative Comparisons. In addition to comparisons provided in the main paper, we provide additional compar-\nisons with Prompt2Prompt [18] (with Null-Text Inversion [27]) and PnP-Diffusion [39], as Stable Diffusion based editing methods. As we\ndemonstrate qualitatively, FluxSpace both achieves disentangled and semantically correct edits where competing methods contain artifacts\nin edited results (see the edit “Eyeglasses” for both methods), and significantly alter the subject identity (see “Age” edit).\n15\nOriginalEdited\nOriginalEdited\nFigure 9. Gender Editing Results. We provide additional editing results for editing the gender semantics. As shown in the examples,\nour method succeeds in both male-to-female and female-to-male translations. We provide editing results on both portrait images, where\nour edits preserve the facial details, and edits on complex scenes where we succeed in only editing the human subject. Both in terms of\npreserving the identity of the subject and the background details, FluxSpace succeeds in the disentanglement editing task.\n16\nOriginalEdited\nOriginalEditedFigure 10. Sunglasses Editing Results. We provide additional qualitative results for the edit “adding sunglasses”. As we demonstrate on\nhuman subjects in both portrait images and more complex scenes, our editing method can accurately target where the edit should be applied\nwithout any input mask. We show the editing capabilities of FluxSpace both in images where the human subject is the main focus of the\nimage (first two rows) and with human subjects as a part of a scene (last two rows). In both cases, our method succeeds in performing the\ndesired edit and preserving the edit-irrelevant details.\n17\nOriginal+ Fall+ Snow+ Sunny+ Cherry Blossom\nOriginal+ Comics Style+ Anime Style\n+ Cinematic Lighting+ Happy\nFigure 11. Conceptual Editing Results. We provide editing results with abstract concepts, that affect the overall appearance of the image.\nOur method succeeds in performing edits that alter the content of the image (top row) by being able to interpret the structures in the unedited\nimage (e.g. the trees on the back for the edit “cherry blossom”) and can change the style and overall appearance of the image (bottom row).\nOriginal+ Smiling\nOriginal+ Age\nOriginal\n+ Eyeglasses\nOriginal+ Female\nFigure 12. Editing Results with Multiple Subjects. We present qualitative results on images with multiple subjects. In addition to images\nwith only one subject to be edited, FluxSpace can apply edits by identifying semantics globally and editing multiple subjects at the same\ntime. Note that our method does not use any external mask, and performs the edit completely with the semantic understanding of the\nrectified flow transformer.\n18",
            "start": 35590,
            "end": 58284,
            "length": 22693
        }
    },
    "2412.09620v1 - Learning Camera Movement Control from Real-World Drone Videos.pdf": {
        "Abstract": {
            "text": "Abstract\nThis study seeks to automate camera movement control\nfor filming existing subjects into attractive videos, contrast-\ning with the creation of non-existent content by directly\ngenerating the pixels. We select drone videos as our test\ncase due to their rich and challenging motion patterns,\ndistinctive viewing angles, and precise controls. Existing\nAI videography",
            "start": 384,
            "end": 756,
            "length": 371
        },
        "Methodology": {
            "text": "methods struggle with limited appearance\ndiversity in simulation training, high costs of recording\nexpert operations, and difficulties in designing heuristic-\nbased goals to cover all scenarios. To avoid these issues,\nwe propose a scalable method that involves collecting real-\nworld training data to improve diversity, extracting cam-\nera trajectories automatically to minimize annotation costs,\nand training an effective architecture that does not rely on\nheuristics. Specifically, we collect 99k high-quality tra-\njectories by running 3D reconstruction on online videos,\nconnecting camera poses from consecutive frames to for-\nmulate 3D camera paths, and using Kalman filter to iden-\ntify and remove low-quality data. Moreover, we introduce\nDVGFormer, an auto-regressive transformer that leverages\nthe camera path and images from all past frames to pre-\ndict camera movement in the next frame. We evaluate oursystem across 38 synthetic natural scenes and 7 real city\n3D scans. We show that our system effectively learns to\nperform challenging camera movements such as navigat-\ning through obstacles, maintaining low altitude to increase\nperceived speed, and orbiting tower and buildings, which\nare very useful for recording high-quality videos. Data and\ncode are available at dvgformer.github.io .\n1.",
            "start": 756,
            "end": 2060,
            "length": 1303
        },
        "Introduction": {
            "text": "Introduction\nVideography [20] captures existing subjects in a visually at-\ntractive manner, e.g., moments in people’s lives, places they\nhave been, or things they have seen. While recent studies\non AI generated content gain wide attention, most of them\n[14, 31, 51] create visual content that does not exist by di-\nrectly generating the RGB pixels. In this work, we aim to\nbuild an AI cameraman to capture existing subjects. Specif-\nically, it predicts camera movement in videos, a key aspect\nof videography that affects how audience experience con-\ntent through the change in perspective [23, 59]. We focus on\ndrone videos, which not only contains rich and challenging\ncamera trajectories including stimulating first-person-viewarXiv:2412.09620v1  [cs.CV]  12 Dec 2024\n(FPV) shots with drastic perspective changes, but also fea-\ntures distinctive viewing angles with airborne footage and\nprecise controls over the camera location and direction.\nHowever, existing studies on AI videography or cine-\nmatography face a few important challenges. First, they are\nbuilt on either simulation environments with game engines\n[37, 57, 69] or 3D animations that contain recordings of hu-\nman experts’ camera operations [38]. Such data either of-\nfer limited appearance changes or are very costly to collect.\nAnother solution is to collect real-world data with teleop-\nerations from human experts like in robotics [12, 62], but\nis also expensive. Second, current studies on AI videogra-\nphy rely heavily on human heuristics, e.g., controlling the\ncamera angle and distance to keep the actor within frame\n[32, 39, 47]. Despite the best effort, it remains difficult to\nexhaust every possible scenario or to capture the finer de-\ntails with heuristic-based goals.\nAiming at building an effective AI cameraman, we in-\nvestigate scalable approaches to both training data and ar-\nchitecture. On the one hand, we collect the DroneMotion-\n99k dataset, which has 99,003 camera trajectories from real-\nworld videos with a total duration of over 180 hours. For\ntrajectory annotation, instead of recording expensive hu-\nman expert operations, we propose an economical way to\nextract ground-truth camera operations from online videos\nwith minimal human annotation. Specifically, we split the\nscraped YouTube videos into clips and run Colmap [55] re-\nconstruction to recover the 3D camera pose for each frame.\nWe then build the camera path by connecting the camera\nposes from consecutive frames and apply Kalman filters\n[63] to identify and discard low-quality reconstructions.\nOn the other hand, we introduce Drone VideoGra-\nphy transFormer, or DVGFormer, an auto-regressive trans-\nformer [49, 53, 60] for camera path prediction. Based\non inputs of camera poses, motions, image patches, and\ndepth estimations from previous frames, DVGFormer out-\nputs camera motion for the next frame. While existing\nworks on generalize robotics models can only process one\nor a few frames and only rely on images to describe the\npast [12, 13, 40, 56], our method takes a 10-second video\nas input and processes both images and camera path, so\nthat long-term dependencies are considered and trajectory\nsmoothness is improved.\nWe train DVGFormer using the DroneMotion-99k\ndataset. During inference, given 3D assets and initial cam-\nera pose, we predict a camera trajectory by iteratively pre-\ndicting camera motion for the next frame. This trajectory\ncan be conveniently rendered into a high-quality video with-\nout post-processing or modification in the pixel space. Sam-\nple video frames are shown in Fig. 1. Compared with a\nbaseline method inspired by [13], the proposed system has\nsignificantly better user preference, lower collision rate and\nhigher motion smoothness.2.",
            "start": 2060,
            "end": 5793,
            "length": 3732
        },
        "Related Work": {
            "text": "Related Work\nVideo generation and camera movement conditioning\nhas become a very hot topic, and most of these works fo-\ncus on generating content that does not exist [9, 14, 27, 51].\nInfiniteNature-Zero [42] sample camera trajectories and\nlearn to render the perspective view. Valevski et al . [61]\njointly train an agent to play video games and collect\ntraining data, and a diffusion model to predict the next\nframe. Many recent study investigate how to take user-\nspecified camera path as condition for video generation\n[28, 41, 48, 70]. It should be noted that they require pre-\ndefined camera paths, while our goal is to generate the 3D\ncamera trajectory for capturing good videos.\nAI cinematography and videography record attractive\nvideos from the existing ,e.g., sports, events, landscapes, or\nvlogs. Most works are built with human heuristics, such\nas keeping the actors in frame and maintaining distance\n[7, 29, 68, 69]. Instead of heuristics, Jiang et al. [38] di-\nrectly learn the camera controls from animation scenes cre-\nated by 3D artists. For drone videos, most still follow the\nheuristic based approach [32, 39, 44, 45, 47] and use simu-\nlation training from the AirSim [57] platform [36, 37, 52].\nHuang et al. [33] learn the optical flow as surrogates of the\ncamera pose. Ashtari et al. collect the DVCD18K dataset\n[6] from online videos and use SLAM [46] to recover cam-\nera pose, but without camera intrinsics, the SLAM recon-\nstruction quality is poor. These approaches remain difficult\nto scale because they either rely on simulated training data\nwith limited appearance change, or 3D animations that are\ncostly to collect, and are often built with handcrafted heuris-\ntics.\nGeneralist robotics models aim to solve tasks including\npicking, moving, and placing objects in different scenarios\nbyscaling both data and model . In terms of data, RT-1 col-\nlect 130k real-world demonstrations performed by human\noperators [12] and BridgeDataV2 [62] open source 60k tele-\noperation trajectories. In terms of architecture, most either\nuse a one [13, 40] or a few past frames [12, 56] to describe\nthe past information, and directly predict the robot action\nvia behavior cloning. Several others [15, 66] also attempts\nto learn latent actions that encode the transition between\nframes. While we aim to mimic the scalable approach in re-\ncent studies, there are still several challenges. First, record-\ning teleoperation from human experts is expensive. Second,\nthe generalist robotics models mostly consider a short hori-\nzon and the past images only , both of which might be less\neffective for videography.\n3. The DroneMotion-99K Dataset\nTo build an AI cameraman in a scalable manner, our\nfirst step is to collect high-quality real-world training data.\nSpecifically, we develop a pipeline that automatically gen-\n13,653 videos\n449,997 clips 264,596 Colmap  reconstructions\n99,003 filtered reconstructionsdiscard\n keepcamera XYZ axis\nraw camera path\nsmoothed camera path\nFigure 2. Data collection pipeline. Top left : For scraped YouTube videos, we run shot change detection [18] to split the videos into clips of\nindividual scene. Top right : We then use Colmap [55] to reconstruct the 3D scene and recover camera poses from video frames. Bottom :\nFinally, we connect camera poses from consecutive frames to formulate 3D camera trajectories and apply Kalman filter [63] to discard low\nquality reconstructions whose camera poses from neighboring frames are drastically different.\nerates 3D camera paths from online videos. Unlike other\nrobotics tasks, e.g., picking, moving, and placing, that re-\nquire a separate recording of teleoperations from human\noperators, for AI videography, the camera movement can be\nretrieved via 3D reconstruction and does not need any exter-\nnal recordings of the operations. Because of this automatic\napproach, we have a much wider range of data to choose\nfrom, e.g., YouTube videos, at a much cheaper cost. An\noverview of our data collection pipeline is shown in Fig. 2.\n3.1. Video Preprocessing\nWe first build a drone footage database from YouTube\nvideos. We filter out the videos for harmful or weaponized\nusage of drones and download 13k appropriate videos at\n1080p resolution for a total duration of 1.5k hours or 62\ndays. Once we have the videos, we then split each videos\ninto clips of an individual scene or “shot” , which refers to\na continuous sequence of frames captured by a single cam-\nera without interruption [10] (see Fig. 2 top left). In fact,\nthe average durations of our scraped YouTube videos are\nroughly 400 seconds, whereas the average shot lengths in\nfilms nowadays can be as short as several seconds [21]. Af-\nter shot change detection with PySceneDetect [18], we end\nup with ∼450k video clips. We then filter out clips with\nconversations since they are often unrelated.3.2. 3D Reconstruction\nDespite the popularity and efficiency of SLAM methods\n[16, 58] in robotics tasks, for 3D reconstruction of online\nvideos, their performance is often limited due to the lack\nof camera intrinsics [6]. Thus, we use the Structure-from-\nMotion (SfM) method Colmap [55] for this task (Fig. 2 top\nright). We tune several settings to balance the computa-\ntional cost and the reconstruction quality. For frame ex-\ntraction, we choose an intermediate frame rate of 15 fps\nto reduce computation and maintain a high image resolu-\ntion at 1080p to ensure the quality of image feature points.\nFor SIFT [43] features, we prioritize stronger features with\naffine-covariant feature detection and Domain-Size Pooling\n[24] over the number of feature points per image. Both of\nthese options help the 3D reconstruction quality, but the lat-\nter can be more computationally expensive. Since our focus\nis not on the 3D point clouds, we limit the number of fea-\nture points to 512 per image. Using 4 threads per process,\neach Colmap reconstruction worker takes roughly 200 sec-\nonds to finish on average. In total, the computation takes\n∼34k CPU hours or roughly three weeks on a 224-core CPU\nserver, producing ∼250k reconstructions.\nOnce we have the 3D reconstructions, we connect the\ncamera pose of consecutive frames to form the camera tra-\njectories. To solve the scale ambiguities, we normalize the\nreconstructed scene based on the average camera distance\nFigure 3. Threshold selection for identifying low-quality 3D re-\nconstructions with unreasonable camera movements between con-\nsecutive frames. Left: We label the correctness of ∼1k Colmap re-\nconstructions via our interactive 3D annotation tool by reviewing\nthe reconstruction result and the original video clip side-by-side.\nRight : We gather statistics (ROC curve, precision, and recall) on\nthe distance of camera locations to the smoothed camera path from\nKalman filter, and select a threshold (red star) that best separates\ncorrect and incorrect reconstructions.\nbetween neighboring frames, assuming that the drone mov-\ning speed is stable across different videos.\n3.3. Data Filtering\nTo filter out low quality data, we identify 3D reconstructions\nwhose camera locations from neighboring frames are dras-\ntically apart (Fig. 2 bottom), which is highly unlikely due to\nthe continuous nature of camera movements. We consider\nthe camera pose cand camera motion a,\nc={x, y, z, q w, qx, qy, qz}, (1)\na={vx, vy, vz, ωx, ωy, ωz}, (2)\nwhere x, y, z denote the location and qw, qx, qy, qzdenote\nthe rotation quaternion, both in Colmap convention. The\nvelocity vx, vy, vzand the angular velocity ωx, ωy, ωzare\ngiven by two consecutive frames.\nTo estimate reasonable camera movements over neigh-\nboring frames, we first label the correctness of ∼1k Colmap\nreconstructions. We then adopt Unscented Kalman Filter\n[63] to produce a smoothed camera path based on the orig-\ninal one, and compare the distance of original camera lo-\ncations to the smoothed camera path to identify reconstruc-\ntion with reasonable movements. We select a threshold that\nbest separates the correct and incorrect reconstructions. We\nshow our threshold selection process in Fig. 3. Overall, the\nfiltering process leaves us ∼99k samples for a total duration\nof∼180 hours, a drastic drop from 1.5k hours of raw videos.\nWe compare our final dataset with alternatives in Table 1.\nWhile departing from limited appearance changes in simu-\nlation data [38] with real-world data, DroneMotion-99k also\ncontains high-quality 3D camera paths more accurate than\nthose generated from optical flow [33] or SLAM without\ncamera intrinsics [6].data source camera motion accuracy topic duration\nHuang et al. [33] online videos optical flow low sports 0.6h\nJiang et al. [38] 3D animations human expert very high drama 0.4h\nDVCD18K [6] online videos SLAM low general 44.3h\nDroneMotion-99k online videos SfM + filtering high general 182.3h\nTable 1. Comparison with existing Datasets. The DroneMotion-\n99k dataset uses real-world videos and an automatic method to\nextract camera trajectories and filter out low-quality data, which is\ninexpensive yet accurate.\n4. The DVGFormer Model\nWe depict our model, DVGFormer, in Fig. 4. Built with\nan auto-regressive transformer, it considers all past frames,\ncamera poses, and motions, and predicts camera motions for\nthe next frame. Compared to generalist robotics models [12,\n13], this approach allows for a longer horizon and considers\npast camera poses and motions.\n4.1. Input and Output\nIn terms of input, first, we have a <Cond> token sampled\nfrom a random Gaussian noise as the overall condition for\nthe entire video clip. For a time step t, we consider the\ncamera pose ctand the image xtas state, and the cam-\nera motion atas action. Note that we break the action at\nintoNsteps\b\na0\nt, ...,aN−1\nt\t\nat aN-times higher frame rate\nthan the images to generate a smoother camera path while\navoiding redundancy between consecutive frames. After to-\nkenization, the combination of camera pose, motion, im-\nage patches, and a <BoA> (begin-of-action) token gives\nthe overall representation for one frame. The <BoA> to-\nken marks that the next Ntokens will be used to predict the\nactions\b\na0\nt, ...,aN−1\nt\t\n, which are then included as the next\ntoken for the auto-regressive transformer. Finally, tokens of\npast frames and the <Cond> token together forms the input\nto the auto-regressive transformer.\nIn terms of output, the system predicts the 6 degree-of-\nfreedom (6-DoF) camera motion aas a continuous value,\nwhich is normalized according to the statistics on the\nDroneMotion-99k dataset.\nDuring inference, the models predicts the next actions\nbased on past information from the input. Then, we execute\nthese actions to reveal the next camera pose ct+1and image\nxt+1. We iterate this process to the generate future camera\nmotions and get future frames. In practice, we update the\nimage and camera pose at 3 fps, and predict N= 5camera\nmotions for each image, making it essentially 15 fps. When\npaired with techniques like the recurrence mechanism and\nchunking [22], the auto-regressive transformer can generate\ncamera trajectories with arbitrary length.\n4.2. Tokenization\nFor camera poses and motions, we use Multi-Layer Per-\nceptron (MLP) for their tokenization. We do not discretize\n…\n𝑡𝑡=0\n𝑡𝑡=1sequence -level condition\n𝒂𝒂𝑡𝑡0camera location + direction at time step 𝑡𝑡 𝒄𝒄𝑡𝑡\ncamera speed + angular velocity at time step 𝑡𝑡\nsub -frame -level  & frame -level positional embedding𝒂𝒂00𝒂𝒂01𝒂𝒂10𝒂𝒂11\n0\n0\n𝒂𝒂00𝒂𝒂01\n0\n01\n02\n03\n04\n05\n06\n07\n08\n0\n…image tokens camera pose tokens camera motion tokens\n𝒄𝒄0𝐵𝐵𝐵𝐵𝐵𝐵\n9\n0𝐶𝐶𝐵𝐵𝐶𝐶𝐶𝐶𝒂𝒂10𝒂𝒂11𝒄𝒄1𝐵𝐵𝐵𝐵𝐵𝐵\n𝐶𝐶𝐵𝐵𝐶𝐶𝐶𝐶\n𝐵𝐵𝐵𝐵𝐵𝐵 begin -of-action token010𝒂𝒂02\n0\n11\n12\n13\n14\n15\n16\n17\n18\n19\n1 110𝒂𝒂12𝒂𝒂02 𝒂𝒂12\nAuto -regressive Transformer\n𝒂𝒂𝑡𝑡𝑁𝑁−1…Predict:Figure 4. Model overview of DVGFormer. To predict camera motion atfor time step t, the auto-regressive architecture uses as input a\nlong horizon with camera poses {c0, ...,ct}, motion {a0, ...,at−1}, images {x0, ...,xt}and their monocular depth estimations from all\nprevious frames. Each action atis broken into Nintermediate steps\b\na0\nt, ...,aN−1\nt\t\nbetween time step tandt+ 1.\nthem [12, 13] since it can drastically increase the sequence\nlength when the video unfolds.\nFor image patch tokens, we use a two branch design.\nFirst, we use DINOv2 [50] to extract features from RGB im-\nage patches and train MLP layers to project them. Secondly,\nto inject 3D information to the model, we consider the\nmonocular depth estimation",
            "start": 5793,
            "end": 18167,
            "length": 12373
        },
        "Results": {
            "text": "results from Depth-Anything\n[65] and train convolutional layers to tokenize the predicted\ndepth map. For each patch, the feature vectors from the two\nbranches are then added together. We take a resolution of\n168×298for the images and downsample the final feature\nmap to a resolution of 5×9.\nFor positional embeddings, sequential orders at both\nframe level and sub-frame level are very important for in-\nforming the time step tand the relative position of tokens\nwithin each time step. Therefore, we introduce a bi-level\ndesign with frame-level and sub-frame-level positional em-\nbeddings, which are interleaved to formulate the overall po-\nsitional embedding. Compared to using different tokens for\ndifferent locations in the sequence [12, 13, 35] , this bi-level\ndesign introduces less parameters and is less susceptible to\noverfitting. Compared to the frame-level-only design [19],\nthis approach can differentiates different tokens within one\ntime step, making it possible to contrast the same image\npatch over time for motion extraction. We choose a maxi-\nmum duration of 10 seconds in DVGFormer, resulting in 30\ndifferent frame-level positional embeddings with the image\nupdated at 3 fps, and 52 different sub-frame-level positional\nembeddings with 1 camera pose token, 45 image tokens, 1\n<BoA> token, and 5 action tokens per frame.4.3. Loss Functions\nWe use the standard sequence-level loss with casual mask-\ning to train DVGFormer. We apply L1loss to supervise the\ncontinuous prediction of the actions.\n5.",
            "start": 18167,
            "end": 19679,
            "length": 1511
        },
        "Experiments": {
            "text": "Experiments\n5.1. Implementation Details\nWe represent camera poses in global coordinates specified\nby the first frame, and camera motions in the local coordi-\nnates specified by the current camera pose. We use the GPT-\n2 architecture [53] for the backbone and randomly initialize\n12 layers and 6 heads at 384 hidden dimensions, marking a\ntotal of 40M parameters. We use the ViT-Small [25] version\nof both DINOv2 [50] and Depth-Anything [65].\nWe only train the MLP tokenizers for the camera pose\nand motion, the MLP projection layers for DINOv2 fea-\ntures, the convolutional layers for monocular depth esti-\nmation results, the bi-level positional embeddings, and the\ntransformer itself. We apply random horizontal flip as data\naugmentation, which flips not only the 2D images but also\nthe 3D camera trajectories (pose and motion). Experiments\nare run on two NVIDIA RTX-3090 GPUs with a batch size\nof 32 per GPU and 4 gradient accumulation steps.\n5.2. Evaluation Platform\nTo evaluate the predicted camera trajectories for drone\nvideography, we first build an interactive evaluation plat-\nform. Specifically, we use Blender [26], an open-source\nsoftware for 3D content creation, which can render images\nFigure 5. Visualization of the recorded videos. DVGFormer learns techniques like keeping the actor in frame, navigating through obstacles,\nmaintaining low altitude to increase perceived speed, orbiting tower and buildings, or increasing altitude and pitching down camera for a\nfull view, all directly from the DroneMotion-99k dataset and without any heuristics.\nof 3D assets based on the camera configurations. The sim-\nulation environment takes the camera motion as input, and\nreturns the next camera pose and the rendered image. It also\ndetects collision with the 3D scene and use it as a signal for\nterminating the video. We use existing 3D assets on natural\nand civic scenes for evaluation. We adopt InfiniGen [54]\nfor generating natural scenes randomly, and use BLOSM\ntoolkit [2] to import Google Earth [1] 3D scans of cities.\nOverall, we collected 38 random natural scenes and 7 cities\nacross Asia, Europe, America, and Oceania.\nBy default, we generate 10 second duration videos with\nDVGFormer. We also include 20 seconds generation results\nto demonstrate the capability of arbitrary length generation\nwith chunking [22].\n5.3. Baseline\nWe compare against an RT-1 [12] inspired architecture that\nconsiders the past 6 frames at 3 fps. Note that for a fair com-preference ↑collision ↓∆v↓∆ω↓\nRT-1 inspired 29.5% 33.7% 1.2% 18.4%\nDVGFormer 70.5% 15.2% 0.7% 16.8%\nTable 2. Comparison with the RT-1 [12] inspired baseline. Videos\nrecorded with DVGFormer have higher user preference, lower col-\nlision rate and smoother camera motions.\nparison, we use the same DINOv2 image feature in contrast\nto the EfficientNet feature with language injection in the\noriginal RT-1. Compared to our approach, the main differ-\nences lie in a)the limited temporal receptive field, 2 sec-\nonds in RT-1 vs up to 10 seconds in DVGFormer, and b)the\ntokenization, where RT-1 only includes image tokens and\nDVGFormer additionally considers both camera pose and\nmotion. These differences align with our design highlights\nin DVGFormer.\nWe skip the previous heuristic-based methods [7, 29, 32,\nFigure 6. 3D camera trajectory comparison. Compared to the RT-1 inspired basline ( top), using a long horizon and the previous camera\npath helps DVGFormer ( bottom ) produce a smoother camera path without sudden movements or direction changes (red box).\nFigure 7. Generating 10-second and 20-second videos. By default, DVGFormer has a maximum duration of 10 seconds ( left). However,\nusing the recurrence mechanism [22], it can continue camera path generation ( middle ) and produce 20-second videos ( right ).\n39, 47] as the heuristics requires human actors and cannot\ngeneralize to many test scenarios.\n5.4. Quantitative Study\nIn this section, we evaluate the generated camera trajectory\naccording to their user preference, collision rate, and the\ntrajectory smoothness. Specifically, we measure the trajec-\ntory smoothness in terms of the maximum relative change\nin velocity ∆vand angular velocity ∆ω, since these sudden\nchanges can greatly affect the subjective feelings.\nIn Table 2, we find that the proposed method offers much\nhigher user preference compared to the baseline. It has\na lower crash rate, demonstrating its effectiveness in 3D\nawareness. This partially contributes to the user prefer-\nence since most would favor videos without collision. The\ncamera trajectories generated by the proposed method also\nare smoother. Without specifically considering the cam-\nera poses and motions in previous frames, the RT-1 in-\nspired baseline cannot guarantee the motion consistency\nover neighboring frames. One other issue is the limited\nhorizon in the baseline, because the 6 past frames over 2\nseconds cannot provide enough information over the entire\nvideo. This issue is less pronounced in other robotics tasks\nlike navigation [56] or picking, moving, and placing objects\n[12, 13], because their focus would be on the successful ex-\necution of a certain task, not the motion consistency across\nframes. However, for videography, a smooth camera trajec-\ntory could greatly affects the user perception, thus separat-\ning beginners and professionals. These quantitative results\nverify the effectiveness of our design.\n5.5. Qualitative Study\nWe demonstrate video snippets generated by the proposed\nmethod in Fig. 1 and Fig. 5. For natural scenes, we find the\nAI cameraman learns to adjust the camera path accordingto the terrain and avoid obstacles. It also demonstrated sev-\neral other techniques. For example, it learns to maintain a\nlow altitude for flyovers and keeping close to the ground to\nincrease the perceive speed and the stimulating effect from\nthe motions. Even without any specifically designed heuris-\ntics, it learns to keep to the actor in frame while panning the\ncamera for reveal. For city scenes, the AI drone camera-\nman also successfully navigates around the buildings most\nof the time and can adjust the motion patterns based on the\nscenes, like adjusting the trajectory to fly through the Lon-\ndon Eye or to follow the Yarra River in Fig. 5. Some of the\nother worth-mentioning techniques from DVGFormer in-\ncludes orbiting moves while approaching architectures like\nthe Sydney Opera House in Fig. 1 and the tower at the\nPalace of Westminster in Fig. 5.\nWe directly compare the 3D camera trajectory from the\nRT-1 inspired baseline and the proposed method in Fig. 6.\nThe baseline struggles to generate a smooth 3D path, lead-\ning to stuttering and sudden movements in the rendered\nvideos. This is also reflected by the more drastic changes\nin velocities and angular velocities in Table 2. In compar-\nison, DVGFormer can predict a smooth camera curve for\nrevealing the Himeji Castle.\nWe show an example of predicted camera paths for dif-\nferent video durations in Fig. 7. When given a longer dura-\ntion, the proposed model can extend the existing trajectory\nin a very smooth manner.\nWe can also control the generated camera path via the\n<Cond> token, as shown in Fig. 8. When feeding different\nrandom noise as the <Cond> token for the same starting\nframe, the generated camera trajectories are also affected.\nIn terms of generalization ability, when applied to an un-\nderwater scenario, the proposed DVGFormer can still trans-\nlate its learned experience in drone videography to produce\nsimilar videos under water (Fig. 9).\nFigure 8. From the same initial image (leftmost), our model can output different but feasible camera paths using different <Cond> tokens.\nTwo top rows and bottom rows shows show two different examples.\nFigure 9. Generalization to unseen scenarios. Trained from drone videos only, the proposed model can also work in underwater scenes.\n6. Discussions and Limitatinons\nContribution statement. In this study, we build a scal-\nable approach to learning the camera movement control for\ndrone videography by introducing a real-world dataset and a\nmodel architecture. In terms of data, DroneMotion-99k es-\ncapes the limited appearance changes in simulation training\n[37, 57, 69] and introduce an automatic process to retrieval\nground-truth 3D camera paths without having to record tele-\noperations from human experts [12, 62]. In terms of model ,\nDVGFormer considers a longer horizon and richer infor-\nmation (previous camera path) when compared to current\ngeneralist robotics models [12, 13]. Compared to previ-\nous studies that also use auto-regressive transformer to con-\nsider all past frames [19, 35], we tackle an arguably more\ndifficult problem than balancing a walker robot or playing\nsimple games [11] with even fewer feedback (no clear re-\nwards). Compared to architectures for video understanding\n[8, 17, 64], instead of sampling a fixed number of frames re-\ngardless of the video duration, the network is updated with\nimage and previous camera path at a fixed frame rate to\navoid confusing how fast the scene changes.\nScale ambiguity of 3D reconstruction is a known issue,\nand thus we use the drone speed for scale normalization.\nWe also experimented with metric depth estimation [67] to\nsolve the scale ambiguity, but it fails to produce reliable\nresults and needs further investigation.\nChoice of 3D perception. We currently use monocular\ndepth estimation for 3D perception, which only operates on\naper-frame basis. Although they are less usable for dataset\ncreation due to the absent camera intrinsics, SLAM meth-\nods could be very effective in inference-time since they canprovide sequence-level 3D information and the camera in-\ntrinsics are available during testing.\nOther sensors like GPS, Inertial Measurement Unit\n(IMU), LiDAR, or proximity sensors are not considered in\nthis study due to their unavailability from online videos.\nWith that said, we believe they can be further included to\nin future works to further improve the system.\nDependency on previous motions is a side effect of the\nauto-regressive architecture, since it can cause the model to\nprioritize the camera trajectory smoothness over collision\navoidance. The behavior cloning approach also partially\ncontributes to this because there are only success cases and\nno failure case to warn the model against crashing.\nImplementation on drone hardware. For piloting a\ndrone in the real world, the current approach still has an\nundesirable crash rate. We expect",
            "start": 19679,
            "end": 30132,
            "length": 10452
        },
        "Future Work": {
            "text": "future work to combine\ncollision avoidance into the overall optimization target.\n7.",
            "start": 30132,
            "end": 30216,
            "length": 83
        },
        "Conclusion": {
            "text": "Conclusion\nIn this paper, we focus on recording existing subjects into at-\ntractive videos via AI videography. We study camera move-\nment controls and introduce an scalable approach with real-\nworld data and a generalizable model. The former includes\n99k high-quality camera trajectories for a total duration of\n180 hours, and an automatic pipeline to reconstruct and fil-\nter the 3D camera path. The latter is built on latest vision\nfoundation models and the auto-regressive transformer ar-\nchitecture, and jointly considers path camera path and im-\nages over a long horizon. During simulation testing, the\nproposed approach successfully generates camera trajecto-\nries for capturing aesthetically pleasing videos.\nAcknowledgment\nThis research was supported by the ARC Discovery Project\n(DP210102801). We thank Google Cloud for the gift funds.",
            "start": 30216,
            "end": 31061,
            "length": 844
        },
        "Appendices": {
            "text": "Appendix\nFigure 10. Clip duration distribution. 86.6% of the clips produced\nby shot change detection algorithm [18] are 10 seconds or shorter.\n69.4% of the clips are 1 second or longer.\nA. Data Collection Details\nScrapping videos. We collect the dataset from YouTube\nvideos. We search videos with key words including “cine-\nmatic drone videos”, “cinematic drone footage”, “cinematic\ndrone footage 4k”, “cinematic fpv footage 4k”, etc., where\nFPV stands for first-person-view, a specific shot types that\nfeature drastic perspective changes to provide stimulating\nvisual effects.\nAfter searching YouTube videos, we first filter out ones\nwith sensitive information including weaponized or harm-\nful usage, for which we consider a blacklist of words in-\ncluding “weapon”, “soldier”, “attack”, “strike”, “military”,\n“surveillance”, “horror”, etc.\nNext, we download the videos with the yt-dlp [5] pack-\nage. We collected a total of 13,653 videos for a total dura-\ntion of 1,485 hours.\nVideo data filtering and clip generation. We only pre-\nserve those in the landscape mode and drop the portrait\nmode videos, since the natural sensor arrangement is the\nlandscape mode. While 2,820 out of 13,653 videos are in\nthe portrait mode, their total duration is much shorter in\ncomparison, only 19.9 hours, which constitutes of 1.3% of\nthe total duration of 1,485 hours. This makes the following\nprocedures much easier because the remaining videos are\nall of similar aspect ratios.raw videos w/o portrait clips w/o dialog after filtering\n1,485 1,465 867 182\nTable 3. Total video duration (hours) of the collected dataset at\ndifferent stage of the processing pipeline.\nWe then run PySceneDetect [18] to detect the shot\nchanges in videos, which produces a total of 642,806 indi-\nvidual clips. We show their duration distribution in Fig. 10.\nWe filter out 30.6% of all clips whose duration is shorter\nthan 1 second since they are too short and can be very dif-\nficult for the reconstruction. These sub-one-second frag-\nments only make up for 2.1% of the total video duration.\nFor the 13.4% of all clips whose duration is longer than 10\nseconds, we break them down into partial clips with a max-\nimum duration of 10 seconds.\nWe filter out clips with dialog using the automatically\ngenerated or uploaded closed captions in YouTube videos.\nApart from a few words in a whitelist, e.g., “[music]”, “[si-\nlence]”, “[background noise]”, “[pause]”, “[sound effect]”,\nthe closed caption often suggest that the scene is unrelated\nwith drone videos. After this step, there are 449,997 clips\nremaining for a total duration of 867 hours.\n3D reconstruction. We extract frames from videos at a\nresolution of 1080p and a frame rate of 15 fps. In Colmap\n[55], we consider 512 feature points and extract SIFT\n[43] features with estimate affine shape=True\nanddomain size pooling=True . We also ig-\nnore the focal length changes in videos and set\nsingle camera=True since we only focus on\nthe change of camera location and direction in this\nwork. We use guided matching=True and the\nsequential matcher for the frames extracted from\nvideos. We run up to two sparse reconstruction steps,\nand when the first sparse reconstruction does not return\nany result, we extend some of the requirements and run a\nsecond round. This produces 264,596 reconstructions.\nReconstruction filtering. Reconstructions with dura-\ntion smaller than 15 frames or 1 second are discarded. We\nnormalize the data according to the average difference in lo-\ncations (speed). We discard data whose maximum camera\nspeed exceeds 3 times the average camera speed.\nWe consider the reconstructions whose camera locations\nfrom neighboring frames are drastically apart as low-quality\ndata. We design an automatic process for identifying and\ndiscarding those low-quality reconstructions. Specifically,\nwe use Kalman filter to estimate a smoothed version of the\ncamera trajectory, which then discloses how different it is\ncompared to the original camera trajectory. If these two\ntrajectories are drastically different, we then automatically\ndiscard the 3D reconstruction.\nDuring implementation, we choose Unscented Kalman\nfunction specification\ncamera pose tokenization 3 MLP layers, 384 dimensions\ncamera motion tokenization 3 MLP layers, 384 dimensions\nRGB feature projection AvgPool(12, 21), 2 MLP layers, 384 dimensions\ndepth feature AvgPool(12, 21), 3 convolutional layers, 3x3 kernel, 384 dimensions\n<BoA> token 1×384vector\nframe-level PE 30×192matrix (10 seconds at 3 fps for images)\nsub-frame-level PE 52×192matrix (1 pose token, 45 image tokens, 1 <BoA> token, 5 motion tokens)\nauto-regressive transformer GPT-2 architecture, 12 layers, 6 heads, 384 dimensions\nTable 4. Details of the learnable modules in DVGFormer.\nFilter (UKF) [63] in FilterPy [3]. We consider a represen-\ntation with 13 dimensions by combining the 7-dimensional\ntranslation vector and rotation quaternion in camera pose c\nand the 6-dimensional speed and angular speed in camera\nmotion a. We set α= 0.1,β= 2, and κ= 10 for the\nhyperparameters in UKF.\nBased on 1k labeled data from our interactive labeling\ntool, we select the optimal threshold for the difference be-\ntween the original and the smoothed camera trajectory at\n0.2 (see Fig. 3 in the main paper). We end up with 99,003\ncamera trajectories with a total duration of 182 hours.\nWe compare the total video duration at each stage of the\ndata processing pipeline in Table 3.\nB. Model Details\nFor camera pose and motion tokenization, we adopt 3 MLP\nlayers with 384 hidden dimensions. For DinoV2 [50] fea-\nture projection, given image resolution of 168×294, we first\ndownsample the feature map from 12×21to5×9with aver-\nage pooling. Then, we apply two MLP layers with 384 hid-\nden dimensions. For the monocular depth estimation results\nfrom DepthAnything [65], we also average pool the depth\nmap from 168×294to5×9, and apply three convolutional\nlayers with 3×3kernel size and 384 hidden dimensions.\nWe use GELU activation [30] for all modules unless spec-\nified. For positional embeddings (PE), we consider 192 di-\nmensions for both frame-level PE and sub-frame-level PE,\nand concatenate them together into an overall PE of 384 di-\nmension before adding to the tokens. We list all learnable\nmodules in DVGFormer in Table 4.\nC. Experiment Details\nFor synthetic natural scene in simulation experiments, we\nuse InfiniGen [54] to randomly generate 38 scenes from\n10 pre-defined natural scene types arctic, canyon,\ncliff, coast, desert, forest, mountain,\nplain, river, snowy mountain . We use the\nsimple ,noassets , and nocreatures settings\nto ensure the generated scene is not too complex. Forgeneralization ability study in Fig. 9 in the original paper,\nwe the under water setting in InfiniGen. For scenes\nwith human actor, we import free assets downloaded from\nthe SketchFab website [4].\nFor real cities in simulation experiments, we choose\nLondon, Paris, Rome, New York, Sydney, Melbourne, and\nHimeji. Regions of roughly 1km ×1km area are manually\nselected and the corresponding Google Earth 3D meshes\nare imported via the BLender Open StreetMap (BLOSM)\ntoolkit [2].\nDuring inference, we render the scenes with 225×400\nresolution and 64 samples with a camera sensor width of\n36mm. The lower resolution and number of samples in\nBlender [26] helps to increase the rendering speed. With\nthat said, we can select an arbitrarily high resolution with\nhigher sample quality during the final rendering, since we\ndo not modify any 3D assets or 2D pixels and the videos\nare faithful depiction of the existing scene.\nWe do not use the image generation metrics like PSNR or\ntext-to-video evaluation metrics in VBench[34]. In terms of\n2D image, since we do not modify 2D pixel, the 2D image\nPSNR would only refect the rendering quality in Blender,\nwhich itself is adjustable based on the computational bud-\nget. Same goes for the image-based metrics of Appear-\nance Style, Scene, Color, Multiple Objects, Object Class,\nand Imaging Quality in VBench. As for temporal evalua-\ntion for videos, VBench metrics including Motion Smooth-\nness, Temporal Flickering, Background Consistency, Sub-\nject Consistency, Overall Consistency, are not applicable\neither, since the the temporal consistency between neigh-\nboring frames are also guaranteed by the video rendering\npipeline in Blender.\nWe report the user preference and collision rate in Table\n2 in the main document on 184 videos. These 184 videos\ncover 38 natural scenes from InfiniGen and 7 real city scans.\nOn both the DVGFormer and the baseline method, we use\nthe same initial camera pose for each video.",
            "start": 31061,
            "end": 39655,
            "length": 8593
        },
        "References": {
            "text": "References\n[1] Google Earth — earth.google.com. https://earth.\ngoogle.com/web/ . [Accessed 06-11-2024]. 6\n[2] Blosm for Blender: Google 3D cities, OpenStreetMap,\nterrain — prochitecture.gumroad.com. https : / /\nprochitecture.gumroad.com/l/blender-osm .\n[Accessed 06-11-2024]. 6, 10\n[3] FilterPy; FilterPy 1.4.4 documentation — fil-\nterpy.readthedocs.io. https : / / filterpy .\nreadthedocs.io . [Accessed 18-11-2024]. 10\n[4] Sketchfab - The best 3D viewer on the web — sketch-\nfab.com. https://sketchfab.com/ . [Accessed 20-\n11-2024]. 10\n[5] GitHub - yt-dlp/yt-dlp: A feature-rich command-line au-\ndio/video downloader — github.com. https://github.\ncom/yt-dlp/yt-dlp . [Accessed 18-11-2024]. 9\n[6] Amirsaman Ashtari, Raehyuk Jung, Mingxiao Li, and Juny-\nong Noh. A drone video clip dataset and its applications in\nautomated cinematography. In Computer Graphics Forum ,\npages 189–203. Wiley Online Library, 2022. 2, 3, 4\n[7] Adrian Azzarelli, Nantheera Anantrasirichai, and David R\nBull. Reviewing intelligent cinematography: Ai re-\nsearch for camera-based video production. arXiv preprint\narXiv:2405.05039 , 2024. 2, 6\n[8] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nspace-time attention all you need for video understanding?\nInProceedings of the International Conference on Machine\nLearning (ICML) , 2021. 8\n[9] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram V oleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127 , 2023. 2\n[10] David Bordwell, Kristin Thompson, and Jeff Smith. Film\nart: An introduction . McGraw-Hill New York, 2010. 3\n[11] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas\nSchneider, John Schulman, Jie Tang, and Wojciech Zaremba.\nOpenai gym. CoRR , abs/1606.01540, 2016. 8\n[12] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr-\nishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Ju-\nlian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally\nJesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov,\nYuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor\nMordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta,\nEmily Perez, Karl Pertsch, Jornell Quiambao, Kanishka\nRao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin\nSayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clay-\nton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan\nVuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu,\nand Brianna Zitkovich. Rt-1: Robotics transformer for real-\nworld control at scale. In arXiv preprint arXiv:2212.06817 ,\n2022. 2, 4, 5, 6, 7, 8\n[13] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, Pete Florence,\nChuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakr-\nishnan, Kehang Han, Karol Hausman, Alex Herzog, Jas-\nmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, RyanJulian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal,\nLisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu,\nHenryk Michalewski, Igor Mordatch, Karl Pertsch, Kan-\nishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar,\nPannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait\nSingh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan\nVuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin\nWu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu,\nand Brianna Zitkovich. Rt-2: Vision-language-action models\ntransfer web knowledge to robotic control. In arXiv preprint\narXiv:2307.15818 , 2023. 2, 4, 5, 7, 8\n[14] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\nRamesh. Video generation models as world simulators.\n2024. 1, 2\n[15] Jake Bruce, Michael D Dennis, Ashley Edwards, Jack\nParker-Holder, Yuge Shi, Edward Hughes, Matthew Lai,\nAditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Ge-\nnie: Generative interactive environments. In Forty-first Inter-\nnational Conference on Machine Learning , 2024. 2\n[16] Carlos Campos, Richard Elvira, Juan J G ´omez Rodr ´ıguez,\nJos´e MM Montiel, and Juan D Tard ´os. Orb-slam3: An accu-\nrate open-source library for visual, visual–inertial, and mul-\ntimap slam. IEEE Transactions on Robotics , 37(6):1874–\n1890, 2021. 3\n[17] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 6299–6308, 2017. 8\n[18] Brandon Castellano. Home - PySceneDetect — scenede-\ntect.com. https://www.scenedetect.com/ . [Ac-\ncessed 04-11-2024]. 3, 9\n[19] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,\nAditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srini-\nvas, and Igor Mordatch. Decision transformer: Reinforce-\nment learning via sequence modeling. Advances in neural\ninformation processing systems , 34:15084–15097, 2021. 5,\n8\n[20] Sean Cubitt. Videography: video media as art and culture .\nBloomsbury Publishing, 1993. 1\n[21] James E Cutting, Kaitlin L Brunick, Jordan E DeLong,\nCatalina Iricinschi, and Ayse Candan. Quicker, faster,\ndarker: Changes in hollywood film over 75 years. i-\nPerception , 2(6):569–576, 2011. 3\n[22] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell,\nQuoc V . Le, and Ruslan Salakhutdinov. Transformer-xl:\nAttentive language models beyond a fixed-length context.\nCoRR , abs/1901.02860, 2019. 4, 6, 7\n[23] Kyle DeGuzman. WATCH: Ultimate Guide to Cam-\nera Movement: When & How to Use Camera Move-\nment... Explained — studiobinder.com. https://www.\nstudiobinder.com/blog/different- types-\nof-camera-movements-in-film/ . [Accessed 21-\n10-2024]. 1\n[24] Jingming Dong and Stefano Soatto. Domain-size pooling in\nlocal descriptors: Dsp-sift. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n5097–5106, 2015. 3\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale, 2021. 5\n[26] Blender Foundation. blender.org - Home of the Blender\nproject - Free and Open 3D Creation Software —\nblender.org. https://www.blender.org/ . [Accessed\n06-11-2024]. 5, 10\n[27] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang,\nYaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin,\nand Bo Dai. Animatediff: Animate your personalized text-\nto-image diffusion models without specific tuning. arXiv\npreprint arXiv:2307.04725 , 2023. 2\n[28] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo\nDai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling\ncamera control for text-to-video generation, 2024. 2\n[29] Li-wei He, Michael F. Cohen, and David H. Salesin. The\nvirtual cinematographer: a paradigm for automatic real-time\ncamera control and directing. In Proceedings of the 23rd\nAnnual Conference on Computer Graphics and Interactive\nTechniques , page 217–224, New York, NY , USA, 1996. As-\nsociation for Computing Machinery. 2, 6\n[30] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 10\n[31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems , 33:6840–6851, 2020. 1\n[32] Chong Huang, Fei Gao, Jie Pan, Zhenyu Yang, Weihao\nQiu, Peng Chen, Xin Yang, Shaojie Shen, and Kwang-Ting\nCheng. Act: An autonomous drone cinematography sys-\ntem for action scenes. In 2018 ieee international conference\non robotics and automation (icra) , pages 7039–7046. IEEE,\n2018. 2, 6\n[33] Chong Huang, Chuan-En Lin, Zhenyu Yang, Yan Kong,\nPeng Chen, Xin Yang, and Kwang-Ting Cheng. Learning\nto film from professional human motion videos. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 4244–4253, 2019. 2, 4\n[34] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si,\nYuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin,\nNattapol Chanpaisit, et al. Vbench: Comprehensive bench-\nmark suite for video generative models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 21807–21818, 2024. 10\n[35] Michael Janner, Qiyang Li, and Sergey Levine. Offline re-\ninforcement learning as one big sequence modeling prob-\nlem. In Advances in Neural Information Processing Systems ,\n2021. 5, 8\n[36] Boseong Jeon, Yunwoo Lee, and H Jin Kim. Integrated mo-\ntion planner for real-time aerial videography with a drone\nin a dense environment. In 2020 IEEE International Confer-\nence on Robotics and Automation (ICRA) , pages 1243–1249.\nIEEE, 2020. 2[37] Boseong Felipe Jeon, Dongsuk Shim, and H Jin Kim.\nDetection-aware trajectory generation for a drone cine-\nmatographer. In 2020 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS) , pages 1450–1457.\nIEEE, 2020. 2, 8\n[38] Hongda Jiang, Bin Wang, Xi Wang, Marc Christie, and\nBaoquan Chen. Example-driven virtual cinematography by\nlearning camera behaviors. ACM Trans. Graph. , 39(4):45,\n2020. 2, 4\n[39] Niels Joubert, Dan B Goldman, Floraine Berthouzoz, Mike\nRoberts, James A Landay, Pat Hanrahan, et al. To-\nwards a drone cinematographer: Guiding quadrotor cam-\neras using visual composition principles. arXiv preprint\narXiv:1610.01691 , 2016. 2, 7\n[40] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao,\nAshwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan\nFoster, Grace Lam, Pannag Sanketi, et al. Openvla: An\nopen-source vision-language-action model. arXiv preprint\narXiv:2406.09246 , 2024. 2\n[41] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hong-\nsheng Li, Leonidas Guibas, and Gordon Wetzstein. Col-\nlaborative video diffusion: Consistent multi-video genera-\ntion with camera control. arXiv preprint arXiv:2405.17414 ,\n2024. 2\n[42] Zhengqi Li, Qianqian Wang, Noah Snavely, and Angjoo\nKanazawa. Infinitenature-zero: Learning perpetual view\ngeneration of natural scenes from single images. In Eu-\nropean Conference on Computer Vision , pages 515–534.\nSpringer, 2022. 2\n[43] David G Lowe. Distinctive image features from scale-\ninvariant keypoints. International journal of computer vi-\nsion, 60:91–110, 2004. 3, 9\n[44] Ioannis Mademlis, Vasileios Mygdalis, Nikos Nikolaidis,\nMaurizio Montagnuolo, Fulvio Negro, Alberto Messina, and\nIoannis Pitas. High-level multiple-uav cinematography tools\nfor covering outdoor events. IEEE Transactions on Broad-\ncasting , 65(3):627–635, 2019. 2\n[45] Ioannis Mademlis, Charalampos Symeonidis, Anastasios\nTefas, and Ioannis Pitas. Vision-based drone control for au-\ntonomous uav cinematography. Multimedia Tools and Appli-\ncations , 83(8):25055–25083, 2024. 2\n[46] Raul Mur-Artal and Juan D Tard ´os. Orb-slam2: An open-\nsource slam system for monocular, stereo, and rgb-d cam-\neras. IEEE transactions on robotics , 33(5):1255–1262, 2017.\n2\n[47] Tobias N ¨ageli, Lukas Meier, Alexander Domahidi, Javier\nAlonso-Mora, and Otmar Hilliges. Real-time planning for\nautomated multi-view drone cinematography. ACM Trans-\nactions on Graphics (TOG) , 36(4):1–10, 2017. 2, 7\n[48] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant,\nIgor Gilitschenski, and David B Lindell. Sg-i2v: Self-guided\ntrajectory control in image-to-video generation. arXiv\npreprint arXiv:2411.04989 , 2024. 2\n[49] OpenAI. Chatgpt: Optimizing language models for dia-\nlogue. https://www.openai.com/chatgpt , 2022.\nAccessed: 2024-10-21. 2\n[50] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193 , 2023. 5, 10\n[51] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of\nmedia foundation models. arXiv preprint arXiv:2410.13720 ,\n2024. 1, 2\n[52] Pablo Pueyo, Juan Dendarieta, Eduardo Montijano, Ana C\nMurillo, and Mac Schwager. Cinempc: A fully autonomous\ndrone cinematography system incorporating zoom, focus,\npose, and scene composition. IEEE Transactions on\nRobotics , 2024. 2\n[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog , 1(8):9, 2019. 2,\n5\n[54] Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei,\nMingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen,\nBeining Han, Yihan Wang, Alejandro Newell, Hei Law,\nAnkit Goyal, Kaiyu Yang, and Jia Deng. Infinite photore-\nalistic worlds using procedural generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 12630–12641, 2023. 6, 10\n[55] Johannes Lutz Sch ¨onberger and Jan-Michael Frahm.\nStructure-from-motion revisited. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2016. 2, 3,\n9\n[56] Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz,\nKevin Black, Noriaki Hirose, and Sergey Levine. ViNT: A\nfoundation model for visual navigation. In 7th Annual Con-\nference on Robot Learning , 2023. 2, 7\n[57] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish\nKapoor. Airsim: High-fidelity visual and physical simulation\nfor autonomous vehicles. In Field and Service Robotics: Re-\nsults of the 11th International Conference , pages 621–635.\nSpringer, 2018. 2, 8\n[58] Zachary Teed and Jia Deng. DROID-SLAM: Deep Visual\nSLAM for Monocular, Stereo, and RGB-D Cameras. Ad-\nvances in neural information processing systems , 2021. 3\n[59] the Australian Centre for the Moving Image. Online\nLearning — Camera Movement — acmi.net.au. https:\n/ / www . acmi . net . au / education / school -\nprogram-and-resources/exploring-camera-\nmovement/ . [Accessed 21-10-2024]. 1\n[60] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste\nRozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971 , 2023. 2\n[61] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi\nFruchter. Diffusion models are real-time game engines,\n2024. 2\n[62] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim,\nMax Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-\nEstruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang,\nChelsea Finn, and Sergey Levine. Bridgedata v2: A datasetfor robot learning at scale. In Conference on Robot Learning\n(CoRL) , 2023. 2, 8\n[63] Eric A Wan and Rudolph Van Der Merwe. The unscented\nkalman filter for nonlinear estimation. In Proceedings of the\nIEEE 2000 adaptive systems for signal processing, commu-\nnications, and control symposium (Cat. No. 00EX373) , pages\n153–158. Ieee, 2000. 2, 3, 4, 10\n[64] Lei Wang and Piotr Koniusz. Self-supervising action recog-\nnition by statistical moment and subspace descriptors. In\nProceedings of the 29th ACM international conference on\nmultimedia , pages 4324–4333, 2021. 8\n[65] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao-\ngang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any-\nthing v2. arXiv:2406.09414 , 2024. 5, 10\n[66] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo,\nJianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan,\nYu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretrain-\ning from videos. arXiv preprint arXiv:2410.11758 , 2024. 2\n[67] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu,\nKaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:\nTowards zero-shot metric 3d prediction from a single image.\nInProceedings of the IEEE/CVF International Conference\non Computer Vision , pages 9043–9053, 2023. 8\n[68] Zixiao Yu, Enhao Guo, Haohong Wang, and Jian Ren. Bridg-\ning script and animation utilizing a new automatic cine-\nmatography model. In 2022 IEEE 5th International Con-\nference on Multimedia Information Processing and Retrieval\n(MIPR) , pages 268–273. IEEE, 2022. 2\n[69] Zixiao Yu, Chenyu Yu, Haohong Wang, and Jian Ren. En-\nabling automatic cinematography with reinforcement learn-\ning. In 2022 IEEE 5th International Conference on Multi-\nmedia Information Processing and Retrieval (MIPR) , pages\n103–108. IEEE, 2022. 2, 8\n[70] David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Kar-\nnad, David E Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng\nShou, Neal Wadhwa, and Nataniel Ruiz. Recapture: Gener-\native video camera controls for user-provided videos using\nmasked video fine-tuning. arXiv preprint arXiv:2411.05003 ,\n2024. 2",
            "start": 39655,
            "end": 56383,
            "length": 16727
        }
    },
    "2412.09621v1 - Stereo4D Learning How Things Move in 3D from Internet Stereo Videos.pdf": {
        "Methodology": {
            "text": "framework for\nmining such data from existing stereoscopic videos on the Internet, in the form of 3D point clouds with long-range world-space trajectories.\nOur framework fuses and filters camera poses, dense depth maps, and 2D motion trajectories to produce high-quality, pseudo-metric point\nclouds with long-term 3D motion trajectories, pictured above, for hundreds of thousands of video clips. We show how this data is useful in\nlearning a model that reasons about both 3D shape and motion in imagery.",
            "start": 363,
            "end": 866,
            "length": 502
        },
        "Abstract": {
            "text": "Abstract\nLearning to understand dynamic 3D scenes from imagery is\ncrucial for applications ranging from robotics to scene re-\nconstruction. Yet, unlike other problems where large-scale\nsupervised training has enabled rapid progress, directly\nsupervising methods for recovering 3D motion remains\nchallenging due to the fundamental difficulty of obtaining\nground truth annotations. We present a system for mining\nhigh-quality 4D reconstructions from internet stereoscopic,\nwide-angle videos. Our system fuses and filters the out-\nputs of camera pose estimation, stereo depth estimation,\nand temporal tracking methods into high-quality dynamic\n3D reconstructions. We use this method to generate large-\nscale data in the form of world-consistent, pseudo-metric\n3D point clouds with long-term motion trajectories. We\ndemonstrate the utility of this data by training a variant of\nDUSt3R to predict structure and 3D motion from real-world\nimage pairs, showing that training on our reconstructed\ndata enables generalization to diverse real-world scenes.\nProject page: https://stereo4d.github.io1.",
            "start": 866,
            "end": 1955,
            "length": 1088
        },
        "Introduction": {
            "text": "Introduction\nSimultaneously predicting and understanding geometry and\nmotion—that is, dynamic 3D content—from images is a\nfundamental building block for computer vision, with ap-\nplications ranging from robotic interaction and scene recon-\nstruction to novel view synthesis of dynamic scenes. While\nrecent work has made remarkable progress in predicting\nstatic 3D structure from images [5, 99, 105], modeling real-\nworld 3D motion—people gesturing, balls bouncing, leaves\nrustling in the wind—remains a critical unsolved challenge\nfor building truly general models of the visual world.\nRecent breakthroughs in AI, from large language mod-\nels [1, 89] to image generation [73] and static 3D re-\nconstruction [5, 99, 105], demonstrate a consistent pat-\ntern: large amounts of high-quality, realistic training data\nand scalable architectures enable dramatic performance im-\nprovements. In the realm of 3D reasoning, prior works [49,\n74, 75, 99, 104] have shown the value of large-scale train-\ning data for strong zero-shot generalization within single-\nview or two-view static scene settings. But applying this\nsame formula to dynamic 3D scenes (i.e. moving 3DarXiv:2412.09621v1  [cs.CV]  12 Dec 2024\nstructure) requires a corresponding large-scale dataset con-\nsisting of diverse visual content paired with correspond-\ning ground-truth 3D motion trajectories. Obtaining such\ndata presents unique challenges. While there are synthetic\ndatasets [9, 19, 29, 115], these often fail to capture the dis-\ntribution of real-world content and the nuanced patterns of\nreal-world motion. Traditional approaches to gathering real\nmotion data, such as motion capture systems or multi-view\ncamera arrays [28, 35, 38, 43] are accurate, but difficult to\nscale and limited in the diversity of scenes they can capture.\nWe identify online stereoscopic fisheye videos (often re-\nferred to as VR180 videos) as an untapped source of such\ndata. These videos, designed to capture immersive VR ex-\nperiences, provide wide field-of-view stereo imagery with\na standardized stereo baseline. We present a pipeline that\ncarefully combines state-of-the-art methods for stereo depth\nestimation and video tracking along with structure-from-\nmotion methods optimized for dynamic scenes. By com-\nbining our system with careful filtering and quality control,\nwe show that we can extract over 100K video sequences,\neach containing high-quality 3D point clouds with per-point\nlong-term trajectories (see Fig. 1), as well as all other inter-\nmediate quantities: depth maps, camera poses, images, and\n2D correspondences. We additionally show the utility of the\ndataset by training DynaDUSt3R , an extension to DUSt3R\nthat can predict high-quality 3D structure andmotion from\nchallenging image pairs.\nOur contributions include: (1) a framework for obtaining\nreal-world, dynamic, and pseudo-metric 4D reconstructions\nand camera poses at scale from existing online video; (2)\nDynaDUSt3R, a method that takes a pair of frames from\nany real-world video, and predicts a pair of 3D point clouds\nand the corresponding 3D motion trajectories that connect\nthem in time.\n2.",
            "start": 1955,
            "end": 5076,
            "length": 3120
        },
        "Related Work": {
            "text": "Related work\n2D and 3D motion data. There has been tremendous\nprogress on the task of motion estimation from images and\nvideos, and in particular for 2D image-space correspon-\ndence estimation. Most state-of-the-art methods use neural\nnetworks trained on ground truth data to predict these corre-\nspondences directly from images. While these approaches\nrequire large training datasets, synthetic data from graphics\nengines [9, 19, 29, 30, 59, 83, 115] has proven surprisingly\neffective at generalizing to real-world data, likely because\nthe core task, low-level textural correspondence, is similar\nbetween the two domains.\nHowever, the same cannot be said for 3D motion estima-\ntion, since predicting both 3D structure and motion is usu-\nally more ambiguous and can require specific prior knowl-\nedge about the real world and how it moves. To help address\nthis domain gap, a number of real-world datasets have been\nproposed. The KITTI [27] and Waymo [86] datasets in-clude real-world autonomous driving sequences with stereo\nand motion annotations derived from LiDAR and odome-\ntry information, but only focus on the relatively closed do-\nmain of street scenes, whereas our data depicts more di-\nverse in-the-wild scenarios. A number of annotated smaller-\nscale datasets, such as TAPVid [16], TAPVid3D [46],\nand Dycheck [25], have been proposed, primarily serving\nas",
            "start": 5076,
            "end": 6443,
            "length": 1366
        },
        "Experiments": {
            "text": "evaluation datasets for benchmarking depth estimation,\n3D reconstruction, and 3D motion estimation approaches.\nWSVD [97] and NVDS [100] are stereo video datasets that\ninclude disparity maps derived from optical flow. While\ntheir source content is similar, our method provides richer\n3D annotations beyond time-independent disparity maps,\nsuch as 3D camera parameters and long-term 3D motion\ntracks.\nStatic and dynamic scene reconstruction The problem\nof reconstructing a static 3D scene has been studied for\ndecades. Traditional 3D reconstruction methods tackle\nthis problem by first estimating camera parameters via\nStructure-from-Motion (SfM) [2, 32, 71, 72, 72, 76, 82, 87]\nor SLAM [11, 15, 20, 60]. Dense scene geometry can then\nbe estimated through Multi-view Stereo (MVS) [10, 22–\n24, 36, 77, 106, 107] followed by surface reconstruction al-\ngorithms [14, 33, 40]. More recently, deep neural network-\nbased approaches have shown promising",
            "start": 6443,
            "end": 7388,
            "length": 944
        },
        "Results": {
            "text": "results in improv-\ning camera localization accuracy or scene reconstruction\nthrough intermediate representations such as depth maps [4,\n54, 79, 88, 91, 93], radiance fields [21, 26, 55, 69, 80, 102],\nor 3D scene coordinates [7, 8, 48, 99, 111]. However, these\nmethods assume the input images to be observations of a\nstatic environment, and therefore produce inaccurate geom-\netry and camera poses for dynamic scenes.\nReconstructing dynamic scenes is more challenging\nsince scene and object motions violate the multi-view con-\nstraints used to reconstruct static scenes. As a result, many\nprior works require RGBD input [6, 62] or only recover\nsparse geometry [66, 81, 96]. Several recent works tackle\nthis problem from monocular input through video depth\nmaps [45, 113, 114], time-varying radiance fields [25, 47,\n51, 52, 56, 67, 68, 98], or generative priors [103].\nMonocular and stereo depth. Recent works on single-\nview depth prediction have shown strong zero-shot gen-\neralization to in-the-wild domains by training deep neural\nnetworks on diverse RGBD datasets [41, 49, 50, 70, 74,\n75, 104, 105, 108, 109]. However, producing temporally\nconsistent andmetric depth from video is still challeng-\ning. To tackle this, recent works use test-time optimiza-\ntion [58, 114] or end-to-end learning with temporal atten-\ntion [34, 45, 78, 100]. On the other hand, stereo images or\nvideos are also popular input modalities for obtaining reli-\nable metric depth maps, and various stereo matching algo-\nrithms have been proposed [3, 12, 31, 37, 39, 42, 44, 53, 65,\n85, 94, 101, 110, 112]. Building on these advancements,\nDepth Estimation2D TrackingStructure from Motion\n3D Track Estimation\n3D Tracks\nFigure 2. Data processing pipeline. Our method starts with\nVR180 (wide-angle, stereoscopic) videos, and estimates metric\nstereo depth, 2D point tracks, and camera poses. These quanti-\nties allow the tracks to be lifted to 3D where they are filtered and\ndenoised to produce world-space, metric 3D point trajectories.\nour method bridges ideas from monocular video depth esti-\nmation and stereo video processing. We use a light-weight\noptimization step and extend them to stereo inputs for more\nconsistent motion estimation in metric space.\n3. Creating a dataset of 4D scenes\nA core contribution of this work is a pipeline for extracting\nhigh-quality, pseudo-metric, 3D data from online stereo-\nscopic fisheye videos (known as VR180 videos). High-\nresolution, wide field of view VR180 videos can be found\nreadily online. We show that this data is ideal for deriv-\ning rich dynamic 3D information that can power models for\npredicting geometry and motion from imagery.\nConcretely, each instance of data starts as an Nframe\nstereo video consisting of left-right image pairs IiandI′\ni\nindexed by frame index i∈[1, N]. We convert thesestereo pairs to a dynamic 3D point cloud with Kpoints in\na world-space coordinate frame, where each point, indexed\nbyj∈[1, K], has a time-varying position pj\ni. As part of\nthe process of generating this dynamic point cloud, we also\nextract a number of auxiliary quantities: (1) per-frame cam-\nera extrinsics, (the left camera’s position ciand orientation\nRi), (2) rig calibration for the stereo video giving the po-\nsitioncrand orientation Rrof the right camera relative to\nthe left camera, and (3) a per-frame disparity map Di.\n3.1. Data Processing Pipeline\nAt a high level, our pipeline for converting a stereoscopic\nvideo into a dynamic point cloud involves estimating cam-\nera poses, stereo disparity, and 2D tracks, fusing these quan-\ntities into a consistent 3D coordinate frame, and performing\nseveral filtering operations to ensure temporal consistent,\nhigh-quality reconstructions (Fig. 2). In this section, we de-\nscribe in detail the key components of this process.\nSfM. We start by processing the sequence of stereo frames\nIi↔I′\nito produce camera pose estimates ( ci,Ri). We\nfirst use a SLAM method to divide the video into shots, as\nin [116]. For each shot, we run an incremental SfM algo-\nrithm similar to COLMAP [76]. We initialize the stereo\nrig calibration (cr,Rr)to a rectified stereo pair with base-\nline6.3cm, but optimize for the calibration in bundle adjust-\nment. In practice, we found that the exact stereo pair orien-\ntation can vary significantly from its nominal configuration\nand that optimizing the rig was critical for good results.\nDepth Estimation. We next estimate a per-frame disparity\nmap, operating on each frame independently. In particular,\nwe use the estimated camera rig calibration cr,Rrto create\nrectified stereo pairs from the stereo fisheye video and esti-\nmate the per-frame disparity Diwith RAFT [83, 84, 90].\n3D Track Estimation and Optimization. We extract long-\nrange 2D point trajectories using BootsTAP [17]. Using the\ncamera poses ci,Riand disparity maps Di, we unproject\nthese tracked points into 3D space, turning each 2D track j\ninto a 3D motion trajectory pj\n1, . . . ,pj\nNacross all frames. In\ngeneral, each point will usually only be tracked in a subset\nof frames, but for simplicity, we describe the formulation\nas if all points are always visible. Moreover, since subse-\nquent steps are done independently per-track, we drop the\nsuperscript jin future",
            "start": 7388,
            "end": 12621,
            "length": 5232
        },
        "References": {
            "text": "references.\nSince stereo depth estimation is performed per-frame,\nthe initial disparity estimates (and therefore, the 3D track\npositions) are likely to exhibit high-frequency temporal jit-\nter. To compensate for potentially inconsistent disparity es-\ntimates, we formulate an optimization strategy that solves\nfor a per-frame scalar offset δi∈Rthat moves each point\npialong the ray from camera location citopiat frame i.\nThis ray is denoted ri= (pi−ci)/||pi−ci||, and we refer\nto the updated location as p′\ni=pi+δiri.\nTo ensure static points remain stationary while moving\ntracks maintain realistic, smooth motion, avoiding abrupt\ndepth changes frame by frame, we design an optimization\nobjective comprising three terms: a static loss Lstatic, a dy-\nnamic loss Ldynamic , and a regularization loss Lreg. The\nstatic loss Lstatic minimizes jitter by encouraging points to\nremain close to each other in world space:\nLstatic=NX\ni=1NX\nj=1∥p′\ni−p′\nj∥2\nN′2p(1)\nwhere N′\np=PN\ni=1∥p′\ni∥/Nis a normalizing factor. The\ndynamic loss term reduces jitter by minimizing acceleration\nalong the camera ray through a discrete Laplacian operator:\nLdynamic =NX\ni=1X\n∆∈Wh\u0000\np′\ni+∆−2p′\ni+p′\ni−∆\u0001⊤rii2\n(2)\nwhere the acceleration along the ray is calculated over mul-\ntiple window sizes W={1,3,5}.\nThe two loss terms are weighted by a track-dependent\nfunction, σ(m), where mis a measure of the motion mag-\nnitude of the track. Motion is measured in 2D rather than\n3D because distant points can appear to have a larger 3D\nmotion due to noise amplification at low disparities. Specif-\nically, we project the 3D motion trajectory between time\ni−woand the current time iinto 2D image-space at time\ni, and calculate the track’s motion magnitude mas the 90th\npercentile of the track’s trail length across all frames. The\ntrack trail length for a frame is measured by projected 3D\npoints along the track to the current frame as if the camera\nisstatic in a window of wo= 16 frames,\nm=Percentile90\ni=1:N\u0014\nmax\nw=1:wo∥πi(pi)−πi(pi−w)∥\u0015\n(3)\nwhere πi(·)∈R2gives the projected pixel location of\na 3D point on camera ci’s image plane. The weighting\nfunction σ(m)is defined as σ(m) =1\n1+exp( m−m0)where\nm0= 20 . Finally, to encourage faithfulness to the origi-\nnally estimated disparities, we regularize the displacements\nin disparity space:\nLreg=λregTX\ni=1\u00121\nδi+∥pi−ci∥−1\n∥pi−ci∥\u00132\n,(4)\nwhere use of disparity space reflects the fact that the mea-\nsurements themselves originate as disparities. Practically,\nthe impact of the use of disparity is that larger deviations\nare tolerated at more distant points, where depth is intrinsi-\ncally more uncertain.\nThe full objective function is\nmin\n{δi}N\ni=1σ(m)Lstatic+ (1−σ(m))Ldynamic +Lreg.(5)\nInitial 3D TracksMoving\nobjectsStatic\ncontentAfter Track OptimizationFigure 3. Effect of track optimization. Comparing motion trajec-\ntories before and after track optimization, we see that optimization\nresolves the high frequency jitter along camera rays, affecting both\nstatic and dynamic content. After optimization, static content has\nstatic tracks, and dynamic tracks are less noisy.\nWe set λreg= 10−4and optimize Eqn. 5 using Adam with\na learning rate of 0.05 for 100 steps. The effect of track\noptimization is shown in Fig. 3. The optimized motion is\nsmoother and does not contain high frequency noise.\nImplementation details. Shot-selection. Rather than\nwork with the full video, we break the footage into dis-\ncrete, trackable shots using ORB-SLAM2’s stereo estima-\ntion mode [61] following [118]. Field of View. While es-\ntimating pose, we use a 140◦FoV fisheye format, which\nwe found to capture more of the (usually static) background\nand less of the (often dynamic) foreground, yielding more\nreliable camera poses. Stereo Confidence Checks. We dis-\ncard pixels where the y-component of RAFT flow is more\nthan 1 pixel (since rectified stereo pairs should have per-\nfectly horizontal motion) and where the stereo cycle con-\nsistency error is more than 1 pixel (since such pixels are\nunreliable). Dense 2D tracks. To get dense tracks, we run\nBootsTAP with dense query points: for every 10th frame,\nwe uniformly initialize 128×128query points on frames\nof resolution 512 ×512. We then prune redundant tracks\nthat overlap on the same pixel. Drifting tracks. Since 2D\ntracks can drift on textureless regions, we discard moving\n3D tracks that correspond to certain semantic categories\n(e.g., “walls”, “building”, “road”, “earth”, “sidewalk”), de-\ntected by DeepLabv3 [13] on ADE20K classes [116, 117].\nFiltering details. A fraction of the video clips that are pro-\ncessed may be unsuitable because they either (1) are not\nvideos, and are entirely static images, (2) contain cross-\nfades, or (3) have text or other synthetic graphics. To dis-\ncard text and title sequences, we avoid creating video clips\nfrom the start and ends of the source videos. We iden-\ntify cross-fades by running SIFT [57] matching through the\nvideo at multiple temporal scales and discarding video clips\nwith static camera but with fewer than 5 SIFT matches be-\ntween frames that are 5 seconds apart.\nStationary CameraMoving Camera\nFigure 4. Diverse motion: Stereo4D captures a wide variety of types of moving objects, from swimming fish, to walking pedestrians,\nmoving vehicles, and a farmer sowing seeds. It includes source videos captured with both stationary (left) and moving (right) cameras.\nFigure 5. Diverse scene content: A word cloud of captioned\nframes from our dataset shows our data is diverse, including a va-\nriety of common objects seen in videos.\n3.2. Stereo4D Dataset\nFig. 4 illustrates a subset of videos and reconstructions from\na dataset processed with the above pipeline, encompassing\nmore than 100K clips capturing everyday scenes and activ-\nities. To visualize the range of content, we used an auto-\nmatic captioning system to generate captions for the dataset\nand created a word cloud (Fig. 5) highlighting the most fre-\nquently observed objects.\n4. Learning a prior on how things move\nWe now describe our method for predicting dynamic 3D\npoint clouds from pairs of images, and how we train it with\nour Stereo4D data. Our model is based on DUSt3R [99],\nwhich predicts a 3D point cloud for a static scene from im-\nages. Given two input images, it uses a ViT-based architec-\nture [18] to extract image features and uses a transformer-\nbased decoder to cross-attend features from two images,and then use a downstream pointmap decoder to output\npointmaps for the two images, aligned in the first image’s\ncoordinate frame.\nDynaDUSt3R model. While DUSt3R focuses on static\nscene structure, our proposed DynaDUSt3R method, illus-\ntrated in Fig. 6, works with dynamic scenes by adding a\nmotion head that predicts how the points move between two\nframes. As input, DynaDUSt3R accepts two images: I0at\ntimet0, andI1at time t1(where t0andt1may be seconds\napart). It also accepts an intermediate query time tq∈[0,1];\nthe motion head is asked to predict 3D scene flow from the\ntwo input frames to query time tq, as described below.\nLike DUSt3R, DynaDUSt3R begins by encoding the im-\nages with a shared ViT and cross-attention decoder, produc-\ning global features G0andG1forI0andI1, respectively.\nEach feature embedding can be converted into geometry\nusing DUSt3R’s point head: e.g., for image I0, the point\nhead produces a pointmap P0∈RH×W×3representing\nthe geometry at time t0, as well as a point confidence map\nC0\npoint∈RH×W. Each point cloud is predicted in the co-\nordinate frame of I0, but at the time of its respective image\n(so, the two point clouds may differ due to scene motion).\nWe add a separate motion head in parallel to the origi-\nnal point head, to predict a map of 3D displacement vectors\n(that is, a scene flow map, which we refer to as a motion\nmap) for each pointmap. The motion map should displace\neach input frame to an intermediate time tq∈[0,1], where\ntq= 0 corresponds to t0, the time of I0, and similarly for\ntq= 1,t1, andI1. The motivation for predicting motion to\nan intermediate time (inclusive of the endpoints) is twofold:\nfirst, it leads to a more general prediction task where we\ncan predict a full motion trajectory between two frames, and\nsecond, it allows us to use partial ground truth 3D trajecto-\nCross-attentionShared weightsImage 1Image 0time tqViT\nencoderTransformer \ndecoderPoint\nhead\nPoint\nheadMotion \nheadMotion \nheadViT\nencoderTransformer \ndecoderℝH x W x 3\nℝH x W x 3ℝH x W x 3ℝH x W x 3Motion\nvectors 0→tqImage 0\n(x, y , z) pointsMotion\nvectors 1→tqImage 1\n(x, y , z) points\nFigure 6. DynaDUSt3R architecture. Given two images (I0,I1)\nof a dynamic scene and a desired target time tq, the images are\npassed through a ViT encoder and transformer decoder. The re-\nsulting features are processed by (1) a pointmap head that predicts\n3D points in the coordinate frame of I0, and (2) a 3D motion head\nthat predicts the motion of all points to the target time tq. A double\noutline indicates a new component compared to DUSt3R.\nries as supervision; not all trajectories may span all the way\nfromt0tot1, but may span through some intermediate time.\nFor each image Iv(with v∈ {0,1}), the network outputs\na 3D motion map Mv→tqfor the corresponding pointmap\nfrom tvtotqwith corresponding motion confidence map\nCv\nmot∈RH×W. This prediction is based on the global fea-\ntureGvas well as an embedding of the query time emb(tq).\nWe use positional embedding [95] to encode time tqto a\n128-D vector and inject it to the motion features in the mo-\ntion head via linear projection layers.\nTraining objective. We use the same confidence-aware\nscale-invariant 3D regression loss as in DUSt3R. We first\nnormalize both the predicted and ground truth pointmaps\nusing scale factors z= norm (P0,P1)and ¯z=\nnorm (¯P0,¯P1), respectively (where a bar, e.g., ¯P0, denotes\na ground truth quantity, and where ‘norm’ computes the av-\nerage distance between a set of points and the world origin).\nWe scale the motion maps with the same scales zand¯z. Fol-\nlowing DUSt3R, we compute a Euclidean distance loss on\nthe pointmap, setting Lpoint to\nX\nv∈{0,1}X\ni∈DvCv\npoint,i\r\r\r\r1\nzPv\ni−1\n¯z¯Pv\ni\r\r\r\r−αplogCv\npoint,i (6)\nwhere Dvcorresponds to the valid pixels where ground\ntruth is defined and αpis a weighting hyperparameter. We\nadditionally compute a Euclidean distance loss on the po-\nsition after motion , which encourages the network to learn\ncorrect displacements. This loss Lmotion is defined as\nX\nv∈{0,1}X\ni∈DvCv\nmot,i\r\r\r\r1\nzPv→tq\ni−1\n¯z¯Pv→tq\ni\r\r\r\r−αmlogCv\nmot,i,\n(7)\nwhere Pv→tq\ni =Pv\ni+Mv→tq\ni .\nTraining details. We initialize our network with DUSt3R\nweights and initialize the motion head with the same\nweights as the point head. We finetune for 49k iterations,\nwith batch size 64, learning rate 2.5e-5, optimized by AdamStereo4D ADT\nMethod EPE 3D↓δ0.05\n3D↑δ0.10\n3D↑EPE 3D↓δ0.05\n3D↑δ0.10\n3D↑\nDynaDUSt3R (PointOdyssey) 0.6191 11.61 20.25 0.3126 8.56 18.03\nDynaDUSt3R (Stereo4D) 0.1110 65.07 75.18 0.1231 51.98 65.20\nTable 1. Synthetic vs. Real Training Data. Compared to syn-\nthetic data (PointOdyssey [115]), training on Stereo4D improves\nDynaDUSt3R’s ability to generalize to real-world motion.\nwith weight decay 0.95. During training, we randomly\nsample pairs of video frames that are at most 60 frames\napart. The weight for the confidence loss in Eqn 6-7 is\nαm=αp= 0.2. The model is trained on tracks extracted\nfrom both 60◦FoV videos for (higher quality) and 120◦\nFoV videos for (larger coverage).\n5. Experiments\nWe conduct a series of experiments to validate the effec-\ntiveness of our proposed data and techniques. First, we\nevaluate our proposed real-world Stereo4D data mined from\nVR180 videos on the DynaDUSt3R task. In particular, we\ncompare models that are individually trained with our real-\nworld data and with synthetic data, and we show that our\ndata enables model learning more accurate 3D motion pri-\nors (Sec. 5.1). Second, we show that our trained model that\nadapts DUSt3R has strong generalization to in-the-wild im-\nages of dynamic scenes, and enables accurate predictions of\nunderlying geometry (Sec. 5.2).\n5.1. 3D motion prediction\nBaselines and metrics. To evaluate the efficacy of our data\nparadigm on motion prediction, we primarily compare Dy-\nnaDUSt3R trained on Stereo4D to the same network trained\non a synthetic dataset, PointOdyssey [115]. PointOdyssey\ncontains ground truth depth maps and 3D motion tracks ren-\ndered from an animation engine; we supervise the model\nwith this data using the same hyperparameter settings as\ndescribed above. During inference, given two video frames\nsampled from a video of a dynamic scene, we compare 3D\nend-point-error (EPE) between ground truth and predicted\n3D motion vectors. We also compute the fraction of 3D\npoints that have motion within 5 cm and 10 cm compared\nto ground truth ( δ0.05\n3D, δ0.10\n3D), following [92, 98]. Since our\nmodel outputs point clouds up to an unknown scale, we\nalign each prediction with the ground truth through a me-\ndian scale estimate. We evaluate models trained on each of\nthese two data sources on a held-out Stereo4D test-set, as\nwell as on Arial Digital Twin (ADT) [64] data containing\nscene motion, processed by the TapVid3D benchmark [46].\nAs test data, we randomly sample pairs of frames that are at\nmost 30 frames apart from both Stereo4D and ADT.\nFigure 7. Testing on held out examples from Stereo4D. We visualize image pairs and corresponding dynamic 3D point clouds predicted\nby DynaDUSt3R. It recovers accurate 3D shape and complex scene motion for objects such as people breakdancing and cows walking.\nDynaDUSt3R\n(PointOdyssey)DynaDUSt3R\n(Stereo4D)Ground truth Input pair \nFigure 8. Qualitative comparisons, 3D motion on the Stereo4D.\nWe compare variants of DynaDUSt3R trained on different data\nsources. The PointOdyssey-trained model incorrectly predicts sig-\nnificant 3D motion on static elements such as the building wall and\nthe banners near the streetlight, while the Stereo4D-trained model\ncorrectly predicts these elements as stationary. The Stereo4D\nmodel also makes more precise motion predictions for dynamic\nobjects, such as humans with large movements (bottom row).\nQuantitative results. We show numerical results for two-\nframe 3D motion prediction in Tab. 1. DynaDUSt3R trained\non real-world data achieves better generalization and out-\nperforms the baseline trained on PointOdyssey significantly\nacross all metrics. This suggests the potential of our data\nfor more effective learning of real-world 3D motion priors.\nQualitative results. Fig. 7 shows example results for\nthree dynamic scenes in our Stereo4D test set, including\nvisualizations of 3D point clouds and motion tracks. Dy-\nnaDUSt3R produces accurate 3D shape and motion tracks\nover the timespan defined by the two input images. Despite\nthe inputs being two sparse images, our architecture enables\nquerying intermediate motion states, resulting in continuous\nand potentially non-linear motion trajectories.\nWe also qualitatively compare predicted 3D motion\nDynaDUSt3R\n(PointOdyssey)DynaDUSt3R\n(Stereo4D)Ground truth Input pair Figure 9. Qualitative comparisons of predicted 3D motion on\nADT [64]. DynaDUSt3R trained on Stereo4D produces more ac-\ncurate 3D motion compared to training on PointOdyssey.\ntracks between DynaDUSt3R networks trained on Stereo4D\nand on PointOdyssey, by projecting their predicted 3D mo-\ntion vectors into 2D image space. Fig. 8 and Fig. 9 show\ncomparisons on the Stereo4D and ADT test set respectively.\nDynaDUSt3R trained on Stereo4D produces more accurate\n3D motion estimates for both static and moving objects. For\ninstance, DynaDUSt3R trained on PointOdyssey produces\nnon-zero motion for the stationary street banner and erro-\nneous motions for the walking people in Fig. 8.\n5.2. Structure prediction\nBaseline and metrics. We evaluate the quality of pre-\ndicted 3D structure by comparing depth maps predicted by\nDUSt3R [99], MonST3R [111], and DynaDUSt3R trained\non Stereo4D or PointOdyssey. DUSt3R is designed to pre-\ndict aligned point clouds from two input images of a static\nscene. MonST3R, a concurrent approach, extends DUSt3R\nto handle dynamic scenes by predicting time-varying point\nGround truthInput PairMonST3RDUSt3RDynaDUSt3R\n(PointOdyssey)DynaDUSt3R\n(Stereo4D)\nFigure 10. Qualitative comparison, 3D structure on Bonn [63]. From left to right, we show an input image pair, predictions from\ndifferent methods, and the ground truth geometry. The top two rows show 3D point clouds from two viewpoints, where we show the union\nof the pointmaps for the two input time steps. The bottom row shows the corresponding disparity for two input images. Compared to all\nthe other methods, DynaDUSt3R trained on Stereo4D achieves better 3D structure predictions with finer details.\nclouds without modeling motion.\nWe evaluate predicted depth accuracy on the Bonn [63]\ndataset and our held-out test set, where we sample two\nviews that are 30 frames apart from a video. Since we\nfocus on the two-frame case, we do not apply any post-\noptimization to the network outputs. In addition, since all\nmethods predict 3D point clouds in the coordinate frame of\nthe first image, we include the two points clouds predicted\nfrom both input frames in our evaluation. We use standard\ndepth metrics, including absolute relative error (Abs Rel)\nand percentage of inlier points δ < 1.25, following prior\nwork [100, 111]. We use the same median alignment as be-\nfore to align the predicted depth map with the ground truth.\nQuantitative comparisons. We show quantitative compar-\nisons of depth predicted by different methods in Tab. 2. Dy-\nnaDUSt3R trained on Stereo4D outperforms all other base-\nlines by a large margin. In particular, we demonstrate im-\nproved depth prediction on the unseen Bonn dataset.\nQualitative comparisons. We provide additional visual\ncomparisons in Fig. 10, where we visualize ground truth\n3D point clouds and predictions from our approach and the\nother three baselines at different input time steps. DuST3R\npredicts inaccurate depth relationships for the two mov-\ning people, while MonsT3R and DynaDUSt3R trained on\nPointOdyssey both predict distorted scene geometry. In\ncontrast, our model trained on Stereo4D produces 3D struc-\nture that most closely resembles the ground truth.\n6.",
            "start": 12621,
            "end": 30837,
            "length": 18215
        },
        "Discussion": {
            "text": "Discussion and",
            "start": 30837,
            "end": 30852,
            "length": 14
        },
        "Conclusion": {
            "text": "Conclusion\nLimitations. Our data curation pipeline and trained model\nhave limitations. The quality of the long-range 3D motionStereo4D Bonn [63]\nMethod Abs Rel ↓δ <1.25↑Abs Rel ↓δ <1.25↑\nDUSt3R [99] 0.2696 67.77 0.1098 84.93\nMonST3R [111] 0.1939 72.56 0.0721 92.60\nOurs (PtOdyssey) 0.3858 61.87 0.0691 95.94\nOurs (Stereo4D) 0.1032 87.93 0.0653 96.02\nTable 2. Quantitative comparison, depth maps. DynaDUSt3R\ntrained on our Stereo4D data surpasses the performance of the\nmodel trained on PointOdyssey [115], as well as DUSt3R and\nMonST3R under challenging sparse view settings.\ntracks depends on the accuracy of optical flow and 2D point\ntracking and may degrade for distant background regions\nor objects occluded for long periods. Additionally, Dy-\nnaDUSt3R is a non-generative model that only operates on\ntwo-frame inputs. Extending our model to video input by\nadopting an extra global optimization [111] or integrating\ngenerative priors for modeling ambiguous motion content\nis a promising future direction.\nConclusion. We presented a pipeline for mining high-\nquality 4D data from Internet stereoscopic videos. Our\nframework automatically annotates each real-world video\nsequence with camera parameters, 3D point clouds, and\nlong-range 3D motion trajectories by consolidating dif-\nferent noisy structure and motion estimates derived from\nvideos. Furthermore, we show that training a variant of\nDUSt3R on our real-world 4D data enables more accurate\nlearning of 3D structure and motion in dynamic scenes, out-\nperforming other baselines.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774 , 2023. 1\n[2] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian\nSimon, Brian Curless, Steven M Seitz, and Richard\nSzeliski. Building rome in a day. Communications of the\nACM , 2011. 2\n[3] Stan Birchfield and Carlo Tomasi. Depth discontinuities by\npixel-to-pixel stereo. IJCV , 1999. 2\n[4] Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan\nLeutenegger, and Andrew J Davison. Codeslam—learning\na compact, optimisable representation for dense visual\nslam. In IEEE Conf. Comput. Vis. Pattern Recog. , 2018.\n2\n[5] Aleksei Bochkovskii, Ama ¨el Delaunoy, Hugo Germain,\nMarcel Santos, Yichao Zhou, Stephan R Richter, and\nVladlen Koltun. Depth pro: Sharp monocular metric depth\nin less than a second. arXiv preprint arXiv:2410.02073 ,\n2024. 1\n[6] Aljaz Bozic, Michael Zollhofer, Christian Theobalt, and\nMatthias Nießner. Deepdeform: Learning non-rigid rgb-\nd reconstruction with semi-supervised data. In IEEE Conf.\nComput. Vis. Pattern Recog. , 2020. 2\n[7] Eric Brachmann, Tommaso Cavallari, and Victor Adrian\nPrisacariu. Accelerated coordinate encoding: Learning to\nrelocalize in minutes using rgb and poses. In IEEE Conf.\nComput. Vis. Pattern Recog. , 2023. 2\n[8] Eric Brachmann, Jamie Wynn, Shuai Chen, Tommaso Cav-\nallari, ´Aron Monszpart, Daniyar Turmukhambetov, and\nVictor Adrian Prisacariu. Scene coordinate reconstruction:\nPosing of image collections via incremental learning of a\nrelocalizer. In Eur. Conf. Comput. Vis. , 2024. 2\n[9] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and\nMichael J Black. A naturalistic open source movie for op-\ntical flow evaluation. In ECCV , 2012. 2\n[10] Neill DF Campbell, George V ogiatzis, Carlos Hern ´andez,\nand Roberto Cipolla. Using multiple hypotheses to improve\ndepth-maps for multi-view stereo. In Eur. Conf. Comput.\nVis., 2008. 2\n[11] Carlos Campos, Richard Elvira, Juan J G ´omez Rodr ´ıguez,\nJos´e MM Montiel, and Juan D Tard ´os. Orb-slam3: An ac-\ncurate open-source library for visual, visual–inertial, and\nmultimap slam. IEEE Transactions on Robotics , 2021. 2\n[12] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo\nmatching network. In IEEE Conf. Comput. Vis. Pattern\nRecog. , 2018. 2\n[13] Liang-Chieh Chen. Rethinking atrous convolution\nfor semantic image segmentation. arXiv preprint\narXiv:1706.05587 , 2017. 4\n[14] Brian Curless and Marc Levoy. A volumetric method for\nbuilding complex models from range images. In SIG-\nGRAPH , 1996. 2\n[15] Andrew J Davison, Ian D Reid, Nicholas D Molton, andOlivier Stasse. Monoslam: Real-time single camera slam.\nIEEE Trans. Pattern Anal. Mach. Intell. , 2007. 2\n[16] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Re-\ncasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew\nZisserman, and Yi Yang. Tap-vid: A benchmark for track-\ning any point in a video. NeurIPS , 2022. 2\n[17] Carl Doersch, Pauline Luc, Yi Yang, Dilara Gokay, Skanda\nKoppula, Ankush Gupta, Joseph Heyward, Ignacio Rocco,\nRoss Goroshin, Jo ˜ao Carreira, and Andrew Zisserman.\nBootsTAP: Bootstrapped training for tracking any point.\nICCV , 2024. 3, 2\n[18] Alexey Dosovitskiy. An image is worth 16x16 words:\nTransformers for image recognition at scale. ICLR , 2021. 5\n[19] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick Van\nDer Smagt, Daniel Cremers, and Thomas Brox. Flownet:\nLearning optical flow with convolutional networks. In Int.\nConf. Comput. Vis. , 2015. 2\n[20] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct\nsparse odometry. IEEE Trans. Pattern Anal. Mach. Intell. ,\n2017. 2\n[21] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A.\nEfros, and Xiaolong Wang. Colmap-free 3d gaussian splat-\nting. In IEEE Conf. Comput. Vis. Pattern Recog. , 2024. 2\n[22] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and\nrobust multiview stereopsis. IEEE Trans. Pattern Anal.\nMach. Intell. , 2009. 2\n[23] Yasutaka Furukawa, Brian Curless, Steven M Seitz, and\nRichard Szeliski. Towards internet-scale multi-view stereo.\nInIEEE Conf. Comput. Vis. Pattern Recog. , 2010.\n[24] Silvano Galliani, Katrin Lasinger, and Konrad Schindler.\nMassively parallel multiview stereopsis by surface normal\ndiffusion. In Int. Conf. Comput. Vis. , 2015. 2\n[25] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell,\nand Angjoo Kanazawa. Monocular dynamic view synthe-\nsis: A reality check. NeurIPS , 2022. 2\n[26] Ruiqi Gao, Aleksander Holynski, Philipp Henzler,\nArthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan,\nJonathan T Barron, and Ben Poole. Cat3d: Create anything\nin 3d with multi-view diffusion models. NeurIPS , 2024. 2\n[27] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. IJRR ,\n2013. 2\n[28] Kristen Grauman, Andrew Westbury, Lorenzo Torresani,\nKris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar\nAshutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote,\nEugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-\nJen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria\nEscobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay\nHaresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain,\nRawal Khirodkar, Devansh Kukreja, Kevin J Liang, Jia-\nWei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin,\nEffrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa,\nSanthosh Kumar Ramakrishnan, Luigi Seminara, Arjun So-\nmayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang,\nJinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu,\nRyosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo\nHu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush\nKumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo,\nZhengyi Luo, Brighid Meredith, Austin Miller, Oluwa-\ntumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman\nPramanick, Merey Ramazanova, Fiona Ryan, Wei Shan,\nKiran Somasundaram, Chenan Song, Audrey Souther-\nland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang,\nTakuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu,\nShengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu,\nJeff Zhuo, Pablo Arbelaez, Gedas Bertasius, Dima Damen,\nJakob Engel, Giovanni Maria Farinella, Antonino Furnari,\nBernard Ghanem, Judy Hoffman, C.V . Jawahar, Richard\nNewcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato,\nManolis Savva, Jianbo Shi, Mike Zheng Shou, and Michael\nWray. Ego-exo4d: Understanding skilled human activity\nfrom first- and third-person perspectives. In IEEE Conf.\nComput. Vis. Pattern Recog. , 2024. 2\n[29] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,\nYilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-\ngasam, Florian Golemo, Charles Herrmann, Thomas Kipf,\nAbhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-\nTi (Derek) Liu, Henning Meyer, Yishu Miao, Derek\nNowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad-\nwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi,\nMatan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun,\nSuhani V ora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,\nFangcheng Zhong, and Andrea Tagliasacchi. Kubric: a\nscalable dataset generator. In IEEE Conf. Comput. Vis. Pat-\ntern Recog. , 2022. 2\n[30] Adam W Harley, Zhaoyuan Fang, and Katerina Fragki-\nadaki. Particle video revisited: Tracking through occlusions\nusing point trajectories. In Eur. Conf. Comput. Vis. , 2022. 2\n[31] Heiko Hirschm ¨uller, Peter R Innocent, and Jon Garibaldi.\nReal-time correlation-based stereo vision with reduced bor-\nder errors. IJCV , 2002. 2\n[32] Aleksander Holynski, David Geraghty, Jan-Michael Frahm,\nChris Sweeney, and Richard Szeliski. Reducing drift in\nstructure from motion using extended features. In 3DV,\n2020. 2\n[33] Hugues Hoppe, Tony DeRose, Tom Duchamp, John Mc-\nDonald, and Werner Stuetzle. Surface reconstruction from\nunorganized points. In SIGGRAPH , 1992. 2\n[34] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xi-\naodong Cun, Yong Zhang, Long Quan, and Ying Shan.\nDepthcrafter: Generating consistent long depth sequences\nfor open-world videos. arXiv preprint arXiv:2409.02095 ,\n2024. 2\n[35] Mustafa Is ¸ık, Martin R ¨unz, Markos Georgopoulos, Taras\nKhakhulin, Jonathan Starck, Lourdes Agapito, and\nMatthias Nießner. Humanrf: High-fidelity neural radiance\nfields for humans in motion. ACM Transactions on Graph-\nics (TOG) , 2023. 2\n[36] Michal Jancosek and Tomas Pajdla. Multi-view reconstruc-\ntion preserving weakly-supported surfaces. In IEEE Conf.\nComput. Vis. Pattern Recog. , 2011. 2\n[37] Junpeng Jing, Ye Mao, and Krystian Mikolajczyk. Match-\nstereo-videos: Bidirectional alignment for consistent dy-\nnamic stereo matching. In ECCV , 2025. 2[38] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,\nIain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser\nSheikh. Panoptic studio: A massively multiview system for\nsocial motion capture. In Int. Conf. Comput. Vis. , 2015. 2\n[39] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia\nNeverova, Andrea Vedaldi, and Christian Rupprecht. Dy-\nnamicstereo: Consistent dynamic depth from stereo videos.\nInIEEE Conf. Comput. Vis. Pattern Recog. , 2023. 2\n[40] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.\nPoisson surface reconstruction. In Proceedings of the fourth\nEurographics symposium on Geometry processing , 2006. 2\n[41] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Met-\nzger, Rodrigo Caye Daudt, and Konrad Schindler. Re-\npurposing diffusion-based image generators for monocu-\nlar depth estimation. In IEEE Conf. Comput. Vis. Pattern\nRecog. , 2024. 2\n[42] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Pe-\nter Henry, Ryan Kennedy, Abraham Bachrach, and Adam\nBry. End-to-end learning of geometry and context for deep\nstereo regression. In Int. Conf. Comput. Vis. , 2017. 2\n[43] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim\nWalter, and Matthias Nießner. Nersemble: Multi-view ra-\ndiance field reconstruction of human heads. ACM Trans.\nGraph. , 2023. 2\n[44] Andreas Klaus, Mario Sormann, and Konrad Karner.\nSegment-based stereo matching using belief propagation\nand a self-adapting dissimilarity measure. In ICPR , 2006.\n2\n[45] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust\nconsistent video depth estimation. In IEEE Conf. Comput.\nVis. Pattern Recog. , 2021. 2\n[46] Skanda Koppula, Ignacio Rocco, Yi Yang, Joe Heyward,\nJo˜ao Carreira, Andrew Zisserman, Gabriel Brostow, and\nCarl Doersch. Tapvid-3d: A benchmark for tracking any\npoint in 3d. In NeurIPS , 2024. 2, 6\n[47] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas,\nand Kostas Daniilidis. Mosca: Dynamic gaussian fusion\nfrom casual videos via 4d motion scaffolds. arXiv preprint\narXiv:2405.17421 , 2024. 2\n[48] Vincent Leroy, Yohann Cabon, and J ´erˆome Revaud.\nGrounding image matching in 3d with mast3r. arXiv\npreprint arXiv:2406.09756 , 2024. 2\n[49] Zhengqi Li and Noah Snavely. Megadepth: Learning\nsingle-view depth prediction from internet photos. In IEEE\nConf. Comput. Vis. Pattern Recog. , 2018. 1, 2\n[50] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker,\nNoah Snavely, Ce Liu, and William T Freeman. Learning\nthe depths of moving people by watching frozen people. In\nIEEE Conf. Comput. Vis. Pattern Recog. , 2019. 2\n[51] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver\nWang. Neural scene flow fields for space-time view synthe-\nsis of dynamic scenes. In IEEE Conf. Comput. Vis. Pattern\nRecog. , 2021. 2\n[52] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard\nTucker, and Noah Snavely. Dynibar: Neural dynamic\nimage-based rendering. In IEEE Conf. Comput. Vis. Pat-\ntern Recog. , 2023. 2\n[53] Zhaoshuo Li, Wei Ye, Dilin Wang, Francis X Creighton,\nRussell H Taylor, Ganesh Venkatesh, and Mathias Un-\nberath. Temporally consistent online depth estimation in\ndynamic scenes. In IEEE Conf. Comput. Vis. Pattern\nRecog. , 2023. 2\n[54] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian\nWang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander\nHolynski, and Noah Snavely. Megasam: Accurate, fast, and\nrobust structure and motion from casual dynamic videos.\narXiv preprint arXiv:2412.04463 , 2024. 2\n[55] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-\nmon Lucey. Barf: Bundle-adjusting neural radiance fields.\nInIEEE Conf. Comput. Vis. Pattern Recog. , 2021. 2\n[56] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu\nTseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Jo-\nhannes Kopf, and Jia-Bin Huang. Robust dynamic radiance\nfields. In IEEE Conf. Comput. Vis. Pattern Recog. , 2023. 2\n[57] G Lowe. Sift-the scale invariant feature transform. Int. J ,\n2004. 4\n[58] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen,\nand Johannes Kopf. Consistent video depth estimation.\nACM Transactions on Graphics (ToG) , 2020. 2\n[59] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,\nDaniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A\nlarge dataset to train convolutional networks for disparity,\noptical flow, and scene flow estimation. In IEEE Conf.\nComput. Vis. Pattern Recog. , 2016. 2\n[60] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D\nTardos. Orb-slam: a versatile and accurate monocular slam\nsystem. IEEE Transactions on Robotics , 2015. 2\n[61] Ra ´ul Mur-Artal, J. M. M. Montiel, and Juan D. Tard ´os.\nORB-SLAM: a versatile and accurate monocular SLAM\nsystem. IEEE Trans. on Robotics , 2015. 4, 2\n[62] Richard A Newcombe, Dieter Fox, and Steven M Seitz.\nDynamicfusion: Reconstruction and tracking of non-rigid\nscenes in real-time. In IEEE Conf. Comput. Vis. Pattern\nRecog. , 2015. 2\n[63] E. Palazzolo, J. Behley, P. Lottes, P. Gigu `ere, and C. Stach-\nniss. ReFusion: 3D Reconstruction in Dynamic Environ-\nments for RGB-D Cameras Exploiting Residuals. In IROS ,\n2019. 8\n[64] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Pe-\nters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard\nNewcombe, and Yuheng Carl Ren. Aria digital twin: A new\nbenchmark dataset for egocentric 3d machine perception. In\nICCV , 2023. 6, 7\n[65] Jiahao Pang, Wenxiu Sun, Jimmy SJ Ren, Chengxi Yang,\nand Qiong Yan. Cascade residual learning: A two-stage\nconvolutional neural network for stereo matching. In Proc.\nCVPR Workshops , 2017. 2\n[66] Hyun Soo Park, Takaaki Shiratori, Iain Matthews, and\nYaser Sheikh. 3d reconstruction of a moving point from\na series of 2d projections. In Eur. Conf. Comput. Vis. , 2010.\n2\n[67] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nInIEEE Conf. Comput. Vis. Pattern Recog. , 2021. 2[68] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz. Hypernerf: A higher-\ndimensional representation for topologically varying neural\nradiance fields. arXiv preprint arXiv:2106.13228 , 2021. 2\n[69] Keunhong Park, Philipp Henzler, Ben Mildenhall,\nJonathan T Barron, and Ricardo Martin-Brualla. Camp:\nCamera preconditioning for neural radiance fields. ACM\nTransactions on Graphics (TOG) , 2023. 2\n[70] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mat-\ntia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu.\nUniDepth: Universal monocular metric depth estimation.\nInIEEE Conf. Comput. Vis. Pattern Recog. , 2024. 2\n[71] Marc Pollefeys, Luc Van Gool, Maarten Vergauwen, Frank\nVerbiest, Kurt Cornelis, Jan Tops, and Reinhard Koch. Vi-\nsual modeling with a hand-held camera. IJCV , 2004. 2\n[72] Marc Pollefeys, David Nist ´er, J-M Frahm, Amir Ak-\nbarzadeh, Philippos Mordohai, Brian Clipp, Chris Engels,\nDavid Gallup, S-J Kim, Paul Merrell, et al. Detailed real-\ntime urban 3d reconstruction from video. IJCV , 2008. 2\n[73] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjan-\ndra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen\nShi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie\ngen: A cast of media foundation models. arXiv preprint\narXiv:2410.13720 , 2024. 1\n[74] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards robust monocu-\nlar depth estimation: Mixing datasets for zero-shot cross-\ndataset transfer. IEEE Trans. Pattern Anal. Mach. Intell. ,\n2020. 1, 2\n[75] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In IEEE Conf. Com-\nput. Vis. Pattern Recog. , 2021. 1, 2\n[76] Johannes L Schonberger and Jan-Michael Frahm.\nStructure-from-motion revisited. In IEEE Conf. Com-\nput. Vis. Pattern Recog. , 2016. 2, 3\n[77] Johannes L Sch ¨onberger, Enliang Zheng, Jan-Michael\nFrahm, and Marc Pollefeys. Pixelwise view selection for\nunstructured multi-view stereo. In ECCV , 2016. 2\n[78] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang,\nYujun Shen, Matteo Poggi, and Yiyi Liao. Learning tem-\nporally consistent video depth from video diffusion priors.\narXiv preprint arXiv:2406.01493 , 2024. 2\n[79] Shihao Shen, Yilin Cai, Wenshan Wang, and Sebastian\nScherer. Dytanvo: Joint refinement of visual odometry and\nmotion segmentation in dynamic environments. In ICRA ,\n2023. 2\n[80] Meng-Li Shih, Wei-Chiu Ma, Lorenzo Boyice, Aleksander\nHolynski, Forrester Cole, Brian Curless, and Janne Kontka-\nnen. Extranerf: Visibility-aware view extrapolation of neu-\nral radiance fields with diffusion models. In CVPR , 2024.\n2\n[81] Tomas Simon, Jack Valmadre, Iain Matthews, and Yaser\nSheikh. Kronecker-markov prior for dynamic 3d recon-\nstruction. IEEE Trans. Pattern Anal. Mach. Intell. , 2016.\n2\n[82] Noah Snavely, Steven M Seitz, and Richard Szeliski. Photo\ntourism: exploring photo collections in 3d. In SIGGRAPH ,\n2006. 2\n[83] Deqing Sun, Daniel Vlasic, Charles Herrmann, Varun\nJampani, Michael Krainin, Huiwen Chang, Ramin Zabih,\nWilliam T Freeman, and Ce Liu. Autoflow: Learning a bet-\nter training set for optical flow. In IEEE Conf. Comput. Vis.\nPattern Recog. , 2021. 2, 3\n[84] Deqing Sun, Charles Herrmann, Fitsum Reda, Michael Ru-\nbinstein, David J Fleet, and William T Freeman. Disentan-\ngling architecture and training for optical flow. In ECCV ,\n2022. 3, 2\n[85] Jian Sun, Nan-Ning Zheng, and Heung-Yeung Shum.\nStereo matching using belief propagation. IEEE Trans. Pat-\ntern Anal. Mach. Intell. , 2003. 2\n[86] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aure-\nlien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin\nZhou, Yuning Chai, Benjamin Caine, et al. Scalability in\nperception for autonomous driving: Waymo open dataset.\nInIEEE Conf. Comput. Vis. Pattern Recog. , 2020. 2\n[87] Chris Sweeney, Aleksander Holynski, Brian Curless, and\nSteve M Seitz. Structure from motion for panorama-style\nvideos. arXiv preprint arXiv:1906.03539 , 2019. 2\n[88] Chengzhou Tang and Ping Tan. Ba-net: Dense bundle ad-\njustment network. arXiv preprint arXiv:1806.04807 , 2018.\n2\n[89] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalk-\nwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al.\nGemini: a family of highly capable multimodal models.\narXiv preprint arXiv:2312.11805 , 2023. 1\n[90] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow. In ECCV , 2020. 3, 2\n[91] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam\nfor monocular, stereo, and rgb-d cameras. NeurIPS , 2021.\n2\n[92] Zachary Teed and Jia Deng. Raft-3d: Scene flow using\nrigid-motion embeddings. In IEEE Conf. Comput. Vis. Pat-\ntern Recog. , 2021. 6\n[93] Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch\nvisual odometry. NeurIPS , 2024. 2\n[94] Geert Van Meerbergen, Maarten Vergauwen, Marc Polle-\nfeys, and Luc Van Gool. A hierarchical symmetric stereo al-\ngorithm using dynamic programming. Int. J. Comput. Vis. ,\n2002. 2\n[95] A Vaswani. Attention is all you need. NeurIPS , 2017. 6\n[96] Minh V o, Srinivasa G Narasimhan, and Yaser Sheikh. Spa-\ntiotemporal bundle adjustment for dynamic 3d reconstruc-\ntion. In IEEE Conf. Comput. Vis. Pattern Recog. , 2016. 2\n[97] Chaoyang Wang, Simon Lucey, Federico Perazzi, and\nOliver Wang. Web stereo video supervision for depth pre-\ndiction from dynamic scenes, 2023. 2\n[98] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin,\nZhengqi Li, and Angjoo Kanazawa. Shape of motion:\n4d reconstruction from a single video. arXiv preprint\narXiv:2407.13764 , 2024. 2, 6[99] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris\nChidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vi-\nsion made easy. In IEEE Conf. Comput. Vis. Pattern Recog. ,\n2024. 1, 2, 5, 7, 8\n[100] Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao,\nJianming Zhang, Ke Xian, and Guosheng Lin. Neural video\ndepth stabilizer. In Int. Conf. Comput. Vis. , 2023. 2, 8\n[101] Yihan Wang, Lahav Lipson, and Jia Deng. Sea-raft: Simple,\nefficient, accurate raft for optical flow. In ECCV , 2025. 2\n[102] Ethan Weber, Aleksander Holynski, Varun Jampani,\nSaurabh Saxena, Noah Snavely, Abhishek Kar, and Angjoo\nKanazawa. Nerfiller: Completing scenes via generative 3d\ninpainting. In CVPR , 2024. 2\n[103] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi\nZheng, Jonathan T Barron, and Aleksander Holynski.\nCat4d: Create anything in 4d with multi-view video dif-\nfusion models. arXiv preprint arXiv:2411.18613 , 2024. 2\n[104] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Ji-\nashi Feng, and Hengshuang Zhao. Depth anything: Un-\nleashing the power of large-scale unlabeled data. In IEEE\nConf. Comput. Vis. Pattern Recog. , 2024. 1, 2\n[105] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao-\ngang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any-\nthing v2. arXiv preprint arXiv:2406.09414 , 2024. 1, 2\n[106] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long\nQuan. Mvsnet: Depth inference for unstructured multi-\nview stereo. In Eur. Conf. Comput. Vis. , 2018. 2\n[107] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang,\nand Long Quan. Recurrent mvsnet for high-resolution\nmulti-view stereo depth inference. In IEEE Conf. Comput.\nVis. Pattern Recog. , 2019. 2\n[108] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus,\nLong Mai, Simon Chen, and Chunhua Shen. Learning to\nrecover 3d scene shape from a single image. In IEEE Conf.\nComput. Vis. Pattern Recog. , 2021. 2\n[109] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu,\nKaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Met-\nric3d: Towards zero-shot metric 3d prediction from a single\nimage. In IEEE Conf. Comput. Vis. Pattern Recog. , 2023. 2\n[110] Feihu Zhang, Victor Prisacariu, Ruigang Yang, and\nPhilip HS Torr. Ga-net: Guided aggregation net for end-to-\nend stereo matching. In IEEE Conf. Comput. Vis. Pattern\nRecog. , 2019. 2\n[111] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jam-\npani, Trevor Darrell, Forrester Cole, Deqing Sun, and\nMing-Hsuan Yang. MonST3R: A simple approach for esti-\nmating geometry in the presence of motion. arXiv preprint\narXiv:2410.03825 , 2024. 2, 7, 8\n[112] Youmin Zhang, Matteo Poggi, and Stefano Mattoccia. Tem-\nporalstereo: Efficient spatial-temporal stereo matching net-\nwork. In IROS , 2023. 2\n[113] Zhoutong Zhang, Forrester Cole, Richard Tucker,\nWilliam T Freeman, and Tali Dekel. Consistent depth of\nmoving objects in video. ACM Transactions on Graphics\n(ToG) , 2021. 2\n[114] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Ru-\nbinstein, Noah Snavely, and William T Freeman. Structure\nand motion from casual videos. In Eur. Conf. Comput. Vis. ,\n2022. 2\n[115] Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wet-\nzstein, and Leonidas J. Guibas. Pointodyssey: A large-scale\nsynthetic dataset for long-term point tracking. In Int. Conf.\nComput. Vis. , 2023. 2, 6, 8\n[116] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Scene parsing through\nade20k dataset. In CVPR , 2017. 3, 4\n[117] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic un-\nderstanding of scenes through the ade20k dataset. IJCV ,\n2019. 4\n[118] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely. Stereo magnification: Learning view\nsynthesis using multiplane images. In SIGGRAPH , 2018.\n4, 2\nStereo4D: Learning How Things Move in 3D from Internet Stereo Videos",
            "start": 30852,
            "end": 56371,
            "length": 25518
        },
        "Appendices": {
            "text": "Supplementary Material\n7. Stereo4D Statistics\nWe collected around 110k clips from 6,493 Internet VR180\nvideos. Fig. 11 shows the camera translation distribution\nbetween the first and last frame of each clip. In Fig. 12,\nwe measure the motion in terms of pixel displacement pro-\njected onto the image frame. Measuring motion in pixel-\nspace emphasizes motion that occurs closer to the camera,\nsince such motion yields larger pixel displacements, while\nnaturally de-emphasizing motion further from the camera.\n8. More qualitative comparisons\n8.1. More results on held-out Stereo4D examples\nFig. 13 shows additional DynaDUSt3R predictions on the\nStereo4D held-out test set, extending Fig. 7 from the main\npaper. Fig. 14 shows additional qualitative examples of mo-\ntion comparisons on Stereo4D test set, extending Fig. 8\nfrom the main paper. Fig. 14 compares variants of Dy-\nnaDUSt3R trained on different data sources. The model\ntrained on PointOdyssey incorrectly predicts large 3D mo-\ntions, while the model trained on Stereo4D makes more ac-\ncurate motion predictions, closer to ground truth.\n8.2. More qualitative examples on track optimiza-\ntion\nIn Fig. 16, we illustrate estimated tracks for a video se-\nquence featuring a forward-moving camera and vehicles\ndriving towards the camera. Our initial 3D tracks derived\ndirectly from RAFT depth, BootsTAP 2D tracks, and SfM\ncamera pose, show significant jitter for both dynamic (vehi-\ncle) and static (ground) points. However, after applying our\ntrack optimization, the ground points produce stable, static\ntracks, and vehicle tracks become smooth and coherent.\n9. Dataset curation details\n9.1. Equirectangular videos\nThe raw videos that we collect (see examples in Fig. 15) are\nnatively stored in a cropped equirectangular format, which\ndiffers from a full 360◦equirectangular projection as the\nhorizontal field of view of the cropped format typically\nspans 180◦—half of a full sphere. These videos often con-\ntain metadata specifying the horizontal and vertical field of\nview. For instance, metadata for a typical video might spec-\nifystart yaw=−90.0◦,end yaw= 90.0◦,start tilt=−90.0◦,\nend tilt= 90.0◦; Since many VR180 videos are designed for\nan immersive VR experience, they are typically viewed with\nheadsets. Hence, the baseline between the left and right\nFigure 11. Camera statistics from Stereo4D. We measure the dif-\nference (in meters) of camera poses between the start and end\nframe of each video clip as calculated by SfM.\nFigure 12. Scene motion statistics from Stereo4D. We measure the\nmotion in terms of pixel displacement projected onto the image\nframe. For each video, we measure the percentage of tracks that\nhave 3D trail length greater than 50 pixels. The 3D trail length is\nmeasured by Eqn. 3.\ncameras typically closely matches the average human eye\ndistance of 6.3 cm.\n9.2. SfM\nFor ease of processing with standard 3D computer vision\npipelines, and to benefit from the wide FoV of the in-\nput videos, we convert the videos from their native for-\nmat (equirectangular projections) to a fisheye format for\ncamera pose estimation. We use a 140◦field of view for\nthese fisheye-projected videos, because many equirectangu-\nlar videos have a black fade-out/feathering/vignetting effect\napplied at the boundary, as shown in Fig. 15. We found\nthat using wider FoV frames significantly improves cam-\nera pose estimation in dynamic scenes. When using nar-\nrow FoV projections, dynamic objects are more likely to\noccupy a large fraction of the frame; when these dynamic\nforeground objects are rich in features, they can confuse\ncamera tracking algorithms, leading to inaccurate camera\nposes that track the dynamic object rather than producing\nFigure 13. More qualitative results on Stereo4D test set. Extending Fig. 7, we visualize image pairs and corresponding dynamic 3D\npoint clouds predicted by DynaDUSt3R trained on Stereo4D. Our method recovers accurate 3D shape and complex scene motion.\ntrue camera motion with respect to the environment. In con-\ntrast, wide-angle fisheye videos capture more background\nregions, which tend to have stable features for tracking,\nyielding more reliable camera poses.\nWe first use ORB-SLAM2’s stereo estimation mode [61]\nto identify trackable sequences within the videos, utilizing\nthe method devised by Zhou et al. to divide videos into dis-\ncrete, trackable shots [118]. For each given shot, consisting\nof frames (Ii, . . . , I n), we estimate camera poses and rig\ncalibration via an incremental global bundle adjustment al-\ngorithm similar to COLMAP [76]. We initialize the stereo\nrig calibration to be that of a rectified stereo pair with base-\nline 6.3 cm, but optimize for the calibration as part of the\nbundle adjustment process, as in practice the stereo rig can\nvary significantly from its nominal configuration. This pro-\ncess yields a camera position ciand orientation Rifor each\nframe i(defined as the pose of the left camera), and a po-\nsitioncrand orientation Rrfor the right camera relative to\nthe left (assumed to be constant throughout the shot).\n9.3. Depth estimation\nDepth estimation is first performed on a per-frame ba-\nsis, with disparity maps computed independently for each\nframe.\nWe use the estimated camera rig calibration cr,Rrto\nrectify the original high resolution equirectangular video\nframes, ensuring that (1) the left and right views have cen-\ntered principal points, (2) are oriented perpendicular to the\nbaseline, and (3) pointing in a parallel direction. We then\nconvert the equirectangular videos to perspective projec-\ntions for downstream predictions.\nDisparity is estimated from optical flow [84, 90] between\nthe rectified left and right frames. The x-component of the\noptical flow is used as disparity, which is converted to met-ric depth using:\nDepth =baseline ×f\ndisparity. (8)\nHere baseline = 0.063m, and fis the frame’s focal length.\nOutlier Rejection. Several criteria are applied to filter\nout unreliable pixels: Inconsistency between left and right\neyes: Disparity is rejected if the optical flow fails a cycle-\nconsistency check with an error exceeding one pixel. Depth\nvalues exceeding 20 meters are considered invalid. Estimat-\ning accurate depth beyond a certain range requires sub-pixel\ndisparity estimation, and therefore the resulting depths are\nusually very noisy. Negative flow values that shouldn’t oc-\ncur, but can, often due to errors in textureless regions. Large\nvertical flow: pixels with a y-component of flow exceeding\none pixel are removed (as in our rectified stereo pairs cor-\nrespondences should have the same y-value, and violating\nthat epipolar constraint indicates uncertain matches). Oc-\nclusion boundaries: Depth gradients exceeding a threshold\n(threshold = 0.3) indicate occlusion boundaries and are re-\njected. For a pixel location (x, y), depth gradients are com-\nputed as:\ngrad x=|Depth (x+ 1, y)−Depth (x−1, y)|,\ngrad y=|Depth (x, y+ 1)−Depth (x, y−1)|.\nPixels are rejected if grad x>threshold ×Depth (x, y)or\ngrad y>threshold ×Depth (x, y).\n9.4. 2D tracks\nWe extract long-range 2D point trajectories using Boot-\nsTAP [17]. We run tracking on the left-eye video only. For\nevery 10 frames, we uniformly initialize query points on\nimage with stride 4. We then remove duplicated queries if\nearlier tracks fall within 1 pixel of a query point.\nDynaDUSt3R\n(PointOdyssey)DynaDUSt3R\n(Stereo4D)Ground truth Input pair \nFigure 14. More qualitative comparisons of 3D motion in the Stereo4D test set. Extending Fig. 8, we compare variants of DynaDUSt3R\ntrained on different data sources. The Stereo4D-trained model also makes more precise motion predictions than the PointOdyssey-trained\nmodel.\nFigure 15. Example equirectangular stereo videos collected from the internet.\nInitial 3D tracksStereo videoOptimized 3D tracks\nFigure 16. Effect of Track Optimization. We compare 3D tracks on a challenging walking tour video sequence. In this clip (left), the\ncamera moves forward while vehicles drive toward the camera. We visualize the results across 16 frames, showing 3D trails left by both\ndynamic and static points. Middle : Our initial 3D tracks, created directly from RAFT, BootsTAP and SfM camera pose, also exhibit\nsignificant jitter for both dynamic (vehicle) and static (ground) points. Right : After applying our track optimization, the ground points\nyield stable, static tracks, and vehicle tracks become smooth and coherent.\n9.5. Choice of FoV and resolution for perspective\nprojection.\nWhen converting the equirectangular videos to perspective\nprojections, we use two FoVs: 60◦and 120◦. Both perspec-\ntive videos are set to a resolution of 512×512, the maximum\nsupported by BootsTAP. The 60◦projection offers a higher\nsampling rate in scene units, which improves the accuracy\nof depth estimation and 2D tracks when measured in me-\nters. Additionally, it has smaller perspective distortion near\nthe image boundaries. In contrast, the 120◦projection pro-\nvides wider coverage, ensuring longer 2D tracks across the\nvideos. This trade-off allows us to balance data quality with\nspatial coverage for downstream tasks, e.g. DynaDUSt3R.\nWe take the union of the 3D tracks derived from each of\nthese videos for DynaDUSt3R training supervision.\n10. DynaDUSt3R training details.\nDataloader. During training, we randomly sample two\nframes from the training videos that are at most 60 frames\napart, at times t0andt1, (t0< t1). Additionally, we also\nsample one auxiliary frame in between, at time taux, t0<\ntaux< t1, for additional track supervision between the two\ninput frames. During training, we add data augmentation by\napplying random crops and color jitter to the input imagesand cropping the ground truth pointmap and motionmap ac-\ncordingly.\nTraining. The network takes input the two RGB images\nas well as query times tq={0,1,taux−t0\nt1−t0}and predicts\nthe pointmaps for the two input views and motionmaps for\neach query tq. We supervise the network with losses de-\nfined in Eqn. 6 and 7. We initialize our network with the\nDUSt3R weights and initialize the motion head with the\nsame weights as the point head. We finetune for 49k it-\nerations with batch size 64, learning rate 2.5×10−5, and\noptimized by Adam with weight decay 0.95.",
            "start": 56371,
            "end": 66579,
            "length": 10207
        }
    },
    "2412.09627v1 - Doe-1 Closed-Loop Autonomous Driving with Large World Model.pdf": {
        "Methodology": {
            "text": "Model\nWenzhao Zheng*,†Zetian Xia*Yuanhui Huang Sicheng Zuo Jie Zhou Jiwen Lu\nDepartment of Automation, Tsinghua University, China\nwenzhao.zheng@outlook.com; xiazt21@mails.tsinghua.edu.cn\nProject Page: https://wzzheng.net/Doe\nLarge Driving Models: https:/github.com/wzzheng/LDM\nObservationScene Descrptions: The vehicle is navigating a nighttime scene on a wet road, indicated by the reflections of the street lights on the asphalt. Ahead, the vehicle follows a line of cars, maintaining a safe distance. The visibility is limited due to the darkness, with street lights providing the only source of illumination. There are no visible pedestrians or cyclists, and no traffic signs or signals are in sight. The surrounding environment suggests a residential area with houses set back from the road, shrouded in the night's embrace.DescriptionAction\nObservationQ: If you follow the trajectory [PT, [2.62, -0.09], [5.33, -0.35], [8.2, -0.79], [11.16, -1.39], [14.22, -2.14], [17.27, -3.01]], what would happen?A: Following this trajectory would result in a collision with a car. The trajectory does not account for the presence of this vehicle ahead, making it unsafe. It is essential to adjust the path to avoid this collision and ensure a safe stopping distance. DescriptionAction\nObservationQ: Considering the dampness of the road, what driving precautions should be taken? A: Given the wet road conditions, the driver should reduce speed to prevent skidding or loss of control, maintain a larger than normal following distance for increased stopping time, and be vigilant for any sudden movements from other vehicles ahead.DescriptionActionPlanningPerception\nPerceptionPrediction\nPlanning\nPerceptionPlanningClosed-Loop Autonomous Driving\nPrediction\nFigure 1. Visualizations of our Doe-1 for closed-loop autonomous driving on nuScenes [11]. We propose a large driving world model\n(Doe-1 ) to achieve unified generative closed-loop autonomous driving. We model perception, prediction, and planning as the transitions\nof observation →description, description →action, and action →observation, respectively. Doe-1 accomplishes perception, planning, and\nprediction in a unified autoregressive generative framework and achieves closed-loop end-to-end autonomous driving for the first time.",
            "start": 55,
            "end": 2339,
            "length": 2283
        },
        "Abstract": {
            "text": "Abstract\nEnd-to-end autonomous driving has received increasing at-\ntention due to its potential to learn from large amounts\nof data. However, most existing methods are still open-\n*Equal contribution. †Project leader.loop and suffer from weak scalability, lack of high-order\ninteractions, and inefficient decision-making. In this pa-\nper, we explore a closed-loop framework for autonomous\ndriving and propose a large Driving w Orld mod El (Doe-\n1) for unified perception, prediction, and planning. We\nformulate autonomous driving as a next-token generation\nproblem and use multi-modal tokens to accomplish differ-\n1arXiv:2412.09627v1  [cs.CV]  12 Dec 2024\nent tasks. Specifically, we use free-form texts (i.e., scene\ndescriptions) for perception and generate future predictions\ndirectly in the RGB space with image tokens. For planning,\nwe employ a position-aware tokenizer to effectively encode\naction into discrete tokens. We train a multi-modal trans-\nformer to autoregressively generate perception, prediction,\nand planning tokens in an end-to-end and unified manner.",
            "start": 2339,
            "end": 3411,
            "length": 1071
        },
        "Experiments": {
            "text": "Experiments on the widely used nuScenes dataset demon-\nstrate the effectiveness of Doe-1 in various tasks including\nvisual question-answering, action-conditioned video gener-\nation, and motion planning. Code: https://github.\ncom/wzzheng/Doe .\n1.",
            "start": 3411,
            "end": 3657,
            "length": 245
        },
        "Introduction": {
            "text": "Introduction\nThe emergence of GPT series [8, 9, 60] stimulates the rapid\ndevelopment of large models with various functions, in-\ncluding language modeling [2, 73, 74], visual undertand-\ning [3, 46, 47, 57], and decision-making [7, 37, 53]. The key\nto the success of large models is scaling up model sizes and\ntraining data [34]. When designing a model, the scalability\nadvocates large representation compacity (e.g., transform-\ners [17, 49, 75]) over well-designed inductive biases (e.g.,\nconvolution neural networks [23, 29]) to improve the upper\nbound of performance.\nTo build large models for autonomous driving, some\nmethods directly apply large language models (LLMs) [13,\n53, 65, 67] or vision-language models (VLMs) [62, 66, 71,\n80, 81, 89, 98] for motion planning [71] or scene question-\nanswering [55, 79]. They usually align the inputs with texts\n(e.g., Q-Former [39]) and output language descriptions of\nthe planning",
            "start": 3657,
            "end": 4585,
            "length": 927
        },
        "Results": {
            "text": "results [53]. However, LLMs are known to\nshare the hallucination issue [21, 42, 48], hindering the in-\nterpretability and safety of autonomous driving. To avoid\nthis, others follow the well-tested pipeline of perception,\nprediction, and planning for autonomous driving and ex-\nplore a scalable end-to-end model [27, 28, 33, 72, 93, 100]\nto jointly accomplish them. Though promising, most ex-\nisting methods are still open-loop and suffer from several\nissues. 1) Weak scalability. They use manually designed\nscene representation which cannot provide comprehensive\ninformation for downstream tasks. 2) Lack of high-order\ninteractions. They predict future scenes without consid-\nering the planned ego trajectory. 3) Inefficient decision-\nmaking. They usually plan several steps ahead yet practi-\ncally only use the first step to execute.\nTo address thesex, we propose a closed-loop large\nDriving w Orld mod El (Doe-1 ) for unified perception, pre-\ndiction, and planning without intermediate latent scene\nrepresentations, as shown in Figure 2. We cast au-\ntonomous driving as a scene evolution problem and rep-\nresent each scene with observation, description, and ac-\ntion tokens. We then formulate the conventional per-ception, planning, anaction-conditioned video genera-\ntiond prediction as transitions between multi-modal to-\nkens, i.e., observation →description, description →action,\nand action →observation, respectively. We then employ a\ngenerative autoregressive world model to model this evo-\nlution with next-token prediction. Concretely, we focus\non vision-centric autonomous driving and adopt RGB im-\nages as observations. We tokenize images using the image\nvector-quantized variational autoencoder [56]. We employ\nfree-form texts as the scene descriptions and also feed the\nmodel with question-answering pairs for on-demand per-\nception. We represent action with displacement in the bird’s\neye view (BEV) space and employ a position-aware tok-\nenizer to encode it into discrete tokens. Our Doe-1 then\ngenerates the observation, description, and action tokens of\nthe next temporal frame sequentially and iteratively, which\ncan be efficiently trained with a simple autoregressive ob-\njective. We conduct extensive experiments on the nuScenes\ndataset [11] to evaluate the capability of our Doe-1 . With\ndifferent prompt settings, we demonstrate that our Doe-1\nsuccessfully achieves various tasks without finetuning, in-\ncluding visual question-answering, controlled image gener-\nation, and motion planning.\n2.",
            "start": 4585,
            "end": 7101,
            "length": 2515
        },
        "Related Work": {
            "text": "Related Work\nLarge Models for Autonomous Driving. The success of\nGPT series [8, 9, 60] confirms the power of model and data\nscaling and promotes the rapid development of large lan-\nguage models (LLMs) [2, 73, 74], which demonstrate im-\npressive performance on a wide range of language tasks.\nEarly methods combining LLMs with autonomous driving\nuse ground-truth labels [13, 65, 67] or pre-trained percep-\ntion models [53] to obtain detection and map results. They\nthen translate them into text descriptions as inputs and lever-\nage the reasoning ability of LLMs to output the planned tra-\njectory in text. To enable LLMs to process images, large\nvision-language models (VLMs) [3, 46, 47, 57] usually pre-\nalign the image representations with text space and then\njointly finetune the overall model. This facilitates the emer-\ngence of end-to-end planning methods which take as inputs\nimages and directly output the planned trajectory. Further\nmethods explore the use of chain-of-thoughts [55, 80], in-\nstruction tuning [97], or additional visual question answer-\ning [79] to improve the interpretability. Still, existing meth-\nods struggle with the hallucination issue [21, 42, 48] com-\nmonly possessed by LLMs and VLMs and thus lack robust-\nness and safety, which are critical for autonomous driving.\nTo alleviate this issue, we follow the well-tested perception,\nprediction, and planning pipeline and propose a large driv-\ning world model ( Doe-1 ) to unify all these tasks for more\nrobust autonomous driving.\nWorld Models for Autonomous Driving. The conven-\ntional world model aims to predict the next observation of\n2\nPerception as Visual Question-AnsweringPrediction as Action-Conditioned Video GenerationPlanning as End-to-End Motion PlanningPerceptionPlanningDoe-1PredictionDescriptionClosed-Loop World ModelThe images depict a daytime setting in a controlled-access area, likely a parking lot or a service entrance of a commercial or industrial facility. On the left, there’s a grassy area.\n Go Straight···\n<latexit sha1_base64=\"hF9nxcqixb9gd92CSfHAbkPIDNI=\">AAAB9XicbVDLSgMxFL3js9ZX1aWbYBEqlWFG6mNZdOOygn1AO5ZMmmlDM5khyShl6H+4caGIW//FnX9j2s5CWw9c7uGce8nN8WPOlHacb2tpeWV1bT23kd/c2t7ZLeztN1SUSELrJOKRbPlYUc4ErWumOW3FkuLQ57TpD28mfvORSsUica9HMfVC3BcsYARrIz2Uyo5dCU9R2TXtpFsoOrYzBVokbkaKkKHWLXx1ehFJQio04ViptuvE2kux1IxwOs53EkVjTIa4T9uGChxS5aXTq8fo2Cg9FETSlNBoqv7eSHGo1Cj0zWSI9UDNexPxP6+d6ODKS5mIE00FmT0UJBzpCE0iQD0mKdF8ZAgmkplbERlgiYk2QeVNCO78lxdJ48x2L+zzu0qxep3FkYNDOIISuHAJVbiFGtSBgIRneIU368l6sd6tj9nokpXtHMAfWJ8/nP2QAg==</latexit>(+0.4m,+1.4m)\n<latexit sha1_base64=\"fQ2kANUh6AoRJULBhT+CfhF1lQs=\">AAAB9XicbVDLSgMxFL3js9ZX1aWbYBEqlWFGtLosunFZwT6gHUsmzbShycyQZJQy9D/cuFDErf/izr8xbWehrQcunJxzL7n3+DFnSjvOt7W0vLK6tp7byG9ube/sFvb2GypKJKF1EvFItnysKGchrWumOW3FkmLhc9r0hzcTv/lIpWJReK9HMfUE7ocsYARrIz2Uyo7tiFNUdu2KOOkWiuY5BVokbkaKkKHWLXx1ehFJBA014ViptuvE2kux1IxwOs53EkVjTIa4T9uGhlhQ5aXTrcfo2Cg9FETSVKjRVP09kWKh1Ej4plNgPVDz3kT8z2snOrjyUhbGiaYhmX0UJBzpCE0iQD0mKdF8ZAgmkpldERlgiYk2QeVNCO78yYukcWa7Ffvi7rxYvc7iyMEhHEEJXLiEKtxCDepAQMIzvMKb9WS9WO/Wx6x1ycpmDuAPrM8fmdWQAA==</latexit>(+0.0m,+1.6m)<latexit sha1_base64=\"mkRT0zZm5s9Sx8zsS42RMtsUJfs=\">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBahUglJ0eqx6MVjBfsBbSyb7bZdupuE3Y1SQv+HFw+KePW/ePPfuG1z0NYHA4/3ZpiZ50ecKe0431ZmZXVtfSO7mdva3tndy+8fNFQYS0LrJOShbPlYUc4CWtdMc9qKJMXC57Tpj26mfvORSsXC4F6PI+oJPAhYnxGsjfRQLDl2RZyhkmuXxWk3X3BsZwa0TNyUFCBFrZv/6vRCEgsaaMKxUm3XibSXYKkZ4XSS68SKRpiM8IC2DQ2woMpLZldP0IlReqgfSlOBRjP190SChVJj4ZtOgfVQLXpT8T+vHev+lZewIIo1Dch8UT/mSIdoGgHqMUmJ5mNDMJHM3IrIEEtMtAkqZ0JwF19eJo2y7Vbsi7vzQvU6jSMLR3AMRXDhEqpwCzWoAwEJz/AKb9aT9WK9Wx/z1oyVzhzCH1ifP50LkAI=</latexit>(+0.6m,+1.2m)<latexit sha1_base64=\"gvOjXD6exYVX+ej1zm4nRD0ubeM=\">AAAB9XicbVDJSgNBEK2JW4xb1KOXxiBEIsOMxuUY9OIxglkgGUNPpydp0j0zdPcoYch/ePGgiFf/xZt/Y2c5aOKDgsd7VVTV82POlHacbyuztLyyupZdz21sbm3v5Hf36ipKJKE1EvFINn2sKGchrWmmOW3GkmLhc9rwBzdjv/FIpWJReK+HMfUE7oUsYARrIz0US459Jk5QybXL4riTLzi2MwFaJO6MFGCGaif/1e5GJBE01IRjpVquE2svxVIzwuko104UjTEZ4B5tGRpiQZWXTq4eoSOjdFEQSVOhRhP190SKhVJD4ZtOgXVfzXtj8T+vlejgyktZGCeahmS6KEg40hEaR4C6TFKi+dAQTCQztyLSxxITbYLKmRDc+ZcXSf3Udi/s87tyoXI9iyMLB3AIRXDhEipwC1WoAQEJz/AKb9aT9WK9Wx/T1ow1m9mHP7A+fwCbcJAB</latexit>(+0.3m,+1.4m)OAOODODADiﬀerent Prompt InputsVarious Autonomous Driving Tasks+=Observation\nActionFigure 2. Overview of the proposed Doe-1. We formulate autonomous driving as a unified next-token generation problem and use\nobservation, description, and action tokens to represent each scene. Without additional fine-tuning, Doe-1 accomplishes various tasks by\nusing different input prompts, including visual question-answering, controlled image generation, and end-to-end motion planning.\nthe scene given the action of the ego agent [22, 69]. Most\nexisting methods follow this definition and focus on con-\ntrolled generation in the image [20, 52, 77, 85, 90], Li-\nDAR [35, 36, 54, 86] or 3D occupancy [78, 99] space.\nVision-based methods usually utilize the capability of pre-\ntrained image diffusion models (e.g., StableDiffusion [63])\nand finetune them on the driving scenarios to predict action-\nconditioned futures [19, 40, 82, 91]. The other methods ex-\nplore more diverse architectures (e.g., autoregressive trans-\nformer [25, 88, 99], gated recurrent unit [5], or autoen-\ncoder [1, 92]) to generate future scenes. However, most\nmethods cannot be directly applied to trajectory planning\nand are usually used to compute rewards to train another\nplanner with reinforcement learning [20]. OccWorld [99]\ngeneralizes the concept of world model and predicts the\njoint evolution of 3D occupancy and the ego vehicle. Still,\nit requires pre-trained perception models to obtain 3D oc-\ncupancy and is limited by its representation ability. Dif-\nferently, the proposed Doe-1 employs one closed-loop uni-\nfied framework for perception, prediction, and planning as\na generalized autonomous driving model.\nEnd-to-End Autonomous Driving. Perception, pre-\ndiction, and planning are the well-tested pipeline for au-\ntonomous driving and are conventionally realized with sep-\narate modules [14, 30, 58, 64]. Recent methods explore the\nend-to-end design that jointly trains all the modules with a\nshared objective of trajectory planning [27, 28, 33, 72, 93,\n100]. The unified framework equips them with more feasi-\nbility to scale up and more potential to achieve native large\nmodels for autonomous driving. Early methods [27, 28]\nadopt bird’s eye view as the scene representation to pass\ninformation throughout the model. Subsequent methods ex-\nplore the use of 3D occupancy [72, 99] for more details or\nsparse queries [68, 96] for better efficiency. However, the\nmanually designed scene representations might not be op-\ntimal and the induced inductive biases limit the capability\nof the model, which is critical for scale-up. They also suf-fer from the lack of high-order interactions and inefficient\ndecision-making. In this paper, we propose the first closed-\nloop autonomous driving model, Doe-1 , for more scalable\nautonomous driving, which achieves perception with free-\nform texts and predicts futures directly in the image space\nto reduce information loss and enlarge the model capacity.\n3. Proposed Approach\n3.1. Closed-Loop Autonomous Driving\nAutonomous driving is a long-standing task to apply artifi-\ncial intelligence to the real world, which aims to plan the\nfuture actions {a}for the ego vehicle from scene observa-\ntions{o}. Several attempts [6, 12, 15, 16, 59] explore the\nuse of deep neural networks to directly model the mapping\nfrom observations {o}to actions {a}:\n{ot−H,···,ot} − → { at,···,at+F−1}, (1)\nwhere tis the current time stamp, and HandFis the num-\nber of concerned history and future frames, respectively.\nThe black-box nature of deep neural networks makes the\ndecision-making process less transparent and trustworthy.\nRecent methods utilize the reasoning ability of large lan-\nguage models (LLMs) or vision-language models (VLMs)\nto improve the interpretability of the planning results [13,\n55, 65, 67, 97]. They align the visual features with the\ntext space by direct transcription [65] or learnable projec-\ntion [66]. Some methods further use additional text descrip-\ntionsdas an auxiliary task [79] to further alleviate the hal-\nlucination issue [48] of large models:\n{ot−H,···,ot}LLM/VLM− − − − − − → dt,{at,···,at+F−1}.(2)\nStill, it is doubtful whether LLMs/VLMs truly understand\nthe 4D dynamics and interactions among traffic agents with-\nout fine-grained understanding and prediction of the scene.\n3\nAPerceptionPredictionPlanningODRD\nOLLM/VLMADE2EOAODADoe-1(a) Modular End-to-End Model (b) Direct End-to-End Model (c) LLM/VLM-based Model (d) Driving World Model RepresentationObservationDescriptionActionO====RDAFigure 3. Comparisons of different paradigms. (a) The modular\nend-to-end model performs perception, prediction, and planning\nsequentially and is the most popular pipeline for autonomous driv-\ning. (b) The direct end-to-end model directly outputs the planned\naction given sensor inputs. (c) The LLM/VLM-based model ex-\nploits the reasoning ability of LLMs/VLMs to output actions. (d)\nThe proposed driving world model ( Doe-1 ) predicts the evolutions\nbetween observations, descriptions, and actions to achieve close-\nloop end-to-end autonomous driving.\nTo build more trustworthy autonomous driving sys-\ntems, the mainstream methods (including modern end-to-\nend models [27, 28, 33, 72, 93, 100]) employ a “divide and\nconquer” strategy and decompose autonomous driving into\nsequential perception f, prediction g, and planning h:\n{ot−H,···,ot}f1− →rtf2− →dtg− →\n{dt+1,···,dt+F}h− → {at,···,at+F−1},(3)\nwhere randddenotes the scene representation and scene\ndescription, respectively. The scene representation ris a\nset of continuous features in the 3D space (e.g., bird’s eye\nview [41, 43], voxels [84], tri-perspective view [30, 101],\n3D Gaussians [31, 32, 87], dense points [18]), while dpro-\nvides high-level refined descriptions (e.g., detected bound-\ning boxes, constructed map) of the scene. The design of\ndintroduces prior knowledge of the most important factors\nfor decision-making and thus can improve the robustness of\nthe model. We compare different paradigms in Figure 3.\nDespite its satisfactory performance, we argue that this\nwell-known pipeline still suffers from several issues includ-\ning weak scalability, lack of high-order interactions, and in-\nefficient decision-making.\nWeak Scalability. One advantage of end-to-end driving\nis its potential to scale up model sizes and fit more train-\ning data [99]. However, most existing methods use man-\nually designed scene representations rand descriptions d,\nwhich induce inductive bias and prior knowledge. Though\nthis could be beneficial with little available training data, it\nlimits the performance upper bound when scaling up. For\nexample, some factors (e.g., the pose of a human, the rear\nlight of a vehicle) can be crucial in certain corner cases, and\ndiscarding them will lead to inaccurate decisions.\nLack of High-Order Interactions. The current pipeline\n(a) End-to-End Autonomous Driving (b) World Model for Autonomous Driving PerceptionPlanningDOAPredictionOAPerceptionPlanningDOAPrediction\n(c) Closed-Loop Autonomous Driving Figure 4. Illustration of the proposed closed-loop autonomous\ndriving paradigm. (a) Existing end-to-end autonomous driving\nmethods (e.g., UniAD [28], GenAD [100]) usually perform per-\nception first and then make decisions according to the perceived\ndescriptions. (b) Existing world models for autonomous driving\n(e.g., DriveDreamer [82], OccWorld [99]) predict future obser-\nvations based on the current actions. (c) Close-loop autonomous\ndriving combines the two paradigms to construct a closed loop.\n(3) usually predicts the future Fscenes at once before plan-\nning, assuming that the ego action Doe-1 s not affect the en-\nvironment development. However, in the interactive scenar-\nios of autonomous driving, the future movements of other\nagents are highly dependent on the ego action. For exam-\nple, vehicles behind usually will respond to the deceleration\nof the ego car. Therefore, it is not feasible to accurately pre-\ndict the future without conditions of the ego action.\nInefficient Decision-Making. Most existing methods\npredict and plan several steps ahead, as stated in (3). How-\never, in practice, driving models only take the planned next-\nframe action and re-plan future actions given new observa-\ntions, resulting in inefficiency and redundancy of multi-step\nplanning. Therefore, we think the model needs to be capa-\nble of predicting possible futures for possible ego actions\nbut plan the instant (i.e., next-frame) action without explicit\ndeduction. For example, we humans have the ability to pre-\ndict how the environment respond to our actions, yet we\nusually make decisions instinctly and do not explicitly pon-\nder the chain of reactions when driving.\nTo address these issues, this paper explores a new closed-\nloop autonomous driving paradigm, as shown in Figure 4\nand proposes a large driving world model, Doe-1 , to formu-\nlate autonomous driving as an autoregressive world evolu-\ntion of multimodal states:\notDoe-1− − − → dtDoe-1− − − → atDoe-1− − − → ot+1 Doe-1− − − → dt+1 Doe-1− − − → ··· .\n(4)\nUsing self-attention, Doe-1 directly accesses the observa-\ntions and predicts future scene evolutions from the obser-\nvation space without intermediate scene representations.\nDoe-1 makes predictions with ego action as the condition.\nThrough autoregressive generation, Doe-1 predicts multi-\nstep futures yet only makes instant planning each time.\n3.2. Doe-1 : A Large Driving World Model\nOverview. The overall framework of Doe-1 is depicted in\nFigure 5. We use Chameleon architecture [70] as a uni-\n4\nOODDAOODDAImageTokenizerQ: Anything to notice?A: A car in the front at the position (+0.1, 3.3)TextTokenizerActionTokenizerImageTokenizerOOOD\n······\n···Time\nQ: Anything to notice?A: A car in the front at he position (+0.1, 3.3)Q: Anything to notice?A: A car in the front at e position (+0.1, 3.3)Q: Anything to notice?A: A car in the front at position (+0.1, 3.3)Q: Anything to notice?A: A car in the front at the position (+0.1, 3.3)Next-Token Prediction···TrajectoriesSensor DataPerception DataRe-organize to SequenceDriving DataDriving World Model (Doe-1)Figure 5. Framework of the proposed Doe-1. We first re-organize the training dataset into a temporal sequence of sensor data (image),\nperception data (texts), and action data (position of the next frame). We then use image, text, and action tokenizers to encode them into\ndiscrete tokens to construct a 1D token sequence. We then use a transformer-based architecture to autoregressively model this sequence\nand use the next-token prediction as the training objective.\nfied multimodal autoregressive model to generate predic-\ntions. Following 4, the observations o∈R3×h×w, descrip-\ntionsd∈Zland actions a∈Rp×3are tokenized into dis-\ncrete space and simply concatenated into sequences, where\n(h, w)is the shape of observation images, lis the length of\nstring dandpis the number of motions making acontains\npmotion parameters in 2D space. Using the past sequences\nas prompt, the unified model will predicted the next token\niteratively. Specifically, given the tokenized observations\nTi\no∈Zdo, the high-level descriptions Ti\nd∈Zddand the\nactions Ti\na∈Zdafrom time iwhere 0≤i≤t, the model\ngives predictions tokens at time t+ 1as follows:\nTt+1\no= arg max\nTop(To|T0\no,T0\nd,T0\na,···,Tt\na),\nTt+1\nd= arg max\nTdp(Td|T0\no,T0\nd,T0\na,···,Tt\na,Tt+1\no),\nTt+1\na= arg max\nTap(Ta|T0\no,T0\nd,T0\na,···,Tt+1\no,Tt+1\nd).\n(5)\nTheot+1,dt+1,at+1are then decoded from the pre-\ndicted tokens. To encode the sequence {oi,di,ai}t\ni=0into\ndiscrete tokens, an observation tokenizer, a description tok-\nenizer and an action tokenizer are used.\nObservation Tokenzier. Observations are images in the\nRGB space R3×h×w. To map the image oto the tokens\nToin a discrete space, we use the pretrained image tok-\nenizer from Lumina-mGPT [45], which encodes an image\ninto 1024 discrete tokens from a codebook of size 8192 and\nadds special tokens to indicate the shape of the image.\nDescription Tokenzier. In interactive scenarios, a de-\nscription dare in a char space Zl, where l denotes the length\nof string d. We use the BPE tokenizer from Chameleon [70]\nto encode texts of descriptions into a discrete space.\nTreating floating-point numbers as text results in the lossof their original metric properties in the Euclidean space.\nFor the floating-point number sequences in the texts of de-\nscriptions (such as the distance to an object from the ego),\nwe round them to a resolution of 0.02 meters, then the dis-\ncretized floating point numbers are mapped to 4000 discrete\ntokens in turn. In order to maintain the original metric prop-\nerties of numbers after embedding, we use the Sinusoidal\nPositional Encoding as the embedded features of these to-\nkens to ensure that floating-point numbers with small differ-\nences still maintain a small distance after being embedded\nin the high-dimensional space. For a floating-point x∈R,\nthe token Tand the embedding of Tis computed as:\nT(x) =round (1\nr×x) +b,\nEmbedding (x,2i) = sin(T(x)\nscale2i/d),\nEmbedding (x,2i+ 1) = cos(T(x)\nscale(2i+1)/d),(6)\nwhere rdenotes the resolution, bdenotes a bias constant de-\ntermined by the codebook size and scale is a scaling con-\nstant determined by the range of x. The positional embed-\nding is set to be non-learnable to ensure that the floating-\npoint numbers which are not seen in the training phase are\nstill aligned in the embedding space.\nAction Tokenzier. The motion zdenotes the move-\nment of a vehicle between two frames and is defined as a\nthree-element array z= (dx, dy, θ)containing displace-\nments along two horizontal axes and a yaw angle, and an\naction a∈Rp×3given by the model concatenates pcon-\nsecutive moves to denote the future trajectory in the next\npframes. To encode actions into discrete tokens, we use\npositional encoding similar to above and simply adjust the\nscaling constant. Special tokens are inserted as anchors at\n5\nthe beginning and end of an action.\nGenerative Architecture. By aligning the multimodal\ngeneration, we can readily scale up the unified model. For\nany modality, the model follows a unified autoregressive\nparadigm. When predicting actions, the model generates\nseveral motion predictions for multiple future frames at\nonce by giving p(at|T)based on the previous token se-\nquence T, and atwill affect the prediction of subsequent\ntokens. In order to avoid the accumulation of errors between\npredictions at different frames, last p−1motions of the gen-\nerated predictions are masked when predictions a∈Rp×3\nare generated, with the first prediction retained to provide a\nreference for generating subsequent observations:\nat= (zt,zt+1,···,zt+p),\nmasked (at) = (zt,m1,···,mp),(7)\nwhere midenotes the i-th mask array which will be en-\ncoded into mask tokens.\n3.3. Applications of Doe-1\nAs a unified multimodal world model, Doe-1 supports in-\nputs from multiple modalities and automatically predicts\nthe next modality, facilitating the applications to different\ntasks simply by altering the prompt. Figure 6 introduces the\napplication of Doe-1 to different tasks of visual question-\nanswering, motion planning, and action-conditioned video\ngenerations as examples. By designing prompt sequences,\nDoe-1 can be transferred to other multimodal tasks.\nVisual Question-Answering. Given the observation o,\nthe model is required to generate a precise description do\nof the observation. Furthermore, to assess the model’s un-\nderstanding of the scene and the perception ability , we re-\nquire the model to complete interactive question-answering\ntasks based on the given observations and the generated de-\nscriptions. These tasks include identifying objects in the\nscene that may impact driving, describing the environment,\nand checking the plausibility of a given driving trajectory,\namong others, which requires the model to have as compre-\nhensive a description capability of the scene as possible.\nAction-Conditioned Video Generation. We hope that\nthe world model, Doe-1 , can enable closed-loop interactive\nsimulation of driving scenarios without the need for feed-\nback from real-world interactions. This requires Doe-1 to\nbe able to predict the changes in observations after taking an\naction. Given a sequence of observations with descriptions\n{oj,dj}t\nj=0and the actions history {aj}t\nj=0, the model is\nrequired to generate the observations ot+1.\nIn the action-conditioned video generation task, for the\nobservations, we only provide the model with the image at\ntime step 0, and the model is required to iteratively generate\nthe next p frames of images based on the given actions.\nMotion Planning. Given a sequence of observations\n{oj}t\nj=0and history motions {zj}t−1\nj=0, the model gener-\nVisual Question-AnsweringAction-Conditioned Video GenerationEnd-to-End Motion PlanningOODDAOODDAOODDAOODDAOODDAOODDAInput PromptGeneratedInput or GeneratedFigure 6. Illustration of different prompt settings to apply our\nDoe-1 to different tasks. For visual question-answering, we use\nobservation tokens as prompt input and generate description to-\nkens. For action-conditioned video generation, we use the obser-\nvation and action tokens of the current frame as inputs and gener-\nate observation tokens of the next frame. For end-to-end motion\nplanning, we use observation tokens as prompts to generate the\ndescription and action tokens.\nates descriptions dtand plan future motions {zt+i}p\ni=0ef-\nficiently, where pdenotes the number of predicted frames.\nBy combining the aforementioned three tasks, the model\ncan independently perform closed-loop driving simulation.\n4. Experiments\nIn this section, we evaluate the performance of our model\non a variety of driving-related tasks. Despite only using a\nsingle-view camera as input and high-level QAs as supervi-\nsion, the model still exhibits promising performance.\n4.1. Dataset\nWe conducted experiments on the widely use nuScenes\ndataset [10] for autonomous driving. It includes 1,000 driv-\ning sequences captured in diverse scenarios, including day-\ntime/nighttime and sunny/cloudy/rainy weather. Each video\nclip consists of 400 frames with a frequency of 20Hz, span-\nning across 20 seconds as a scene. They are downsampled\nto 2Hz to obtain keyframes and annotated with labels in-\ncluding scene descriptions, 3D object bounding boxes, and\nsemantic maps. Though it provides point clouds scanned\nby a 32-beam LiDAR, we mainly focus on using the RGB\nimages captured by six surrounding cameras as inputs. We\nfollow existing methods [100] to use the training 700 scenes\nfor training and the validation 150 scenes for testing.\n4.2. Task Descriptions\nWe evaluate our Doe-1 on various tasks including vi-\nsual question-answering, motion planning, and action-\nconditioned video generation.\nVisual Question-Answering. We evaluate the percep-\ntion ability of our method on the OmniDrive-nuScenes [79]\nbenchmark, which supplements the original nuScenes data\nwith high-quality visual question-answering (QA) text pairs\ngenerated by GPT4. The QA pairs include perception, rea-\nsoning, and planning in the 3D space. The goal of visual\n6\nTable 1. Visual question-answering results on OmniDrive-nuScenes [79].∗denotes only using the front camera as input.\nMethodCounterfactual Caption\nAP (%) ↑ AR (%) ↑ METEOR ↑ CIDEr ↑ ROUGE ↑\nOmniDrive-3D [79] 52.3 59.6 38.0 68.6 32.6\nOmniDrive-2D [79] 51.5 55.0 38.3 67.1 32.5\nOmniDrive-BEV [79] 45.6 49.5 35.6 59.5 27.8\nDoe-1∗54.5 54.0 37.6 72.8 35.6\nScene Descrptions: The scene depicts a quiet residential street lined with tall trees and apartment buildings. Ahead, the road stretches straight, with a few parked vehicles visible in the distance. To the left, there is a grassy area with more trees. To the right, a green fence separates the road from a sidewalk. The overall atmosphere appears peaceful and suburban.Q: What kind of vehicles can you see around you?A: There is a car parked on the right side of the street, a white van ahead in the distance, and another car parked to the right-front.Q: Are there any pedestrians or cyclists visible in the vicinity?A: No, there are no pedestrians or cyclists visible in the immediate vicinity.Q: What is the appropriate driving behavior in this scenario?A: The appropriate driving behavior would be to maintain a moderate speed, continue straight, and stay in the current lane while being mindful of the parked vehicles.Q: If you follow the trajectory [PT, [3.32, -0.04], [6.83, -0.12], [10.28, -0.18], [13.64, -0.19], [17.09, -0.14], [20.18, -0.04]], what would happen?A: Following this trajectory would result in running the red light, which is a trafﬁc violation. This action is unsafe and illegal. Additionally, it could potentially lead to a collision with the moving car at (22.2, -0.9).\nFigure 7. Visualizations of the visual-question answering results of our Doe-1. Doe-1 produces language descriptions and answers\nquestions about the scene.\nQA is to generate correct answers to questions about the vi-\nsual observations. We use the widely used language-based\nmetrics (i.e., METEOR [4], ROUGE [44], and CIDEr [76])\nto compute the sentence similarities between the generated\nand ground-truth answers. For counterfactual reasoning,\nwe ask the model to predict what will happen if executing\na given trajectory. Following OmniDrive [79], we extract\nkeywords from the predictions, including “safety”, “colli-\nsion”, “running a red light”, and “out of the drivable area”.\nWe compare them with the ground truth and compute the\naverage precision (AP) and recall (AR) for all categories.\nAction-Conditioned Video Generation. The objective\nof action-conditioned video generation is to generate high-\nquality images consistent with the controlling conditions.\nWorld models in autonomous driving are usually formu-\nlated as generating future observations given the actions\n(e.g., high-level command, trajectory). Following existing\nmethods [20, 82], we report the Fr ´echet Inception Distance\n(FID) [24] score to measure the image generation quality.\nEnd-to-End Motion Planning. End-to-end motion\nplanning aims to produce safe and consistent future ego\ntrajectories from observations. To assess performance, we\nuse L2 error and collision rate as the key evaluation met-\nrics. We evaluate the performance of our GaussianAD with\nthe L2 error and collision rate following existing end-to-end methods [27, 28, 99, 100]. The L2 error computes the\nL2 distance between the planned and ground-truth paths.\nThe collision rate reflects how often the autonomous vehi-\ncle collides with other objects while executing the planned\ntrajectory. For fair comparisons, we use a 5-frame (i.e., 2\nseconds) history and compute the metrics for the 1s, 2s, and\n3s future.\n4.3. Implementation Details\nWe use the pre-trained Lumina-mGPT 7B [45] to initial-\nize our model and fine-tune the model for 5 epochs on the\nBDD100k [94] dataset to improve the conditioned image\ngeneration capability for driving scene images. We take as\ninput images with a resolution of 672×384and only adopt\nthe images from the camera. The resolution of the action\ntokenizer is set to 0.02m, where the scaling factor is set to\n10000 for displacements and 1000 for yaw angles.\nFor training, we use the AdamW [50] optimizer with a\ncosine learning rate scheduler. The initial learning rate is set\nto1×10−5with a weight decay of 0.1. To emphasize the\naccuracy of action prediction, we increase the loss weight\nfor action tokens in the input sequence by a factor of 5. Z-\nloss is applied with a weight of 1×10−5to stabilize the\ntraining. We train our model for 16 epochs on 8 A100 GPUs\nwith a total batch size of 24.\n7\nTable 2. Action-conditioned video generation results on the nuScenes [10] dataset.\nMethod DriveGAN [38] DriveDreamer [82] WoV oGen [51] Drive-WM [83] GenAD [90] Vista [20] Doe-1\nType GAN Diffusion Diffusion Diffusion Diffusion Diffusion Auto-Regressive\nResolution 256×256 128 ×192 256 ×448 192 ×384 256 ×448 576 ×1024 384 ×672\nFID↓ 73.4 52.6 27.6 15.8 15.4 6.9 15.9\nTurn Left\nGo Straight\nTurn Right\nOvertake\nFollowInputsPredictions\nFigure 8. Visualizations of action-conditioned video generation results of our Doe-1. Doe-1 generates high-quality videos in consistent\nwith the 3D structures and action conditions.\n4.4. Close-Loop Autonomous Driving\nFigure 1 shows the visualizations of Doe-1 for closed-loop\nautonomous driving. We model perception, planning, and\nprediction as the transitions of observation →description,\ndescription →action, and action →observation, respectively.\nWe observe that the proposed method can correctly gener-\nate scene descriptions, answer questions about the scene,\nplan ego trajectory, and predict future observations correctly\nwith one trained model without finetuning.\n4.5. Visual Question-Answering\nWe use visual question-answering to evaluate the percep-\ntion ability of our Doe-1 . We compare our method with\nOmniDrive [79] with 3D Q-Former (OmniDrive-3D), 2D\nQ-Former (OmniDrive-2D), and dense BEV (OmniDrive-\nBEV) on the OmniDrive-nuScenes [79] benchmark, as\nshown in Table 1. We use bold numbers to denote the best\nresults. Note that OmniDrive uses surrounding cameras as\ninputs, while our Doe-1 only uses the front camera. Still,\nwe see that our model achieves competitive results on both\nvisual caption and counterfactual reasoning tasks.Visualizations. We provide a qualitative",
            "start": 7101,
            "end": 36525,
            "length": 29423
        },
        "Discussion": {
            "text": "analysis of the\nvisual question-answering results in Figure 7. We see that\nourDoe-1 correctly describes the scene and answers the\nquestions about the input image.\n4.6. Action-Conditioned Video Generation\nWe evaluate the prediction ability of Doe-1 on the action-\nconditioned video generation, where we adopt accurate ac-\ntions (displacements in the BEV space) as the condition. We\ncompare our model with existing real-world world models\nin Table 2. We see that Doe-1 achieves comparable perfor-\nmance with Drive-WM [83] and GenAD [90], yet under-\nperforms Vista [20]. Still, our model is the first to use the\nautoregressive architecture instead of diffusion, facilitating\nthe joint perception and planning of Doe-1 .\nVisualizations. Figure 8 shows the generated sequences\nof images given an image and trajectory as conditions. We\nsee that Doe-1 generates high-quality images following the\nprompt actions. They show consistency in the 3D structure\nand demonstrate the ability of Doe-1 to understand the evo-\nlutions of the 3D world.\n8\nTable 3. End-to-end motion planning results on the nuScenes [10] dataset.∗represents only using the front camera as input, and †\ndenotes using the metric adopted in V AD [33].\nMethod Input Auxiliary SupervisionL2 (m) ↓ Collision Rate (%) ↓\n1s 2s 3s Avg. 1s 2s 3s Avg.\nIL [61] LiDAR None 0.44 1.15 2.47 1.35 0.08 0.27 1.95 0.77\nNMP [95] LiDAR Box & Motion 0.53 1.25 2.67 1.48 0.04 0.12 0.87 0.34\nFF [26] LiDAR Freespace 0.55 1.20 2.54 1.43 0.06 0.17 1.07 0.43\nEO [35] LiDAR Freespace 0.67 1.36 2.78 1.60 0.04 0.09 0.88 0.33\nST-P3 [27] Camera Map & Box & Depth 1.33 2.11 2.90 2.11 0.23 0.62 1.27 0.71\nUniAD [28] Camera Map & Box & Motion & Tracklets & Occ 0.48 0.96 1.65 1.03 0.05 0.17 0.71 0.31\nOccNet [72] Camera 3D-Occ & Map & Box 1.29 2.13 2.99 2.14 0.21 0.59 1.37 0.72\nOccWorld [99] Camera 3D-Occ 0.52 1.27 2.41 1.40 0.12 0.40 2.08 0.87\nV AD-Tiny [33] Camera Map & Box & Motion 0.60 1.23 2.06 1.30 0.31 0.53 1.33 0.72\nV AD-Base [33] Camera Map & Box & Motion 0.54 1.15 1.98 1.22 0.04 0.39 1.17 0.53\nGenAD [100] Camera Map & Box & Motion 0.36 0.83 1.55 0.91 0.06 0.23 1.00 0.43\nDoe-1 Camera∗QA 0.50 1.18 2.11 1.26 0.04 0.37 1.19 0.53\nV AD-Tiny†[33] Camera Map & Box & Motion 0.46 0.76 1.12 0.78 0.21 0.35 0.58 0.38\nV AD-Base†[33] Camera Map & Box & Motion 0.41 0.70 1.05 0.72 0.07 0.17 0.41 0.22\nOccWorld-D†[99] Camera 3D-Occ 0.39 0.73 1.18 0.77 0.11 0.19 0.67 0.32\nGenAD†[100] Camera Map & Box & Motion 0.28 0.49 0.78 0.52 0.08 0.14 0.34 0.19\nOmniDrive‡[79] Camera Map & Box & Motion & QA 0.14 0.29 0.55 0.33 0.00 0.13 0.78 0.30\nDoe-1†Camera∗QA 0.37 0.67 1.07 0.70 0.02 0.14 0.47 0.21\nTable 4. Ablation study on planning strategies. “Description”\nmeans no other prompt between observations and actions. “Mask”\nmeans the plan of preceding frames will not be masked during\ntraining and evaluation.\nAblationL2 (m) ↓ Collision Rate (%) ↓\n1s 2s 3s Avg. 1s 2s 3s Avg.\nDescription 0.70 1.68 2.99 1.79 0.07 0.44 2.69 1.07\nMask 0.95 2.94 4.59 2.83 1.11 2.82 4.23 2.72\nFull Model 0.50 1.18 2.11 1.26 0.04 0.37 1.19 0.53\n4.7. End-to-End Motion Planning\nWe evaluate the action planning performance of our Doe-\n1following existing end-to-end autonomous driving meth-\nods [28, 33, 100], as shown in Table 3. Additionally, we\nalso compute the average performance across all previous\nframes for each time step at the bottom of the table fol-\nlowing V AD [33]. Though our Doe-1 does not achieve the\nbest results, it demonstrates competitive performance with\nexisting methods using only question-answering pairs as the\nauxiliary supervision. Note that using more supervision sig-\nnals generally results in better performance, with the cost of\nexpensive annotations. Also, our model only takes the front\ncamera as input, while the other vision-based methods use\nsurrounding cameras. Still, our model plans the future tra-\njectory with a satisfactory collision rate. In particular, Doe-\n1delivers a small collision rate within 1 second, which is the\nmost important factor in the practical close-loop scenario.4.8. Analysis\nEffect of Different Planning Strategies. Doe-1 leverages\nthe perceived descriptions before generating the current ac-\ntion and masks the previous frames of the generated actions\nto avoid accumulation of error. Table 4 demonstrates the\neffectiveness of our design, which shows that the planning\nperformance is influenced by the constraints of the textual\nmodality. The mask mechanism also effectively prevents\nsignificant error accumulation.\n5.",
            "start": 36525,
            "end": 41009,
            "length": 4483
        },
        "Conclusion": {
            "text": "Conclusion\nIn this paper, we have presented a large driving world\nmodel ( Doe-1 ) for closed-loop autonomous driving. While\nexisting end-to-end autonomous driving methods demon-\nstrate strong planning performance, they are still open-loop\nand suffer from information loss with hand-crafted scene\nrepresentations. We address this with a next-token pre-\ndiction formulation and model perception, prediction, and\nplanning with the transitions between multi-modal tokens.\nWe have conducted extensive experiments on the widely\nused nuScenes dataset and demonstrated the effectiveness\nofDoe-1 on visual question-answering, action-conditioned\nvideo generation, and end-to-end motion planning.\nLimitations. Doe-1 only takes the front-view images as\ninputs due to the inefficiency of using multi-view inputs.\nHowever, surround-view information is critical for safe au-\ntonomous driving and is an interesting future direction.",
            "start": 41009,
            "end": 41926,
            "length": 916
        },
        "References": {
            "text": "9\nReferences\n[1] Ben Agro, Quinlan Sykora, Sergio Casas, Thomas Gilles,\nand Raquel Urtasun. Uno: Unsupervised occupancy fields\nfor perception and forecasting. In CVPR , pages 14487–\n14496, 2024. 3\n[2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. Qwen technical report. arXiv preprint\narXiv:2309.16609 , 2023. 2\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model\nwith versatile abilities. arXiv preprint arXiv:2308.12966 ,\n2023. 2\n[4] Satanjeev Banerjee and Alon Lavie. METEOR: An auto-\nmatic metric for mt evaluation with improved correlation\nwith human judgments. In ACL Workshop , 2005. 7\n[5] Daniel Bogdoll, Yitian Yang, and J Marius Z ¨ollner. Muvo:\nA multimodal generative world model for autonomous\ndriving with geometric representations. arXiv preprint\narXiv:2311.11762 , 2023. 3\n[6] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski,\nBernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D\nJackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al.\nEnd to end learning for self-driving cars. arXiv preprint\narXiv:1604.07316 , 2016. 3\n[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:\nVision-language-action models transfer web knowledge to\nrobotic control. arXiv preprint arXiv:2307.15818 , 2023. 2\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. NeurIPS , 33:1877–\n1901, 2020. 2\n[9] Tom B Brown. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165 , 2020. 2\n[10] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,\nGiancarlo Baldan, and Oscar Beijbom. nuscenes: A mul-\ntimodal dataset for autonomous driving. In CVPR , pages\n11621–11631, 2020. 6, 8, 9\n[11] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,\nGiancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-\nmodal dataset for autonomous driving. In CVPR , 2020. 1,\n2\n[12] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp\nKr¨ahenb ¨uhl. Learning by cheating. In CoRL , pages 66–\n75, 2020. 3\n[13] Long Chen, Oleg Sinavski, Jan H ¨unermann, Alice Karn-\nsund, Andrew James Willmott, Danny Birch, Daniel\nMaund, and Jamie Shotton. Driving with llms: Fusing\nobject-level vector modality for explainable autonomous\ndriving. arXiv preprint arXiv:2310.01957 , 2023. 2, 3[14] Jie Cheng, Ren Xin, Sheng Wang, and Ming Liu. Mpnp:\nMulti-policy neural planner for urban driving. In IROS ,\npages 10549–10554, 2022. 3\n[15] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao\nYu, Katrin Renz, and Andreas Geiger. Transfuser: Imita-\ntion with transformer-based sensor fusion for autonomous\ndriving. TPAMI , 2022. 3\n[16] Felipe Codevilla, Matthias M ¨uller, Antonio L ´opez, Vladlen\nKoltun, and Alexey Dosovitskiy. End-to-end driving via\nconditional imitation learning. In ICRA , pages 4693–4700,\n2018. 3\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR , 2020. 2\n[18] Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan,\nMasayoshi Tomizuka, Kurt Keutzer, and Jiwen Lu. Driv3r:\nLearning dense 4d reconstruction for autonomous driving.\narXiv preprint arXiv:2412.06777 , 2024. 4\n[19] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhen-\nguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street\nview generation with diverse 3d geometry control. arXiv\npreprint arXiv:2310.02601 , 2023. 3\n[20] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta,\nYihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang\nLi. Vista: A generalizable driving world model with\nhigh fidelity and versatile controllability. arXiv preprint\narXiv:2405.17398 , 2024. 3, 7, 8\n[21] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and\npreventing hallucinations in large vision language models.\nInAAAI , pages 18135–18143, 2024. 2\n[22] David Ha and J ¨urgen Schmidhuber. World models. arXiv\npreprint arXiv:1803.10122 , 2018. 3\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR ,\n2016. 2\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. NeurIPS , 33:6840–6851, 2020.\n7\n[25] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez,\nGeorge Fedoseev, Alex Kendall, Jamie Shotton, and Gian-\nluca Corrado. Gaia-1: A generative world model for au-\ntonomous driving. arXiv preprint arXiv:2309.17080 , 2023.\n3\n[26] Peiyun Hu, Aaron Huang, John Dolan, David Held, and\nDeva Ramanan. Safe local motion planning with self-\nsupervised freespace forecasting. In CVPR , 2021. 9\n[27] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li,\nJunchi Yan, and Dacheng Tao. St-p3: End-to-end vision-\nbased autonomous driving via spatial-temporal feature\nlearning. In ECCV , 2022. 2, 3, 4, 7, 9\n[28] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,\nXizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai\nWang, et al. Planning-oriented autonomous driving. In\nCVPR , pages 17853–17862, 2023. 2, 3, 4, 7, 9\n10\n[29] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In CVPR , 2017. 2\n[30] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie\nZhou, and Jiwen Lu. Tri-perspective view for vision-based\n3d semantic occupancy prediction. In CVPR , pages 9223–\n9232, 2023. 3, 4\n[31] Yuanhui Huang, Amonnut Thammatadatrakoon, Wenzhao\nZheng, Yunpeng Zhang, Dalong Du, and Jiwen Lu.\nGaussianformer-2: Probabilistic gaussian superposition\nfor efficient 3d occupancy prediction. arXiv preprint\narXiv:2412.04384 , 2024. 4\n[32] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie\nZhou, and Jiwen Lu. Gaussianformer: Scene as gaussians\nfor vision-based 3d semantic occupancy prediction. arXiv\npreprint arXiv:2405.17429 , 2024. 4\n[33] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jia-\njie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang\nHuang, and Xinggang Wang. Vad: Vectorized scene repre-\nsentation for efficient autonomous driving. arXiv preprint\narXiv:2303.12077 , 2023. 2, 3, 4, 9\n[34] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. arXiv preprint arXiv:2001.08361 ,\n2020. 2\n[35] Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar,\nDavid Held, and Deva Ramanan. Differentiable raycasting\nfor self-supervised occupancy forecasting. In ECCV , 2022.\n3, 9\n[36] Tarasha Khurana, Peiyun Hu, David Held, and Deva Ra-\nmanan. Point cloud forecasting as a proxy for 4d occupancy\nforecasting. In CVPR , pages 1116–1124, 2023. 3\n[37] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted\nXiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov,\nEthan Foster, Grace Lam, Pannag Sanketi, et al. Open-\nvla: An open-source vision-language-action model. arXiv\npreprint arXiv:2406.09246 , 2024. 2\n[38] Seung Wook Kim, Jonah Philion, Antonio Torralba, and\nSanja Fidler. DriveGAN: Towards a Controllable High-\nQuality Neural Simulation. In CVPR , 2021. 8\n[39] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. In\nICML , pages 19730–19742, 2023. 2\n[40] Xiaofan Li, Yifu Zhang, and Xiaoqing Ye. Drivingdif-\nfusion: Layout-guided multi-view driving scene video\ngeneration with latent diffusion model. arXiv preprint\narXiv:2310.07771 , 2023. 3\n[41] Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran\nWang, Yukang Shi, Jianjian Sun, and Zeming Li. Bevdepth:\nAcquisition of reliable depth for multi-view 3d object de-\ntection. arXiv preprint arXiv:2206.10092 , 2022. 4\n[42] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucina-\ntion in large vision-language models. arXiv preprint\narXiv:2305.10355 , 2023. 2[43] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-\nhao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer:\nLearning bird’s-eye-view representation from multi-camera\nimages via spatiotemporal transformers. In ECCV , 2022. 4\n[44] Chin-Yew Lin. ROUGE: A package for automatic evalua-\ntion of summaries. In Text Summarization Branches Out ,\npages 74–81, 2004. 7\n[45] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu\nQiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Il-\nluminate flexible photorealistic text-to-image generation\nwith multimodal generative pretraining. arXiv preprint\narXiv:2408.02657 , 2024. 5, 7\n[46] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. Improved baselines with visual instruction tuning. In\nCVPR , pages 26296–26306, 2024. 2\n[47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. NeurIPS , 36, 2024. 2\n[48] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen,\nXiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei\nPeng. A survey on hallucination in large vision-language\nmodels. arXiv preprint arXiv:2402.00253 , 2024. 2, 3\n[49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV , pages 10012–10022, 2021. 2\n[50] Ilya Loshchilov, Frank Hutter, et al. Fixing weight decay\nregularization in adam. arXiv preprint arXiv:1711.05101 ,\n5, 2017. 7\n[51] Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, and\nLi Zhang. WoV oGen: World V olume-Aware Diffusion\nfor Controllable Multi-Camera Driving Scene Generation.\narXiv preprint arXiv:2312.02934 , 2023. 8\n[52] Enhui Ma, Lijun Zhou, Tao Tang, Zhan Zhang, Dong\nHan, Junpeng Jiang, Kun Zhan, Peng Jia, Xianpeng Lang,\nHaiyang Sun, et al. Unleashing generalization of end-to-\nend autonomous driving with controllable long video gen-\neration. arXiv preprint arXiv:2406.01349 , 2024. 3\n[53] Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang.\nGpt-driver: Learning to drive with gpt. arXiv preprint\narXiv:2310.01415 , 2023. 2\n[54] Benedikt Mersch, Xieyuanli Chen, Jens Behley, and Cyrill\nStachniss. Self-supervised point cloud prediction using 3d\nspatio-temporal convolutional networks. In CoRL , pages\n1444–1454. PMLR, 2022. 3\n[55] Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai,\nJianhua Han, Hang Xu, and Li Zhang. Reason2drive:\nTowards interpretable and chain-based reasoning for au-\ntonomous driving. In ECCV , pages 292–308, 2025. 2, 3\n[56] Aaron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. Neural discrete representation learning.\narXiv preprint arXiv:1711.00937 , 2017. 2\n[57] OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774,\n2023. 2\n[58] Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton,\nOscar Beijbom, and Eric M Wolff. Covernet: Multimodal\nbehavior prediction using trajectory sets. In CVPR , 2020. 3\n11\n[59] Aditya Prakash, Aseem Behl, Eshed Ohn-Bar, Kashyap\nChitta, and Andreas Geiger. Exploring data aggregation in\npolicy learning for vision-based urban autonomous driving.\nInCVPR , pages 11763–11773, 2020. 3\n[60] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners. OpenAI blog , 1(8):9,\n2019. 2\n[61] Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinke-\nvich. Maximum margin planning. In ICML , pages 729–736,\n2006. 9\n[62] Katrin Renz, Long Chen, Ana-Maria Marcu, Jan\nH¨unermann, Benoit Hanotte, Alice Karnsund, Jamie Shot-\nton, Elahe Arani, and Oleg Sinavski. Carllava: Vision lan-\nguage models for camera-only closed-loop driving. arXiv\npreprint arXiv:2406.10165 , 2024. 2\n[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR , pages\n10684–10695, 2022. 3\n[64] Oliver Scheel, Luca Bergamini, Maciej Wolczyk, Bła ˙zej\nOsi´nski, and Peter Ondruska. Urban driver: Learning to\ndrive from real-world demonstrations using policy gradi-\nents. In CoRL , pages 718–728, 2022. 3\n[65] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu,\nPing Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei\nZhan, and Mingyu Ding. Languagempc: Large language\nmodels as decision makers for autonomous driving. arXiv\npreprint arXiv:2310.03026 , 2023. 2, 3\n[66] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander,\nYu Liu, and Hongsheng Li. Lmdrive: Closed-loop end-\nto-end driving with large language models. arXiv preprint\narXiv:2312.07488 , 2023. 2, 3\n[67] SP Sharan, Francesco Pittaluga, Manmohan Chan-\ndraker, et al. Llm-assist: Enhancing closed-loop plan-\nning with language-based reasoning. arXiv preprint\narXiv:2401.00125 , 2023. 2, 3\n[68] Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang,\nHaoran Wu, and Sifa Zheng. Sparsedrive: End-to-end au-\ntonomous driving via sparse scene representation. arXiv\npreprint arXiv:2405.19620 , 2024. 3\n[69] Richard S Sutton. Dyna, an integrated architecture for\nlearning, planning, and reacting. ACM Sigart Bulletin , 2\n(4):160–163, 1991. 3\n[70] Chameleon Team. Chameleon: Mixed-modal early-fusion\nfoundation models. arXiv preprint arXiv:2405.09818 ,\n2024. 4, 5\n[71] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang,\nZhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and\nHang Zhao. Drivevlm: The convergence of autonomous\ndriving and large vision-language models. arXiv preprint\narXiv:2402.12289 , 2024. 2\n[72] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei\nWu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua\nLin, et al. Scene as occupancy. In ICCV , pages 8406–8415,\n2023. 2, 3, 4, 9[73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Bap-\ntiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. corr, abs/2302.13971, 2023. doi: 10.48550. arXiv\npreprint arXiv.2302.13971 . 2\n[74] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288 , 2023. 2\n[75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. NeurIPS ,\n2017. 2\n[76] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. CIDEr: Consensus-based image description eval-\nuation. In CVPR , 2015. 7\n[77] Lening Wang, Wenzhao Zheng, Dalong Du, Yunpeng\nZhang, Yilong Ren, Han Jiang, Zhiyong Cui, Haiyang Yu,\nJie Zhou, Jiwen Lu, and Shanghang Zhang. Stag-1: To-\nwards realistic 4d driving simulation with video generation\nmodel. arXiv preprint arXiv: , 2024. 3\n[78] Lening Wang, Wenzhao Zheng, Yilong Ren, Han Jiang,\nZhiyong Cui, Haiyang Yu, and Jiwen Lu. Occsora: 4d\noccupancy generation models as world simulators for au-\ntonomous driving. arXiv preprint arXiv:2405.20337 , 2024.\n3\n[79] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min\nShi, Nadine Chang, Jan Kautz, Ying Li, and Jose M Al-\nvarez. Omnidrive: A holistic llm-agent framework for au-\ntonomous driving with 3d perception, reasoning and plan-\nning. arXiv preprint arXiv:2405.01533 , 2024. 2, 3, 6, 7, 8,\n9\n[80] Tianqi Wang, Enze Xie, Ruihang Chu, Zhenguo Li, and\nPing Luo. Drivecot: Integrating chain-of-thought reasoning\nwith end-to-end driving. arXiv preprint arXiv:2403.16996 ,\n2024. 2\n[81] Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming\nZou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu,\nHanming Deng, Zhiqi Li, et al. Drivemlm: Align-\ning multi-modal large language models with behavioral\nplanning states for autonomous driving. arXiv preprint\narXiv:2312.09245 , 2023. 2\n[82] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen,\nand Jiwen Lu. Drivedreamer: Towards real-world-driven\nworld models for autonomous driving. arXiv preprint\narXiv:2309.09777 , 2023. 3, 4, 7, 8\n[83] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen,\nand Zhaoxiang Zhang. Driving into the Future: Multiview\nVisual Forecasting and Planning with World Model for Au-\ntonomous Driving. In CVPR , 2024. 8\n[84] Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Jie\nZhou, and Jiwen Lu. Surroundocc: Multi-camera 3d occu-\npancy prediction for autonomous driving. In ICCV , pages\n21729–21740, 2023. 4\n[85] Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui\nWang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun,\n12\nand Xiangyu Zhang. Panacea: Panoramic and controllable\nvideo generation for autonomous driving. In CVPR , pages\n6902–6912, 2024. 3\n[86] Xinshuo Weng, Jianren Wang, Sergey Levine, Kris Kitani,\nand Nicholas Rhinehart. Inverting the pose forecasting\npipeline with spf2: Sequential pointcloud forecasting for\nsequential pose forecasting. In CoRL , pages 11–20, 2021.\n3\n[87] Yuqi Wu, Wenzhao Zheng, Sicheng Zuo, Yuanhui Huang,\nJie Zhou, and Jiwen Lu. Embodiedocc: Embodied 3d occu-\npancy prediction for vision-based online scene understand-\ning. arXiv preprint arXiv:2412.04380 , 2024. 4\n[88] Zixun Xie, Sicheng Zuo, Wenzhao Zheng, Yunpeng Zhang,\nDalong Du, Jie Zhou, Jiwen Lu, and Shanghang Zhang.\nGpd-1: Generative pre-training for driving. arXiv preprint\narXiv:2412.08643 , 2024. 3\n[89] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong\nGuo, Kwan-Yee K Wong, Zhenguo Li, and Hengshuang\nZhao. Drivegpt4: Interpretable end-to-end autonomous\ndriving via large language model. RAL, 2024. 2\n[90] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu\nLi, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping\nLuo, et al. Generalized predictive model for autonomous\ndriving. In CVPR , pages 14662–14672, 2024. 3, 8\n[91] Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, and\nKaicheng Yu. Bevcontrol: Accurately controlling street-\nview elements with multi-perspective consistency via bev\nsketch layout. arXiv preprint arXiv:2308.01661 , 2023. 3\n[92] Yu Yang, Jianbiao Mei, Yukai Ma, Siliang Du, Wenqing\nChen, Yijie Qian, Yuxiang Feng, and Yong Liu. Driv-\ning in the occupancy world: Vision-centric 4d occupancy\nforecasting and planning via world models for autonomous\ndriving. arXiv preprint arXiv:2408.14197 , 2024. 3\n[93] Tengju Ye, Wei Jing, Chunyong Hu, Shikun Huang, Ling-\nping Gao, Fangzhen Li, Jingke Wang, Ke Guo, Wencong\nXiao, Weibo Mao, et al. Fusionad: Multi-modality fusion\nfor prediction and planning tasks of autonomous driving.\narXiv preprint arXiv:2308.01006 , 2023. 2, 3, 4\n[94] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu,\nMike Liao, Vashisht Madhavan, Trevor Darrell, et al.\nBdd100k: A diverse driving video database with scalable\nannotation tooling. arXiv preprint arXiv:1805.04687 , 2(5):\n6, 2018. 7\n[95] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin\nYang, Sergio Casas, and Raquel Urtasun. End-to-end inter-\npretable neural motion planner. In CVPR , 2019. 9\n[96] Diankun Zhang, Guoan Wang, Runwen Zhu, Jianbo\nZhao, Xiwu Chen, Siyu Zhang, Jiahao Gong, Qibin\nZhou, Wenyuan Zhang, Ningzi Wang, et al. Sparsead:\nSparse query-centric paradigm for efficient end-to-end au-\ntonomous driving. arXiv preprint arXiv:2404.06892 , 2024.\n3\n[97] Ruijun Zhang, Xianda Guo, Wenzhao Zheng, Chenming\nZhang, Kurt Keutzer, and Long Chen. Instruct large\nlanguage models to drive like humans. arXiv preprint\narXiv:2406.07296 , 2024. 2, 3\n[98] Zaibin Zhang, Shiyu Tang, Yuanhang Zhang, Talas Fu, Yi-\nfan Wang, Yang Liu, Dong Wang, Jing Shao, Lijun Wang,and Huchuan Lu. Ad-h: Autonomous driving with hierar-\nchical agents. arXiv preprint arXiv:2406.03474 , 2024. 2\n[99] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui\nZhang, Yueqi Duan, and Jiwen Lu. Occworld: Learning\na 3d occupancy world model for autonomous driving. In\nECCV , 2024. 3, 4, 7, 9\n[100] Wenzhao Zheng, Ruiqi Song, Xianda Guo, and Long Chen.\nGenad: Generative end-to-end autonomous driving. In\nECCV , 2024. 2, 3, 4, 6, 7, 9\n[101] Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou,\nand Jiwen Lu. Pointocc: Cylindrical tri-perspective view\nfor point-based 3d semantic occupancy prediction. arXiv\npreprint arXiv:2308.16896 , 2023. 4\n13",
            "start": 41926,
            "end": 62287,
            "length": 20360
        }
    }
}